Mongoloid (/ˈmɒŋɡəˌlɔɪd/)[1] is an obsolete racial grouping of various peoples indigenous to large parts of Asia, the Americas, and some regions in Europe and Oceania. The term is derived from a now-disproven theory of biological race.[2] In the past, other terms such as "Mongolian race", "yellow", "Asiatic" and "Oriental" have been used as synonyms.
The concept of dividing humankind into the Mongoloid, Caucasoid, and Negroid races was introduced in the 1780s by members of the Göttingen school of history. It was further developed by Western scholars in the context of racist ideologies during the age of colonialism.[3][4] With the rise of modern genetics, the concept of distinct human races in a biological sense has become obsolete. In 2019, the American Association of Biological Anthropologists stated: "The belief in 'races' as natural aspects of human biology, and the structures of inequality (racism) that emerge from such beliefs, are among the most damaging elements in the human experience both today and in the past."[4]
The term Mongoloid has had a second usage referencing people with Down syndrome, now generally regarded as highly offensive.[5][6][7][8] Those affected were often referred to as "Mongoloids" or in terms of "Mongolian idiocy" or "Mongolian imbecility".
Mongolian as a term for race was first introduced in 1785 by Christoph Meiners, a scholar at the then modern Göttingen University. Meiners divided humanity into two races he labeled "Tartar-Caucasians" and "Mongolians", believing the former to be beautiful, the latter to be "weak in body and spirit, bad, and lacking in virtue".[9]: 34
His more influential Göttingen colleague Johann Friedrich Blumenbach borrowed the term Mongolian for his division of humanity into five races in the revised 1795 edition of his De generis humani varietate nativa (On the Natural Variety of Mankind). Although Blumenbach's concept of five races later gave rise to scientific racism, his arguments were basically anti-racist,[10] since he underlined that humankind as a whole forms one single species,[11] and points out that the transition from one race to another is so gradual that the distinctions between the races presented by him are "very arbitrary".[12] In Blumenbach's concept, the Mongolian race comprises the peoples living in Asia east of the Ob River, the Caspian Sea and the Ganges River, with the exception of the Malays, who are considered to be transitional between Caucasian and Ethiopian.[13][14] Of peoples living outside Asia, he includes the "Eskimos" in northern America and the European Finns, among whom he includes the "Lapps".[15]
Discussions on race among Western scholars during the 19th century took place against the background of the debate between monogenists and polygenists, the former arguing for a single origin of all humankind, the latter holding that each human race had a specific origin. Monogenists based their arguments either on a literal interpretation of the biblical story of Adam and Eve or on secular research. Since polygenism stressed the perceived differences, it was popular among white supremacists, especially slaveholders in the US.[17]
British biologist Thomas Huxley, a strong advocate of Darwinism and a monogenist, presented the views of polygenists in 1865: "[S]ome imagine their assumed species of mankind were created where we find them... the Mongolians from the Orangs".[18]
During the 19th century, diverging opinions were pronounced whether Native Americans or Malays should be included in the grouping which was sometimes called "Mongolian" and sometimes "Mongoloid". For example, D. M. Warren in 1856 used a narrow definition which did not include either the "Malay" or the "American" races,[19] while Huxley (1870)[20] and Alexander Winchell (1881) included both Malays and indigenous Americans.[21] In 1861, Isidore Geoffroy Saint-Hilaire added the Australian as a secondary race (subrace) of the principal race of Mongolian.[22]
In his Essai sur l'inégalité des races humaines (Essay on the Inequality of the Human Races, published 1853–55), which would later influence Adolf Hitler, the French aristocrat Arthur de Gobineau defined three races which he called "white", "black", and "yellow". His "yellow race", corresponding to other writers' "Mongoloid race", consisted of "the Altaic, Mongol, Finnish and Tartar branches".[23][24] While he saw the "white race" as superior, he claimed that the "yellow race" was physically and intellectually mediocre but had an extremely strong materialism that allowed them to achieve certain results.[25]: 100
According to the Meyers Konversations-Lexikon (1885–90), peoples included in the Mongoloid race are North Mongol, Chinese and Indochinese, Japanese and Korean, Tibetan and Burmese, Malay, Polynesian, Maori, Micronesian, Eskimo, and Native American.[26]
In 1909, a map published based on racial classifications in South Asia conceived by Herbert Hope Risley classified inhabitants of Bengal and parts of Odisha as Mongolo-Dravidians, people of mixed Mongoloid and Dravidian origin.[27] Similarly in 1904, Ponnambalam Arunachalam claimed the Sinhalese people of Sri Lanka were a people of mixed Mongolian and Malay racial origins as well as Indo-Aryan, Dravidian and Vedda origins.[28] Howard S. Stoudt in The Physical Anthropology of Ceylon (1961) and Carleton S. Coon in The Living Races of Man (1966) classified the Sinhalese as partly Mongoloid.[29][30]
German physical anthropologist Egon Freiherr von Eickstedt, an influential proponent of Rassenkunde (racial studies) in Nazi Germany, classified people from Nepal, Bhutan, Bangladesh, East India, parts of Northeast India, western Myanmar and Sri Lanka as East Brachid, referring to people of mixed Indid and South Mongolid origins.[31] Eickstedt also classified the people of central Myanmar, Yunnan, southern Tibet, Thailand and parts of India as Palaungid deriving from the name of the Palaung people of Myanmar. He also classified the Burmese, Karen, Kachin, Shan, Sri Lankans, Tai, South Chinese, Munda and Juang, and others as having "mixed" with the Palaungid phenotype.[32]
Commenting on the situation of the United States in the early 20th century, Leonard Lieberman said that the notion of the whole world being composed of three distinct races, Caucasoid, Mongoloid, and Negroid, seemed credible because of the history of immigration to the United States with most immigrants coming from three areas, Southeast China, Northwest Europe, and West Africa. This made the point of view of three races appear to be "true, natural, and inescapable".[33]
In 1950, UNESCO published their statement The Race Question. It condemned all forms of racism, naming "the doctrine of inequality of men and races"[34]: 1  among the causes of World War II and proposing to replace the term "race" with "ethnic groups" because "serious errors ... are habitually committed when the term 'race' is used in popular parlance".[34]: 6
Alfred L. Kroeber (1948), Emeritus Professor of Anthropology at the University of California, Berkeley, referring to the racial classification of humankind on the basis of physical features, said that there are basically "three grand divisions." Kroeber indicated that, within the three-part classification, the Mongoloid, the Negroid, and the Caucasian are the three "primary racial stocks of mankind." Kroeber said that the following are the divisions of the Mongoloid stock: the "Mongolian proper of East Asia," the "Malaysian of the East Indies," and the "American Indian." Kroeber alternatively referred to the divisions of the Mongoloid stock as the following: "Asiatic Mongoloids," "Oceanic Mongoloids," and "American Mongoloids." Kroeber said that the differences among the three divisions of the Mongoloid stock are not very large. Kroeber said that the Malaysian and the American Indian are generalized type peoples while the Mongolian proper is the most extreme or pronounced form. Kroeber said that the original Mongoloid stock must be regarded as being more like the current Malaysians, the current American Indians, or an intermediate type between these two. Kroeber said that it is from these generalized type peoples, who kept more nearly the ancient type, that peoples such as the Chinese gradually diverged, who added the oblique eye, and a "certain generic refinement of physique." Kroeber said that, according to most anthropometrists, the Eskimo is the most particularized sub-variety out of the American Mongoloids. Kroeber said that in the East Indies, and in particular the Philippines, there can at times be distinguished a less specifically Mongoloid strain, which has been called the "Proto-Malaysian," and a more specifically Mongoloid strain, which has been called the "Deutero-Malaysian." Kroeber said that Polynesians appear to have primary Mongoloid connections by way of the Malaysians. Kroeber said that the Mongoloid element of Polynesians is not a specialized Mongoloid. Kroeber said that the Mongoloid element in Polynesians appears to be larger than the definite Caucasian strain in Polynesians. Speaking of Polynesians, Kroeber said that there are locally possible minor Negroid absorptions, as the ancestral Polynesians had to pass by or through archipelagoes which are presently Papuo-Melanesian Negroid to get to the central Pacific.[35][36]
American anthropologist Carleton S. Coon published his much debated[37]: 248  Origin of Races in 1962. Coon divided the species Homo sapiens into five groups: Besides the Caucasoid, Mongoloid, and Australoid races, he posited two races among the indigenous populations of sub-Saharan Africa: the Capoid race in the south and the Congoid race.
Coon's thesis was that Homo erectus had already been divided into five different races or subspecies. "Homo Erectus then evolved into Homo Sapiens not once but five times, as each subspecies, living in its own territory, passed a critical threshold from a more brutal to a more sapient state."[38]
Since Coon followed the traditional methods of physical anthropology, relying on morphological characteristics, and not on the emerging genetics to classify humans, the debate over Origin of Races has been "viewed as the last gasp of an outdated scientific methodology that was soon to be supplanted."[37]: 249 [39]
The fact that there are no sharp distinctions between the supposed racial groups had been observed by Blumenbach and later by Charles Darwin.[40]
With the availability of new data due to the development of modern genetics, the concept of races in a biological sense has become untenable. Problems of the concept include: It "is not useful or necessary in research",[33] scientists are not able to agree on the definition of a certain proposed race, and they do not even agree on the number of races, with some proponents of the concept suggesting 300 or even more "races".[33] Also, data are not reconcilable with the concept of a treelike evolution[41] nor with the concept of "biologically discrete, isolated, or static" populations.[4]
After discussing various criteria used in biology to define subspecies or races, Alan R. Templeton concludes in 2016: "[T]he answer to the question whether races exist in humans is clear and unambiguous: no."[42]: 360
The last edition of the German encyclopedia Meyers Konversations-Lexikon (1971–79, 25 volumes) lists the following characteristics of the "Mongoloid" populations of Asia: "Flat face with a low nasal root, accentuated zygomatic arches, flat-lying eyelids (which are often slanting), thick, tight, dark hair, dark eyes, yellow-brownish skin, usually short, stocky build."[43]
In 2004, British anthropologist Caroline Wilkinson gave a description of "Mongoloid" skulls in her book on forensic facial reconstruction: "The Mongoloid skull shows a round head shape with a medium-width nasal aperture, rounded orbital margins, massive cheekbones, weak or absent canine fossae, moderate prognathism, absent brow ridges, simple cranial sutures, prominent zygomatic bones, broad, flat, tented nasal root, short nasal spine, shovel-shaped upper incisor teeth (scooped out behind), straight nasal profile, moderately wide palate shape, arched sagittal contour, wide facial breadth and a flatter face."[44]
In 1950, Carleton S. Coon, Stanley M. Garn, and Joseph B. Birdsell proposed that the relative flatness of "Mongoloid" faces was caused by adaption to the extreme cold of subarctic and arctic conditions.[45]: 132 [46]: 66  They supposed that "Mongoloid" eye sockets have been extended vertically to make room for adipose tissue around the eyeballs, and that the "reduced" brow ridges decrease the size of the air spaces inside of the brow ridges known as the frontal sinuses which are "vulnerable" to the cold. They also supposed that "Mongoloid" facial features reduce the surface area of the nose by having nasal bones that are flat against the face and having enlarged cheekbones that project forward which effectively reduce the external projection of the nose.[45]
Still, in 1965 a study by A. T. Steegmann showed that the so-called cold-adapted Mongoloid face provided no greater protection against frostbite than the facial structure of Europeans.[46]: 66
In 1858, the California State Legislature enacted the first bill of several that prohibited the attendance of "Negroes, Mongolians and Indians" from public schools.[47]
In 1885, the California State Legislature amended its code to make separate schools for "children of Mongoloid or Chinese descent."[47]
In 1911, the Bureau of Immigration and Naturalization was using the term "Mongolic grand division," not only to include Mongols, but "in the widest sense of all," to include Malays, Chinese, Japanese, and Koreans. In 1911, the Bureau of Immigration and Naturalization was placing all "East Indians," a term which included the peoples of "India, Farther India, and Malaysia," in the "Mongolic" grand division.[48]
In 1985, Michael P. Malone of the FBI Laboratory said that the FBI Laboratory is in a good position for the examination of Mongoloid hairs, because it does most of the examinations for Alaska, which has a large Mongoloid population, and it conducts examinations for the majority of Indian reservations in the United States.[49]
In 1987, a report to the National Institute of Justice indicated that the following skeletal collections were of the "Mongoloid" "Ethnic Group": Arctic Eskimo, Prehistoric North American Indian, Japanese, and Chinese.[50]
In 2005, an article in a journal by the FBI Laboratory defined the term "Mongoloid," as the term is used in forensic hair examinations. It defined the term as, "an anthropological term designating one of the major groups of human beings originating from Asia, excluding the Indian subcontinent and including Native American Indians."[51][52]
"Mongoloid" has had a second usage, now generally avoided as highly offensive: until the late 20th century, people with Down syndrome[5][6][7][8] were often referred to as "Mongoloids", or in terms of "Mongolian idiocy" or "Mongolian imbecility". The term was motivated by the observation that people with Down syndrome often have epicanthic folds.[53]
Coined in 1908, the term remained in medical usage until the 1950s. In 1961, its use was deprecated by a group of genetic experts in an article in The Lancet due to its "misleading connotations".[54] The term continued to be used as a pejorative in the second half of the 20th century, with shortened versions such as mong in slang usage.[55]
In the 21st century, this usage of the term is deemed "unacceptable" in the English-speaking world and has fallen out of common use[56] because of its offensive and misleading implications. The terminology change was brought about both by scientific and medical experts[57] as well as people of Asian ancestry,[57] including those from Mongolia.[58]

A hum /hʌm/; (ⓘ) Latin: murmur, The sound of giraffes humming (ⓘ) is a sound made by producing a wordless tone with the mouth closed, forcing the sound to emerge from the nose.  To hum is to produce such a sound, often with a melody. It is also associated with thoughtful absorption, 'hmm'.
A hum has a particular timbre (or sound quality), usually a monotone or with slightly varying tones.  There are other similar sounds not produced by human singing that are also called hums, as the sound produced by machinery in operation, such as a microwave, or by an insect in flight.  The hummingbird was named for the sound that bird makes in flight which sounds like a hum.
A 'hum' or 'humming' by humans is created by the resonance of air in various parts of passages in the head and throat, in the act of breathing. The 'hum' that a hummingbird creates is also created by resonance: in this case by air resistance against wings in the actions of flying, especially of hovering.
Joseph Jordania suggested that humming could have played an important role in the early human (hominid) evolution as contact calls.[1] Many social animals produce seemingly haphazard and indistinct sounds (like chicken cluck) when they are going about their everyday business (foraging, feeding). These sounds let group members know that they are among kin and there is no danger. In the case of the appearance of any signs of danger (such as suspicious sounds or movements in a forest), the animal that notices danger first, stops moving, stops producing sounds, remains silent and looks in the direction of the danger sign. Other animals quickly follow suit and very soon all the group is silent and is scanning the environment for possible danger.
Charles Darwin was the first to notice this phenomenon on the example of the wild horses and the cattle.[2] Joseph Jordania suggested that for humans, as for many social animals, silence can be a sign of danger, and that's why gentle humming and musical sounds relax humans (see the use of gentle music in music therapy, lullabies).[3]
In Pirahã, the only surviving dialect of the Mura language, there is a special register of speech which uses solely humming, with no audible release.[4]
Humming is often used in music of genres, from classical (for example, the famous chorus at the end of Act 2 of Giacomo Puccini's Madama Butterfly) to jazz to R&B.
Another form of music derived from basic humming is the humwhistle.  Folk art, also known as "whistle-hum," produces a high pitch and low pitch simultaneously.  The two-tone sound is related to field holler, overtone singing, and yodeling the music.

Guns, Germs, and Steel: The Fates of Human Societies (subtitled A Short History of Everybody for the Last 13,000 Years in Britain) is a 1997 transdisciplinary nonfiction book by the American author Jared Diamond. The book attempts to explain why Eurasian and North African civilizations have survived and conquered others, while arguing against the idea that Eurasian hegemony is due to any form of Eurasian intellectual, moral, or inherent genetic superiority. Diamond argues that the gaps in power and technology between human societies originate primarily in environmental differences, which are amplified by various positive feedback loops. When cultural or genetic differences have favored Eurasians (for example, written language or the development among Eurasians of resistance to endemic diseases), he asserts that these advantages occurred because of the influence of geography on societies and cultures (for example, by facilitating commerce and trade between different cultures) and were not inherent in the Eurasian genomes.
In 1998, it won the Pulitzer Prize for general nonfiction and the Aventis Prize for Best Science Book. A documentary based on the book, and produced by the National Geographic Society, was broadcast on PBS in July 2005.[1]
The prologue opens with an account of Diamond's conversation with Yali, a Papua New Guinean politician. The conversation turned to the differences in power and technology between Papua New Guineans and the Europeans who dominated the region for two centuries, differences that neither of them considered due to European genetic superiority. Yali asked, using the local term cargo for inventions and manufactured goods, "Why is it that you white people developed so much cargo and brought it to New Guinea, but we black people had little cargo of our own?"[2]: 14
Diamond realized the same question seemed to apply elsewhere: "People of Eurasian origin ... dominate ... the world in wealth and power." Other peoples, after having thrown off colonial domination, still lag in wealth and power. Still others, he says, "have been decimated, subjugated, and in some cases even exterminated by European colonialists."[2]: 15
The peoples of other continents (sub-Saharan Africans, Indigenous people of the Americas, Aboriginal Australians, New Guineans, and the original inhabitants of tropical Southeast Asia) have been largely conquered, displaced and in some extreme cases – referring to Native Americans, Aboriginal Australians, and South Africa's indigenous Khoisan peoples – largely exterminated by farm-based societies such as Eurasians and Bantu. He believes this is due to these societies' technological and immunological advantages, stemming from the early rise of agriculture after the last ice age.
The book's title is a reference to the means by which farm-based societies conquered populations and maintained dominance though sometimes being vastly outnumbered, so that imperialism was enabled by guns, germs, and steel.
Diamond argues geographic, climatic and environmental characteristics which favored early development of stable agricultural societies ultimately led to immunity to diseases endemic in agricultural animals and the development of powerful, organized states capable of dominating others.
Diamond argues that Eurasian civilization is not so much a product of ingenuity, but of opportunity and necessity. That is, civilization is not created out of superior intelligence, but is the result of a chain of developments, each made possible by certain preconditions.
The first step towards civilization is the move from nomadic hunter-gatherer to rooted agrarian society. Several conditions are necessary for this transition to occur: access to high-carbohydrate vegetation that endures storage; a climate dry enough to allow storage; and access to animals docile enough for domestication and versatile enough to survive captivity. Control of crops and livestock leads to food surpluses. Surpluses free people to specialize in activities other than sustenance and support population growth. The combination of specialization and population growth leads to the accumulation of social and technological innovations which build on each other. Large societies develop ruling classes and supporting bureaucracies, which in turn lead to the organization of nation-states and empires.[2]
Although agriculture arose in several parts of the world, Eurasia gained an early advantage due to the greater availability of suitable plant and animal species for domestication. In particular, Eurasia has barley, two varieties of wheat, and three protein-rich pulses for food; flax for textiles; and goats, sheep, and cattle. Eurasian grains were richer in protein, easier to sow, and easier to store than American maize or tropical bananas.
As early Western Asian civilizations developed trading relationships, they found additional useful animals in adjacent territories, such as horses and donkeys for use in transport. Diamond identifies 13 species of large animals over 100 pounds (45 kg) domesticated in Eurasia, compared with just one in South America (counting the llama and alpaca as breeds within the same species) and none at all in the rest of the world. Australia and North America suffered from a lack of useful animals due to extinction, probably by human hunting, shortly after the end of the Pleistocene, and the only domesticated animals in New Guinea came from the East Asian mainland during the Austronesian settlement around 4,000–5,000 years ago. Biological relatives of the horse, including zebras and onagers, proved untameable; and although African elephants can be tamed, it is very difficult to breed them in captivity.[2][3] Diamond describes the small number of domesticated species (14 out of 148 "candidates") as an instance of the Anna Karenina principle: many promising species have just one of several significant difficulties that prevent domestication. He argues that all large mammals that could be domesticated, have been.[2]: 168–174
Eurasians domesticated goats and sheep for hides, clothing, and cheese; cows for milk; bullocks for tillage of fields and transport; and benign animals such as pigs and chickens. Large domestic animals such as horses and camels offered the considerable military and economic advantages of mobile transport.
Eurasia's large landmass and long east–west distance increased these advantages. Its large area provided more plant and animal species suitable for domestication. Equally important, its east–west orientation has allowed groups of people to wander and empires to conquer from one end of the continent to the other while staying at the same latitude. This was important because similar climate and cycle of seasons let them keep the same "food production system" – they could keep growing the same crops and raising the same animals all the way from Scotland to Siberia. Doing this throughout history, they spread innovations, languages and diseases everywhere.
By contrast, the north–south orientation of the Americas and Africa created countless difficulties adapting crops domesticated at one latitude for use at other latitudes (and, in North America, adapting crops from one side of the Rocky Mountains to the other). Similarly, Africa was fragmented by its extreme variations in climate from north to south: crops and animals that flourished in one area never reached other areas where they could have flourished, because they could not survive the intervening environment. Europe was the ultimate beneficiary of Eurasia's east–west orientation: in the first millennium BCE, the Mediterranean areas of Europe adopted Southwestern Asia's animals, plants, and agricultural techniques; in the first millennium CE, the rest of Europe followed suit.[2][3]
The plentiful supply of food and the dense populations that it supported made division of labor possible. The rise of non-farming specialists such as craftsmen and scribes accelerated economic growth and technological progress. These economic and technological advantages eventually enabled Europeans to conquer the peoples of the other continents in recent centuries by using guns and steel, particularly after the devastation of native populations by the epidemic diseases from germs.
Eurasia's dense populations, high levels of trade, and living in close proximity to livestock resulted in widespread transmission of diseases, including from animals to humans. Smallpox, measles, and influenza were the result of close proximity between dense populations of animals and humans. Natural selection endowed most Eurasians with genetic variations making them less susceptible to some diseases, and constant circulation of diseases meant adult individuals had developed immunity to a wide range of pathogens. When Europeans made contact with the Americas, European diseases (to which Americans had no immunity) ravaged the indigenous American population, rather than the other way around. The "trade" in diseases was a little more balanced in Africa and southern Asia, where endemic malaria and yellow fever made these regions notorious as the "white man's grave".[4] Some researchers say syphilis was known to Hippocrates,[5] and others think it was brought from the Americas by Columbus and his successors.[6] The European diseases from germs obliterated indigenous populations so that relatively small numbers of Europeans could maintain dominance.[2][3]
Diamond proposes geographical explanations for why western European societies, rather than other Eurasian powers such as China, have been the dominant colonizers.[2][7] He said Europe's geography favored balkanization into smaller, closer nation-states, bordered by natural barriers of mountains, rivers, and coastline. Advanced civilization developed first in areas whose geography lacked these barriers, such as China, India and Mesopotamia. There, the ease of conquest meant they were dominated by large empires in which manufacturing, trade and knowledge flourished for millennia, while balkanized Europe remained more primitive.
However, at a later stage of development, western Europe's fragmented governmental structure actually became an advantage. Monolithic, isolated empires without serious competition could continue mistaken policies – such as China squandering its naval mastery by banning the building of ocean-going ships – for long periods without immediate consequences. In Western Europe, by contrast, competition from immediate neighbors meant that governments could not afford to suppress economic and technological progress for long; if they did not correct their mistakes, they were out-competed and/or conquered relatively quickly. While the leading powers alternated, a constant was rapid development of knowledge which could not be suppressed. For instance, the Chinese Emperor could ban shipbuilding and be obeyed, ending China's Age of Discovery, but the Pope could not keep Galileo's Dialogue from being republished in Protestant countries, or Kepler and Newton from continuing his progress; this ultimately enabled European merchant ships and navies to navigate around the globe. Western Europe also benefited from a more temperate climate than Southwestern Asia where intense agriculture ultimately damaged the environment, encouraged desertification, and hurt soil fertility.
Guns, Germs, and Steel argues that cities require an ample supply of food, and thus are dependent on agriculture. As farmers do the work of providing food, division of labor allows others freedom to pursue other functions, such as mining and literacy.
The crucial trap for the development of agriculture is the availability of wild edible plant species suitable for domestication. Farming arose early in the Fertile Crescent since the area had an abundance of wild wheat and pulse species that were nutritious and easy to domesticate. In contrast, American farmers had to struggle to develop corn as a useful food from its probable wild ancestor, teosinte.
Also important to the transition from hunter-gatherer to city-dwelling agrarian societies was the presence of "large" domesticable animals, raised for meat, work, and long-distance communication. Diamond identifies a mere 14 domesticated large mammal species worldwide. The five most useful (cow, horse, sheep, goat, and pig) are all descendants of species endemic to Eurasia. Of the remaining nine, only two (the llama and alpaca both of South America) are indigenous to a land outside the temperate region of Eurasia.
Due to the Anna Karenina principle, surprisingly few animals are suitable for domestication. Diamond identifies six criteria including the animal being sufficiently docile, gregarious, willing to breed in captivity and having a social dominance hierarchy. Therefore, none of the many African mammals such as the zebra, antelope, cape buffalo, and African elephant were ever domesticated (although some can be tamed, they are not easily bred in captivity). The Holocene extinction event eliminated many of the megafauna that, had they survived, might have become candidate species, and Diamond argues that the pattern of extinction is more severe on continents where animals that had no prior experience of humans were exposed to humans who already possessed advanced hunting techniques (such as the Americas and Australia).
Smaller domesticable animals such as dogs, cats, chickens, and guinea pigs may be valuable in various ways to an agricultural society, but will not be adequate in themselves to sustain a large-scale agrarian society. An important example is the use of larger animals such as cattle and horses in plowing land, allowing for much greater crop productivity and the ability to farm a much wider variety of land and soil types than would be possible solely by human muscle power. Large domestic animals also have an important role in the transportation of goods and people over long distances, giving the societies that possess them considerable military and economic advantages.
Diamond argues that geography shaped human migration, not simply by making travel difficult (particularly by latitude), but by how climates affect where domesticable animals can easily travel and where crops can ideally grow easily due to the sun. The dominant Out of Africa theory holds that modern humans developed east of the Great Rift Valley of the African continent at one time or another. The Sahara kept people from migrating north to the Fertile Crescent, until later when the Nile River valley became accommodating. Diamond continues to describe the story of human development up to the modern era, through the rapid development of technology, and its dire consequences on hunter-gathering cultures around the world.
Diamond touches on why the dominant powers of the last 500 years have been West European rather than East Asian, especially Chinese. The Asian areas in which big civilizations arose had geographical features conducive to the formation of large, stable, isolated empires which faced no external pressure to change which led to stagnation. Europe's many natural barriers allowed the development of competing nation states. Such competition forced the European nations to encourage innovation and avoid technological stagnation.
In the later context of the European colonization of the Americas, 95% of the indigenous populations are believed to have been killed off by diseases brought by the Europeans. Many were killed by infectious diseases such as smallpox and measles. Similar circumstances were observed in Australia and South Africa. Aboriginal Australians and the Khoikhoi population were devastated by smallpox, measles, influenza, and other diseases.[8][9]
Diamond questions how diseases native to the American continents did not kill off Europeans, and posits that most of these diseases were developed and sustained only in large dense populations in villages and cities. He also states most epidemic diseases evolve from similar diseases of domestic animals. The combined effect of the increased population densities supported by agriculture, and of close human proximity to domesticated animals leading to animal diseases infecting humans, resulted in European societies acquiring a much richer collection of dangerous pathogens to which European people had acquired immunity through natural selection (such as the Black Death and other epidemics) during a longer time than was the case for Native American hunter-gatherers and farmers.
He mentions the tropical diseases (mainly malaria) that limited European penetration into Africa as an exception. Endemic infectious diseases were also barriers to European colonisation of Southeast Asia and New Guinea.
Guns, Germs, and Steel focuses on why some populations succeeded. Diamond's later book, Collapse: How Societies Choose to Fail or Succeed, focuses on environmental and other factors that have caused some populations to fail.
In the 1930s, the Annales School in France undertook the study of long-term historical structures by using a synthesis of geography, history, and sociology. Scholars examined the impact of geography, climate, and land use. Although geography had been nearly eliminated as an academic discipline in the United States after the 1960s, several geography-based historical theories were published in the 1990s.[10]
In 1991, Jared Diamond already considered the question of "why is it that the Eurasians came to dominate other cultures?" in The Third Chimpanzee: The Evolution and Future of the Human Animal (part four).
Many noted that the large scope of the work makes some oversimplification inevitable while still praising the book as a very erudite and generally effective synthesis of multiple different subjects. Paul R. Ehrlich and E. O. Wilson both praised the book.[11]
Northwestern University economic historian Joel Mokyr interpreted Diamond as a geographical determinist but added that the thinker could never be described as "crude" like many determinists. For Mokyr, Diamond's view that Eurasia succeeded largely because of a uniquely large stock of domesticable plants is flawed because of the possibility of crop manipulation and selection in the plants of other regions: the drawbacks of an indigenous North American plant such as sumpweed could have been bred out, Mokyr wrote, since "all domesticated plants had originally undesirable characteristics" eliminated via "deliberate and lucky selection mechanisms". Mokyr dismissed as unpersuasive Diamond's theory that breeding specimens failing to fix characteristics controlled by multiple genes "lay at the heart of the geographically challenged societies". Mokyr also states that in seeing economic history as centered on successful manipulation of environments, Diamond downplays the role of "the option to move to a more generous and flexible area", and speculated that non-generous environments were the source of much human ingenuity and technology. However, Mokyr still argued that Guns, Germs, and Steel is "one of the more important contributions to long-term economic history and is simply mandatory to anyone who purports to engage Big Questions in the area of long-term global history". He lauded the book as full of "clever arguments about writing, language, path dependence and so on. It is brimming with wisdom and knowledge, and it is the kind of knowledge economic historians have always loved and admired."[12]
Berkeley economic historian Brad DeLong described the book as a "work of complete and total genius".[13] Harvard International Relations (IR) scholar Stephen Walt in a Foreign Policy article called the book "an exhilarating read" and put it on a list of the ten books every IR student should read.[14] Tufts University IR scholar Daniel W. Drezner listed the book on his top ten list of must-read books about international economic history.[15]
International Relations scholars Iver B. Neumann (of the London School of Economics and Political Science) and Einar Wigen (of University of Oslo) use Guns, Germs, and Steel as a foil for their own inter-disciplinary work. They write that "while empirical details should, of course, be correct, the primary yardstick for this kind of work cannot be attention to detail." According to the two writers, "Diamond stated clearly that any problematique of this magnitude had to be radically multi-causal and then set to work on one complex of factors, namely ecological ones", and note that Diamond "immediately came in for heavy criticism from specialists working in the disparate fields on which he drew". But Neumann and Wigen also stated, "Until somebody can come up with a better way of interpreting and adding to Diamond's material with a view to understanding the same overarching problematique, his is the best treatment available of the ecological preconditions for why one part of the world, and not another, came to dominate."[16] Historian Tonio Andrade writes that Diamond's book "may not satisfy professional historians on all counts" but that it "does make a bold and compelling case for the different developments that occurred in the Old World versus the New (he is less convincing in his attempts to separate Africa from Eurasia)."[17]
Historian Tom Tomlinson wrote that the magnitude of the task makes it inevitable that Professor Diamond would "[use] very broad brush-strokes to fill in his argument", but ultimately commended the book. Taking the account of prehistory "on trust" because it was not his area of expertise, Tomlinson stated that the existence of stronger weapons, diseases, and means of transport is convincing as an "immediate cause" of Old World societies and technologies being dominant, but questioned Diamond's view that the way this has transpired has been through certain environments causing greater inventiveness which then caused more sophisticated technology. Tomlinson noted that technology spreads and allows for military conquests and the spread of economic changes, but that in Diamond's book this aspect of human history "is dismissed as largely a question of historical accident". Writing that Diamond gives meager coverage to the history of political thought, the historian suggested that capitalism (which Diamond classes as one of 10 plausible but incomplete explanations) has perhaps played a bigger role in prosperity than Diamond argues.[18]
Tomlinson speculated that Diamond underemphasizes cultural idiosyncrasies as an explanation, and argues (with regards to the "germs" part of Diamond's triad of reasons) that the Black Death of the 14th century, as well as smallpox and cholera in 19th century Africa, rival the Eurasian devastation of indigenous populations as overall "events of human diffusion and coalescence". Tomlinson also found contentious Diamond's view that humanity's future can one day be foreseen with scientific rigor since this would involve a search for general laws that new theoretical approaches deny the possibility of establishing: "The history of humans cannot properly be equated with the history of dinosaurs, glaciers or nebulas, because these natural phenomena do not consciously create the evidence on which we try to understand them". Tomlinson still described these flaws as "minor", however, and wrote that Guns, Germs, and Steel "remains a very impressive achievement of imagination and exposition".[18][19]
Another historian, professor J. R. McNeill, complimented the book for "its improbable success in making students of international relations believe that prehistory is worth their attention", but likewise thought Diamond oversold geography as an explanation for history and under-emphasized cultural autonomy.[3][20] McNeill wrote that the book's success "is well-deserved for the first nineteen chapters–excepting a few passages–but that the twentieth chapter carries the argument beyond the breaking point, and excepting a few paragraphs, is not an intellectual success." But McNeill concluded, "While I have sung its praises only in passing and dwelt on its faults, [...] overall I admire the book for its scope, for its clarity, for its erudition across several disciplines, for the stimulus it provides, for its improbable success in making students of international relations believe that prehistory is worth their attention, and, not least, for its compelling illustration that human history is embedded in the larger web of life on earth." Tonio Andrade described McNeill's review as "perhaps the fairest and most succinct summary of professional world historians' perspectives on Guns, Germs, and Steel".[17]
In 2010, Tim Radford of The Guardian called the book "exhilarating" and lauded the passages about plants and animals as "beautifully constructed".[21]
A 2023 study in the Quarterly Journal of Economics assessed Diamond's claims about topography influencing Chinese unification and contributing to European fragmentation. The study's model found that topography was a sufficient condition for the varied outcomes in Asia and Europe, but that it was not a necessary condition.[22]
The anthropologist Jason Antrosio described Guns, Germs, and Steel as a form of "academic porn", writing, "Diamond's account makes all the factors of European domination a product of a distant and accidental history" and "has almost no role for human agency—the ability people have to make decisions and influence outcomes. Europeans become inadvertent, accidental conquerors. Natives succumb passively to their fate." He added, "Jared Diamond has done a huge disservice to the telling of human history. He has tremendously distorted the role of domestication and agriculture in that history. Unfortunately his story-telling abilities are so compelling that he has seduced a generation of college-educated readers."[23]
In his last book, published in 2000, the anthropologist and geographer James Morris Blaut criticized Guns, Germs, and Steel, among other reasons, for reviving the theory of environmental determinism, and described Diamond as an example of a modern Eurocentric historian.[24] Blaut criticizes Diamond's loose use of the terms "Eurasia" and "innovative", which he believes misleads the reader into presuming that Western Europe is responsible for technological inventions that arose in the Middle East and Asia.[25]
Anthropologist Kerim Friedman wrote, "While it is interesting and important to ask why technologies developed in some countries as opposed to others, I think it overlooks a fundamental issue: the inequality within countries as well as between them." Timothy Burke, an instructor in African history at Swarthmore College wrote: "Anthropologists and historians interested in non-Western societies and Western colonialism also get a bit uneasy with a big-picture explanation of world history that seems to cancel out or radically de-emphasize the importance of the many small differences and choices after 1500 whose effects many of us study carefully."[11]
Economists Daron Acemoğlu, Simon Johnson and James A. Robinson have written extensively about the effect of political institutions on the economic well-being of former European colonies. Their writing finds evidence that, when controlling for the effect of institutions, the income disparity between nations located at various distances from the equator disappears through the use of a two-stage least squares regression quasi-experiment using settler mortality as an instrumental variable. Their 2001 academic paper explicitly mentions and challenges the work of Diamond,[26] and this critique is brought up again in Acemoğlu and Robinson's 2012 book Why Nations Fail.[27]
The book Questioning Collapse (Cambridge University Press, 2010) is a collection of essays by fifteen archaeologists, cultural anthropologists, and historians criticizing various aspects of Diamond's books Collapse: How Societies Choose to Fail or Succeed and Guns, Germs and Steel.[28] The book was a result of 2006 meeting of the American Anthropological Association in response to the misinformation they felt Diamond's popular science publications were causing and the association decided to combine experts from multiple fields of research to cover the claims made in Diamond's and debunk them. The book includes research from indigenous peoples of the societies Diamond discussed as collapsed and also vignettes of living examples of those communities, in order to showcase the main theme of the book on how societies are resilient and change into new forms over time, rather than collapsing.[29][30]
Guns, Germs, and Steel won the 1997 Phi Beta Kappa Award in Science.[31] In 1998, it won the Pulitzer Prize for General Nonfiction, in recognition of its powerful synthesis of many disciplines, and the Royal Society's Rhône-Poulenc Prize for Science Books.[32][33]
Guns, Germs, and Steel was first published by W. W. Norton in March 1997. It was published in Great Britain with the title Guns, Germs, and Steel: A Short History of Everybody for the Last 13,000 Years by Vintage in 1998.[34] It was a selection of Book of the Month Club, History Book Club, Quality Paperback Book Club, and Newbridge Book Club.[35]
In 2003 and 2007, updated English-language editions were released without changing any conclusions.[36]
The National Geographic Society produced a documentary, starring Jared Diamond, based on the book and of the same title, that was broadcast on PBS in July 2005.[1][37]

Peter Forster FRSB (born 27 June 1967) is a geneticist researching the prehistoric origins and ancestry of mankind. In addition to archaeogenetics, he has published on the reconstruction and spread of prehistoric languages and in the field of forensic genetics.
Peter Forster studied chemistry at the Christian-Albrechts-University in Kiel and the University of Hamburg. At the Heinrich-Pette-Institut for Virology and Immunology in Hamburg, he specialised in genetics and obtained his PhD degree in 1997 in biology on the topic of "Dispersal and differentiation of modern Homo sapiens analysed with mitochondrial DNA". After postdoctoral research at the Institute for Legal Medicine at the Westphalian Wilhelms-University in Münster, he was appointed Research Fellow at the McDonald Institute for Archaeological Research in Cambridge University in 1999, and furthermore a Fellow at Murray Edwards College at the University of Cambridge.  He is on the editorial board of the International Journal of Legal Medicine, and a director of Roots for Real. Peter Forster was elected a life member of the German Academy of Sciences Leopoldina. In January 2016 the Royal Society of Biology elected Peter Forster as a Fellow.
Modern humans have existed in Africa for around 200,000 years. Peter Forster discovered on the basis of modern and ancient DNA that there has only been a single successful migration out of Africa during prehistory, and he dated this emigration to around 60,000 years ago. The size of 
this emigrant group, according to his estimate, was less than 200 people (BBC 2009: African tribe populated rest of the world). Their descendants travelled on average about 200 to 1,000 metres per year and reached Europe and Australia just over 40,000 years ago, and America around 20,000 years ago. Due to the small numbers of founders, and due to their subsequent isolation on separate continents, differences between populations accumulated, yielding the distinctive sets of features that are perceived today as human races.[1]
On the basis of geographic DNA patterns, Forster discovered that the current language areas on all continents arose primarily through the prehistoric spread of culturally or militarily dominant men, whose languages were evidently favoured by the local women and passed on to their children. Hence, there is a statistical relationship today between the language and the Y chromosome types of modern males, but no such relationship between mtDNA and language in females today.[2]
Peter Forster has also applied his statistical evolutionary approach on languages directly and has calculated that the Celtic languages spread in the Bronze Age from about 3000 BC, and that the Germanic languages spread during the Iron Age from about 600 BC, as far as Britain.[3]
To obtain these results, Forster has compiled, proofread and corrected DNA- and language databases, and developed, in collaboration with his colleagues, phylogenetic network analysis of mitochondrial DNA, Y-chromosomal DNA, and linguistic data, as well as the concept of the mtDNA- and Y-chromosomal "clock". He has developed practical applications of his research in the shape of DNA ancestry tests, geographical ancestry tests and relationship tests for casework in genealogy, family research and forensics.
Elisabeth Hamel (2007) Das Werden der Völker in Europa. Tenea-Verlag, Berlin.

Events/Artifacts
(north to south)
Events/Artifacts
Artifacts
Since H. Otley Beyer first proposed his wave migration theory, numerous scholars have approached the question of how, when and why humans first came to the Philippines. The current scientific consensus favors the "Out of Taiwan" model, which broadly match linguistic, genetic, archaeological, and cultural evidence.
Modern theories of the peopling of the Philippines islands are interpreted against the wider backdrop of the migrations of the Austronesian peoples. They comprise two major schools of thought, the "Out of Sundaland" models and the "Out of Taiwan" model. Of the two, however, the most widely accepted hypothesis is the Out-of-Taiwan model, which largely corresponds to linguistic, genetic, archaeological, and cultural evidence.[2] It has since been strengthened by genetic and archaeological studies that broadly agree with the timeline of the Austronesian expansion.[1][3][4][5]
The various "Out of Sundaland" hypotheses, posited by a minority of modern authors and differing slightly in the details, is similar to F. Landa Jocano's "Core Population" hypothesis. However, instead of the Philippines, they assume the origin of the Austronesian peoples as being the now sunken Sundaland landmass (modern Sumatra, Java, Borneo, and the Malay Peninsula). These models have been criticized as relying only on mtDNA genetic data without accounting for admixture events, thus having results that mistakenly combine the much older Paleolithic Negrito populations with the newer Neolithic Austronesian peoples.[6][7]
A notable model among the "Out of Sundaland" hypothesis is Wilhelm Solheim II's "Nusantao Maritime Trading and Communication Network" (NMTCN). It posited an alternative model based on maritime movement of people over different directions and routes. It suggests that people with distant origins from 50,000 years ago in the area of present-day coastal eastern Vietnam and Southern China had moved to the area of the Bismarck Islands south and east of Mindanao and developed into the Austronesian cultures. They supposedly later spread among seafarers from the area to the rest of Island Southeast Asia and areas along the South China Sea. In support of this idea Solheim notes there is little or no indication that Pre- or Proto Malayo-Polynesian was present in Taiwan.  According to Solheim, "The one thing I feel confident in saying is that all native Southeast Asians are closely related culturally, genetically and to a lesser degree linguistically."[8][9][10][11]
Solheim's concept of the Nusantao Maritime Trading and Communication Network, while not strictly a theory regarding the biological ancestors of modern Southeast Asians, does suggest that the patterns of cultural diffusion throughout the Asia-Pacific region are not what would be expected if such cultures were to be explained by simple migration.  Where Bellwood based his analysis primarily on linguistic analysis, Solheim's approach was based on artifact findings.  On the basis of a careful analysis of artifacts, he suggests the existence of a trade and communication network that first spread in the Asia-Pacific region during its Neolithic age (c.8,000 to 500 BC). According to Solheim's NMTCN theory, this trade network, consisting of both Austronesian and non-Austronesian seafaring peoples, was responsible for the spread of cultural patterns throughout the Asia-Pacific region, not the simple migration proposed by the Out-of-Taiwan hypothesis.[12]
Solheim came up with four geographical divisions delineating the spread of the NMTCN over time, calling these geographical divisions "lobes." Specifically, these were the central, northern, eastern and western lobes.
The central lobe was further divided into two smaller lobes reflecting phases of cultural spread: the Early Central Lobe and the Late Central Lobe. Instead of Austronesian peoples originating from Taiwan, Solheim placed the origins of the early NMTCN peoples in the "Early Central Lobe," which was in eastern coastal Vietnam, at around 9000 BC.
He then suggests the spread of peoples around 5000 BC towards the "Late central lobe", including the Philippines, via island Southeast Asia, rather than from the north as the Taiwan theory suggests. Thus, from the Point of view of the Philippine peoples, the NMTCN is also referred to as the Island Origin Theory.
This "late central lobe" included southern China and Taiwan, which became "the area where Austronesian became the original language family and Malayo-Polynesian developed." In about 4000 to 3000 BC, these peoples continued spreading east through Northern Luzon to Micronesia to form the Early Eastern Lobe, carrying the Malayo-Polynesian languages with them.  These languages would become part of the culture spread by the NMTCN in its expansions Malaysia and western towards Malaysia before 2000 BC, continuing along coastal India and Sri Lanka up to the western coast of Africa and Madagascar; and over time, further eastward towards its easternmost borders at Easter Island. Thus, as in the case of Bellwood's theory, the Austronesian languages spread eastward and westward from the area around the Philippines.  Aside from the matter of the origination of peoples, the difference between the two theories is that Bellwood's theory suggests a linear expansion, while Solheim's suggests something more akin to concentric circles, all overlapping in the geographical area of the late central lobe which includes the Philippines.[citation needed]
The most widely accepted hypothesis today is the "Out of Taiwan" model, first proposed by Peter Bellwood. Although originally largely based on linguistic evidence, it has corresponded to archaeological, cultural, and genetic findings later on;[13] including whole genome sequencing data, rather than the mtDNA sequencing relied upon by "Out of Sundaland" proponents.[13][11]
In this hypothesis, the first Austronesians reached the Philippines at around 2200 BC from Taiwan, settling the Batanes Islands and northern Luzon. From there, they rapidly spread downwards to the rest of the islands of the Philippines and Southeast Asia, as well as voyaging further east to reach the Northern Mariana Islands by around 1500 BC.[1][14][13] They assimilated the earlier Negrito groups  which arrived during the Paleolithic, resulting in the modern Filipino ethnic groups which all display various ratios of genetic admixture between Austronesian and Negrito groups.[11]
A 2021 genetic study, which examined representatives of 115 indigenous communities, found evidence of at least five independent waves of early human migration. Negrito groups, divided between those in Luzon and those in Mindanao, may come from a single wave and diverged subsequently, or through two separate waves. This likely occurred sometime after 46,000 years ago. Another Negrito migration entered Mindanao sometime after 25,000 years ago. Two early East Asian waves (Austroasiatic and possible Austric) were detected, one most strongly evidenced among the Manobo people who live in inland Mindanao, and the other in the Sama-Bajau and related people of the Sulu archipelago, Zamboanga Peninsula, and Palawan. The admixture found in the Sama people indicates a relationship with the Lua and Mlabri people of mainland Southeast Asia, and reflects a similar genetic signal found in western Indonesia. These happened sometime after 15,000 years ago and 12,000 years ago respectively, around the time the last glacial period was coming to an end.[15]
Austronesians, either from Southern China or Taiwan, were found to have come in at least two distinct waves. The first, occurring perhaps between 10,000 and 7,000 years ago, brought the ancestors of indigenous groups that today live around the Cordillera Central mountain range. Later migrations brought other Austronesian groups, along with agriculture, and the languages of these recent Austronesian migrants effectively replaced those existing populations. Papuan ancestry was also detected among the ethnic Blaan and Sangir people of Mindanao, suggesting that there was westward expansion of peoples from Papua New Guinea into the Philippines. In all cases, new immigrants appear to have mixed to some degree with existing populations. The integration of Southeast Asia into Indian Ocean trading networks around 2,000 years ago also shows some impact, with South Asian genetic signals present within some Sama-Bajau communities.[15] After these initial migratory waves that occurred in the precolonial era, there were also modest scales of immigration from Europe and Latin America.[16][17][18] among Filipinos.
The most widely known theory of the prehistoric peopling of the Philippines is that of H. Otley Beyer, founder of the Anthropology Department of the University of the Philippines. Heading that department for 40 years, Professor Beyer became the unquestioned expert on Philippine prehistory, exerting early leadership in the field and influencing the first generation of Filipino historians and anthropologists, archaeologists, paleontologists, geologists, and students the world over.[19] According to Dr. Beyer, the ancestors of the Filipinos came in different "waves of migration", as follows:[20]
There is no definite evidence, archaeological or historical, to support this migration theory, and the passage of time has made that more unlikely. Key issues with this theory include Beyer's reliance on 19th-century theories of progressive evolution and migratory diffusion[clarification needed] that have been shown in other contexts to be overly simplistic and unreliable and his reliance on incomplete archaeological findings and conjecture.[21]
His claims that the Malays were the original settlers of the lowland regions and the dominant cultural transmitter now seem untenable, no subsequent evidence has emerged to support his "Dawn Man",[21] and improved bathymetric soundings have established that there was almost certainly not a land bridge to Sundaland,[22] although most of the islands were connected and could be accessed across the Mindoro Strait and Sibutu Passage. Writing in 1994, Philippine historian William Scott concluded that "it is probably safe to say that no anthropologist accepts the Beyer Wave Migration Theory today."[23]
A German scientist who has studied the Philippines, Fritjof Voss, has even argued that the present soundings are probably a generous overestimate of the earlier situation, as the Philippines have steadily risen over known geologic history.
In February 1976, Fritjof Voss, a German scientist who studied the geology of the Philippines, questioned the validity of the theory of land bridges. He maintained that the Philippines was never part of mainland Asia. He claimed that it arose from the bottom of the sea and, as the thin Pacific crust moved below it, continued to rise. It continues to rise today. The country lies along great Earth faults that extend to deep submarine trenches. The resulting violent earthquakes caused what is now the land masses forming the Philippines to rise to the surface of the sea. Dr. Voss also pointed out that when scientific studies were done on the Earth's crust from 1964 to 1967, it was discovered that the 35-kilometer- thick crust underneath China does not reach the Philippines. Thus, the latter could not have been a land bridge to the Asian mainland.[24] The matter of who the first settlers were has not been really resolved. This is being disputed by anthropologists, as well as Professor H. Otley Beyer, who claims that the first inhabitants of the Philippines came from the Malay Peninsula. The Malays now constitute the largest portion of the populace and what Filipinos now have is an Austronesian culture.
Philippine historian William Henry Scott has pointed out that Palawan and the Calamianes Islands are separated from Borneo by water nowhere deeper than 100 meters, that south of a line drawn between Saigon and Brunei does the depth of the South China Sea nowhere exceeds 100 meters, and that the Strait of Malacca reaches 50 meters only at one point.[25] Scott also asserts that the Sulu Archipelago is not the peak of a submerged mountain range connecting Mindanao and Borneo, but the exposed edge of three small ridges produced by tectonic tilting of the sea bottom in recent geologic times. According to Scott, it is clear that Palawan and the Calamianes do not stand on a submerged land bridge, but were once a hornlike protuberance on the shoulder of a continent whose southern shoreline used to be the present islands of Java and Borneo. Mindoro and the Calamianes are separated by a channel more than 500 meters deep. [26]
A less rigid version of the earlier wave migration theory is the Core Population Theory first proposed by anthropologist Felipe Landa Jocano of the University of the Philippines.[27] This theory holds that there weren't clear discrete waves of migration. Instead it suggests early inhabitants of Southeast Asia were of the same ethnic group with similar culture, but through a gradual process over time driven by environmental factors, differentiated themselves from one another.[28][29][30]
Jocano contends that what fossil evidence of ancient men show is that they not only migrated to the Philippines, but also to New Guinea, Borneo, and Australia. He says that there is no way of determining if they were Negritos at all. However, what is sure is that there is evidence the Philippines was inhabited tens of thousands of years ago. In 1962, a skull cap and a portion of a jaw, presumed to be those of a human being, were found in Tabon Cave in Palawan.[31][32]
The nearby charcoal from cooking fires have been dated to c. 22,000 years ago. While Palawan was connected directly to Sundaland during the last ice age (and separated from the rest of the Philippines by the Mindoro Strait), Callao Man's still-older remains (c. 67,000 B.P.) were discovered in northern Luzon. Some have argued that this may show settlement of the Philippines earlier than that of the Malay Peninsula.[32]
Jocano further believes that the present Filipinos are products of the long process of cultural evolution and movement of people. This not only holds true for Filipinos, but for the Indonesians and the Malays of Malaysia, as well. No group among the three is culturally or genetically dominant. Hence, Jocano says that it is not correct to attribute the Filipino culture as being Malayan in orientation.[27]
According to Jocano's findings, the people of the prehistoric islands of Southeast Asia were of the same population as the combination of human evolution that occurred in the islands of Southeast Asia about 1.9 million years ago. The claimed evidence for this is fossil material found in different parts of the region and the movements of other people from the Asian mainland during historic times. He states that these ancient men cannot be categorized under any of the historically identified ethnic groups (Malays, Indonesians, and Filipinos) of today.[27]
Other prominent anthropologists like Robert Bradford Fox, Alfredo E. Evangelista, Jesus Peralta, Zeus A. Salazar, and Ponciano L. Bennagen agreed with Jocano.[30][33] Some still preferred Beyer's theory as the more acceptable model, including anthropologist E. Arsenio Manuel.[30]

in West Africa (white)
Yorubaland (Yoruba: Ilẹ̀ Káàárọ̀-Oòjíire) is the homeland and cultural region of the Yoruba people in West Africa. It spans the modern-day countries of Nigeria, Togo and Benin, and covers a total land area of 142,114 km2 (54,871 sq mi). Of this land area, 106,016 km2 (74.6%) lies within Nigeria, 18.9% in Benin, and the remaining 6.5% is in Togo. Prior to European colonization, a portion of this area was known as Yoruba country. The geo-cultural space contains an estimated 55 million people, the majority of this population being ethnic Yoruba.
Geo-physically, Yorubaland spreads north from the Gulf of Guinea and west from the Niger River into Benin and Togo. In the northern section, Yorubaland begins in the suburbs just west of Lokoja and continues unbroken up to the Ogooué River tributary of the Mono River in Togo, a distance of around 610 km. In the south, it begins in an area just west of the Benin and Osse (Ovia) river occupied by the Ilaje Yorubas and continues uninterrupted up to Porto Novo, a total distance of about 280 km as the crow flies. West of Porto Novo Gbe speakers begin to predominate. The northern section is thus more expansive than the southern coastal section.
The land is characterized by mangrove forests, estuaries and coastal plains in the south, which rise steadily northwards into rolling hills and a jagged highland region in the interior, commonly known as the Yorubaland plateau or Western upland. The highlands are pronounced in the Ekiti area of the region, especially around the Effon ridge and the Okemesi fold belt, which have heights in excess of 732 m (2,400 ft) and are characterized by numerous waterfalls and springs such as Olumirin waterfall, Arinta waterfall, and Effon waterfall.[1][2] The highest elevation is found at the Idanre Inselberg Hills, which have heights in excess of 1,050 metres (3,440 ft). In general, the landscape of the interior is made up of undulating terrain with occasional inselbergs jutting out dramatically from the surrounding expanse. Some include: Okeagbe hills: 790m, Olosunta in Ikere Ekiti: 690m, Saki and Igbeti hills.
With coastal plains, southern lowlands, and interior highlands, Yorubaland has several large rivers and streams that crisscross the terrain.[1] These rivers flow in two general directions within the Yoruba country; southwards into the lagoons, estuaries and creeks which empty into the Atlantic Ocean, and northwards into the Niger river. Some southward flowing rivers include; The Osun and Shasha rivers which empty into the Lekki Lagoon, the Ogun River and its major tributaries; the Oyan and Ofiki which empties into the Lagos Lagoon, the upper Mono River, Oba River, Erinle River, Yewa River which discharges into the Badagry creek, Okpara River which forms part of the Nigeria-Benin border before fully re-entering Benin to join the Ouémé River (Ofe in Yoruba) which drains into Lake Nokoué and the Porto-Novo creek. On the eastern flank, the Owena (Siluko), Ofosu and Ose rivers empty into the Benin river creek. Those which flow in a northerly direction into the Niger include the Moshi river, Oyun, Oshin, Awun, Asa, Ero and Oyi.[3]
The Nigerian part of Yorubaland comprises today's Ọyọ, Ọṣun, Ogun, Kwara, Ondo, Ekiti, Lagos as well as parts of Kogi.[1] The Beninese portion consists of Ouémé Department, Plateau Department, Collines Department, Tchaourou commune of Borgou Department, Bassila commune of Donga Department, Ouinhi and Zogbodomey commune of Zou Department, and Kandi commune of Alibori Department. The Togolese portions are the Ogou, Anié and Est-Mono prefectures in Plateaux Region, and the Tchamba prefecture in Centrale Region.
The climate of Yorubaland varies from north to south. The southern, central and eastern portions of the territory is tropical high forest, known as the Yoruba lowland forests ecoregion.[4] The characteristic vegetation is verdant closed-canopy forests composed of many varieties of hardwood trees including Milicia excelsa which is more commonly known locally as iroko, Antiaris africana, Terminalia superba which is known locally as afara, Entandrophragma or sapele, Lophira alata, Triplochiton scleroxylon (or obeche), Khaya grandifoliola (or African mahogany), Symphonia globulifera, and numerous other species. Some non-native species such as Tectona grandis (teak) and Gmelina arborea (pulp wood) have been introduced into the ecosystem and are being extensively grown in several large forest plantations.
The coastal section of this area features an area covered by swamp flats and dominated by such plants as mangroves and other stilt plants as well as palms, ferns and coconut trees on the beaches. This portion includes most of Ondo, Ekiti, Ogun, Osun, Lagos states and is characterised by generally high levels of precipitation defined by a double maxima (peak period); March–July and September–November. Annual rainfall in Ijebu Ode in the middle of Ogun state, for example, averages 2,020 millimetres or 80 inches.[5] The area is the center of thriving cocoa, natural rubber, kola nut and oil palm production industry, as well as lucrative logging. Ondo, Ekiti and Osun states are the leading producers of cocoa in Nigeria,[6][7] while the southern portions of Ogun and Ondo states (Odigbo, Okitipupa and Irele) play host to large plantations of oil palm and rubber.
The northern and western portions of the region is characterized by tropical woodland savanna climate (Aw), with a single rainfall maxima. This area covers the northern two-thirds of Oyo, northwestern Ogun, Kwara, Kogi, Collines (Benin), northern half of Plateau department (Benin) and central Togo. It is part of the Guinean forest–savanna mosaic ecoregion, a transitional zone between West Africa's coastal forests and interior savannas.[8] Part of this region is derived savanna which was once covered in forest but has lost tree cover due to agricultural and other pressures on land. Annual rainfall here hovers between 1,100 and 1,500 millimetres (43 and 59 in). Annual precipitation in Ilorin for example is 1,220 millimetres or 48.03 inches.[9] Tree species here include the Blighia sapida more commonly known as ackee in English and ishin in Yoruba, and Parkia biglobosa which is the locust bean tree used in making iru or ogiri, a local cooking condiment.
The monsoon (rainy period) in both climatic zones is followed by a drier season characterized by northwest trade winds that bring the harmattan (cold dust-laden windstorms) that blow from the Sahara. They normally affect all areas except a small portion of the southern coast. Nonetheless, it has been reported that the harmattan has reached as far as Lagos in some years.
Oduduwa is regarded as the legendary progenitor of the Yoruba, and almost every Yoruba settlement traces its origin to princes of Ile-Ife in Osun State, Nigeria. As such, Ife can be regarded as the cultural and spiritual homeland of the Yoruba nation, both within and outside Nigeria. According to an Oyo account, Oduduwa was a Yoruba emissary; said to have come from the east, sometimes understood by some sources as the "vicinity" true east on the cardinal points, but more likely signifying the region of the Ekiti and Okun sub-communities in Yorubaland, Nigeria.[12]
On the other hand, linguistic evidence seems to corroborate the fact that the eastern half of Yorubaland was settled at an earlier time in history than the western regions, as the Northwest and Southwest Yoruba dialects show more linguistic innovations than their central and eastern counterparts.[citation needed]
Between 1100 and 1400, the Yoruba Kingdom of Ife experienced a golden age, part of which was a sort of artistic and ideological renaissance.[citation needed] It was then surpassed by the Oyo Empire as the dominant Yoruba military and political power between 1700 and 1900. Yoruba people generally feel a deep sense of culture and tradition that unifies and helps identify them.[citation needed] There are sixteen established kingdoms, states that are said to have been descendants of Oduduwa himself. The other sub-kingdoms and chiefdoms that exist are second order branches of the original sixteen kingdoms.[citation needed]
There are various groups and subgroups in Yorubaland based on the many distinct dialects of the Yoruba language, which although all mutually intelligible, have peculiar differences. The governments of these diverse people are quite intricate and each group and subgroup varies in their pattern of governance. In general, government begins at home with the immediate family. The next level is the extended family with its own head, an Olori-Ebi. A collection of distantly related extended families makes up a town. The individual chiefs that serve the towns as corporate entities, called Olóyès, are subject to the Baálès that rule over them. A collection of distantly related towns makes up a clan. A separate group of Oloyes are subject to the Oba that rules over an individual clan, and this Oba may himself be subject to another Oba, depending on the grade of the Obaship.[citation needed]
In this, government begins at home. The father of the family is considered the "head" and his first wife is the mother of the house. If her husband chooses to marry another wife, that wife must show proper respect to the first wife even if the first wife is chronologically younger. Children are taught to have respect for all those who are older than they are. This includes their parents, aunts, uncles, elder siblings, and cousins who they deal with every day. ... Any adult presumably has as much authority over a child as the child's parents do. All members of a particular clan live in the same compound and share family resources, rights, and possessions such as land
Ife was surpassed by the Oyo Empire as the dominant Yoruba military and political power between the year 1600 and 1800. The nearby kingdom of Benin was also a powerful force between 1300 and 1850. Most of the city states were controlled by Obas, priestly monarchs, and councils made up of Oloyes, recognised leaders of royal, noble and, often, even common descent, who joined them in ruling over the kingdoms through a series of guilds and sects. Different states saw differing ratios of power between the kingship and the chiefs' council. Some, such as Oyo, had powerful, autocratic monarchs with almost total control, while in others the senatorial councils were supreme and the Ọba served as something of a figurehead. In all cases, however, Yoruba monarchs were subject to the continuing approval of their constituents as a matter of policy, and could be easily compelled to abdicate for demonstrating dictatorial tendencies or incompetence. The order to vacate the throne was usually communicated through an aroko or symbolic message, which usually took the form of parrot eggs delivered in a covered calabash bowl by the Basorun the head of Oyomesi (the lawmakers) after Judgements from the Ogbonis which were in the judiciary wing. In most cases, the message would compel the Oba to take his own life, which he was bound by oath to do.
Following a jihad (known as the Fulani War) led by Uthman Dan Fodio (1754–1817) and a rapid consolidation of the Hausa city-states of contemporary northern Nigeria, the Fulani Sokoto Caliphate annexed the buffer Nupe Kingdom and began to press southwards towards the Oyo Empire. Shortly after, they overran the Yoruba city of Ilorin and then sacked Ọyọ-Ile, the capital city of the Oyo Empire. Further attempts by the Sokoto Caliphate to expand southwards were checked by the Yoruba who had rallied to resist under the military leadership of the city-state of Ibadan, which rose from the old Oyo Empire, and of the Ijebu kingdom.
However, the Oyo hegemony had been dealt a mortal blow. The other Yoruba city-states broke free of Oyo dominance, and subsequently became embroiled in a series of internecine wars, a period when millions of individuals were forcibly transported to the Americas and the Caribbean, eventually ending up in such countries as the Bahamas, Cuba, the Dominican Republic, Puerto Rico, Brazil, Haiti and Venezuela, the United States, among others.
During the 19th century, the British Empire gradually colonized Yorubaland. In 1892, the British declared war on the Ijebu Kingdom in response to its barriers on trade. The British emerged victorious in the conflict and occupied the Ijebu capital.[14] After British colonization, the capital served as an administrative center for colonial officials as the kingdom was annexed to the colony of Southern Nigeria. The colony was gradually expanded by protectorate treaties. These treaties proved decisive in the eventual annexation of the rest of Yorubaland and, eventually, of southern Nigeria and the Cameroons.[citation needed]
In 1960, greater Yorubaland was subsumed into the Federal Republic of Nigeria.[15]
According to Yoruba historians, by the time the British came to colonize and subjugate Yorubaland first to itself and later to the Fulani of Northern Nigeria, the Yoruba were getting ready to recover from what is popularly known as the Yoruba Civil War. One of the lessons of the internecine Yoruba wars was the opening of Yorubaland to Fulani hegemony whose major interest was the imposition of sultanistic despotism on Old Oyo Ile and present-day Ilorin. The most visible consequence of this was the adding of almost one-fifth of Yorubaland from Offa to Old Oyo to Kabba to the then-Northern Nigeria of Lord Frederick Lugard and the subsequent subjugation of this portion of Yorubaland under the control of Fulani feudalism.[16]
[1]

The prehistory of Australia is the period between the first human habitation of the Australian continent and the colonisation of Australia in 1788, which marks the start of consistent written documentation of Australia. This period has been variously estimated, with most evidence suggesting that it goes back between 50,000 and 65,000 years. This era is referred to as prehistory rather than history because knowledge of this time period does not derive from written documentation. However, some argue that Indigenous oral tradition should be accorded an equal status.[1]
Human habitation of the Australian continent began with the migration of the ancestors of today's Aboriginal Australians by land bridges and short sea crossings from what is now Southeast Asia.[2] It is uncertain how many waves of immigration may have contributed to these ancestors of modern Aboriginal Australians.[3][4] The Madjedbebe rock shelter in Arnhem Land is perhaps the oldest site showing the presence of humans in Australia.[5][6] The oldest human remains found are at Lake Mungo in New South Wales, which have been dated to around 41,000 years ago.[7][8]
At the time of first European contact, estimates of the Aboriginal population range from 300,000 to one million.[9][10][11] They were complex hunter-gatherers with diverse economies and societies. There were about 600 tribes or nations and 250 languages with various dialects.[12][13] Certain groups engaged in fire-stick farming,[14] fish farming,[15] and built semi-permanent shelters.[16][17] The extent to which some groups engaged in agriculture is controversial.[18][19][20]
The Torres Strait Islander people first settled their islands around 4,000 years ago. Culturally and linguistically distinct from mainland Aboriginal peoples, they were seafarers and obtained their livelihood from seasonal horticulture and the resources of their reefs and seas. Agriculture also developed on some islands and villages appeared by the 1300s.[21][22]
The earliest evidence of humans in Australia has been variously estimated, with most scholars, as of 2023, dating it between 50,000 and 65,000 years BP.[23][24][25][26]
There is considerable discussion among archaeologists as to the route taken by the first migrants to Australia, widely taken to be ancestors of the modern Aboriginal peoples. Migration took place during the closing stages of the Pleistocene, when sea levels were much lower than they are today. Repeated episodes of extended glaciation during the Pleistocene epoch resulted in decreases of sea levels by more than 100 metres in Australasia.[27] People appear to have arrived by sea during a period of glaciation, when New Guinea and Tasmania were joined to the continent of Australia. The continental coastline extended much further out into the Timor Sea, and Australia and New Guinea formed a single landmass (known as Sahul), connected by an extensive land bridge across the Arafura Sea, Gulf of Carpentaria and Torres Strait. Nevertheless, the sea still presented a major obstacle so it is theorised that these ancestral people reached Australia by island hopping.[27] Two routes have been proposed. One follows an island chain between Sulawesi and New Guinea and the other reaches North Western Australia via Timor.[28] Rupert Gerritsen has suggested an alternative theory, involving accidental colonisation as a result of tsunamis.[29] The journey still required sea travel, however, making them some of the world's earliest mariners.[30]
In the 2013 book First Footprints: The Epic Story of the First Australians, Scott Cane writes that the first wave may have been prompted by the eruption of Toba, and if they arrived around 70,000 years ago, they could have crossed the water from Timor, when the sea level was low – but if they came later, around 50,000 years ago, a more likely route would be through the Moluccas to New Guinea. Given that the likely landfall regions have been under around 50 metres of water for the last 15,000 years, it is unlikely that the timing will ever be established with certainty.[31]
The minimum widely accepted time-frame for the arrival of humans in Australia is placed at least 48,000 years ago.[32] Many sites dating from this time period have been excavated. In Arnhem Land Madjedbebe (formerly known as Malakunanja II) fossils and a rock shelter have been dated to around 65,000 years old, although a study in 2020 argues that this dating is unreliable.[24][33] According to mitochondrial DNA research, Aboriginal people reached Eyre Peninsula (South Australia) 49,000–45,000 years ago from both the east (clockwise, along the coast, from northern Australia) and the west (anti-clockwise).[34]: 189
Radiocarbon dating suggests that they lived in and around Sydney for at least 30,000 years.[35] In Parramatta, Western Sydney, it was found that some Aboriginal peoples used charcoal, stone tools and possible ancient campfires.[36] Near Penrith, a far western suburb of Sydney, numerous Aboriginal stone tools were found in Cranebrook Terraces gravel sediments having dates of 45,000 to 50,000 years BP. This would mean that there was human settlement in Sydney earlier than thought.[37]
Archaeological evidence indicates human habitation at the upper Swan River, Western Australia by about 40,000 years ago.[38] A 2018 study using archaeobotany dated evidence of human habitation at Karnatukul (Serpent's Glen) in the Carnarvon Range in the Little Sandy Desert in WA at around 50,000 years (20,000 years earlier than previously thought), and it was shown that human habitation had been continuous at the site since then.[39][40][41]
Tasmania, which was connected to the continent by a land bridge, was inhabited at least 40,000 years ago. The oldest known site is Wareen Cave, dated to this time.[42][43]
A 2021 study by researchers at the Australian Research Council Centre of Excellence for Australian Biodiversity and Heritage has mapped the likely migration routes of the peoples as they moved across the Australian continent to its southern reaches of what is now Tasmania, but back then part of the mainland. The modelling is based on data from archaeologists, anthropologists, ecologists, geneticists, climatologists, geomorphologists, and hydrologists, and it is intended to compare the modelling with the oral histories of Aboriginal peoples, including Dreaming stories, as well as Australian rock art and linguistic features of  the many Aboriginal languages. The routes, dubbed "superhighways" by the authors, are similar to current highways and stock routes in Australia. Lynette Russell of Monash University sees the new model as a starting point for collaboration with Aboriginal people to help uncover their history. The new models suggest that the first people may have first landed in the Kimberley region in what is now Western Australia about 60,000 years ago, and had settled across the continent within 6,000 years.[44][45]
Archeological evidence indicates that the ancestors of today's Aboriginal Australians first migrated to the continent 50,000 to 65,000 years ago.[46][47][48][49] Genomic studies suggest that the peopling of Australia happened between 43,000 to 60,000 years ago.[50][51][52][53]
DNA studies have suggested that the ancestors of Aboriginal Australians belonged to the southern route dispersal following the "out of Africa" exit, which expanded into the South and Southeast Asia region and subsequently diverged rapidly into the ancestors of Ancient Ancestral South Indians (AASI), Andamanese, East Asians, other Australasians, such as Papuans.[54][55][56]
It is unknown how many populations settled in Australia prior to European colonisation. Both "trihybrid" and single-origin hypotheses have received extensive discussion.[57] Keith Windschuttle, known for his belief that Aboriginal pre-history has become politicised, argues that the assumption of a single origin is tied into ethnic solidarity, and multiple entry was suppressed because it could be used to justify white seizure of Aboriginal lands,[58] but this hypothesis is not supported by scientific studies.
Human genomic differences are being studied to find possible answers, but there is still insufficient evidence to distinguish a "wave invasion model" from a "single settlement" one.[59]
A 2012 paper by Alan J. Redd et al. on the topic of migration from India around 4,000 years ago notes that the indicated influx period corresponds to the timing of various other changes, specifically mentioning "The divergence times reported here correspond with a series of changes in the Australian anthropological record between 5,000 years ago and 3,000 years ago, including the introduction of the dingo; the spread of the Australian Small Tool tradition; the appearance of plant-processing technologies, especially complex detoxification of cycads; and the expansion of the Pama-Nyungan language over seven-eighths of Australia". Although previously linked to the pariah dogs of India, recent testing of the mitochondrial DNA of dingoes shows a closer connection to the dogs of Eastern Asia and North America, suggesting an introduction as a result of the Austronesian expansion from Southern China to Timor over the last 5,000 years.[59] A 2007 finding of kangaroo ticks on the pariah dogs of Thailand suggested that this genetic expansion may have been a two-way process.[60]
The dingo reached Australia about 4,000 years ago, and around the same time there were changes in language, with the Pama-Nyungan language family spreading over most of the mainland, and stone tool technology, with the use of smaller tools. Human contact has thus been inferred, and genetic data of two kinds have been proposed to support a gene flow from India to Australia: firstly, signs of South Asian components in Aboriginal Australian genomes, reported on the basis of genome-wide SNP data; and secondly, the existence of a Y chromosome (male) lineage, designated haplogroup C∗, with the most recent common ancestor around 5,000 years ago.[61] The first type of evidence comes from a 2013 study by the Max Planck Institute for Evolutionary Anthropology using large-scale genotyping data from a pool of Aboriginal Australians, New Guineans, island Southeast Asians and Indians. It found that the New Guinea and Mamanwa (Philippines area) groups diverged from the Aboriginal about 36,000 years ago (and supporting evidence that these populations are descended from migrants taking an early "southern route" out of Africa, before other groups in the area), and also that the Indian and Australian populations mixed well before European contact, with this gene flow occurring during the Holocene (c.4,230 years ago).[62] The researchers had two theories for this: either some Indians had contact with people in Indonesia who eventually transferred those genes from India to Aboriginal Australians, or that a group of Indians migrated all the way from India to Australia and intermingled with the locals directly.[63][64]
However, a 2016 study in Current Biology by Anders Bergström et al. excluded the Y chromosome as providing evidence for recent gene flow from India into Australia. The study authors sequenced 13 Aboriginal Australian Y chromosomes using recent advances in gene sequencing technology, investigating their divergence times from Y chromosomes in other continents, including comparing the haplogroup C chromosomes. The authors concluded that, although the results do not disprove the presence of any Holocene gene flow or non-genetic influences from South Asia at that time, and the appearance of the dingo does provide strong evidence for foreign arrivals, the evidence overall is consistent with a complete lack of gene flow, and points to indigenous origins for the technological and linguistic changes. Gene flow across the island-dotted 150-kilometre (93 mi)-wide Torres Strait is plausible and while Y chromosome divergence times are consistent with a lack of gene flow across the Strait since its formation due to rising sea levels, the authors assert that the Torres Strait Islander ancestry of the men from which the times were derived means external contact occurred more recently. It could not be determined from the study when within the last 10,000 years this may have occurred – newer analytical techniques have the potential to address such questions.[61]
Archaeological evidence from ash deposits in the Coral Sea indicates that fire was already a significant part of the Australian landscape over 100,000 years BP.[65] There is evidence of the deliberate use of fire to shape the Australian environment 46,000 years ago. One explanation being the use by hunter-gatherers as a tool to drive game, to produce a green flush of new growth to attract animals, and to open up impenetrable forest.[66] In The Biggest Estate on Earth: How Aborigines made Australia, Bill Gammage claims that dense forest became more open sclerophyll forest, open forest became grassland and fire-tolerant species became more predominant: in particular, eucalyptus, acacia, banksia, casuarina and grasses.[67]
The changes to the fauna were even more dramatic: the megafauna, species significantly larger than humans, disappeared, and many of the smaller species disappeared too. All told, about 60 different vertebrates became extinct, including the genus Diprotodon (very large marsupial herbivores that looked rather like hippos), several large flightless birds, carnivorous kangaroos, Wonambi naracoortensis, a five-metre snake, a five-metre lizard and Meiolania.[68]
The direct cause of the mass extinctions is uncertain: it may have been fire, hunting, climate change or a combination of all or any of these factors. The degree of human agency in these extinctions is still a matter of discussion.[69][70] With no large herbivores to keep the understorey vegetation down and rapidly recycle soil nutrients with their dung, fuel build-up became more rapid and fires burned hotter, further changing the landscape. Against this theory is the evidence that in fact careful seasonal fires from Aboriginal land management practices reduced fuel loads, and prevented wildfires like those seen since European colonisation.[71]
The Aboriginal population was confronted with significant changes to climate and environment. About 30,000 years ago, sea levels began to fall, temperatures in the south-east of the continent dropped by as much as 9 degrees Celsius, and the interior of Australia became more arid. About 20,000 years ago, New Guinea and Tasmania were connected to the Australian continent, which was more than a quarter larger than today.[72]
About 19,000 years ago temperatures and sea levels began to rise. Tasmania became separated from the mainland some 14,000 years ago, and between 8,000 and 6,000 years ago thousands of islands in the Torres Strait and around the coast of Australia were formed.[72][73] Josephine Flood writes that the flooding and loss of land as coastlines receded might have led to greater emphasis on territorial boundaries separating groups, stronger clan identity, and the development of the Rainbow Serpent and other mythologies.[74]
The warmer climate was associated with new technologies. Small back-bladed stone tools appeared 15–19 thousand years ago. Wooden javelins and boomerangs have been found dating from 10,000 years ago. Stone points for spears have been found dating from 5–7 thousand years ago. Spear throwers were probably developed more recently than 6,500 years ago.[75]
Sea levels stabilised at around their current level about 6,500 years ago. Warmer weather, wetter conditions and the new coastlines led to significant changes in Aboriginal social and economic organisation. New coastal societies emerged around tidal reefs, estuaries and flooded river valleys, and coastal islands were incorporated into local economies.[76]  There was a proliferation of stone tool, plant processing and landscape modification technologies. Elaborate fish and eel traps involving channels up to three kilometres long were in use in western Victoria from about 6,500 years ago. Semi-permanent collections of wooden huts on mounds also appeared in western Victoria, associated with a more systematic exploitation of new food sources in the wetlands.[76]
Aboriginal Tasmanians were isolated from the mainland from about 14,000 years ago. As a result, they only possessed one quarter of the tools and equipment of the adjacent mainland and were without hafted axes, grinding technology, stone tipped weapons, spear throwers and the boomerang. By 3,700 BP they had ceased to eat fish and use bone tools. Coastal Tasmanians switched from fish to abalone and crayfish and more Tasmanians moved to the interior.[77] The Tasmanians built watercraft from reeds and bark and journeyed up to 10 kilometres offshore to visit islands and hunt for seals and muttonbirds.[78]
Around 4,000 years ago the first phase of occupation of the Torres Strait Islands began. By 2,500 years ago more of the islands were occupied and a distinctive Torres Strait Island maritime culture emerged. Agriculture also developed on some islands and by 700 years ago villages appeared.[79]
On the Australian mainland, some innovations were imported from neighbouring cultures. The dingo was introduced about 4,000 years ago. Shell fish hooks appeared in Australia about 1,200 years ago and were probably introduced from the Torres Strait or by Polynesian seafarers. From the mid-1660s fishing vessels from Indonesia regularly visited the north coast of Australia in search of trepang (sea cucumber). Trade and social relationships developed which were reflected in Aboriginal art, ceremonies and oral traditions. Aboriginal people adopted dugout canoes and metal harpoon heads from the Indonesians which allowed them to better hunt dugong and turtle off the coast and nearby islands.[80]
Despite these interactions with neighbouring cultures, the basic structure of Aboriginal society was unchanged. Family groups were joined in bands and clans averaging about 25 people, each with a defined territory for foraging.  Clans were attached to tribes or nations, associated with particular languages and country. At the time of European contact there were about 600 tribes or nations and 250 distinct languages with various dialects.[81][82][83][84][85]
Aboriginal society was egalitarian with no formal government or chiefs. Authority rested with elders who held extensive ritual knowledge gained over many years. Group decisions were generally made through the consensus of elders. The traditional economy was cooperative, with males generally hunting large game while females gathered local staples such as small animals, shellfish, vegetables, fruits, seeds and nuts. Food was shared within groups and exchanged across groups.[86]
Aboriginal groups were semi-nomadic, generally ranging over a specific territory defined by natural features. Members of a group would enter the territory of another group through rights established by marriage and kinship or by invitation for specific purposes such as ceremonies and sharing abundant seasonal foods. As all natural features of the land were created by ancestral beings, a group's particular country provided physical and spiritual nourishment.[87][82]
According to Australian Aboriginal mythology and the animist framework developed in Aboriginal Australia, the Dreaming is a sacred era in which ancestral totemic spirit beings formed The Creation. The Dreaming established the laws and structures of society and the ceremonies performed to ensure continuity of life and land.[88]
The extent to which some Aboriginal societies were agricultural is controversial.[89][90][91][92] In the Lake Condah region of western Victoria the inhabitants built elaborate eel and fish traps and hundreds gathered in semi-permanent stone and bark huts during the eel season. However, these groups still moved across their territory several times a year to exploit other seasonal food sources.[93] In semi-arid areas, millet was harvested, stacked and threshed and the seeds stored for later use. In tropical areas the tops of yams were replanted. Flood argues that such practices are better classified as resource management than agriculture and that Aboriginal societies did not develop the systematic cultivation of crops or permanent villages such as existed in the Torres Strait Islands. Elizabeth Williams has called the inhabitants of the more settled regions of the Murray valley "complex hunter gatherers".[94]
Behaviour was governed by strict rules regarding responsibilities to and from uncles, aunts, brothers and sisters as well as in-laws. The kinship systems observed by many communities included a division into moieties, with restrictions on intermarrying dictated by the moiety an individual belonged to.[95]
Male initiation usually occurred at puberty and the rites often included penile subincision, depilation or tooth avulsion.[96] Female initiation often involved purification through smoke or bathing, and sometimes scarification or the removal of finger joints.[97]
Abortion and infanticide were widely practised as a means of birth control or dealing with deformities, injuries or illness which might impair the functioning of the group.[98][99] Some Aboriginal and Torres Strait Islander groups practised ritual cannibalism in rare circumstances, notably mortuary cannibalism. There is no credible evidence that the gustatory cannibalism described by colonists was practised.[100]
Describing prehistoric Aboriginal culture and society during her 1999 Boyer Lecture, Australian historian and anthropologist Inga Clendinnen explained:
"They [...] developed steepling thought-structures – intellectual edifices so comprehensive that every creature and plant had its place within it. They travelled light, but they were walking atlases, and walking encyclopedias of natural history. [...] Detailed observations of nature were elevated into drama by the development of multiple and multi-level narratives: narratives which made the intricate relationships between these observed phenomena memorable.
These dramatic narratives identified the recurrent and therefore the timeless and the significant within the fleeting and the idiosyncratic. They were also very human, charged with moral significance but with pathos, and with humour, too – after all, the Dreamtime creatures were not austere divinities, but fallible beings who happened to make the world and everything in it while going about their creaturely business. Traditional Aboriginal culture effortlessly fuses areas of understanding which Europeans 'naturally' keep separate: ecology, cosmology, theology, social morality, art, comedy, tragedy – the observed and the richly imagined fused into a seamless whole."[101]
Aboriginal people have no cultural memory of living anywhere outside Australia. Nevertheless, the people living along the northern coastline of Australia, in the Kimberley, Arnhem Land, Gulf of Carpentaria and Cape York had encounters with various visitors for many thousands of years. People and traded goods moved freely between Australia and New Guinea up to and even after the eventual flooding of the land bridge by rising sea levels, which was completed about 6,000 years ago.[citation needed]
However, trade and intercourse between the separated lands continued across the newly formed Torres Strait, whose 150 km-wide channel remained readily navigable with the chain of Torres Strait Islands and reefs affording intermediary stopping points.[citation needed] The islands were settled by different seafaring Melanesian cultures such as the Torres Strait Islanders over 2500 years ago, and cultural interactions continued via this route with the Aboriginal people of northeast Australia.
Indonesian "Bajau" fishermen from the Spice Islands (e.g. Banda) have fished off the coast of Australia for hundreds of years. Macassan traders from Sulawesi regularly visited the coast of northern Australia to fish for trepang, an edible sea cucumber to trade with the Chinese since at least the early 18th century.
There was a high degree of cultural exchange, evidenced in Aboriginal rock and bark paintings, the introduction of technologies such as dug-out canoes and items such as tobacco and tobacco pipes, Macassan words in Aboriginal languages (e.g. Balanda for white person), and descendants of Malay people in Australian Aboriginal communities and vice versa, as a result of intermarriage and migration.[102]
The myths of the people of Arnhem Land have preserved accounts of the trepang-catching, rice-growing Baijini people, who, according to the myths, were in Australia in the earliest times, before the Macassans. The Baijini have been variously interpreted by modern researchers as a different group of presumably South East Asian people, such as Bajau visitors to Australia who may have visited Arnhem Land before the Macassans,[103][104] as a mythological reflection of the experiences of some Yolŋu people who have travelled to Sulawesi with the Macassans and came back,[105] or, in more fringe views, even as visitors from China.[106]
In 1944, a small number of copper coins with Arabic inscriptions were discovered on a beach in Jensen Bay on Marchinbar Island, part of the Wessel Islands of the Northern Territory. These coins were later identified as from the Kilwa Sultanate of east Africa. Only one such coin had ever previously been found outside east Africa (unearthed during an excavation in Oman). The inscriptions on the Jensen Bay coins identify a ruling Sultan of Kilwa, but it is unclear whether the ruler was from the 10th century or the 14th century. This discovery has been of interest to those historians who believe it likely that people made landfall in Australia or its offshore islands before the first generally accepted such discovery, by the Dutch sailor Willem Janszoon in 1606.[107]
This article incorporates text by Anders Bergström et al. available under the CC BY 4.0 license.

Early human migrations are the earliest migrations and expansions of archaic and modern humans across continents. They are believed to have begun approximately 2 million years ago with the early expansions out of Africa by Homo erectus. This initial migration was followed by other archaic humans including H. heidelbergensis, which lived around 500,000 years ago and was the likely ancestor of Denisovans and Neanderthals as well as modern humans. Early hominids had likely crossed land bridges that have now sunk.
Within Africa, Homo sapiens dispersed around the time of its speciation, roughly 300,000 years ago.[note 1] The recent African origin theory suggests that the anatomically modern humans outside of Africa descend from a population of Homo sapiens migrating from East Africa roughly 70–50,000 years ago and spreading along the southern coast of Asia and to Oceania by about 50,000 years ago. Modern humans spread across Europe about 40,000 years ago.
Early Eurasian Homo sapiens fossils have been found in Israel and Greece, dated to 194,000–177,000 and 210,000 years old respectively. These fossils seem to represent failed dispersal attempts by early Homo sapiens, who were likely replaced by local Neanderthal populations.[3][4][5]
The migrating modern human populations are known to have interbred with earlier local populations, so that contemporary human populations are descended in small part (below 10% contribution) from regional varieties of archaic humans.[note 2]
After the Last Glacial Maximum, North Eurasian populations migrated to the Americas about 20,000 years ago.[9][10]  Arctic Canada and Greenland were reached by the Paleo-Eskimo expansion around 4,000 years ago. Finally, Polynesia was populated within the past 2,000 years in the last wave of the Austronesian expansion.
The earliest humans developed out of australopithecine ancestors about 3 million years ago, most likely in the area of the Kenyan Rift Valley, where the oldest known stone tools have been found. Stone tools recently discovered at the Shangchen site in China and dated to 2.12 million years ago are claimed to be the earliest known evidence of hominins outside Africa, surpassing Dmanisi in Georgia by 300,000 years.[11]
Between 2 and less than a million years ago, Homo spread throughout East Africa and to Southern Africa (Homo ergaster), but not yet to West Africa. Around 1.8 million years ago, Homo erectus migrated out of Africa via the Levantine corridor and Horn of Africa to Eurasia. This migration has been proposed as being related to the operation of the Saharan pump, around 1.9 million years ago.[citation needed] Homo erectus dispersed throughout most of the Old World, reaching as far as Southeast Asia. Its distribution is traced by the Oldowan lithic industry, by 1.3 million years ago extending as far north as the 40th parallel (Xiaochangliang).
Key sites for this early migration out of Africa are Riwat in Pakistan (~2 Ma?[12]), Ubeidiya in the Levant (1.5 Ma) and Dmanisi in the Caucasus (1.81 ± 0.03 Ma, p=0.05[13]).
China shows evidence of Homo erectus from 2.12 mya in Gongwangling, in Lantian county.[14] Two Homo erectus incisors have been found near Yuanmou, southern China, and are dated to 1.7 mya, and a cranium from Lantian has been dated to 1.63 mya. Artefacts from Majuangou III and Shangshazui in the Nihewan basin, northern China, have been dated to 1.6–1.7 mya.[14][15] The archaeological site of Xihoudu (西侯渡) in Shanxi province is the earliest recorded use of fire by Homo erectus, which is dated 1.27 million years ago.[16]
Southeast Asia (Java) was reached about 1.7 million years ago (Meganthropus). Western Europe was first populated around 1.2 million years ago (Atapuerca).[17]
Robert G. Bednarik has suggested that Homo erectus may have built rafts and sailed oceans, a theory that has raised some controversy.[18]
One million years after its dispersal, H. erectus was diverging into new species. H. erectus is a chronospecies and was never extinct, so its "late survival" is a matter of taxonomic convention. Late forms of H. erectus are thought to have survived until after about 0.5 million ago to 143,000 years ago at the latest,[note 3] with derived forms classified as H. antecessor in Europe around 800,000 years ago and H. heidelbergensis in Africa around 600,000 years ago. H. heidelbergensis in its turn spread across East Africa (H. rhodesiensis) and to Eurasia, where it gave rise to Neanderthals and Denisovans.
H. heidelbergensis, Neanderthals and Denisovans expanded north beyond the 50th parallel (Eartham Pit, Boxgrove 500kya, Swanscombe Heritage Park 400kya, Denisova Cave 50 kya). It has been suggested that late Neanderthals may even have reached the boundary of the Arctic, by c. 32,000 years ago, when they were being displaced from their earlier habitats by H. sapiens, based on 2011 excavations at the site of Byzovaya in the Urals (Komi Republic, 65°01′N 57°25′E﻿ / ﻿65.02°N 57.42°E﻿ / 65.02; 57.42).[20]
Other archaic human species are assumed to have spread throughout Africa by this time, although the fossil record is sparse. Their presence is assumed based on traces of admixture with modern humans found in the genome of African populations.[8][21][22][23] Homo naledi, discovered in South Africa in 2013 and tentatively dated to about 300,000 years ago, may represent fossil evidence of such an archaic human species.[24]
Neanderthals spread across the Near East and Europe, while Denisovans appear to have spread across Central and East Asia and to Southeast Asia and Oceania. There is evidence that Denisovans interbred with Neanderthals in Central Asia where their habitats overlapped.[25] Neanderthal evidence has also been found quite late at 33,000 years ago at the 65th latitude of the Byzovaya site in the Ural Mountains. This is far outside of any otherwise known habitat, during a high ice cover period, and perhaps reflects a refugia of near extinction.
Homo sapiens are believed to have emerged in Africa about 300,000 years ago, based in part on thermoluminescence dating of artifacts and remains from Jebel Irhoud, Morocco, published in 2017.[note 4][27] The Florisbad Skull from Florisbad, South Africa, dated to about 259,000 years ago, has also been classified as early Homo sapiens.[28][29][30][31] Previously, the Omo remains, excavated between 1967 and 1974 in Omo National Park, Ethiopia, and dated to 200,000 years ago, were long held to be the oldest known fossils of Homo sapiens.[32]
In September 2019, scientists reported the computerized determination, based on 260 CT scans, of a virtual skull shape of the last common human ancestor to anatomically modern humans, representative of the earliest modern humans, and suggested that modern humans arose between 260,000 and 350,000 years ago through a merging of populations in East and South Africa.[33][34]
In July 2019, anthropologists reported the discovery of 210,000 year old remains of a H. sapiens and 170,000 year old remains of a H. neanderthalensis in Apidima Cave in southern Greece, more than 150,000 years older than previous H. sapiens finds in Europe.[35][36][37][38]
Early modern humans expanded to Western Eurasia and Central, Western and Southern Africa from the time of their emergence. While early expansions to Eurasia appear not to have persisted,[39][25] expansions to Southern and Central Africa resulted in the deepest temporal divergence in living human populations. Early modern human expansion in sub-Saharan Africa appears to have contributed to the end of late Acheulean (Fauresmith) industries at about 130,000 years ago, although very late coexistence of archaic and early modern humans, until as late as 12,000 years ago, has been argued for West Africa in particular.[40]
The ancestors of the modern Khoi-San expanded to Southern Africa before 150,000 years ago, possibly as early as before 260,000 years ago,[note 5] so that by the beginning of the MIS 5 "megadrought", 130,000 years ago, there were two ancestral population clusters in Africa, bearers of mt-DNA haplogroup L0 in southern Africa, ancestral to the Khoi-San, and bearers of haplogroup L1-6 in central/eastern Africa, ancestral to everyone else. There was a significant back-migration of bearers of L0 towards eastern Africa between 120 and 75 kya.[note 6]
Expansion to Central Africa by the ancestors of the Central African forager populations (African Pygmies) most likely took place before 130,000 years ago, and certainly before 60,000 years ago.[42][43][44][45][note 7] Wet forest environments were not a major ecological barrier for Homo sapiens as early as around 150,000 years ago.[47][non-primary source needed][48]
The situation in West Africa is difficult to interpret due to a scarcity of fossil evidence. Homo sapiens seems to have reached the western Sahelian zone by 130,000 years ago, while tropical West African sites associated with H. sapiens are known only from after 130,000 years ago. Unlike elsewhere in Africa, archaic Middle Stone Age sites appear to persist until very late, down to the Holocene boundary (12,000 years ago), pointing to the possibility of late survival of archaic humans, and late hybridization with H. sapiens in West Africa.[40]
Populations of Homo sapiens migrated to the Levant and to Europe[dubious – discuss] between 130,000 and 115,000 years ago, and possibly in earlier waves as early as 185,000 years ago.[note 8]
A fragment of a jawbone with eight teeth found at Misliya Cave has been dated to around 185,000 years ago. Layers dating from between 250,000 and 140,000 years ago in the same cave contained tools of the Levallois type which could put the date of the first migration even earlier if the tools can be associated with the modern human jawbone finds.[49][50]
These early migrations do not appear to have led to lasting colonisation and receded by about 80,000 years ago.[25] There is a possibility that this first wave of expansion may have reached China (or even North America[dubious – discuss][51]) as early as 125,000 years ago, but would have died out without leaving a trace in the genome of contemporary humans.[25]
There is some evidence that modern humans left Africa at least 125,000 years ago using two different routes: through the Nile Valley, the Sinai Peninsula and the Levant (Qafzeh Cave: 120,000–100,000 years ago); and a second route through the present-day Bab-el-Mandeb Strait on the Red Sea (at that time, with a much lower sea level and narrower extension), crossing to the Arabian Peninsula[52][53] and settling in places like the present-day United Arab Emirates (125,000 years ago)[54] and Oman (106,000 years ago),[55] and possibly reaching the Indian Subcontinent (Jwalapuram: 75,000 years ago.) Although no human remains have yet been found in these three places, the apparent similarities between the stone tools found at Jebel Faya, those from Jwalapuram and some from Africa suggest that their creators were all modern humans.[56] These findings might give some support to the claim that modern humans from Africa arrived at southern China about 100,000 years ago (Zhiren Cave, Zhirendong, Chongzuo City: 100,000 years ago;[note 9] and the Liujiang hominid (Liujiang County): controversially dated at 139,000–111,000 years ago [61]). Dating results of the Lunadong (Bubing Basin, Guangxi, southern China) teeth, which include a right upper second molar and a left lower second molar, indicate that the molars may be as old as 126,000 years.[62][63]
Since these previous exits from Africa did not leave traces in the results of genetic analyses based on the Y chromosome and on MtDNA, it seems that those modern humans did not survive in large numbers and were assimilated by our major antecessors. An explanation for their extinction (or small genetic imprint) may be the Toba eruption (74,000 years ago), though some argue it scarcely affected human population.[64]
The so-called "recent dispersal" of modern humans took place about 70–50,000 years ago.[65][66][67] It is this migration wave that led to the lasting spread of modern humans throughout the world.
A small group from a population in East Africa, bearing mitochondrial haplogroup L3 and numbering possibly fewer than 1,000 individuals,[68][69] crossed the Red Sea strait at Bab-el-Mandeb, to what is now Yemen, after around 75,000 years ago.[70] A recent review has also shown support for the northern route through the Sinai Peninsula and the Levant.[25] Their descendants spread along the coastal route around Arabia and Persia to South Asia before 55,000 years ago. Other research supports a migration out of Africa between about 65,000 and 50,000 years ago.[65][71][67] The coastal migration between roughly 70,000 and 50,000 years ago is associated with mitochondrial haplogroups M and N, both derivative of L3.
Along the way H. sapiens interbred with Neanderthals and Denisovans,[72] with Denisovan DNA making 0.2% of mainland Asian and Native American DNA.[73]
Migrations continued along the Asian coast to Southeast Asia and Oceania, colonising Australia by around 65,000–50,000 years ago.[74][75][76] By reaching Australia, H. sapiens for the first time expanded its habitat beyond that of H. erectus. Denisovan ancestry is shared by Melanesians, Aboriginal Australians, and smaller scattered groups of people in Southeast Asia, such as the Mamanwa, a Negrito people in the Philippines, suggesting the interbreeding took place in Eastern Asia where the Denisovans lived.[77][78][79] Denisovans may have crossed the Wallace Line, with Wallacea serving as their last refugium.[80][81] Homo erectus had crossed the Lombok gap reaching as far as Flores, but never made it to Australia.[82]
During this time sea level was much lower and most of Maritime Southeast Asia formed one land mass known as Sunda. Migration continued Southeast on the coastal route to the straits between Sunda and Sahul, the continental land mass of present-day Australia and New Guinea. The gaps on the Weber Line are up to 90 km wide,[83] so the migration to Australia and New Guinea would have required seafaring skills. Migration also continued along the coast eventually turning northeast to China and finally reaching Japan before turning inland. This is evidenced by the pattern of mitochondrial haplogroups descended from haplogroup M, and in Y-chromosome haplogroup C.
Sequencing of one Aboriginal genome from an old hair sample in Western Australia revealed that the individual was descended from people who migrated into East Asia between 62,000 and 75,000 years ago. This supports the theory of a single migration into Australia and New Guinea before the arrival of Modern Asians (between 25,000 and 38,000 years ago) and their later migration into North America.[84] This migration is believed to have happened around 50,000 years ago, before Australia and New Guinea were separated by rising sea levels approximately 8,000 years ago.[85][86] This is supported by a date of 50,000–60,000 years ago for the oldest evidence of settlement in Australia,[74][87] around 40,000 years ago for the oldest human remains,[74] the earliest humans artifacts which are at least 65,000 years old[88] and the extinction of the Australian megafauna by humans between 46,000 and 15,000 years ago argued by Tim Flannery,[89] which is similar to what happened in the Americas. The continued use of Stone Age tools in Australia has been much debated.[90]
The population brought to South Asia by coastal migration appears to have remained there for some time, during roughly 60,000 to 50,000 years ago, before spreading further throughout Eurasia. This dispersal of early humans, at the beginning of the Upper Paleolithic, gave rise to the major population groups of the Old World and the Americas.
Towards the West, Upper Paleolithic populations associated with mitochondrial haplogroup R and its derivatives, spread throughout Asia and Europe, with a back-migration of M1 to North Africa and the Horn of Africa several millennia ago. [dubious – discuss]
Presence in Europe is certain after 40,000 years ago, possibly as early as 43,000 years ago,[91] rapidly replacing the Neanderthal population. Contemporary Europeans have Neanderthal ancestry, but it seems likely that substantial interbreeding with Neanderthals ceased before 47,000 years ago, i.e. took place before modern humans entered Europe.[92]
There is evidence from mitochondrial DNA that modern humans have passed through at least one genetic bottleneck, in which genome diversity was drastically reduced. Henry Harpending has proposed that humans spread from a geographically restricted area about 100,000 years ago, the passage through the geographic bottleneck and then with a dramatic growth amongst geographically dispersed populations about 50,000 years ago, beginning first in Africa and thence spreading elsewhere.[93] Climatological and geological evidence suggests evidence for the bottleneck. The explosion of Toba, the largest volcanic eruption of the Quaternary, may have created a 1,000 year cold period, potentially reducing human populations to a few tropical refugia. It has been estimated that as few as 15,000 humans survived. In such circumstances genetic drift and founder effects may have been maximised. The greater diversity amongst African genomes may reflect the extent of African refugia during the Toba incident.[94] However, a recent review highlights that the single-source hypothesis of non-African populations is less consistent with ancient DNA analysis than multiple sources with genetic mixing across Eurasia.[25]
The recent expansion of anatomically modern humans reached Europe around 40,000 years ago from Central Asia and the Middle East, as a result of cultural adaption to big game hunting of sub-glacial steppe fauna.[95] Neanderthals were present both in the Middle East and in Europe, and the arriving populations of anatomically modern humans (also known as "Cro-Magnon" or European early modern humans) interbred with Neanderthal populations to a limited degree. Populations of modern humans and Neanderthal overlapped in various regions such as the Iberian peninsula and the Middle East. Interbreeding may have contributed Neanderthal genes to palaeolithic and ultimately modern Eurasians and Oceanians.
An important difference between Europe and other parts of the inhabited world was the northern latitude. Archaeological evidence suggests humans, whether Neanderthal or Cro-Magnon, reached sites in Arctic Russia by 40,000 years ago.[96]
Cro-Magnon are considered the first anatomically modern humans in Europe. They entered Eurasia by the Zagros Mountains (near present-day Iran and eastern Turkey) around 50,000 years ago, with one group rapidly settling coastal areas around the Indian Ocean and another migrating north to the steppes of Central Asia.[97] Modern human remains dating to 45,000-47,000 have been found in Germany,[98] while finds of 43,000–45,000 years ago have been discovered in Italy[99] and Britain,[100] as well as in the European Russian Arctic from 40,000 years ago.[96][101]
Humans colonised the environment west of the Urals, hunting reindeer especially,[102] but were faced with adaptive challenges; winter temperatures averaged from −20 to −30 °C (−4 to −22 °F) with fuel and shelter scarce. They travelled on foot and relied on hunting highly mobile herds for food. These challenges were overcome through technological innovations: tailored clothing from the pelts of fur-bearing animals; construction of shelters with hearths using bones as fuel; and digging "ice cellars" into the permafrost to store meat and bones.[102][103]
However, from recent research it is believed that the ecological crisis resulting from the eruption in c. 38,000 BCE of the super-volcano in the Phlegrean Fields near Naples, which left much of eastern Europe covered in ash, wiped out both the last Neanderthal and the first Homo Sapiens populations of the early Upper Paleolithic.[104][105] Modern Europeans of today bear no trace of the genomes of the first Homo Sapiens Europeans, but only of those from after the ecological crisis of 38,000 BCE.[106] Modern humans then repopulated Europe from the east after the eruption and the ice age that took place from 38,000 to 36,000 BCE.[107]
A mitochondrial DNA sequence of two Cro-Magnons from the Paglicci Cave in Italy, dated to 23,000 and 24,000 years old (Paglicci 52 and 12), identified the mtDNA as Haplogroup N, typical of the latter group.[108]
The expansion of modern human population is thought to have begun 45,000 years ago, and it may have taken 15,000–20,000 years for Europe to be colonized.[110][111]
During this time, the Neanderthals were slowly being displaced. Because it took so long for Europe to be occupied, it appears that humans and Neanderthals may have been constantly competing for territory. The Neanderthals had larger brains, and were larger overall, with a more robust or heavily built frame, which suggests that they were physically stronger than modern Homo sapiens. Having lived in Europe for 200,000 years, they would have been better adapted to the cold weather. The anatomically modern humans known as the Cro-Magnons, with widespread trade networks, superior technology and bodies likely better suited to running, would eventually completely displace the Neanderthals, whose last refuge was in the Iberian Peninsula. Neanderthals disappeared about 40,000 years ago.[112]
From the extent of linkage disequilibrium, it was estimated that the last Neanderthal gene flow into early ancestors of Europeans occurred 47,000–65,000 years BP. In conjunction with archaeological and fossil evidence, interbreeding is thought to have occurred somewhere in Western Eurasia, possibly the Middle East.[92] Studies show a higher Neanderthal admixture in East Asians than in Europeans.[113][114] North African groups share a similar excess of derived alleles with Neanderthals as non-African populations, whereas Sub-Saharan African groups are the only modern human populations with no substantial Neanderthal admixture.[note 10] The Neanderthal-linked haplotype B006 of the dystrophin gene has also been found among nomadic pastoralist groups in the Sahel and Horn of Africa, who are associated with northern populations. Consequently, the presence of this B006 haplotype on the northern and northeastern perimeter of Sub-Saharan Africa is attributed to gene flow from a non-African point of origin.[note 11]
"Tianyuan man", an individual who lived in China c. 40,000 years ago, showed substantial Neanderthal admixture. A 2017 study of the ancient DNA of Tianyuan Man found that the individual is related to modern Asian and Native American populations.[118] A 2013 study found Neanderthal introgression of 18 genes within the chromosome 3p21.31 region (HYAL region) of East Asians. The introgressive haplotypes were positively selected in only East Asian populations, rising steadily from 45,000 years ago until a sudden increase of growth rate around 5,000 to 3,500 years ago. They occur at very high frequencies among East Asian populations in contrast to other Eurasian populations (e.g. European and South Asian populations). The findings also suggest that this Neanderthal introgression occurred within the ancestral population shared by East Asians and Native Americans.[119]
A 2016 study presented an analysis of the population genetics of the Ainu people of northern Japan as key to the reconstruction of the early peopling of East Asia. The Ainu were found to represent a more basal branch than the modern farming populations of East Asia, suggesting an ancient (pre-Neolithic) connection with northeast Siberians.[120] A 2013 study associated several phenotypical traits associated with Mongoloids with a single mutation of the EDAR gene, dated to c. 35,000 years ago.[note 12][note 13]
Mitochondrial haplogroups A, B and G originated about 50,000 years ago, and bearers subsequently colonized Siberia, Korea and Japan, by about 35,000 years ago. Parts of these populations migrated to North America during the Last Glacial Maximum. Indeed, the Last Glacial Maximum promoted range contrations toward southern regions, followed by posterior range re-expansions toward the north, in North Asia populations that shaped their spatial genetic gradients.[124]
A review paper by Melinda A. Yang (in 2022) summarized and concluded that a distinctive "Basal-East Asian population" referred to as 'East- and Southeast Asian lineage' (ESEA); which is ancestral to modern East Asians, Southeast Asians, Polynesians, and Siberians, originated in Mainland Southeast Asia at ~50,000BC, and expanded through multiple migration waves southwards and northwards respectively. This ESEA lineage gave rise to various sublineages, and is also ancestral to the Hoabinhian hunter-gatherers of Southeast Asia and the ~40,000 year old Tianyuan lineage found in Northern China, but already differentiated and distinct from European-related and Australasian-related lineages, found in other regions of prehistoric Eurasia. The ESEA lineage trifurcated from an earlier East-Eurasian or "eastern non-African" (ENA) meta-population, which also contributed to the formation of Ancient Ancestral South Indians (AASI) as well as to Australasians.[125]
Around 20,000 years ago, approximately 5,000 years after the Neanderthal extinction, the Last Glacial Maximum forced northern hemisphere inhabitants to migrate to several shelters (refugia) until the end of this period. The resulting populations are presumed to have resided in such refuges during the LGM to ultimately reoccupy Europe, where archaic historical populations are considered their descendants. The composition of European populations was later altered by further migrations, notably the Neolithic expansion from the Middle East, and still later the Chalcolithic population movements associated with Indo-European expansion, as well as admixture with diverse populations from North Africa.[126] A Paleolithic site on the Yana River, Siberia, at 71°N, lies well above the Arctic Circle and dates to 27,000 radiocarbon years before present, during glacial times. This site shows that people adapted to this harsh, high-latitude, Late Pleistocene environment much earlier than previously thought.[127]
Paleo-Indians originated from Central Asia, crossing the Beringia land bridge between eastern Siberia and present-day Alaska.[128] Humans lived throughout the Americas by the end of the last glacial period, or more specifically what is known as the late glacial maximum.[128][129][130][131] Details of Paleo-Indian migration to and throughout the American continent, including the dates and the routes traveled, are subject to ongoing research and discussion.[132]
Conventional estimates have it that humans reached North America at some point between 15,000 and 20,000 years ago.[133][134][135][136] The traditional theory is that these early migrants moved when sea levels were significantly lowered due to the Quaternary glaciation,[129][132] following herds of now-extinct pleistocene megafauna along ice-free corridors that stretched between the Laurentide and Cordilleran ice sheets.[137] Another route proposed is that, either on foot or using primitive boats, they migrated down the Pacific coast to South America as far as Chile.[138] Any archaeological evidence of coastal occupation during the last Ice Age would now have been covered by the sea level rise, up to a hundred metres since then.[139] The recent finding of indigenous Australasian genetic markers in Amazonia supports that a coastal route and subsequent isolation did occur with some migrants.[140]
The Holocene is taken to begin 12,000 years ago, after the end of the Last Glacial Maximum. During the Holocene climatic optimum, beginning about 9,000 years ago, human populations which had been geographically confined to refugia began to migrate. By this time, most parts of the globe had been settled by H. sapiens; however, large areas that had been covered by glaciers were now re-populated.
This period sees the transition from the Mesolithic to the Neolithic stage throughout the temperate zone. The Neolithic subsequently gives way to the Bronze Age in Old World cultures and the gradual emergence of the historical record in the Near East and China beginning around 4,000 years ago.
Large-scale migrations of the Mesolithic to Neolithic era are thought to have given rise to the pre-modern distribution of the world's major language families such as the Niger-Congo, Nilo-Saharan, Afro-Asiatic, Uralic, Sino-Tibetan or Indo-European phyla. The speculative Nostratic theory postulates the derivation of the major language families of Eurasia (excluding Sino-Tibetan) from a single proto-language spoken at the beginning of the Holocene period.
Evidence published in 2014 from genome analysis of ancient human remains suggests that the modern native populations of Europe largely descend from three distinct lineages: "Western Hunter-Gatherers", derivative of the Cro-Magnon population of Europe, Early European Farmers introduced to Europe from the Near East during the Neolithic Revolution and Ancient North Eurasians who expanded to Europe in the context of the Indo-European expansion.[142] The Ancient North Eurasian component was introduced to Western Europe by people related to the Yamnaya culture.[143] Additional ANE ancestry is found in European populations through Paleolithic interactions with Eastern Hunter-Gatherers.[144]
West-Eurasian back-migrations started in the early Holocene or already earlier in the Paleolithic period (30-15kya), followed by pre-Neolithic and Neolithic migration events from the Middle East, mostly affecting Northern Africa, the Horn of Africa, and wider regions of the Sahel zone and East Africa.[145]
The Nilotic peoples are thought to be derived from an earlier undifferentiated Eastern Sudanic unity by the 3rd millennium BCE. The development of the Proto-Nilotes as a group may have been connected with their domestication of livestock. The Eastern Sudanic unity must have been considerably earlier still, perhaps around the 5th millennium BCE (while the proposed Nilo-Saharan unity would date to the Upper Paleolithic about 15kya). The original locus of the early Nilotic speakers was presumably east of the Nile in what is now South Sudan. The Proto-Nilotes of the 3rd millennium BCE were pastoralists, while their neighbors, the Proto-Central Sudanic peoples, were mostly agriculturalists.[146]
The Niger-Congo phylum is thought to have emerged around 6,000 years ago in West or Central Africa. Its expansion may have been associated with the expansion of Sahel agriculture in the African Neolithic period, following the desiccation of the Sahara in c. 3900 BCE.[147] The Bantu expansion has spread the Bantu languages to Central, Eastern and Southern Africa, partly replacing the indigenous populations of these regions, including the African Pygmies, Hadza people and San people. Beginning about 3,000 years ago, it reached South Africa
about 1,700 years ago.[148]
Some evidence (including a 2016 study by Busby et al.) suggests admixture from ancient and recent migrations from Eurasia into parts of Sub-Saharan Africa.[149] Another study (Ramsay et al. 2018) also shows evidence that ancient Eurasians migrated into Africa and that Eurasian admixture in modern Sub-Saharan Africans ranges from 0% to 50%, varying by region and generally higher in the Horn of Africa and parts of the Sahel zone, and found to a lesser degree in certain parts of Western Africa, and Southern Africa (excluding recent immigrants).[150]
The first seaborne human migrations were by the Austronesian peoples [dubious – discuss] originating from Taiwan known as the "Austronesian expansion".[151] Using advanced sailing technologies like catamarans, outrigger boats, and crab claw sails, they built the first sea-going ships and rapidly colonized Island Southeast Asia at around 3000 to 1500 BCE. From the Philippines and Eastern Indonesia they colonized Micronesia by 2200 to 1000 BCE.[151][152]
A branch of the Austronesians reached Island Melanesia between 1600 and 1000 BCE, establishing the Lapita culture (named after the archaeological site in Lapita, New Caledonia, where their characteristic pottery was first discovered). They are the direct ancestors of the modern Polynesians. They ventured into Remote Oceania reaching Vanuatu, New Caledonia, and Fiji by 1200 BCE, and Samoa and Tonga by around 900 to 800 BCE. This was the furthest extent of the Lapita culture expansion. During a period of around 1,500 years, they gradually lost the technology for pottery (likely due to the lack of clay deposits in the islands), replacing it with carved wooden and bamboo containers. Back-migrations from the Lapita culture also merged back Island Southeast Asia in 1500 BCE, and into Micronesia at around 200 BCE. It was not until 700 CE when they started voyaging further into the Pacific Ocean, when they colonized the Cook Islands, the Society Islands, and the Marquesas. From there, they further colonized Hawaii by 900 CE, Rapa Nui by 1000 CE, and New Zealand by 1200 CE.[152][153][154]
In the Indian Ocean, Austronesians from Borneo also colonized Madagascar and the Comoros Islands by around 500 CE. Austronesians remain the dominant ethnolinguistic group of the islands of the Indo-Pacific, and were the first to establish a maritime trade network reaching as far west as East Africa and the Arabian Peninsula. They assimilated earlier Pleistocene to early Holocene human overland migrations through Sundaland like the Papuans and the Negritos in Island Southeast Asia.[151][152] The Austronesian expansion was the last and the most far-reaching Neolithic human migration event.[155]
The Caribbean was one of the last places in the Americas that were settled by humans. The oldest remains are known from the Greater Antilles (Cuba and Hispaniola) dating between 4000 and 3500 BCE, and comparisons between tool-technologies suggest that these peoples moved across the Yucatán Channel from Central America. All evidence suggests that later migrants from 2000 BCE and onwards originated from South America, via the Orinoco region.[156] The descendants of these migrants include the ancestors of the Taíno and Kalinago (Island Carib) peoples.[157]
The earliest inhabitants of North America's central and eastern Arctic are referred to as the Arctic small tool tradition (AST) and existed c. 2500 BCE. AST consisted of several Paleo-Eskimo cultures, including the Independence cultures and Pre-Dorset culture.[158][159]
The Inuit are the descendants of the Thule culture, which emerged from western Alaska around CE 1000 and gradually displaced the Dorset culture.[160][161]

The evolution of human intelligence is closely tied to the evolution of the human brain and to the origin of language. The timeline of human evolution spans approximately seven million years,[1] from the separation of the genus Pan until the emergence of behavioral modernity by 50,000 years ago. The first three million years of this timeline concern Sahelanthropus, the following two million concern Australopithecus and the final two million span the history of the genus Homo in the Paleolithic era.
Many traits of human intelligence, such as empathy, theory of mind, mourning, ritual, and the use of symbols and tools, are somewhat apparent in other great apes, although they are in much less sophisticated forms than what is found in humans like the great ape language.
The great apes (Hominidae) show some cognitive and empathic abilities. Chimpanzees can make tools and use them to acquire foods and for social displays; they have mildly complex hunting strategies requiring cooperation, influence and rank; they are status conscious, manipulative and capable of deception; they can learn to use symbols and understand aspects of human language including some relational syntax, concepts of number and numerical sequence.[2] One common characteristic that is present in species of "high degree intelligence" (i.e. dolphins, great apes, and humans - Homo sapiens) is a brain of enlarged size. Additionally, these species have a more developed neocortex, a folding of the cerebral cortex, and von Economo neurons. Said neurons are linked to social intelligence and the ability to gauge what another is thinking or feeling and are also present in bottlenose dolphins.[3]
Around 10 million years ago, the Earth's climate entered a cooler and drier phase, which led eventually to the Quaternary glaciation beginning some 2.6 million years ago. One consequence of this was that the north African tropical forest began to retreat, being replaced first by open grasslands and eventually by desert (the modern Sahara). As their environment changed from continuous forest to patches of forest separated by expanses of grassland, some primates adapted to a partly or fully ground-dwelling life where they were exposed to predators, such as the big cats, from whom they had previously been safe.
These environmental pressures caused selection to favor bipedalism - walking on hind legs. This gave the Homininae's eyes greater elevation, the ability to see approaching danger further off, and a more efficient means of locomotion.[citation needed] It also freed their arms from the task of walking and made the hands available for tasks such as gathering food. At some point the bipedal primates developed handedness, giving them the ability to pick up sticks, bones and stones and use them as weapons, or as tools for tasks such as killing smaller animals, cracking nuts, or cutting up carcasses. In other words, these primates developed the use of primitive technology. Bipedal tool-using primates from the subtribe Hominina date back to as far as about 5 to 7 million years ago, such as one of the earliest species, Sahelanthropus tchadensis.
From about 5 million years ago, the hominin brain began to develop rapidly in both size and differentiation of function.
There has been a gradual increase in brain volume as humans progressed along the timeline of evolution (see Homininae), starting from about 600 cm3 in Homo habilis up to 1500 cm3 in Homo neanderthalensis. Thus, in general there's a positive correlation between brain volume and intelligence.[4] However, modern Homo sapiens have a brain volume slightly smaller (1250 cm3) than neanderthals, and the Flores hominids (Homo floresiensis), nicknamed hobbits, had a cranial capacity of about 380 cm3 (considered small for a chimpanzee) about a third of that of Homo erectus. It is proposed that they evolved from H. erectus as a case of insular dwarfism. With their three-times-smaller brain, the Flores hominids apparently used fire and made tools as sophisticated as those of their ancestor H. erectus.
Roughly 2.4 million years ago Homo habilis had appeared in East Africa: the first known human species, and the first known to make stone tools, yet the disputed findings of signs of tool use from even earlier ages and from the same vicinity as multiple Australopithecus fossils may put to question how much more intelligent than its predecessors H. habilis was.
The use of tools conferred a crucial evolutionary advantage, and required a larger and more sophisticated brain to co-ordinate the fine hand movements required for this task.[5][6] Our knowledge of the complexity of behaviour of Homo habilis is not limited to stone culture; they also had habitual therapeutic use of toothpicks.[7]
A larger brain requires a larger skull, and thus is accompanied by other morphological and biological evolutionary changes. One such change required for the female to have a wider birth canal for the newborn's larger skull to pass through. The solution to this was to give birth at an early stage of fetal development, before the skull grew too large to pass through the birth canal. Other accompanying adaptations were the smaller maxillary and mandibular bones, smaller and weaker facial muscles, and shortening and flattening of the face resulting in modern-human's complex cognitive and linguistic capabilities as well as the ability to create facial expressions and smile.[6] Consequentially, dental issues in modern humans arise from these morphological changes that are exacerbated by a shift from nomadic to sedentary lifestyles.[6]
Humans' increasingly sedentary lifestyle to protect their more vulnerable offspring led them to grow even more dependent on tool-making to compete with other animals and other humans, and rely less on body size and strength.[6]
About 200,000 years ago Europe and the Middle East were colonized by Neanderthals, extinct by 39,000 years ago following the appearance of modern humans in the region from 40,000 to 45,000 years ago.
History of humans
In the Late Pliocene, hominins were set apart from modern great apes and other closely related organisms by the anatomical evolutionary changes resulting in bipedalism, or the ability to walk upright.[8][9] Characteristics such as a supraorbital torus, or prominent eyebrow ridge, and flat face also makes Homo erectus distinguishable. Their brain size substantially sets them apart from closely related species, such as H. habilis, as seen by an increase in average cranial capacity of 1000 cc. Compared to earlier species, H. erectus developed keels and small crests in the skull showing morphological changes of the skull to support increased brain capacity. It is believed that Homo erectus were, anatomically, modern humans as they are very similar in size, weight, bone structure, and nutritional habits. Over time, however, human intelligence developed in phases that is interrelated with brain physiology, cranial anatomy and morphology, and rapidly changing climate and environments.[9]
The study of the evolution of cognition relies on the archaeological record made up of assemblages of material culture, particularly from the Paleolithic Period, to make inferences about our ancestors' cognition. Paleo-anthropologists from the past half-century have had the tendency of reducing stone tool artifacts to physical products of the metaphysical activity taking place in the brains of hominins. Recently, a new approach called 4E cognition (see Models for other approaches) has been developed by cognitive archaeologists Lambros Malafouris, Thomas G. Wynn, and Karenleigh A. Overmann, to move past the "internal" and "external" dichotomy by treating stone tools as objects with agency in both providing insight to hominin cognition and having a role in the development of early hominin cognition.[10] The 4E cognition approach describes cognition as embodied, embedded, enactive, and extended, to understand the interconnected nature between the mind, body, and environment.[10]
There are four major categories of tools created and used throughout human evolution that are associated with the corresponding evolution of the brain and intelligence. Stone tools such as flakes and cores used by Homo habilis for cracking bones to extract marrow, known as the Oldowan culture, make up the oldest major category of tools from about 2.5 and 1.6 million years ago. The development of stone tool technology suggests that our ancestors had the ability to hit cores with precision, taking into account the force and angle of the strike, and the cognitive planning and capacity to envision a desired outcome.[11]
Acheulean culture, associated with Homo erectus, is composed of bifacial, or double-sided, hand-axes, that "requires more planning and skill on the part of the toolmaker; he or she would need to be aware of principles of symmetry".[11] In addition, some sites show evidence that selection of raw materials involved travel, advanced planning, cooperation, and thus communication with other hominins.[11]
The third major category of tool industry marked by its innovation in tool-making technique and use is the Mousterian culture. Compared to previous tool cultures, in which tools were regularly discarded after use, Mousterian tools, associated with Neanderthals, were specialized, built to last, and "formed a true toolkit".[11] The making of these tools, called the Levallois technique, involves a multi-step process which yields several tools. In combination with other data, the formation of this tool culture for hunting large mammals in groups evidences the development of speech for communication and complex planning capabilities.[11]
While previous tool cultures did not show great variation, the tools of early modern Homo sapiens are robust in the amount of artifacts and diversity in utility. There are several styles associated with this category of the Upper Paleolithic, such as blades, boomerangs, atlatls (spear throwers), and archery made from varying materials of stone, bone, teeth, and shell. Beyond use, some tools have been shown to have served as signifiers of status and group membership. The role of tools for social uses signal cognitive advancements such as complex language and abstract relations to things.[11]
The eldest findings of Homo sapiens in Jebel Irhoud, Morocco date back c. 300,000 years[12][13]
Fossils of Homo sapiens were found in East Africa which are c. 200,000 years old. It is unclear to what extent these early modern humans had developed language, music, religion, etc. The cognitive tradeoff hypothesis proposes that there was an evolutionary tradeoff between short-term working memory and complex language skills over the course of human evolution.[14]
According to proponents of the Toba catastrophe theory, the climate in non-tropical regions of the earth experienced a sudden freezing about 70,000 years ago, because of a huge explosion of the Toba volcano that filled the atmosphere with volcanic ash for several years. This reduced the human population to less than 10,000 breeding pairs in equatorial Africa, from which all modern humans are descended. Being unprepared for the sudden change in climate, the survivors were those intelligent enough to invent new tools and ways of keeping warm and finding new sources of food (for example, adapting to ocean fishing based on prior fishing skills used in lakes and streams that became frozen).[citation needed]
Around 80,000–100,000 years ago, three main lines of Homo sapiens diverged, bearers of mitochondrial haplogroup L1 (mtDNA) / A (Y-DNA) colonizing Southern Africa (the ancestors of the Khoisan/Capoid peoples), bearers of haplogroup L2 (mtDNA) / B (Y-DNA) settling Central and West Africa (the ancestors of Niger–Congo and Nilo-Saharan speaking peoples), while the bearers of haplogroup L3 remained in East Africa.[citation needed]
The "Great Leap Forward" leading to full behavioral modernity sets in only after this separation. Rapidly increasing sophistication in tool-making and behaviour is apparent from about 80,000 years ago, and the migration out of Africa follows towards the very end of the Middle Paleolithic, some 60,000 years ago. Fully modern behaviour, including figurative art, music, self-ornamentation, trade, burial rites etc. is evident by 30,000 years ago. The oldest unequivocal examples of prehistoric art date to this period, the Aurignacian and the Gravettian periods of prehistoric Europe, such as the Venus figurines and cave painting (Chauvet Cave) and the earliest musical instruments (the bone pipe of Geissenklösterle, Germany, dated to about 36,000 years ago).[15]
The human brain has evolved gradually over the passage of time; a series of incremental changes occurring as a result of external stimuli and conditions. It is crucial to keep in mind that evolution operates within a limited framework at a given point in time. In other words, the adaptations that a species can develop are not infinite and are defined by what has already taken place in the evolutionary timeline of a species. Given the immense anatomical and structural complexity of the brain, its evolution (and the congruent evolution of human intelligence), can only be reorganized in a finite number of ways. The majority of said changes occur either in terms of size or in terms of developmental timeframes.[16]
The cerebral cortex is divided into four lobes (frontal, parietal, occipital, and temporal) each with specific functions. The cerebral cortex is significantly larger in humans than in any other animal and is responsible for higher thought processes such as reasoning, abstract thinking, and decision making.[17] Another characteristic that makes humans special and sets them apart from any other species is our ability to produce and understand complex, syntactic language. The cerebral cortex, particularly in the temporal, parietal, and frontal lobes, are populated with neural circuits dedicated to language. There are two main areas of the brain commonly associated with language, namely: Wernicke's area and Broca's area. The former is responsible for the understanding of speech and the latter for the production of speech. Homologous regions have been found in other species (i.e. Area 44 and 45 have been studied in chimpanzees) but they are not as strongly related to or involved in linguistic activities as in humans.[18]
The social brain hypothesis was proposed by British anthropologist Robin Dunbar, who argues that human intelligence did not evolve primarily as a means to solve ecological problems, but rather as a means of surviving and reproducing in large and complex social groups.[19][20] Some of the behaviors associated with living in large groups include reciprocal altruism, deception, and coalition formation. These group dynamics relate to Theory of Mind or the ability to understand the thoughts and emotions of others, though Dunbar himself admits in the same book that it is not the flocking itself that causes intelligence to evolve (as shown by ruminants).[19]
Dunbar argues that when the size of a social group increases, the number of different relationships in the group may increase by orders of magnitude. Chimpanzees live in groups of about 50 individuals whereas humans typically have a social circle of about 150 people, which is also the typical size of social communities in small societies and personal social networks;[21] this number is now referred to as Dunbar's number. In addition, there is evidence to suggest that the success of groups is dependent on their size at foundation, with groupings of around 150 being particularly successful, potentially reflecting the fact that communities of this size strike a balance between the minimum size of effective functionality and the maximum size for creating a sense of commitment to the community.[21] According to the social brain hypothesis, when hominids started living in large groups, selection favored greater intelligence. As evidence, Dunbar cites a relationship between neocortex size and group size of various mammals.[19]
Phylogenetic studies of brain sizes in primates show that while diet predicts primate brain size, sociality does not predict brain size when corrections are made for cases in which diet affects both brain size and sociality. The exceptions to the predictions of the social intelligence hypothesis, which that hypothesis has no predictive model for, are successfully predicted by diets that are either nutritious but scarce or abundant but poor in nutrients.[22] Researchers have found that frugivores tend to exhibit larger brain size than folivores.[22] One potential explanation for this finding is that frugivory requires "extractive foraging", or the process of locating and preparing hard-shelled foods, such as nuts, insects, and fruit.[23] Extractive foraging requires higher cognitive processing, which could help explain larger brain size.[23] However, other researchers argue that extractive foraging was not a catalyst in the evolution of primate brain size, demonstrating that some non primates exhibit advanced foraging techniques.[23] Other explanations for the positive correlation between brain size and frugivory highlight how the high-energy, frugivore diet facilitates fetal brain growth and requires spatial mapping to locate the embedded foods.[22]
Meerkats have far more social relationships than their small brain capacity would suggest. Another hypothesis is that it is actually intelligence that causes social relationships to become more complex, because intelligent individuals are more difficult to learn to know.[24]
There are also studies that show that Dunbar's number is not the upper limit of the number of social relationships in humans either.[25][26]
The hypothesis that it is brain capacity that sets the upper limit for the number of social relationships is also contradicted by computer simulations that show simple unintelligent reactions to be sufficient to emulate "ape politics"[27] and by the fact that some social insects such as the paper wasp do have hierarchies in which each individual has its place (as opposed to herding without social structure) and maintains their hierarchies in groups of approximately 80 individuals with their brains smaller than that of any mammal.[28]
Insects provide an opportunity to explore this since they exhibit an unparalleled diversity of social forms to permanent colonies containing many individuals working together as a collective organism and have evolved an impressive range of cognitive skills despite their small nervous systems.[29][30][31] Social insects are shaped by ecology, including their social environment. Studies aimed to correlating brain volume to complexity have failed to identify clear correlations between sociality and cognition because of cases like social insects. In humans, societies are usually held together by the ability of individuals to recognize features indicating group membership. Social insects, likewise, often recognize members of their colony allowing them to defend against competitors. Ants do this by comparing odors which require fine discrimination of multicomponent variable cues.[32] Studies suggest this recognition is achieved through simple cognitive operations that do not involve long-term memory but through sensory adaptation or habituation.[33] In honeybees, their symbolic 'dance' is a form of communication that they use to convey information with the rest of their colony. In an even more impressive social use of their dance language, bees indicate suitable nest locations to a swarm in search of a new home. The swarm builds a consensus from multiple 'opinions' expressed by scouts with different information, to finally agree on a single destination to which the swarm relocates.[34]
Similar to, but distinct from the social brain hypothesis, is the cultural intelligence or cultural brain hypothesis, which dictates that human brain size, cognitive ability, and intelligence have increased over generations due to cultural information from a mechanism known as social learning.[35] The hypothesis also predicts a positive correlation between species with a higher dependency and more frequent opportunities for social learning and overall cognitive ability.[36] This is because social learning allows species to develop cultural skills and strategies for survival; in this way it can be said that heavily cultural species should in theory be more intelligent.[37]
Humans have been widely acknowledged as the most intelligent species on the planet, with big brains with ample cognitive abilities and processing power which outcompete all other species.[38] In fact, humans have shown an enormous increase in brain size and intelligence over millions of years of evolution.[39] This is because humans have been referred to as an 'evolved cultural species'; one that has an unrivalled reliance on culturally transmitted knowledge due to the social environment around us.[40] This is down to social transmission of information which spreads significantly faster in human populations relative to changes in genetics.[41] Put simply, humans are the most cultural species there is, and are therefore the most intelligent species there is. The key point when concerning evolution of intelligence is that this cultural information has been consistently transmitted across generations to build vast amounts of cultural skills and knowledge throughout the human race.[42] Dunbar's social brain hypothesis on the other hand dictates that our brains evolved primarily due to complex social interactions in groups,[43] so in this way the two hypotheses are distinct from each other in that the cultural intelligence hypothesis focuses more on an in increase in intelligence from socially transmitted information. A shift in focus from 'social' interactions to learning strategies can be seen through this.[36] The hypothesis can also be seen to contradict the idea of human 'general intelligence' by emphasising the process of cultural skills and information being learned from others.[44]
In 2018, Muthukrishna and researchers constructed a model based on the cultural intelligence hypothesis which revealed relationships between brain size, group size, social learning and mating structures.[36] The model had three underlying assumptions:
Using evolutionary simulation, the researchers were able to confirm the existence of hypothesised relationships. Results concerning the cultural intelligence hypothesis model showed that larger brains can store more information and adaptive knowledge, thus supporting larger groups. This abundance of adaptive knowledge can then be used for frequent social learning opportunities.
As previously mentioned, social learning is the foundation of the cultural intelligence hypothesis and can be described simplistically as learning from others. It involves behaviours such as imitation, observational learning, influences from family and friends and explicit teaching from others.[45] What sets humans apart from other species is that, due to our emphasis on culturally acquired information, humans have evolved to already possess significant social learning abilities from infancy. Neurological studies on nine month old infants were conducted by researchers in 2012 to demonstrate this phenomenon.[46] The study involved infants observing a caregiver making a sound with a rattle over a period of one week. The brains of the infants were monitored throughout the study. Researchers found that the infants were able to activate neural pathways associated with making a sound with the rattle without actually doing the action themselves, showing human social learning in action- infants were able to understand the effects of a particular action simply by observing the performance of the action by someone else. Not only does this study demonstrate the neural mechanisms of social learning, but it also demonstrates our inherent ability to acquire cultural skills from those around us from the very start of our lives- it therefore shows strong support for the cultural intelligent hypothesis.
Various studies have been conducted to show the cultural intelligence hypothesis in action on a wider scale. One particular study in 2016 investigated two orangutan species, including the more social Sumatran species and the less sociable Bornean species. The aim was to test the notion that species with a higher frequency of opportunities for social learning should evolve to be more intelligent.[47] Results showed that the Sumatrans consistently performed better in cognitive tests compared to the less sociable Borneans. The Sumatrans also showed greater inhibition and more cautious behaviour within their habitat. This was one of the first studies to show evidence for the cultural intelligence hypothesis in a non human species- frequency of learning opportunities had gradually produced differences in cognitive abilities between the two species.
A study in 2018 proposed an altered variant of the original version of the hypothesis called the 'transformative cultural intelligence hypothesis'.[48] The research involved investigating four year old's problem solving skills in different social contexts. The children were asked to extract a floating object from a tube using water. Nearly all were unsuccessful without cues, however most children succeeded after being shown a pedagogical solution suggesting video. When the same video was shown in a non pedagogical manner however, the children's success in the task did not improve. Crucially, this meant that the children's physical cognition and problem solving ability was therefore affected by how the task was socially presented to them. Researchers thus formulated the transformative cultural intelligence hypothesis, which stresses that our physical cognition is developed and affected by the social environment around us. This challenges the traditional cultural intelligence hypothesis which states that it is human's social cognition and not physical cognition which is superior to our nearest primate relatives;[44] showing unique physical cognition in humans affected by external social factors. This phenomenon has not been seen in other species.
Another theory that tries to explain the growth of human intelligence is the reduced aggression theory (aka self-domestication theory). According to this strand of thought, what led to the evolution of advanced intelligence in Homo sapiens was a drastic reduction of the aggressive drive. This change separated us from other species of monkeys and primates, where this aggressivity is still in plain sight, and eventually lead to the development of quintessential human traits such as empathy, social cognition, and culture.[49][50] This theory has received strong support from studies of animal domestication where selective breeding for tameness has, in only a few generations, led to the emergence of impressive "humanlike" abilities. Tamed foxes, for example, exhibit advanced forms of social communication (following pointing gestures), pedomorphic physical features (childlike faces, floppy ears) and even rudimentary forms of theory of mind (eye contact seeking, gaze following).[51][52] Evidence also comes from the field of ethology (which is the study of animal behavior, focused on observing species in their natural habitat rather than in controlled laboratory settings) where it has been found that animals with a gentle and relaxed manner of interacting with each other – for example stumptailed macaques, orangutans and bonobos – have more advanced socio-cognitive abilities than those found among the more aggressive chimpanzees and baboons.[53] It is hypothesized that these abilities derive from a selection against aggression.[50][54][55][56]
On a mechanistic level, these changes are believed to be the result of a systemic downregulation of the sympathetic nervous system (the fight-or-flight reflex). Hence, tamed foxes show a reduced adrenal gland size and have an up to fivefold reduction in both basal and stress-induced blood cortisol levels.[57][58] Similarly, domesticated rats and guinea pigs have both reduced adrenal gland size and reduced blood corticosterone levels.[59][60] It seems as though the neoteny of domesticated animals significantly prolongs the immaturity of their hypothalamic-pituitary-adrenal system (which is otherwise only immature for a short period when they are pups/kittens) and this opens up a larger "socialization window" during which they can learn to interact with their caretakers in a more relaxed way.
This downregulation of sympathetic nervous system reactivity is also believed to be accompanied by a compensatory increase in a number of opposing organs and systems. Although these are not as well specified, various candidates for such "organs" have been proposed: the parasympathetic system as a whole, the septal area over the amygdala,[49] the oxytocin system,[61] the endogenous opioids[62] and various forms of quiescent immobilization which antagonize the fight-or-flight reflex.[63][64]
One study hypothesizes that reasoning about social exchange between individuals is an adaptation to the human brain. This adaption is predicted to evolve when two parties are both better off than they were before by mutually exchanging things they value less for things they value more. However, selection will only favor social exchange when both parties benefit.[65]
In 2004, psychologist Satoshi Kanazawa argued that g was a domain-specific, species-typical, information processing psychological adaptation,[66] and in 2010, Kanazawa argued that g correlated only with performance on evolutionarily unfamiliar rather than evolutionarily familiar problems, proposing what he termed the "Savanna-IQ interaction hypothesis".[67][68] In 2006, Psychological Review published a comment reviewing Kanazawa's 2004 article by psychologists Denny Borsboom and Conor Dolan that argued that Kanazawa's conception of g was empirically unsupported and purely hypothetical and that an evolutionary account of g must address it as a source of individual differences.[69] In response to Kanazawa's 2010 article, psychologists Scott Barry Kaufman, Colin G. DeYoung, Deirdre Reis, and Jeremy R. Gray gave 112 subjects a 70-item computerized version of the Wason selection task (a logic puzzle) in a social relations context as proposed by Leda Cosmides and John Tooby in The Adapted Mind,[70] and found instead that "performance on non-arbitrary, evolutionarily familiar problems is more strongly related to general intelligence than performance on arbitrary, evolutionarily novel problems".[71][72]
Peter Cathcart Wason originally demonstrated that not even 10% of subjects found the correct solution and his finding was replicated.[73][74] Psychologists Patricia Cheng, Keith Holyoak, Richard E. Nisbett, and Lindsay M. Oliver demonstrated experimentally that subjects who have completed semester-long college courses in propositional calculus do not perform better on the Wason selection task than subjects who do not complete such college courses.[75] Tooby and Cosmides originally proposed a social relations context for the Wason selection task as part of a larger computational theory of social exchange after they began reviewing the previous experiments about the task beginning in 1983.[70] Despite other experimenters finding that some contexts elicited more correct subject responses than others, no theoretical explanation for differentiating between them was identified until Tooby and Cosmides proposed that disparities in subjects performance on contextualized versus non-contextualized variations of the task was an artifact of the task measuring a specialized cheater-detection module.[76][77]
Tooby and Cosmides later noted that whether there are evolved cognitive mechanisms for the content-blind rules of logical inference is disputed,[78][79] and consistently maintained that cognitive adaptations for social exchange were not a by-product of general-purpose reasoning mechanisms, domain-general learning mechanisms, or g.[80][81][82] Relatedly, economist Thomas Sowell has noted that numerous studies finding disparities between the mean test scores of ethnic groups on intelligence tests have found that ethnic groups with lower mean test scores have tended to perform worst on non-verbal, non-informational, or abstract reasoning test items.[83][84] Writing after the completion of the Human Genome Project in 2003, psychologist Earl B. Hunt noted in 2011 that no genes related to differences in cognitive skills across various racial and ethnic groups had ever been discovered,[85] and in 2012, American Psychologist published a review of new findings by psychologists Richard E. Nisbett, Joshua Aronson, Clancy Blair, Diane F. Halpern, and Eric Turkheimer, economist William Dickens, and philosopher James R. Flynn that concluded that almost no single-nucleotide genetic polymorphisms that have been discovered are consistently associated with variation in IQ in the normal range.[86]
Flynn had argued earlier that the Flynn effect presented multiple paradoxes for g as a psychological trait with a biological basis because the increases in the statistical average scores among later birth year cohorts born in the 20th century were occurring without sufficient increases in vocabulary size, general knowledge, and ability to solve arithmetical problems, and that the increases were so large that they would imply the statistically average members of the birth year cohorts in the late 19th century and early 20th century (i.e. the Lost Generation and the Greatest Generation) would have been intellectually disabled as well as more distant human ancestors.[87] Flynn proposed that these paradoxes could be answered by the increasing use of abstraction, logic, and scientific reasoning to address problems,[88] while Nisbett argued that the Flynn effect was largely attributable to increases in formal education among human populations during the 20th century.[89]
In 2010, psychologist David Marks found through 8 statistical analyses that average population IQ scores across race, time, and nationality correlated with literacy rates between a range of 0.79 and 0.99, which led to the conclusion that both the Flynn effect and racial differences in mean scores on intelligence tests were statistical artifacts of uncontrolled variation in literacy rates due to test performance requiring literacy skills.[90][91] However, in reference to the failures of constructivism in mathematics education and whole language in literacy education, psychologist David C. Geary and cognitive scientist Steven Pinker have noted that literacy, numeracy, and formal mathematical and logical reasoning are not psychological adaptations but biologically secondary cognitive skills that require formal, explicit, and direct instruction and extensive practice.[96] While suggesting that the evolution of human intelligence could be explained by intelligence itself being the product of metaphor (stemming from the ability to create arbitrary morphemes) and combinatorial grammar (allowing nesting of verb phrases in syntax) that together enable the infinite composition of sentences,[97][98][99] Pinker has also argued that the Flynn effect is likely caused by increased amounts of formal education in addition to other factors.[100]
This model, which invokes sexual selection, is proposed by Geoffrey Miller who argues that human intelligence is unnecessarily sophisticated for the needs of hunter-gatherers to survive. He argues that the manifestations of intelligence such as language, music and art did not evolve because of their utilitarian value to the survival of ancient hominids. Rather, intelligence may have been a fitness indicator. Hominids would have been chosen for greater intelligence as an indicator of healthy genes and a Fisherian runaway positive feedback loop of sexual selection would have led to the evolution of human intelligence in a relatively short period.[101] Philosopher Denis Dutton also argued that the human capacity for aesthetics evolved by sexual selection.[102]
Evolutionary biologist George C. Williams and evolutionary medicine researcher Randolph M. Nesse cite evolutionary psychologists John Tooby and Leda Cosmides as referring to the emotions as "Darwinian algorithms of the mind",[103] while social psychologist David Buss has argued that the sex-specialized differences in the emotion of jealousy are adaptive strategies for detecting infidelity by a mating partner and anthropologists Donald E. Brown and Ward Goodenough have argued that marriage is a cultural universal that evolved to regulate sexual access to fertile women within a particular culture in response to male intrasexual competition and dominance.[108] Citing cross-cultural research conducted by Buss,[109][110] Miller has argued that if humans prefer altruistic mating partners that would select by mate choice for altruism directly.[111] Additionally, Nesse and theoretical biologist Mary Jane West-Eberhard view sexual selection as a subcategory of social selection,[118] with Nesse and anthropologist Christopher Boehm arguing further that altruism in humans held fitness advantages that enabled evolutionarily extraordinary cooperativeness and the human capability of creating culture, as well as capital punishment by band societies against bullies, thieves, free-riders, and psychopaths.[125]
In many species, only males have impressive secondary sexual characteristics such as ornaments and show-off behavior, but sexual selection is also thought to be able to act on females as well in at least partially monogamous species.[126] With complete monogamy, there is assortative mating for sexually selected traits. This means that less attractive individuals will find other less attractive individuals to mate with. If attractive traits are good fitness indicators, this means that sexual selection increases the genetic load of the offspring of unattractive individuals. Without sexual selection, an unattractive individual might find a superior mate with few deleterious mutations, and have healthy children that are likely to survive. With sexual selection, an unattractive individual is more likely to have access only to an inferior mate who is likely to pass on many deleterious mutations to their joint offspring, who are then less likely to survive.[101]
Sexual selection is often thought to be a likely explanation for other female-specific human traits, for example breasts and buttocks far larger in proportion to total body size than those found in related species of ape.[101] It is often assumed that if breasts and buttocks of such large size were necessary for functions such as suckling infants, they would be found in other species. That human female breasts (typical mammalian breast tissue is small)[127] are found sexually attractive by many men is in agreement with sexual selection acting on human females secondary sexual characteristics.
Sexual selection for intelligence and judging ability can act on indicators of success, such as highly visible displays of wealth. Growing human brains require more nutrition than brains of related species of ape. It is possible that for females to successfully judge male intelligence, they must be intelligent themselves. This could explain why despite the absence of clear differences in intelligence between males and females on average, there are clear differences between male and female propensities to display their intelligence in ostentatious forms.[101]
The sexual selection by the disability principle/fitness display model of the evolution of human intelligence is criticized by certain researchers for issues of timing of the costs relative to reproductive age. While sexually selected ornaments such as peacock feathers and moose antlers develop either during or after puberty, timing their costs to a sexually mature age, human brains expend large amounts of nutrients building myelin and other brain mechanisms for efficient communication between the neurons early in life. These costs early in life build facilitators that reduce the cost of neuron firing later in life, and as a result the peaks of the brain's costs and the peak of the brain's performance are timed on opposite sides of puberty with the costs peaking at a sexually immature age while performance peaks at a sexually mature age. Critical researchers argue the above shows that the cost of intelligence is a signal which reduces the chance of surviving to reproductive age, and does not signal fitness of sexually mature individuals. Since the disability principle is about selection from disabilities in sexually immature individuals, which increases the offspring's chance of survival to reproductive age, disabilities would be selected against and not for by the above mechanism. These critics argue that human intelligence evolved by natural selection citing that unlike sexual selection, natural selection have produced many traits that cost the most nutrients before puberty including immune systems and accumulation and modification for increased toxicity of poisons in the body as a protective measure against predators.[128][129]
The number of people with severe cognitive impairment caused by childhood viral infections like meningitis, protists like Toxoplasma and Plasmodium, and animal parasites like intestinal worms and schistosomes is estimated to be in the hundreds of millions.[130] Even more people with moderate mental damages, such as an inability to complete difficult tasks, that are not classified as 'diseases' by medical standards, may still be considered as inferior mates by potential sexual partners.
Thus, widespread, virulent, and archaic infections are greatly involved in natural selection for cognitive abilities. People infected with parasites may have brain damage and obvious maladaptive behavior in addition to visible signs of disease. Smarter people can more skillfully learn to distinguish safe non-polluted water and food from unsafe kinds and learn to distinguish mosquito infested areas from safe areas. Additionally, they can more skillfully find and develop safe food sources and living environments. Given this situation, preference for smarter child-bearing/rearing partners increases the chance that their descendants will inherit the best resistance alleles, not only for immune system resistance to disease, but also smarter brains for learning skills in avoiding disease and selecting nutritious food. When people search for mates based on their success, wealth, reputation, disease-free body appearance, or psychological traits such as benevolence or confidence; the effect is to select for superior intelligence that results in superior disease resistance.[citation needed]
Another model describing the evolution of human intelligence is ecological dominance-social competition (EDSC),[131] explained by Mark V. Flinn, David C. Geary and Carol V. Ward based mainly on work by Richard D. Alexander. According to the model, human intelligence was able to evolve to significant levels because of the combination of increasing domination over habitat and increasing importance of social interactions. As a result, the primary selective pressure for increasing human intelligence shifted from learning to master the natural world to competition for dominance among members or groups of its own species.
As advancement, survival and reproduction within an increasing complex social structure favored ever more advanced social skills, communication of concepts through increasingly complex language patterns ensued. Since competition had shifted bit by bit from controlling "nature" to influencing other humans, it became of relevance to outmaneuver other members of the group seeking leadership or acceptance, by means of more advanced social skills. A more social and communicative person would be more easily selected.
Human intelligence is developed to an extreme level that is not necessarily adaptive in an evolutionary sense. Firstly, larger-headed babies are more difficult to give birth to and large brains are costly in terms of nutrient and oxygen requirements.[132] Thus the direct adaptive benefit of human intelligence is questionable at least in modern societies, while it is difficult to study in prehistoric societies. Since 2005, scientists have been evaluating genomic data on gene variants thought to influence head size, and have found no evidence that those genes are under strong selective pressure in current human populations.[133] The trait of head size has become generally fixed in modern human beings.[134]
While decreased brain size has strong correlation with lower intelligence in humans, some modern humans have brain sizes as small as with Homo erectus but normal intelligence (based on IQ tests) for modern humans. Increased brain size in humans may allow for greater capacity for specialized expertise.[135]
The two major perspectives on primate brain evolution are the concerted and mosaic approaches.[136] In the concerted evolution approach, cortical expansions in the brain are considered to be a by-product of a larger brain, rather than adaptive potential.[136] Studies have supported the concerted evolution model by finding cortical expansions between macaques and marmosets are comparable to that of humans and macaques.[136] Researchers attribute this result to the constraints on the evolutionary process of increasing brain size.[136] In the mosaic approach, cortical expansions are attributed to their adaptive advantage for the species.[137] Researchers have attributed hominin evolution to mosaic evolution.[137]
Simian primate brain evolution studies show that specific cortical regions associated with high-level cognition have demonstrated the greatest expansion over primate brain evolution.[136] Sensory and motor regions have showcased limited growth.[136] Three regions associated with complex cognition include the frontal lobe, temporal lobe, and the medial wall of the cortex.[136] Studies demonstrate that the enlargement in these regions is disproportionately centered in the temporoparietal junction (TPJ), lateral prefrontal cortex (LPFC), and anterior cingulate cortex (ACC).[136] The TPJ is located in the parietal lobe and is associated with morality, theory of mind, and spatial awareness.[136] Additionally, the Wernicke's area is located in the TPJ.[136] Studies have suggested that the region assists in language production, as well as language processing.[138] The LPFC is commonly associated with planning and working memory functions.[136] The Broca's area, the second major region associated with language processing, is also located in the LPFC.[136] The ACC is associated with detecting errors, monitoring conflict, motor control, and emotion.[136] Specifically, researchers have found that the ACC in humans is disproportionately expanded when compared to the ACC in macaques.[136]
Fossils show that although Homo sapiens' total brain volume approached modern levels as early as 300,000 years ago, parietal lobes and cerebella grew relative to total volume after this point, reaching current levels of variation at some point between the approximate dates of 100,000 and 35,000 years ago.[139]
Studies on cortical expansions in the brain have been used to examine the evolutionary basis of neurological disorders, such as Alzheimer's disease.[136] For example, researchers associate the expanded TPJ region with Alzheimer's disease. However, other researchers found no correlation between expanded cortical regions in the human brain and the development of Alzheimer's disease.[140]
Human brain evolution involves cellular, genetic, and circuitry changes.[141] On a genetic level, humans have a modified FOXP2 gene, which is associated with speech and language development.[142] The human variant of the gene SRGAP2, SRGAP2C, enables greater dendritic spine density which fosters greater neural connections.[143] On a cellular level, studies demonstrate von Economo neurons (VENs) are more prevalent in humans than other primates.[144] Studies show that VENs are associated with empathy, social awareness and self-control.[144] Studies show that the striatum plays a role in understanding reward and pair-bond formation.[145] On a circuitry level, humans exhibit a more complex mirror neuron system, greater connection between the two major language processing areas (Wernicke's area and Broca's area), and a vocal control circuit that connects the motor cortex and brain stem.[141] The mirror neuron system is associated with social cognition, theory of mind, and empathy.[146] Studies have demonstrated the presence of the mirror neuron system in both macaques in humans; However, the mirror neuron system is only activated in macaques when observing transitive movements.[146]
Group selection theory contends that organism characteristics that provide benefits to a group (clan, tribe, or larger population) can evolve despite individual disadvantages such as those cited above. The group benefits of intelligence (including language, the ability to communicate between individuals, the ability to teach others, and other cooperative aspects) have apparent utility in increasing the survival potential of a group.
In addition, the theory of group selection is inherently tied to Darwin's theory of natural selection. Specifically, that "group-related adaptations must be attributed to the natural selection of alternative groups of individuals and that the natural selection of alternative alleles within populations will be opposed to this development".[147]
Between-group selection can be used to explain the changes and adaptations that arise within a group of individuals. Group-related adaptations and changes are a byproduct of between-group selection as traits or characteristics that prove to be advantageous in relation to another group will become increasingly popular and disseminated within a group. In the end, increasing its overall chance of surviving a competing group.
However, this explanation cannot be applied to humans (and other species, predominantly other mammals) that live in stable, established social groupings. This is because of the social intelligence that functioning within these groups requires from the individual. Humans, while they are not the only ones, possess the cognitive and mental capacity to form systems of personal relationships and ties that extend well beyond those of the nucleus of family. The continuous process of creating, interacting, and adjusting to other individuals is a key component of many species' ecology.
These concepts can be tied to the social brain hypothesis, mentioned above. This hypothesis posits that human cognitive complexity arose as a result of the higher level of social complexity required from living in enlarged groups. These bigger groups entail a greater amount of social relations and interactions thus leading to an expanded quantity of intelligence in humans.[22] However, this hypothesis has been under academic scrutiny in recent years and has been largely disproven. In fact, the size of a species' brain can be much better predicted by diet instead of measures of sociality as noted by the study conducted by DeCasien et al. They found that ecological factors (such as: folivory/frugivory, environment) explain a primate brain size much better than social factors (such as: group size, mating system).[22]
Early hominins dating back to pre 3.5 Ma in Africa ate primarily plant foods supplemented by insects and scavenged meat.[10] Their diets are evidenced by their 'robust' dento-facial features of small canines, large molars, and enlarged masticatory muscles that allowed them to chew through tough plant fibers. Intelligence played a role in the acquisition of food, through the use of tool technology such as stone anvils and hammers.[10]
There is no direct evidence of the role of nutrition in the evolution of intelligence dating back to Homo erectus, contrary to dominant narratives in paleontology that link meat-eating to the appearance of modern human features such as a larger brain. However, scientists suggest that nutrition did play an important role, such as the consumption of a diverse diet including plant foods and new technologies for cooking and processing food such as fire.[148]
Diets deficient in iron, zinc, protein, iodine, B vitamins, omega 3 fatty acids, magnesium and other nutrients can result in lower intelligence[149] either in the mother during pregnancy or in the child during development. While these inputs did not have an effect on the evolution of intelligence they do govern its expression. A higher intelligence could be a signal that an individual comes from and lives in a physical and social environment where nutrition levels are high, whereas a lower intelligence could imply a child, its mother, or both, come from a physical and social environment where nutritional levels are low. Previc emphasizes the contribution of nutritional factors to elevations of dopaminergic activity in the brain, which may have been responsible for the evolution of human intelligence since dopamine is crucial to working memory, cognitive shifting, abstract, distant concepts, and other hallmarks of advanced intelligence.[150]

Humorism, the humoral theory, or humoralism, was a system of medicine detailing a supposed makeup and workings of the human body, adopted by Ancient Greek and Roman physicians and philosophers.
Humorism began to fall out of favor in the 17th century and it was definitively disproved with the discovery of microbes.
The concept of "humors" may have origins in Ancient Egyptian medicine,[1] or Mesopotamia,[2] though it was not systemized until ancient Greek thinkers. The word humor is a translation of Greek χυμός,[3] chymos (literally 'juice' or 'sap', metaphorically 'flavor'). Early texts on Indian Ayurveda medicine presented a theory of three or four humors (doṣas),[4][5] which they sometimes linked with the five elements (pañca-bhūta): earth, water, fire, air, and space.[6]
The concept of "humors" (chemical systems regulating human behaviour) became more prominent from the writing of medical theorist Alcmaeon of Croton (c. 540–500 BC). His list of humors was longer and included fundamental elements described by Empedocles, such as water, earth, fire, air, etc. Hippocrates is usually credited with applying this idea to medicine. In contrast to Alcmaeon, Hippocrates suggested that humors are the vital bodily fluids: blood, phlegm, yellow bile, and black bile. Alcmaeon and Hippocrates posited that an extreme excess or deficiency of any of the humors (bodily fluid) in a person can be a sign of illness. Hippocrates, and then Galen, suggested that a moderate imbalance in the mixture of these fluids produces behavioral patterns.[7] One of the treatises attributed to Hippocrates, On the Nature of Man, describes the theory as follows:
The Human body contains blood, phlegm, yellow bile, and black bile. These are the things that make up its constitution and cause its pains and health. Health is primarily that state in which these constituent substances are in the correct proportion to each other, both in strength and quantity, and are well mixed. Pain occurs when one of the substances presents either a deficiency or an excess, or is separated in the body and not mixed with others.[8] The body depends heavily on the four humors because their balanced combination helps to keep people in good health. Having the right amount of humor is essential for health. The pathophysiology of disease is consequently brought on by humor excesses and/or deficiencies.[9]
The existence of fundamental biochemical substances and structural components in the body remains a compellingly shared point with Hippocratic beliefs, despite the fact that current science has moved away from those four Hippocratic humors.[9]
Although the theory of the four humors does appear in some Hippocratic texts, other Hippocratic writers accepted the existence of only two humors, while some refrained from discussing the humoral theory at all.[10] Humoralism, or the doctrine of the four temperaments, as a medical theory retained its popularity for centuries, largely through the influence of the writings of Galen (129–201 AD). The four essential elements—humors—that make up the human body, according to Hippocrates, are in harmony with one another and act as a catalyst for preserving health.[9] Hippocrates' theory of four humors was linked with the popular theory of the four elements (earth, fire, water, and air) proposed by Empedocles, but this link was not proposed by Hippocrates or Galen, who referred primarily to bodily fluids. While Galen thought that humors were formed in the body, rather than ingested, he believed that different foods had varying potential to act upon the body to produce different humors. Warm foods, for example, tended to produce yellow bile, while cold foods tended to produce phlegm. Seasons of the year, periods of life, geographic regions, and occupations also influenced the nature of the humors formed. As such, certain seasons and geographic areas were understood to cause imbalances in the humors, leading to varying types of disease across time and place. For example, cities exposed to hot winds were seen as having higher rates of digestive problems as a result of excess phlegm running down from the head, while cities exposed to cold winds were associated with diseases of the lungs, acute diseases, and "hardness of the bowels", as well as ophthalmies (issues of the eyes), and nosebleeds. Cities to the west, meanwhile, were believed to produce weak, unhealthy, pale people that were subject to all manners of disease.[11] In the treatise, On Airs, Waters, and Places, a Hippocratic physician is described arriving to an unnamed city where they test various factors of nature including the wind, water, and soil to predict the direct influence on the diseases specific to the city based on the season and the individual.[12]
A fundamental idea of Hippocratic medicine was the endeavor to pinpoint the origins of illnesses in both the physiology of the human body and the influence of potentially hazardous environmental variables like air, water, and nutrition, and every humor has a distinct composition and is secreted by a different organ.[13] Aristotle's concept of eucrasia—a state resembling equilibrium—and its relationship to the right balance of the four humors allow for the maintenance of human health, offering a more mathematical approach to medicine.[13]
The imbalance of humors, or dyscrasia, was thought to be the direct cause of all diseases. Health was associated with a balance of humors, or eucrasia. The qualities of the humors, in turn, influenced the nature of the diseases they caused. Yellow bile caused warm diseases and phlegm caused cold diseases. In On the Temperaments, Galen further emphasized the importance of the qualities. An ideal temperament involved a proportionally balanced mixture of the four qualities. Galen identified four temperaments in which one of the qualities (warm, cold, moist, or dry) predominated, and four more in which a combination of two (warm and moist, warm and dry, cold and dry, or cold and moist) dominated. These last four, named for the humors with which they were associated—sanguine, choleric, melancholic and phlegmatic—eventually became better known than the others. While the term temperament came to refer just to psychological dispositions, Galen used it to refer to bodily dispositions, which determined a person's susceptibility to particular diseases, as well as behavioral and emotional inclinations.
Disease could also be the result of the "corruption" of one or more of the humors, which could be caused by environmental circumstances, dietary changes, or many other factors.[14] These deficits were thought to be caused by vapors inhaled or absorbed by the body. Greeks and Romans, and the later Muslim and Western European medical establishments that adopted and adapted classical medical philosophy, believed that each of these humors would wax and wane in the body, depending on diet and activity. When a patient was suffering from a surplus or imbalance of one of the four humors, then said patient's personality and/or physical health could be negatively affected.
Therefore, the goal of treatment was to rid the body of some of the excess humor through techniques like purging, bloodletting, catharsis, diuresis, and others. Bloodletting was already a prominent medical procedure by the first century, but venesection took on even more significance once Galen of Pergamum declared blood to be the most prevalent humor.[15] The volume of blood extracted ranged from a few drops to several litres over the course of several days, depending on the patient's condition and the doctor's practice.[16]
Even though humorism theory had several models that used two, three, and five components, the most famous model consists of the four humors described by Hippocrates and developed further by Galen. The four humors of Hippocratic medicine are black bile (Greek: μέλαινα χολή, melaina chole), yellow bile (Greek: ξανθὴ χολή, xanthe chole), phlegm (Greek: φλέγμα, phlegma), and blood (Greek: αἷμα, haima). Each corresponds to one of the traditional four temperaments. Based on Hippocratic medicine, it was believed that for a body to be healthy, the four humors should be balanced in amount and strength.[17] The proper blending and balance of the four humors was known as eukrasia.[18]
Humorism theory was improved by Galen, who incorporated his understanding of the humors into his interpretation of the human body. He believed the interactions of the humors within the body were the key to investigating the physical nature and function of the organ systems. Galen combined his interpretation of the humors with his collection of ideas concerning nature from past philosophers in order to find conclusions about how the body works. For example, Galen maintained the idea of the presence of the Platonic tripartite soul, which consisted of "thumos (spiritedness), epithumos (directed spiritedness, i.e. desire), and Sophia (wisdom)".[19] Through this, Galen found a connection between these three parts of the soul and the three major organs that were recognized at the time: the brain, the heart, and the liver.[19] This idea of connecting vital parts of the soul to vital parts of the body was derived from Aristotle's sense of explaining physical observations, and Galen utilized it to build his view of the human body. The organs (named organa) had specific functions (called chreiai) that contributed to the maintenance of the human body, and the expression of these functions is shown in characteristic activities (called energeiai) of a person.[20] While the correspondence of parts of the body to the soul was an influential concept, Galen decided that the interaction of the four humors with natural bodily mechanisms were responsible for human development and this connection inspired his understanding of the nature of the components of the body.
Galen recalls the correspondence between humors and seasons in his On the Doctrines of Hippocrates and Plato, and says that, "As for ages and the seasons, the child (παῖς) corresponds to spring, the young man (νεανίσκος) to summer, the mature man (παρακµάζων) to autumn, and the old man (γέρων) to winter".[21] He also related a correspondence between humors and seasons based on the properties of both. Blood, as a humor, was considered hot and wet. This gave it a correspondence to spring. Yellow bile was considered hot and dry, which related it to summer. Black bile was considered cold and dry, and thus related to autumn. Phlegm, cold and wet, was related to winter.[22]
Galen also believed that the characteristics of the soul follow the mixtures of the body, but he did not apply this idea to the Hippocratic humors. He believed that phlegm did not influence character. In his On Hippocrates' The Nature of Man, Galen stated: "Sharpness and intelligence (ὀξὺ καὶ συνετόν) are caused by yellow bile in the soul, perseverance and consistency (ἑδραῖον καὶ βέβαιον) by the melancholic humor, and simplicity and naivety (ἁπλοῦν καὶ ἠλιθιώτερον) by blood. But the nature of phlegm has no effect on the character of the soul (τοῦ δὲ φλέγµατος ἡ φύσις εἰς µὲν ἠθοποιῗαν ἄχρηστος)."[23] He further said that blood is a mixture of the four elements: water, air, fire, and earth.
These terms only partly correspond to modern medical terminology, in which there is no distinction between black and yellow bile, and phlegm has a very different meaning. It was believed that the humors were the basic substances from which all liquids in the body were made. Robin Fåhræus (1921), a Swedish physician who devised the erythrocyte sedimentation rate, suggested that the four humors were based upon the observation of blood clotting in a transparent container. When blood is drawn in a glass container and left undisturbed for about an hour, four different layers can be seen: a dark clot forms at the bottom (the "black bile"); above the clot is a layer of red blood cells (the "blood"); above this is a whitish layer of white blood cells (the "phlegm"); the top layer is clear yellow serum (the "yellow bile").[24]
Many Greek texts were written during the golden age of the theory of the four humors in Greek medicine after Galen. One of those texts was an anonymous treatise called On the Constitution of the Universe and of Man, published in the mid-19th century by J. L. Ideler. In this text, the author establishes the relationship between elements of the universe (air, water, earth, fire) and elements of the man (blood, yellow bile, black bile, phlegm).[25] He said that:
Seventeenth century English playwright Ben Jonson wrote humor plays, where character types were based on their humoral complexion.
It was thought that the nutritional value of the blood was the source of energy for the body and the soul. Blood was believed to consist of small proportional amounts of the other three humors. This meant that taking a blood sample would allow for determination of the balance of the four humors in the body.[26] It was associated with a sanguine nature (enthusiastic, active, and social).[27][28]: 103–05  Blood is considered to be hot and wet, sharing these characteristics with the season of spring.[29]
Yellow bile was associated with a choleric nature (ambitious, decisive, aggressive, and short-tempered).[30] It was thought to be fluid found within the gallbladder, or in excretions such as vomit and feces.[26] The associated qualities for yellow bile are hot and dry with the natural association of summer and fire. It was believed that an excess of this humor in an individual would result in emotional irregularities such as increased anger or irrational behaviour.[31]
Black bile was associated with a melancholy nature, the word melancholy itself deriving from the Greek for 'black bile', μέλαινα χολή (melaina kholé). Depression was attributed to excess or unnatural black bile secreted by the spleen.[32]
Cancer was also attributed to an excess of black bile concentrated in a specific area.[33] The seasonal association of black bile was to autumn as the cold and dry characteristics of the season reflect the nature of man.[29]
Phlegm was associated with all phlegmatic nature, thought to be associated with reserved behavior.[34] The phlegm of humorism is far from phlegm as it is defined today. Phlegm was used as a general term to describe white or colorless secretions such as pus, mucus, saliva, or sweat.[26] Phlegm was also associated with the brain, possibly due to the color and consistency of brain tissue.[26] The French physiologist and Nobel laureate Charles Richet, when describing humorism's "phlegm or pituitary secretion" in 1910, asked rhetorically, "this strange liquid, which is the cause of tumours, of chlorosis, of rheumatism, and cacochymia – where is it? Who will ever see it? Who has ever seen it? What can we say of this fanciful classification of humors into four groups, of which two are absolutely imaginary?"[35] The seasonal association of phlegm is winter due to the natural properties of being cold and wet.[36]
Humors were believed to be produced via digestion as the final products of hepatic digestion. Digestion is a continuous process taking place in every animal, and it can be divided into four sequential stages.[37] The gastric digestion stage, the hepatic digestion stage, the vascular digestion stage, and the tissue digestion stage. Each stage digests food until it becomes suitable for use by the body. In gastric digestion, food is made into chylous, which is suitable for the liver to absorb and carry on digestion. Chylous is changed into chymous in the hepatic digestion stage. Chymous is composed of the four humors: blood, phlegm, yellow bile, and black bile. These four humors then circulate in the blood vessels. In the last stage of digestion, tissue digestion, food becomes similar to the organ tissue for which it is destined.
If anything goes wrong leading up to the production of humors, there will be an imbalance leading to disease. Proper organ functioning is necessary in the production of good humor. The stomach and liver also have to function normally for proper digestion. If there are any abnormalities in gastric digestion, the liver, blood vessels, and tissues cannot be provided with the raw chylous, which can cause abnormal humor and blood composition. A healthy functioning liver is not capable of converting abnormal chylous into normal chylous and normal humors.
Humors are the end product of gastric digestion, but they are not the end product of the digestion cycle, so an abnormal humor produced by hepatic digestion will affect other digestive organs.
According to Hippocratic humoral theory, jaundice is present in the Hippocratic Corpus. Some of the first descriptions of jaundice come from the Hippocratic physicians (icterus).[38] The ailment appears multiple times in the Hippocratic Corpus, where its genesis, description, prognosis, and therapy are given. The five kinds of jaundice mentioned in the Hippocratic Corpus all share a yellow or greenish skin color.[38]
A modern doctor will undoubtedly start to think of the symptoms listed in contemporary atlases of medicine after reading the clinical symptoms of each variety of jaundice listed in the Hippocratic Corpus. Despite the fact that the Hippocratic physicians' therapeutic approaches have little to do with contemporary medical practice, their capacity for observation as they described the various forms of jaundice is remarkable.[38] In the Hippocratic Corpus, the Hippocratic physicians make multiple references to jaundice. At that time, jaundice was viewed as an illness unto itself rather than a symptom brought on by a disease.[38]
Empedocles's theory suggested that there are four elements: earth, fire, water, and air, with the earth producing the natural systems. Since this theory was influential for centuries, later scholars paired qualities associated with each humor as described by Hippocrates/Galen with seasons and "basic elements" as described by Empedocles.[39]
The following table shows the four humors with their corresponding elements, seasons, sites of formation, and resulting temperaments:[40]
Medieval medical tradition in the Golden Age of Islam adopted the theory of humorism from Greco-Roman medicine, notably via the Persian polymath Avicenna's The Canon of Medicine (1025). Avicenna summarized the four humors and temperaments as follows:[41]
The Unani school of medicine, practiced in Perso-Arabic countries, India, and Pakistan, is based on Galenic and Avicennian medicine in its emphasis on the four humors as a fundamental part of the methodologic paradigm.
The humoralist system of medicine was highly individualistic, for all patients were said to have their own unique humoral composition.[43] From Hippocrates onward, the humoral theory was adopted by Greek, Roman and Islamic physicians, and dominated the view of the human body among European physicians until at least 1543 when it was first seriously challenged by Andreas Vesalius, who mostly criticized Galen's theories of human anatomy and not the chemical hypothesis of behavioural regulation (temperament).
Typical 18th-century practices such as bleeding a sick person or applying hot cups to a person were based on the humoral theory of imbalances of fluids (blood and bile in those cases). Methods of treatment like bloodletting, emetics and purges were aimed at expelling a surplus of a humor.[44] Apocroustics were medications intended to stop the flux of malignant humors to a diseased body part.[45]
16th-century Swiss physician Paracelsus further developed the idea that beneficial medical substances could be found in herbs, minerals and various alchemical combinations thereof. These beliefs were the foundation of mainstream Western medicine well into the 17th century. Specific minerals or herbs were used to treat ailments simple to complex, from an uncomplicated upper respiratory infection to the plague. For example, chamomile was used to decrease heat, and lower excessive bile humor. Arsenic was used in a poultice bag to 'draw out' the excess humor(s) that led to symptoms of the plague. Apophlegmatisms, in pre-modern medicine, were medications chewed in order to draw away phlegm and humors.
Although advances in cellular pathology and chemistry criticized humoralism by the 17th century, the theory had dominated Western medical thinking for more than 2,000 years.[46][47] Only in some instances did the theory of humoralism wane into obscurity. One such instance occurred in the sixth and seventh centuries in the Byzantine Empire when traditional secular Greek culture gave way to Christian influences. Though the use of humoralist medicine continued during this time, its influence was diminished in favor of religion.[48]  The revival of Greek humoralism, owing in part to changing social and economic factors, did not begin until the early ninth century.[49]  Use of the practice in modern times is pseudoscience.[50]
Humoral theory was the grand unified theory of medicine, before the invention of modern medicine, for more than 2,000 years. The theory was one of the fundamental tenets of the teachings of the Greek physician-philosopher Hippocrates (460–370 BC), who is regarded as the first practitioner of medicine, appropriately referred to as the "Father of Modern Medicine".[51]
With the advent of the Doctrine of Specific Etiology, the humoral theory's demise hastened even further. This demonstrates that there is only one precise cause and one specific issue for each and every sickness or disorder that has been diagnosed.[51] Additionally, the identification of messenger molecules like hormones, growth factors, and neurotransmitters suggests that the humoral theory has not yet been made fully moribund. Humoral theory is still present in modern medical terminology, which refers to humoral immunity when discussing elements of immunity that circulate in the bloodstream, such as hormones and antibodies.[51]
Modern medicine refers to humoral immunity or humoral regulation when describing substances such as hormones and antibodies, but this is not a remnant of the humor theory. It is merely a literal use of humoral, i.e. pertaining to bodily fluids (such as blood and lymph).
The concept of humorism was not definitively disproven until 1858.[46][47] There were no studies performed to prove or disprove the impact of dysfunction in known bodily organs producing named fluids (humors) on temperament traits simply because the list of temperament traits was not defined up until the end of the 20th century.
Theophrastus and others developed a set of characters based on the humors. Those with too much blood were sanguine. Those with too much phlegm were phlegmatic. Those with too much yellow bile were choleric, and those with too much black bile were melancholic. The idea of human personality based on humors contributed to the character comedies of Menander and, later, Plautus.
Through the neo-classical revival in Europe, the humor theory dominated medical practice, and the theory of humoral types made periodic appearances in drama. The humors were an important and popular iconographic theme in European art, found in paintings, tapestries,[52] and sets of prints.
The humors can be found in Elizabethan works, such as in The Taming of the Shrew, in which the character Petruchio, a choleric man, uses humoral therapy techniques on Katherina, a choleric woman, in order to tame her into the socially acceptable phlegmatic woman.[53] Some examples include: he yells at the servants for serving mutton, a choleric food, to two people who are already choleric; he deprives Katherina of sleep; and he, Katherina and their servant Grumio endure a cold walk home, for cold temperatures were said to tame choleric temperaments.
The theory of the four humors features prominently in Rupert Thomson's 2005 novel Divided Kingdom.

The chimpanzee–human last common ancestor (CHLCA) is the last common ancestor shared by the extant Homo (human) and Pan (chimpanzee and bonobo) genera of Hominini. Estimates of the divergence date vary widely from thirteen to five million years ago.
In human genetic studies, the CHLCA is useful as an anchor point for calculating single-nucleotide polymorphism (SNP) rates in human populations where chimpanzees are used as an outgroup, that is, as the extant species most genetically similar to Homo sapiens.
Despite extensive research, no direct fossil evidence of the CHLCA has been discovered.  Fossil candidates like Sahelanthropus tchadensis, Orrorin tugenensis, and Ardipithecus ramidus have been debated as either being early hominins or close to the CHLCA. However, their classification remains uncertain due to incomplete evidence
The taxon tribe Hominini was proposed to separate humans (genus Homo) from chimpanzees (Pan) and gorillas (genus Gorilla) on the notion that the least similar species should be separated from the other two.  However, later evidence revealed that Pan and Homo are closer genetically than are Pan and Gorilla; thus, Pan was referred to the tribe Hominini with Homo. Gorilla now became the separated genus and was referred to the new taxon 'tribe Gorillini'.
Mann and Weiss (1996), proposed that the tribe Hominini should encompass Pan and Homo, grouped in separate subtribes.[1] They classified Homo and all bipedal apes in the subtribe Hominina and Pan  in the subtribe Panina. (Wood (2010) discussed the different views of this taxonomy.)[2]   A "chimpanzee clade" was posited by Wood and Richmond, who referred it to a tribe Panini, which was envisioned from the family Hominidae being composed of a trifurcation of subfamilies.[3]
Richard Wrangham (2001) argued that the CHLCA species was very similar to the common chimpanzee (Pan troglodytes) – so much so that it should be classified as a member of the genus Pan and be given the taxonomic name Pan prior.[4]
All the human-related genera of tribe Hominini that arose after divergence from Pan are members of the subtribe Hominina, including the genera Homo and Australopithecus. This group represents "the human clade" and its members are called "hominins".[5]
No fossil has yet conclusively been identified as the CHLCA.
Sahelanthropus tchadensis is an extinct hominine with some morphology proposed (and disputed) to be as expected of the CHLCA, and it lived some 7 million years ago – close to the time of the chimpanzee–human divergence. But it is unclear whether it should be classified as a member of the tribe Hominini, that is, a hominin, as an ancestor of Homo and Pan and a potential candidate for the CHLCA species itself, or simply a Miocene ape with some convergent anatomical similarity to many later hominins.
Ardipithecus most likely appeared after the human-chimpanzee split, some 5.5 million years ago, at a time when gene flow may still have been ongoing. It has several shared characteristics with chimpanzees, but due to its fossil incompleteness and the proximity to the human-chimpanzee split, the exact position of Ardipithecus in the fossil record is unclear.[6] It is most likely derived from the chimpanzee lineage and thus not ancestral to humans.[7][8] However, Sarmiento (2010), noting that Ardipithecus does not share any characteristics exclusive to humans and some of its characteristics (those in the wrist and basicranium), suggested that it may have diverged from the common human/African ape stock prior to the human, chimpanzee and gorilla divergence.[9]
Another candidate that has been suggested is Graecopithecus, though this claim is disputed as there is insufficient evidence to support the determination of Graecopithecus as hominin.[10] This would put the CHLCA split in Southeast Europe instead of Africa.[11][12]
The earliest fossils clearly in the human but not the chimpanzee lineage appear between about 4.5 to 4 million years ago, with Australopithecus anamensis.
Few fossil specimens on the "chimpanzee-side" of the split have been found; the first fossil chimpanzee, dating between 545 and 284 kyr (thousand years, radiometric), was discovered in Kenya's East African Rift Valley (McBrearty, 2005).[13] All extinct genera listed in the taxobox[which?] are ancestral to Homo, or are offshoots of such. However, both Orrorin and Sahelanthropus existed around the time of the divergence, and so either one or both may be ancestral to both genera Homo and Pan.
Due to the scarcity of fossil evidence for CHLCA candidates, Mounier (2016) presented a project to create a "virtual fossil" by applying digital "morphometrics" and statistical algorithms to fossils from across the evolutionary history of both Homo and Pan, having previously used this technique to visualize a skull of the last common ancestor of Neanderthal and Homo sapiens.[14][15]
An estimate of 10 to 13 million years for the CHLCA was proposed in 1998,[16] and a range of 7 to 10 million years ago is assumed by White and colleagues in 2009.[17] A 2016 study analyzed transitions at CpG sites in genome sequences, which exhibit a more clocklike behavior than other substitutions, arriving at an estimate for human and chimpanzee divergence time of 12.1 million years.[18] Studies in the early 2020s suggest a more recent CHLCA, such as between 9.3 and 6.5 million years ago in 2021,[19] and studies giving even more recent dates are cited in a 2022 article.[20]
A source of confusion in determining the exact age of the Pan–Homo split is evidence of a more complex speciation process than a clean split between the two lineages. Different chromosomes appear to have split at different times, possibly over as much as a 4-million-year period, indicating a long and drawn out speciation process with large-scale gene flow events between the two emerging lineages as recently as 6.3 to 5.4 million years ago, according to Patterson et al. (2006).[21]
Speciation between Pan and Homo occurred over the last 9 million years. Ardipithecus probably branched off of the Pan lineage in the middle Miocene Messinian.[7][8] After the original divergences, there were, according to Patterson (2006), periods of gene flow between population groups and a process of alternating divergence and gene flow that lasted several million years.[21] Some time during the late Miocene or early Pliocene, the earliest members of the human clade completed a final separation from the lineage of Pan – with date estimates ranging from 13 million[16] to as recent as 4 million years ago.[21] The latter date was in particular based on the similarity of the X chromosome in humans and chimpanzees, a conclusion rejected as unwarranted by Wakeley (2008), who suggested alternative explanations, including selection pressure on the X chromosome in the populations ancestral to the CHLCA.[note 1]
Complex speciation and incomplete lineage sorting of genetic sequences seem to also have happened in the split between the human lineage and that of the gorilla, indicating "messy" speciation is the rule rather than the exception in large primates.[23][24] Such a scenario would explain why the divergence age between the Homo and Pan has varied with the chosen method and why a single point has so far been hard to track down.

Joseph Jordania (Georgian იოსებ ჟორდანია, born February 12, 1954, and also known under the misspelling of Joseph Zhordania) is an Australian–Georgian ethnomusicologist and evolutionary musicologist and professor.[1][2] He is an Honorary Fellow of the Melbourne Conservatorium of Music at the University of Melbourne and the Head of the Foreign Department of the International Research Centre for Traditional Polyphony at Tbilisi State Conservatory. Jordania is known for his model of the origins of human choral singing in the wide context of human evolution and was one of founders of the International Research Centre for Traditional Polyphony in Georgia.
Jordania's academic interests include study of worldwide distribution of choral polyphonic traditions, origins of choral singing, origins of rhythm, origins of human morphology and behaviour, cross-cultural prevalence of stuttering, dyslexia and acquisition of phonological system in children, study of the cognitive threshold between animal and human cognitive abilities. His primary expertise is Georgian and Caucasian traditional music and vocal polyphony.
Jordania was born in Georgia (former Soviet Union). He received a BA degree in ethnomusicology from Tbilisi State Conservatory in 1978. During 1979–1983 he was elected as the President of the Board of Creative Youth of Tbilisi. In 1982 he received his PhD degree in musicology–ethnomusicology from Tbilisi Theatrical Institute, and served as lecturer, senior lecturer, assistant professor, and professor at the Department of Georgian Traditional Music at Tbilisi State Conservatory. For one year (in 1984) he served as a dean of the Faculty of Musicology. In 1991 he received the title D.Mus from Kiev Conservatory. From 1988 until 1995 Jordania was the head of the Musical Sector of the Centre of the Mediterranean Studies at the Tbilisi State University. He published his first monograph on choral polyphony in 1989. In 1984 he was instrumental in organizing the conference "Problems of Folk Polyphony". This conference became the beginning of the series of biannual international conferences (1984, 1986, 1988, 1998, 2000) and symposia (2002, 2004, 2006, 2010, 2012, 2014, 2016, 2018, 2020) on traditional polyphony, and led to establishing the International Research Centre for Traditional Polyphony at Tbilisi State Conservatory in 2003.[3]
In 2009, in recognition of "his contribution to systematic analysis of folk polyphonies of the world, proposing a new model for the origins of traditional choral singing in a broad context of human evolution" Jordania was awarded the Fumio Koizumi Prize for ethnomusicology.[4]

The discovery of human antiquity was a major achievement of science in the middle of the 19th century, and the foundation of scientific paleoanthropology. The antiquity of man, human antiquity, or in simpler language the age of the human race, are names given to the series of scientific debates it involved, which with modifications continue in the 21st century. These debates have clarified and given scientific evidence, from a number of disciplines, towards solving the basic question of dating the first human being.
Controversy was very active in this area in parts of the 19th century, with some dormant periods also. A key date was the 1859 re-evaluation of archaeological evidence that had been published 12 years earlier by Boucher de Perthes. It was then widely accepted, as validating the suggestion that man was much older than had previously been believed, for example than the 6,000 years implied by some traditional chronologies.
In 1863 T. H. Huxley argued that man was an evolved species; and in 1864 Alfred Russel Wallace combined natural selection with the issue of antiquity. The arguments from science for what was then called the "great antiquity of man" became convincing to most scientists, over the following decade. The separate debate on the antiquity of man had in effect merged into the larger one on evolution, being simply a chronological aspect. It has not ended as a discussion, however, since the current science of human antiquity is still in flux.
Modern science has no single answer to the question of how old humanity is. What the question now means indeed depends on choosing genus or species in the required answer. It is thought that the genus of man has been around for ten times as long as our species. Currently, fresh examples of (extinct) species of the genus Homo are still being discovered, so that definitive answers are not available. The consensus view is that human beings are one species, the only existing species of the genus. With the rejection of polygenism for human origins, it is asserted that this species had a definite and single origin in the past. (That assertion leaves aside the point whether the origin meant is of the current species, however. The multiregional hypothesis allows the origin to be otherwise.) The hypothesis of recent African origin of modern humans is now widely accepted, and states that anatomically modern humans had a single origin, in Africa.
The genus Homo is now estimated to be about 2.3 to 2.4 million years old, with the appearance of H. habilis;[1] meaning that the existence of all types of humans has been within the Quaternary.
Once the question is reformulated as dating the transition of the evolution of H. sapiens from a precursor species, the issue can be refined into two further questions. These are: the analysis and dating of the evolution of Archaic Homo sapiens, and of the evolution from "archaic" forms of the species H. sapiens sapiens. The second question is given an answer in two parts: anatomically modern humans are thought to be about 300,000 years old,[2] with behavioral modernity dating back to 40,000[3] or 50,000 years ago. The first question is still subject to debates on its definition.
Discovering the age of the first human is one facet of anthropogeny, the study of human origins, and a term dated by the Oxford English Dictionary to 1839 and the Medical Dictionary of Robert Hooper. Given the history of evolutionary thought, and the history of paleontology, the question of the antiquity of man became quite natural to ask at around this period. It was by no means a new question, but it was being asked in a new context of knowledge, particularly in comparative anatomy and palaeontology. The development of relative dating as a principled method allowed deductions of chronology relative to events tied to fossils and strata. This meant, though, that the issue of the antiquity of man was not separable from other debates of the period, on geology and foundations of scientific archaeology.
The first strong scientific arguments for the antiquity of man as very different from accepted biblical chronology were certainly also strongly controverted. Those who found the conclusion unacceptable could be expected to examine the whole train of reasoning for weak points. This can be seen, for example, in the Systematic Theology of Charles Hodge (1871–3).[4]
For a period, once the scale of geological time had become clear in the 19th century, the "antiquity of man" stood for a theory opposed to the "modern origin of man", for which arguments of other kinds were put forward. The choice was logically independent of monogenism versus polygenism; but monogenism with the modern origin implied time scales on the basis of the geographical spread, physical differences and cultural diversity of humans. The choice was also logically independent of the notion of transmutation of species, but that was considered to be a slow process.
William Benjamin Carpenter wrote in 1872 of a fixed conviction of the "modern origin" as the only reason for resisting the human creation of flint implements.[5] Henry Williamson Haynes writing in 1880 could call the antiquity of man "an established fact".[6]
The Biblical account included
These points were debated by scholars as well as theologians. Biblical literalism was not a given in the medieval and early modern periods, for Christians or Jews.
The Flood could explain extinctions of species at that date, on the hypothesis that the Ark had not contained all species of animal. A Flood that was not universal, on the other hand, had implications for the biblical theory of races and Noah's sons. The theory of catastrophism, which was as much secular as theological in attitude, could be used in analogous ways.
There was interest in matters arising from modification of the biblical narrative, therefore, and it was fuelled by the new knowledge of the world in early modern Europe, and then by the growth of the sciences. One hypothesis was of people not descended from Adam. This hypothesis of polygenism (no unique origin of humans) implied nothing on the antiquity of man, but the issue was implicated in counter-arguments, for monogenism.
Isaac La Peyrère appealed in formulating his Preadamite theory of polygenism to Jewish tradition; it was intended to be compatible with the biblical creation of man. It was rejected by many contemporary theologians.[7][8] This idea of humans before Adam had been current in earlier Christian scholars and those of unorthodox and heretical beliefs; La Peyrère's significance was his synthesis of the dissent.[9] Influentially, he revived the classical idea of Marcus Terentius Varro, preserved in Censorinus, of a three-fold division of historical time into "uncertain" (to a universal flood), "mythical", and "historical" (with certain chronology).[10]
The biblical narrative had implications for ethnology (division into Hamitic, Japhetic and Semitic peoples), and had its defenders, as well as those who felt it made significant omissions. Matthew Hale wrote his Primitive Origination of Mankind (1677) against La Peyrère, it has been suggested, in order to defend the propositions of a young human race and universal Flood, and the Native Americans as descended from Noah.[11] Anthony John Maas writing in the 1913 Catholic Encyclopedia commented that pro-slavery sentiment indirectly supported the Preadamite theories of the middle of the 19th century.[8] The antiquity of man found support in the opposed theories of monogenism of this time that justified abolitionism by discrediting scientific racism.
Already in the 18th century polygenism was applied as a theory of race (see Scientific racism#Blumenbach and Buffon). A variant racist Preadamism was introduced, in particular by Reginald Stuart Poole (The Genesis of the Earth and of Man, London, 1860) and Dominic M'Causland (Adam and the Adamite, or the Harmony of Scripture and Ethnology, London, 1864). They followed the views of Samuel George Morton, Josiah C. Nott, George Gliddon, and Louis Agassiz; and maintained that Adam was the progenitor of the Caucasian race, while the other races descended from Preadamite ancestry.[8]
James Cowles Prichard argued against polygenism, wishing to support the account drawn from the Book of Genesis of a single human origin. In particular he argued that humans were one species, using the interfertility criterion of hybridity.[12] By his use of a form of natural selection to argue for change of human skin colour as a historical process, he also implied a time scale long enough for such a process to have produced the observed differences.[13]
The Early Christian Church contested claims that pagan traditions were older than that of the Bible. Theophilus of Antioch and Augustine of Hippo both argued against Egyptian views that the world was at least 100,000 years old. This figure was too high to be compatible with biblical chronology.[14] One of La Peyrère's propositions, that China was at least 10,000 years old, gained wider currency;[15] Martino Martini had provided details of traditional Chinese chronology, from which it was deduced by Isaac Vossius that Noah's Flood was local rather than universal.[16]
One of the considerations detected in La Peyrère by Otto Zöckler was concern with the Antipodes and their people: were they pre-Adamites, or indeed had there been a second "Adam of the Antipodes"?[17] In a 19th-century sequel, Alfred Russel Wallace in an 1867 book review pointed to the Pacific Islanders as posing a problem for those holding both to monogenism and a recent date for human origins. In other words, he took migration from an original location to remote islands that are now populated to imply a long time scale.[18] A significant consequence of the recognition of the antiquity of man was the greater scope for conjectural history, in particular for all aspects of diffusionism and social evolutionism.[19]
While extinction of species came with the development of geology to be widely accepted in the early 19th century, there was resistance on theological grounds to extinctions after the creation of man. It was argued, in particular in the 1820s and 1830s, that man would not be created into an "imperfect" world as far as design of its collection of species was concerned. This reasoning cut across that which was conclusive for the science of the antiquity of man, a generation later.[20]
The late 18th century was a period in which French and German caves were explored, and remains taken for study:[21] caving was in fashion, if speleology was only in its infancy, and the St. Beatus Caves, for example, attracted many visitors. Caves were a theme of the art of the time, also.[22]
Cave remains proved of great importance to the science of the antiquity of man. Stalagmite formation was a clearcut mechanism of formation of fossils, and its stratigraphy could be understood. Other sites of importance were associated with alluvial deposits of gravel and clay, or peat. The early example of the Gray's Inn Lane Hand Axe was from gravel in a bed of a tributary of the River Thames, but remained isolated for about a century.
The three-age system was in place from about 1820, in the form given to it by Christian Jürgensen Thomsen in his work on the collections that became the National Museum of Denmark. He published his ideas in 1836.[13] Postulating cultural change, in itself and without explaining a rate of change, did not generate reasons to revise traditional chronology.[23] But the concept of Stone Age artifacts became current. Thomsen's book in Danish, Ledetraad til Nordisk Oldkyndighed, was translated into German (Leitfaden zur Nordischen Alterthumskunde, 1837), and English (Guide to Northern Archæology, 1848).[24][25]
John Frere's 1797 discovery of the Hoxne handaxe[26] helped to initiate the 19th century debate,[27] but it started in earnest around 1810.[28] There were then a number of false starts relating to different European sites. William Buckland misjudged what he had found in 1823 with the misnamed Red Lady of Paviland, and explained away the mammoth remains with the find.[29] He also was dismissive of the Kent's Cavern findings of John MacEnery in the later 1820s. In 1829 Philippe-Charles Schmerling discovered a Neanderthal fossil skull (at Engis). At that point, however, its significance was not recognised, and Rudolf Virchow consistently opposed the theory that it was very old. The 1847 book Antiquités Celtiques et Antediluviennes by Boucher de Perthes about Saint-Acheul was found unconvincing in its presentation, until it was reconsidered about a decade later.
The debate moved on only in the context of
It was this combination, "extinct faunal remains" + "human artifacts", that provided the evidence that came to be seen as crucial. A sudden acceleration of research was seen from mid-1858, when the Geological Society set up a "cave committee". Besides Hugh Falconer who had pressed for it, the committee comprised Charles Lyell, Richard Owen, William Pengelly, Joseph Prestwich, and Andrew Ramsay.[30]
On the one hand, lack of uniformity in prehistory is what gave science traction on the question of the antiquity of man; and, on the other hand, there were at the time theories that tended to rule out certain types of lack of regularity. John Lubbock outlined in 1890 the way the antiquity of man had in his time been established as derived from change in prehistory: in fauna, geography and climate.[31] The hypotheses required to establish that these changes were facts of prehistory were themselves in tension with the uniformitarianism that was held to by some scientists; therefore the protean concept "uniformitarianism" was adjusted to accommodate the past changes that could be established.
Zoological uniformity on earth was debated already in the early eighteenth century. 
George Berkeley argued in Alciphron that the lack of human artifacts in deeper excavations suggested a recent origin of man.[32] Evidence of absence was, of course, seen as problematic to establish. Gottfried Leibniz in his Protogaea produced arguments against identification of a species via morphology, without evidence of descent (having in mind a characterisation of humans by possession of reason); and against the discreteness of species and their extinction.[33]
Uniformitarianism held the field against the competitor theories of Neptunism and catastrophism, which partook of Romantic science and theological cosmogony; it established itself as the successor of Plutonism, and became the foundation of modern geology. Its tenets were correspondingly firmly held. Charles Lyell put forward at one point views on what were called "uniformity of kind" and "uniformity of degree" that were incompatible with what was argued later. Lyell's theory, in fact, was of a "steady state" geology, which he deduced from his principles. This went too far in restricting actual geological processes, to a predictable closed system, if it ruled out ice ages (see ice ages#Causes of ice ages), as became clearer not long after Lyell's Principles of Geology appeared (1830–3).[34][35] Of Lubbock's three types of change, the geographical included the theory of migration over land bridges in biogeography, which in general acted as an explanatory stopgap, rather than in most cases being one supported by science. Sea level changes were easier to justify.
The identification of ice ages was important context for the antiquity of man because it was accepted that certain mammals had died out with the last of the ice ages which were clearly marked in the geological record. Georges Cuvier's Recherches sur les ossements fossiles de quadrupèdes (1812) had accepted facts of the extinctions of mammals that were to be relevant to human antiquity. The concept of an ice age was proposed in 1837 by Louis Agassiz, and it opened the way to the study of glacial history of the Quaternary. William Buckland came to see evidence of glaciers in what he had taken to be remains of the biblical Flood. It seemed adequately proved that the woolly mammoth and woolly rhinoceros were mammals of the ice ages, and had ceased to exist with the ice ages: they inhabited Europe when it was tundra, and not afterwards. In fact such extinct mammals were typically found in diluvium as it was then called (distinctive gravel or boulder clay).
Given that the animals were associated with these strata, establishing the date of the strata could be by geological arguments, based on uniformity of stratigraphy; and so the animals' extinction was dated. An extinction can still strictly only be dated on assumptions, as evidence of absence; for a particular site, however, the argument can be from local extinction.
Neither Agassiz nor Buckland adopted the new views on the antiquity of man.
Boucher de Perthes had written up discoveries in the Somme valley in 1847. Joseph Prestwich and John Evans in April 1859, and Charles Lyell with others also in 1859, made field trips to the sites, and returned convinced that humans had coexisted with extinct mammals. In general and qualitative terms, Lyell felt the evidence established the "antiquity of man": that humans were much older than the traditional assumptions had made them.[36] His conclusions were shared by the Royal Society and other British learned institutions, as well as in France. It was this recognition of the early date of Acheulean handaxes that first established the scientific credibility of the deep antiquity of humans.[37]
This debate was concurrent with that over the book On the Origin of Species, published in 1859, and was evidently related; but was not one in which Charles Darwin initially made his own views public. Consolidation of the "antiquity of man" required more work, with stricter methods; and this proved possible over the next two decades. The discoveries of Boucher de Perthes therefore motivated further researches to try to repeat and confirm the findings at other sites. Significant in this were excavations by William Pengelly at Brixham Cavern, and with a systematic approach at Kents Cavern (1865–1880).[38] Another major project, which produced quicker findings, was that of Henry Christy and Édouard Lartet. Lartet in 1860 had published results from a cave near Massat (Ariège) claiming stone tool cuts on bones of extinct mammals, made when the bones were fresh.[39]
When the science was considered reasonably settled as to the existence of "Quaternary Man" (humans of the Pleistocene), there remained the issue as to whether man had existed in the Tertiary, a now obsolete term used for the preceding geological period. The debate on the antiquity of man resonated in the later debate over eoliths, which were supposed proof of the existence of man in the Pliocene (during the Neogene). In this case the sceptical view won out.[45]

other species or subspecies suggested
Human taxonomy is the classification of the human species within zoological taxonomy. The systematic genus, Homo, is designed to include both anatomically modern humans and extinct varieties of archaic humans. Current humans are classified as subspecies to Homo, differentiated, according to some, from the direct ancestor, Homo sapiens idaltu (with some other research instead classifying idaltu and current humans as belonging to the same subspecies[1][2][3]).
Since the introduction of systematic names in the 18th century, knowledge of human evolution has increased significantly, and a number of intermediate taxa have been proposed in the 20th and early 21st centuries. The most widely accepted taxonomy grouping takes the genus Homo as originating between two and three million years ago, divided into at least two species, archaic Homo erectus and modern Homo sapiens, with about a dozen further suggestions for species without universal recognition.
The genus Homo is placed in the tribe Hominini alongside Pan (chimpanzees). The two genera are estimated to have diverged over an extended time of hybridization, spanning roughly 10 to 6 million years ago, with possible admixture as late as 4 million years ago. A subtribe of uncertain validity, grouping archaic "pre-human" or "para-human" species younger than the Homo-Pan split, is Australopithecina (proposed in 1939).
A proposal by Wood and Richmond (2000) would introduce Hominina as a subtribe alongside Australopithecina, with Homo the only known genus within Hominina. Alternatively, following Cela-Conde and Ayala (2003), the "pre-human" or "proto-human" genera of Australopithecus, Ardipithecus, Praeanthropus, and possibly Sahelanthropus, may be placed on equal footing alongside the genus Homo. An even more extreme view rejects the division of Pan and Homo as separate genera, which based on the Principle of Priority would imply the reclassification of chimpanzees as Homo paniscus (or similar).[4]
Categorizing humans based on phenotypes is a socially controversial subject. Biologists originally classified races as subspecies, but contemporary anthropologists reject the concept of race as a useful tool to understanding humanity, and instead view humanity as a complex, interrelated genetic continuum. Taxonomy of the hominins continues to evolve.[5][6]
Human taxonomy on one hand involves the placement of humans within the taxonomy of the hominids (great apes), and on the other the division of archaic and modern humans into species and, if applicable, subspecies. Modern zoological taxonomy was developed by Carl Linnaeus during the 1730s to 1750s. He was the first to develop the idea that, like other biological entities, groups of people could too share taxonomic classifications.[7] He named the human species as Homo sapiens in 1758, as the only member species of the genus Homo, divided into several subspecies corresponding to the great races. The Latin noun homō (genitive hominis) means "human being". The systematic name Hominidae for the family of the great apes was introduced by John Edward Gray (1825).[8] Gray also supplied Hominini as the name of the tribe including both chimpanzees (genus Pan) and humans (genus Homo).
The discovery of the first extinct archaic human species from the fossil record dates to the mid 19th century: Homo neanderthalensis, classified in 1864. Since then, a number of other archaic species have been named, but there is no universal consensus as to their exact number. After the discovery of H. neanderthalensis, which even if "archaic" is recognizable as clearly human, late 19th to early 20th century anthropology for a time was occupied with finding the supposedly "missing link" between Homo and Pan. The "Piltdown Man" hoax of 1912 was the fraudulent presentation of such a transitional species. Since the mid-20th century, knowledge of the development of Hominini has become much more detailed, and taxonomical terminology has been altered a number of times to reflect this.
The introduction of Australopithecus as a third genus, alongside Homo and Pan, in the tribe Hominini is due to Raymond Dart (1925). Australopithecina as a subtribe containing Australopithecus as well as Paranthropus (Broom 1938) is a proposal by Gregory & Hellman (1939). More recently proposed additions to the Australopithecina subtribe include Ardipithecus (1995) and Kenyanthropus (2001). The position of Sahelanthropus (2002) relative to Australopithecina within Hominini is unclear. Cela-Conde and Ayala (2003) propose the recognition of Australopithecus, Ardipithecus, Praeanthropus, and Sahelanthropus (the latter incertae sedis) as separate genera.[9]
Other proposed genera, now mostly considered part of Homo, include:
Pithecanthropus (Dubois, 1894),
Protanthropus (Haeckel, 1895),
Sinanthropus (Black, 1927),
Cyphanthropus (Pycraft, 1928)
Africanthropus (Dreyer, 1935),[10]
Telanthropus (Broom & Anderson 1949),
Atlanthropus (Arambourg, 1954),
Tchadanthropus (Coppens, 1965).
The genus Homo has been taken to originate some two million years ago, since the discovery of stone tools in Olduvai Gorge, Tanzania, in the 1960s. Homo habilis (Leakey et al., 1964) would be the first "human" species (member of genus Homo) by definition, its type specimen being the OH 7 fossils. However, the discovery of more fossils of this type has opened up the debate on the delineation of H. habilis from Australopithecus. Especially, the LD 350-1 jawbone fossil discovered in 2013, dated to 2.8 Mya, has been argued as being transitional between the two.[11] It is also disputed whether H. habilis was the first hominin to use stone tools, as Australopithecus garhi, dated to c. 2.5 Mya, has been found along with stone tool implements.[12] Fossil KNM-ER 1470 (discovered in 1972, designated Pithecanthropus rudolfensis by Alekseyev 1978) is now seen as either a third early species of Homo (alongside H. habilis and H. erectus) at about 2 million years ago, or alternatively as transitional between Australopithecus and Homo.[13]
Wood and Richmond (2000) proposed that Gray's tribe Hominini ("hominins") be designated as comprising all species after the chimpanzee–human last common ancestor by definition, to the inclusion of Australopithecines and other possible pre-human or para-human species (such as Ardipithecus and Sahelanthropus) not known in Gray's time.[14] In this suggestion, the new subtribe of Hominina was to be designated as including the genus Homo exclusively, so that Hominini would have two subtribes, Australopithecina and Hominina, with the only known genus in Hominina being Homo. Orrorin (2001) has been proposed as a possible ancestor of Hominina but not Australopithecina.[15]
Designations alternative to Hominina have been proposed: Australopithecinae (Gregory & Hellman 1939) and Preanthropinae (Cela-Conde & Altaba 2002).[16][17][18]
At least a dozen species of Homo other than Homo sapiens have been proposed, with varying degrees of consensus. Homo erectus is widely recognized as the species directly ancestral to Homo sapiens.[citation needed] Most other proposed species are proposed as alternatively belonging to either Homo erectus or Homo sapiens as a subspecies. This concerns Homo ergaster in particular.[19][20] One proposal divides Homo erectus into an African and an Asian variety; the African is Homo ergaster, and the Asian is Homo erectus sensu stricto. (Inclusion of Homo ergaster with Asian Homo erectus is Homo erectus sensu lato.)[21] There appears to be a recent trend, with the availability of ever more difficult-to-classify fossils such as the Dmanisi skulls (2013) or Homo naledi fossils (2015) to subsume all archaic varieties under Homo erectus.[22][23][24]
The recognition or nonrecognition of subspecies of Homo sapiens has a complicated history. The rank of subspecies in zoology is introduced for convenience, and not by objective criteria, based on pragmatic consideration of factors such as geographic isolation and sexual selection. The informal taxonomic rank of race is variously considered equivalent or subordinate to the rank of subspecies, and the division of anatomically modern humans (H. sapiens) into subspecies is closely tied to the recognition of major racial groupings based on human genetic variation.
A subspecies cannot be recognized independently: a species will either be recognized as having no subspecies at all or at least two (including any that are extinct). Therefore, the designation of an extant subspecies Homo sapiens sapiens only makes sense if at least one other subspecies is recognized. H. s. sapiens is attributed to "Linnaeus (1758)" by the taxonomic Principle of Coordination.[44] During the 19th to mid-20th century, it was common practice to classify the major divisions of extant H. sapiens as subspecies, following Linnaeus (1758), who had recognized H. s. americanus, H. s. europaeus, H. s. asiaticus and H. s. afer as grouping the native populations of the Americas, West Eurasia, East Asia and Sub-Saharan Africa, respectively. Linnaeus also included H. s. ferus, for the "wild" form which he identified with feral children, and two other "wild" forms for reported specimens now considered very dubious (see cryptozoology), H. s. monstrosus and H. s. troglodytes.[45]
There were variations and additions to the categories of Linnaeus, such as H. s. tasmanianus for the native population of Australia.[46] Bory de St. Vincent in his Essai sur l'Homme (1825) extended Linnaeus's "racial" categories to as many as fifteen: Leiotrichi ("smooth-haired"): japeticus (with subraces), arabicus, iranicus, indicus, sinicus, hyperboreus, neptunianus, australasicus, columbicus, americanus, patagonicus; Oulotrichi ("crisp-haired"): aethiopicus, cafer, hottentotus, melaninus.[47] Similarly, Georges Vacher de Lapouge (1899) also had categories based on race, such as priscus, spelaeus (etc.).
Homo sapiens neanderthalensis was proposed by King (1864) as an alternative to Homo neanderthalensis.[48] There have been "taxonomic wars" over whether Neanderthals were a separate species since their discovery in the 1860s. Pääbo (2014) frames this as a debate that is unresolvable in principle, "since there is no definition of species perfectly describing the case."[49] Louis Lartet (1869) proposed Homo sapiens fossilis based on the Cro-Magnon fossils.
There are a number of proposals of extinct varieties of Homo sapiens made in the 20th century. Many of the original proposals were not using explicit trinomial nomenclature, even though they are still cited as valid synonyms of H. sapiens by Wilson & Reeder (2005).[50] These include: Homo grimaldii (Lapouge, 1906),
Homo aurignacensis hauseri (Klaatsch & Hauser, 1910),
Notanthropus eurafricanus (Sergi, 1911), 
Homo fossilis infrasp. proto-aethiopicus (Giuffrida-Ruggeri, 1915),
Telanthropus capensis (Broom, 1917),[51]
Homo wadjakensis (Dubois, 1921), 
Homo sapiens cro-magnonensis, Homo sapiens grimaldiensis (Gregory, 1921),
Homo drennani (Kleinschmidt, 1931),[52]
Homo galilensis (Joleaud, 1931) = Paleanthropus palestinus (McCown & Keith, 1932).[53]
Rightmire (1983) proposed Homo sapiens rhodesiensis.[54]
After World War II, the practice of dividing extant populations of Homo sapiens into subspecies declined. An early authority explicitly avoiding the division of H. sapiens into subspecies was Grzimeks Tierleben, published 1967–1972.[55]
A late example of an academic authority proposing that the human racial groups should be considered taxonomical subspecies is John Baker (1974).[56] The trinomial nomenclature Homo sapiens sapiens became popular for "modern humans" in the context of Neanderthals being considered a subspecies of H. sapiens in the second half of the 20th century. Derived from the convention, widespread in the 1980s, of considering two subspecies, H. s. neanderthalensis and H. s. sapiens, the explicit claim that "H. s. sapiens is the only extant human subspecies" appears in the early 1990s.[57]
Since the 2000s, the extinct Homo sapiens idaltu (White et al., 2003) has gained wide recognition as a subspecies of Homo sapiens, but even in this case there is a dissenting view arguing that "the skulls may not be distinctive enough to warrant a new subspecies name".[58] H. s. neanderthalensis and H. s. rhodesiensis continue to be considered separate species by some authorities, but the 2010s discovery of genetic evidence of archaic human admixture with modern humans has reopened the details of taxonomy of archaic humans.[59]
Homo erectus since its introduction in 1892 has been divided into numerous subspecies, many of them formerly considered individual species of Homo. None of these subspecies have universal consensus among paleontologists.

The Prehistory of The Far Side: A 10th Anniversary Exhibit[1] is a 1989 book chronicling the origin and evolution of The Far Side (including cartoonist Gary Larson's first strip, Nature's Way), giving inside information about the cartooning process and featuring a gallery of Larson's favorite Far Side cartoons from the 1980s.
Larson recounts his childhood by showing several pictures supposedly drawn by him when he was a child. (Example: A picture of a boy sitting on a tire in pitch blackness sports the caption "I believe this is my earliest memory of riding in the car when my family took our annual vacation.") He shows panels of Nature's Way and talks about his early struggles as a cartoonist before he established himself in the field. While on vacation from his regular job as an investigator for the local humane society ("to whom I never disclosed the fact that on the way to the job interview I ran over a dog"), he left his one and only copy of his portfolio at the headquarters of the San Francisco Chronicle, waiting several days until they decided to hire him. After returning home, he received a letter that his local newspaper had decided to discontinue Nature's Way. Larson says that if the letter had arrived a week earlier, "I never would have made the trip to San Francisco. The wind would definitely have gone out of my sails."
Larson explains the creative process he goes through in making cartoons, showing the original doodles in his sketchbook alongside the final published versions. Sometimes he makes large changes to his initial idea, other times the differences are more subtle. (Example: A sketch of "If Dogs Drove," showing canine drivers hanging their heads out the window, was changed to "When dogs go to work," which depicts a bus-full of dogs with their heads hanging out the window.) He discusses the reasons for the changes he makes, and in a few cases he admits that he now prefers an earlier version, or that the final version could have been changed further.
He also shows doodles that he never published, most of which he admits weren't that funny. ("Anyone who attaches more significance to them needs to get out more often.") He then shows several cartoons that were based on little short stories he wrote for a change of pace.
He devotes one section to mistakes that he made, or that editors made when altering his cartoons for one reason or another. These include incomplete drawings, scientific errors (like one featuring polar bears and penguins in the same habitat), typos in the caption, and the right caption set to the wrong cartoon (one time, a Dennis the Menace cartoon featuring children eating sandwiches was inadvertently—or maybe not so inadvertently—set to the Far Side caption, "Oh brother!...Not hamsters again!", while the Far Side cartoon got the Dennis caption—"Lucky thing I learned to make peanut butter sandwiches or we woulda starved to death by now."). In a separate section called "Subtle Things," he discusses the techniques he brings to the cartooning table.
Here Larson shows his cartoons that have provoked controversy, even outrage, and he reprints some of the indignant letters of protest. Some of his cartoons are thought to be in bad taste, as one featuring a couple of dogs playing "Tethercat." In such cases, he argues that readers either were oversensitive or misunderstood the cartoon. One time, a representative of the Jane Goodall Institute attacked a Far Side cartoon in which two chimpanzees are grooming when one finds a human hair and asks, "Conducting a little more 'research' with that Jane Goodall tramp?" Larson was ready to apologize to Goodall, until it came out that Goodall loved the cartoon and had no idea someone in her organization had complained.
A few of his cartoons were controversial simply because they were hard to understand. A particular example was "Cow Tools," which depicts a cow standing by a table on which rest four oddly shaped objects. Larson had found it funny that cows would be really bad at toolmaking. His mistake was making one of the tools resemble a saw, as it led people to believe they had to identify the other tools to understand some kind of deeper meaning. "'Off days' are a part of life," he writes, "whether you're a cartoonist, a neurosurgeon, or an air-traffic controller."
Larson also reprints a letter from an entomologist proposing to name a newly discovered species of chewing lice after Larson, Strigiphilus garylarsoni. The scientist explains that he did this "to honor the enormous contribution that my colleagues and I feel you have made to biology through your cartoons." The image of this insect also appears in the form of a geometric design on the inside front and back covers.
The next section shows a handful of cartoons that his editors rejected (or, in a few cases, that he never even bothered submitting), usually because they were considered in poor taste. Larson responds to everyone who's taken the trouble to complain, with a large print of a boy making a face.
Larson presents a gallery of his own personal favorite Far Side cartoons. "I contemplated making this last section a collection of what I consider the lousiest cartoons I've ever drawn, but space was limited."

A shrunken head is a severed and specially-prepared human head – often decreased to many times smaller than typical size – that is used for trophy, ritual, trade, or other purposes.
Headhunting is believed to have occurred in many regions of the world since time immemorial, but the practice of head shrinking has only been documented in the northwestern region of the Amazon rainforest.[1] Jivaroan peoples, which includes the Shuar, Achuar, Huambisa and Aguaruna tribes from Ecuador and Peru, are known to keep shrunken human heads.[citation needed] While many were probably made from the remains of these peoples, the Shuar people are the only culture in the world that practiced ritualistic head shrinking.[2]
Shuar people call a shrunken head a tsantsa,[3] also transliterated tzantza. Many tribe leaders would display their heads to scare enemies.
Shrunken heads are known for their mandibular prognathism, facial distortion, and shrinkage of the lateral sides of the forehead; these are artifacts of the shrinking process. Among the Shuar, the reduction of the heads was followed by a series of feasts centered on important rituals.[citation needed]
The process of creating a shrunken head begins with removing the skull from the neck. An incision is made on the back of the ear and all the skin and flesh is removed from the cranium. Red seeds are placed underneath the nostrils and the lips are sewn shut. The mouth is held together with three palm pins. Fat from the flesh of the head is removed. Then a wooden ball is placed under the flesh to keep the form. The flesh is then boiled in water that has been saturated with a number of herbs containing tannins. The head is then dried with hot rocks and sand while molding it to retain its human features. The skin is then rubbed with charcoal ash. Decorative beads may be added to the head.[4]
In the head shrinking tradition, it is believed that coating the skin in ash keeps the muisak, or avenging soul, from seeping out.
The practice of preparing shrunken heads originally had religious significance; shrinking the head of an enemy was believed to harness the spirit of that enemy and compel him to serve the shrinker. It was said to prevent the soul from avenging his death.[5]
Shuar believed in the existence of three fundamental spirits:
To block a Muisak spirit from using its powers, they severed their enemies' heads and shrank them. The process also served as a way of warning their enemies. Despite these precautions, the owner of the trophy did not keep it for long. Many heads were later used in religious ceremonies and feasts that celebrated the victories of the tribe. Accounts vary as to whether the heads were discarded or stored.[citation needed]
When Westerner curio collectors created an economic incentive for shrunken heads, there was a sharp increase in the rate of killings in an effort to supply tourists and collectors of ethnographic items.[6][7] The terms 'headhunting' and 'headhunting parties' come from this practice.
Guns were usually what the Shuar acquired in exchange for their shrunken heads, the rate being one gun per head.[citation needed] But weapons were not the only items exchanged. Around 1910, shrunken heads were being sold by a curio shop in Lima for one Peruvian gold pound, equal in value to a British gold sovereign.[8] In 1919, the price in Panama's curio shop for shrunken heads had risen to £5.[8] By the 1930s, when heads were freely exchanged, a shrunken head could be purchased for about 25 U.S. dollars. This was stopped when the Peruvian and Ecuadorian governments cooperated to outlaw head trafficking.[citation needed]
Also encouraged by this trade, people in Colombia and Panama unconnected to the Jívaros began to make counterfeit tsantsas. They used corpses from morgues, or the heads of monkeys or sloths. Some used goatskin. Kate Duncan wrote in 2001, "It has been estimated that about 80 percent of the tsantsas in private and museum hands are fraudulent", including almost all that are female or include an entire torso rather than just a head.[5]
Thor Heyerdahl recounts in The Kon-Tiki Expedition (1948) the various problems of getting into the Jívaro (Shuar) area in Ecuador to get balsa wood for his expedition raft. Local people would not guide his team into the jungle for fear of being killed and having their heads shrunk. In 1951 and 1952 sales of such items in London were being advertised in The Times; one example was priced at $250, a hundredfold appreciation since the early 20th century.[8]
In 1999, the National Museum of the American Indian repatriated the authentic shrunken heads in its collection to Ecuador.[5] Most other countries have also banned the trade. Currently, replica shrunken heads are manufactured as curios for the tourist trade. These are made from leather and animal hides formed to resemble the originals.[citation needed] In 2019 Mercer University repatriated a shrunken head from its collections, crediting the Native American Graves Protection and Repatriation Act as inspiration.[9]
In 2020, Oxford University's Pitt Rivers Museum removed its collection of shrunken heads after an ethical review begun in 2017, as part of an effort to decolonize its collections and avoid stereotyping.[10]
In the novel Moby-Dick, the character Queequeg sells shrunken heads and gives his last as a gift to the narrator, Ishmael, who subsequently sells it himself.
In the 1949 children's novel Amazon Adventure by Willard Price, the character John Hunt buys a shrunken head for the American Museum of Natural History from a Jivaro chief, who explains the shrinking process. The scene mirrors Price's own experience with the Jivaro, described in his 1948 travel book Roving South.
In 1955, Disneyland opened its Jungle Cruise ride. Until 2021, the attraction featured a trader selling shrunken heads (three of his for one of yours).[11][12]
In 1975, Whiting (a Milton Bradley company) released Vincent Price's Shrunken Head Apple Sculpture Kit.[13]
In the 1946 movie The Devil's Mask, a crashed plane with a shrunken head aboard is the only clue to a mystery involving a secret code.
The 1988 movie Beetlejuice featured a ghost of a hunter whose head had been shrunken. At the end of the movie, the title character suffers the same fate. The film's 2024 sequel, Beetlejuice Beetlejuice, features the return of the hunter with the shrunken head, named Bob, alongside many other ghosts with shrunken heads now employed at Betelgeuse's personal call centre.
One of the North American television commercials for the 1990 video game Dr. Mario featured head shrinking, as well as a cover of the song Witch Doctor with slightly different lyrics.[14]
The Goosebumps book, How I Got My Shrunken Head, released in 1996, is about a boy who gets a shrunken head from his aunt that gives him jungle powers.
In the 2004 film adaptation of Harry Potter and the Prisoner of Azkaban, Lenny Henry voices Dre Head, a Jamaican accented shrunken head on the magical Knight Bus. The same film features three more shrunken heads, voiced by Brian Bowles and Peter Serafinowicz, inside the wizard pub The Three Broomsticks.[15]
Both Pirates of the Caribbean: Dead Man's Chest (2006) and Pirates of the Caribbean: At World's End (2007) feature shrunken heads.[16]

The best known cultural archaeological discoveries from the prehistoric period on the territory of modern-day Serbia are the Starčevo and Vinča cultures[1] dating back to 6400–6200 BC.
Serbia's strategic location between two continents has subjected it to invasions by many nations.
The territory of present-day Serbia is situated in the central region of Balkan peninsula. It lies on one of the major migration routes connecting the Middle East with central Europe. Traces of human population in this area go back to at least to 400 000 (see Sićevo Gorge). Morava-Vardar corridor has seen regular waves of migrations throughout its history. It is one of the probable paths of original human expansion into Europe.
This region was home to several important Mesolithic and Paleolithic cultures. Some of the oldest traces of agriculture in Europe are found in this area. Its fertile river valleys are the probable conduit through which agriculture has spread from the Middle East and Asia minor to central Europe.
Archeological sites in present-day Serbia contain some of the earliest examples of metallurgy, especially copper processing.
During the Bronze Age this area has seen several large migrations of various Indo-European groups, some of whom become permanently settled. During the Iron Age lower Morava valley become a Celto-Thraco-Illyrian interaction zone.
The period of prehistory in this area ends with the advance of reliable Greek and later Roman written sources, especially after the area was incorporated into Roman Empire as the province of Moesia.
Paleolithic archeological evidence from the territory of present-day Serbia remain surprisingly scarce.
Two skeletons of Mammoths have been found in Serbia, the first in Kikinda in 1996, the second in Viminacium (Kostolac), June 2009, 1,5 million year old (mammuthus meridionalis)[2][3] thus one of the oldest mammoths of Europe.
A fragment of a human jaw, was found in Sićevo (Mala Balanica) and believed to be up to 525,000–397,000 years old.[4][5][6] Many archaeological sites have been destroyed because of floodings.
Lepenski Vir is a Mesolithic archaeological site of the Iron Gates culture, near Donji Milanovac, dating to 7000 BC with the peak of culture in 5300–4800 BC. Numerous piscine sculptures and peculiar architecture are testimony to a rich social and religious life led by the inhabitants and the high cultural level of these early Europeans. It is assumed that the people of Lepenski Vir culture represent the descendants of the early European population of the Brno-Předmost hunter gatherer culture from the end of the last ice age. Archeological evidence of human habitation of the surrounding caves dates back to around 20,000 BC. The first settlement on the low plateau dates back to 7000 BC, a time when the climate became significantly warmer. Seven successive settlements were discovered on the Lepenski Vir site, with the remains of 136 residential and sacral buildings dating from 6500 to 5500 BC. Among other finds are the many characteristic sculptures, The sculptures can be separated into two distinct categories, one with simple geometric patterns and the other representing humanoid figures. The latter are the most interesting. All of these figural sculptures were modelled in a naturalistic and strongly expressionistic manner. Only the head and face of the human figures were modelled realistically, with strong brow arches, an elongated nose, and a wide, fish-like mouth. Hair, beard, arms and hands can be seen on some of the figures in a stylized form. Many fish-like features can be noticed. Along with the position which these sculptures had in the house shrine, they suggest a connection with river gods.
The Neolithic Starčevo and Vinča cultures existed in or near Belgrade and dominated the Balkans (as well as parts of Central Europe and Asia Minor) about 8,500 years ago.[7][8] Some scholars believe that the prehistoric Vinča signs represent one of the earliest known forms of Writing systems (dating to 6000–4000 BC).[9]
Some of the first evidence of human metallurgy was found, dated to the 5th and 6th millennium BC, in the Vinča culture archaeological sites such as Majdanpek, Jarmovac, Pločnik and Rudna Glava.[10]
The oldest copper axe in Europe was found at Prokuplje, that indicated that human use of metals (Metallurgy) started in Europe around 7,500 years ago (~5,500 BC in the Vincha culture) millennia earlier than Ötzi's axe (previously oldest metalworking)[11][12]
The start of Bronze Age in northern Serbia is marked by the Indo-European invasion, represented by Vučedol culture centered in the region of Syrmia. This culture has been linked with Proto-Illyrian and Mycenaean Greece.
This period spans over a time from the end of the Bronze Age (start of the Iron Age) until the conquering of the Balkans in 168–75 BC (Roman Serbia). The Thracians, most notably Triballi dominated Serbia before the Illyrian migration in the southwest.[13] Greeks colonized the south in the 4th century BC, the northernmost point of the empire of Alexander the Great being the town of Kale.[14]
The tribes of Autariatae and the Celtic Scordisci are thought to have merged into one in the Lower Morava valley, Serbia, after 313 BC, since excavations show that the two groups made burials at the same exact grave field in Pecine, near Kostolac.[15] Nine graves of Autariatae dating to 4th century BC and scattered Autariatae and Celtic graves around these earlier graves show that the two groups mixed rather than made war[16] and this resulted in the lower Morava valley becoming a Celto-Thraco-Illyrian interaction zone.[17]
In 279 BC, after the Gallic invasion of the Balkans, the Scordisci tribal state[18] was formed in Serbia. They took the strategic hill fort of Singidunum, modern Belgrade and built Taurunum (Zemun). They subjugated most of the tribes that came in their way, Illyrians, West Thracians and Paeonians and were at one time the most powerful tribe of the Balkans. From 141 BC and onwards they are fighting with the Romans, they are defeated in 135 BC, victorious in 118 BC against Sextus Pompey and again in 114 BC against Gaius Porcius Cato and then defeated in 107 BC, but are still holding a significant part of Pannonia. They invade Macedonia with the Dardani and Maedi, coming as far as Delphi, plundering the temple but are eventually defeated and driven across the Danube in 88 BC. There they are subjugated by the Dacians under Burebista in 56–50 BC, and finally, in 15 BC they are Roman subjects, beginning their Romanization.
In parts of Moesia (northeast Serbia) the Celtic Scordisci and Thracians lived besides each other, evident in the archeological findings of pits and treasures, spanning from 3rd to 1st centuries BC.[19]

The gibbon–human last common ancestor is the last common ancestor of the superfamily Hominoidea (apes), dating to the split of the Hylobatidae (gibbons) and Hominidae (great apes) families. It is dated to the early Miocene, roughly 20 to 16 million years ago.[1]
Hylobatidae has four gibbon genera (Hylobates with 9 species, Hoolock with 3 species, Nomascus with 7 species and Symphalangus with only 1 species) [1][2] containing 20 different species. Hominidae has two subfamilies, Ponginae (orangutans) and Homininae (African apes, including the human lineage).
A 2014 whole-genome molecular dating analysis indicated that the gibbon lineage diverged from that of great apes (Hominidae) around 17 million years ago (16.8±0.9 Mya), based on certain assumptions about the generation time and mutation rate.[1]
The extinct Bunopithecus sericus was a gibbon or gibbon-like ape.[3] Adaptive divergence associated with chromosomal rearrangements led to rapid radiation of the four genera within the Hylobatidae lineage between about 7 to 5 Mya. Each genus comprises a distinct, well-delineated lineage, but the sequence and timing of divergences among these genera have been hard to resolve due to radiative speciations and extensive incomplete lineage sorting.[1][2]  Recent coalescent-based analysis of both the coding and noncoding parts of the genome suggests that the most likely sequence of species divergences in the Hylobatidae lineage is (Hylobates, (Nomascus, (Hoolock, Symphalangus))).[2] Though other studies have also found different topology. [4]
Hylobates
Nomascus
Hoolock
Symphalangus
Because fossils are so scarce, it is not clear what GHLCA looked like. A 2019 study found that the species was "smaller than previously thought" and about the size of a gibbon.[5]
It is unknown whether GHLCA was tailless and had a broad, flat rib cage like their descendants.[6]: 193  But it is likely that it was a small animal, probably weighing only 12 kilograms (26 lb). This contradicts previous theories that they were the size of chimpanzees and that apes moved to hang and to swing from trees to get off the ground because they were too big. There might have been an arms race in brachiating to reach the best food. Also, the Hominidae, which came later, were smaller than their ancestors, which is contrary to normal evolution where animals get larger over their evolutionary development.[5]

Interbreeding between archaic and modern humans occurred during the Middle Paleolithic and early Upper Paleolithic. The interbreeding happened in several independent events that included Neanderthals and Denisovans, as well as several unidentified hominins.[2]
In Europe, Asia and North Africa, interbreeding between archaic humans and modern humans took place several times. The introgression events into modern humans are estimated to have happened about 47,000–65,000 years ago with Neanderthals and about 44,000–54,000 years ago with Denisovans.
Neanderthal-derived DNA has been found in the genomes of most or possibly all contemporary populations, varying noticeably by region. It accounts for 1–4% of modern genomes for people outside Sub-Saharan Africa, although estimates vary, and either none or up to 0.3% for those in Sub-Saharan Africa.[3] Cushitic and Semitic speaking populations from the Horn of Africa (such as Ethiopians), who derive a large portion of their ancestry from West Eurasians, have ~1% Neanderthal-derived DNA.[4]
Neanderthal-derived DNA is highest in East Asians, intermediate in Europeans, and lower in Southeast Asians.[5] According to some research, it is also lower in Melanesians and Polynesians compared to both East Asians and Europeans.[5] However, other research finds higher Neanderthal admixture in Melanesians, as well as in Native Americans, than in Europeans (though not higher than in East Asians).[6]
Denisovan-derived ancestry is largely absent from modern populations in Africa, Western Asia and Europe. The highest rates, by far, of Denisovan admixture have been found in Oceanian and some Southeast Asian populations. An estimated 4–6% of the genome of modern Melanesians is derived from Denisovans, but the highest amounts detected thus far are found in the Negrito populations of the Philippines. While some Southeast Asian Negrito populations carry Denisovan admixture, others, such as the Andamanese, have none. In addition, low traces of Denisovan-derived ancestry have been found in mainland Asia, with an elevated Denisovan ancestry in South Asian populations compared to other mainland populations.[7]
In Africa, archaic alleles consistent with several independent admixture events in the continent have been found. It is currently unknown who these archaic African hominins were.[5] A 2020 paper found that "despite their very low levels or absence of archaic ancestry, African populations share many Neanderthal and Denisovan variants that are absent from Eurasia, reflecting how a larger proportion of the ancestral human variation has been maintained in Africa."[8]
A 2016 paper in the journal Evolutionary Biology argued that introgression of DNA from other lineages enabled humanity to migrate to, and succeed in, numerous new environments, with the resulting hybridization being an essential force in the emergence of modern humans.[9]
In December 2023, scientists reported that genes inherited by modern humans from Neanderthals and Denisovans may biologically influence the daily routine of modern humans.[10]
On 7 May 2010, following the genome sequencing of three Vindija Neanderthals, a draft sequence of the Neanderthal genome was published and revealed that Neanderthals shared more alleles with Eurasian populations (e.g. French, Han Chinese, and Papua New Guinean) than with sub-Saharan African populations (e.g. Yoruba and San).[11] According to the authors Green et al. (2010), the observed excess of genetic similarity is best explained by recent gene flow from Neanderthals to modern humans after the migration out of Africa.[11] They estimated the proportion of Neanderthal-derived ancestry to be 1–4% of the Eurasian genome.[11] Durand et al. (2011) estimated 1–6% Neanderthal ancestry in non-Africans.[12] Prüfer et al. (2013) estimated the proportion to be 1.5–2.1% for non-Africans.[13] Lohse and Frantz (2014) infer a higher rate of 3.4–7.3% in Eurasia.[14] In 2017, Prüfer et al. revised their estimate to 1.8–2.6% for non-Africans outside Oceania.[15]
According to a later study by Chen et al. (2020), Africans (specifically, the 1000 Genomes African populations) also have Neanderthal admixture,[16] with this Neanderthal admixture in African individuals accounting for 17 megabases,[16] which is 0.3% of their genome.[3] According to the authors, Africans gained their Neanderthal admixture predominantly from a back-migration by peoples (modern humans carrying Neanderthal admixture) that had diverged from ancestral Europeans (postdating the split between East Asians and Europeans).[16] This back-migration is proposed to have happened about 20,000 years ago.[3] However, some scientists, such as geneticist David Reich, have doubts about how extensive the flow of DNA back to Africa would have been, finding the signal of Neanderthal admixture "really weak".[17]
It has been found that 50% of the Neanderthal genome is present among people in India,[18] and 41% has been found in Icelanders.[19]
Previously it was found that about 20% of the Neanderthal genome was found in modern Eurasians,[20] but the figure was also estimated at a third.[21] A 2023 study found an introgression from modern humans to Neanderthals around 250,000 years ago, and estimated that roughly 6% of the Altai Neanderthal genome was inherited from modern humans.[22]
A higher Neanderthal admixture was found in East Asians than in Europeans,[20][23][24][25][26] which is estimated to be about 20% more introgression into East Asians.[20][23][26] This could possibly be explained by the occurrence of further admixture events in the early ancestors of East Asians after the separation of Europeans and East Asians,[5][20][23][24][26] dilution of Neanderthal ancestry in Europeans by populations with low Neanderthal ancestry from later migrations,[5][23][26] or reduced efficacy of purifying selection in the ancestors of East Asians, due smaller effective population sizes as they migrated to East Asia.[5][25][26] Studies simulating admixture models indicate that a reduced efficacy of purifying selection against Neanderthal alleles in East Asians could not account for the greater proportion of Neanderthal ancestry of East Asians, thus favoring more-complex models involving additional pulses of admixture between Neanderthals and the ancestors of East Asians.[27][28] Such models show a pulse to ancestral Eurasians, followed by separation and an additional pulse to ancestral East Asians.[5] It is observed that there is a small but significant variation of Neanderthal admixture rates within European populations, but no significant variation within East Asian populations.[20] Prüfer et al. (2017) remarked that East Asians carry more Neanderthal DNA (2.3–2.6%) than Western Eurasians (1.8–2.4%).[15]
It was later determined by Chen et al. (2020) that East Asians have 8% more Neanderthal ancestry, revised from the previous reports of 20% more Neanderthal ancestry, compared to Europeans.[16] This stems from the fact that Neanderthal ancestry shared with Africans had been masked, because Africans were thought to have no Neanderthal admixture and were therefore used as reference samples.[16] Thus, any overlap in Neanderthal admixture with Africans resulted in an underestimation of Neanderthal admixture in non-Africans and especially in Europeans.[16] The authors give a single pulse of Neanderthal admixture after the out-of-Africa dispersal as the most parsimonious explanation for the enrichment in East Asians, but they add that variation in Neanderthal ancestry may also be attributed to dilution to account for the now-more-modest differences found.[16] As a proportion of the total amount of Neanderthal sequence for each population, 7.2% of the sequence in Europeans is shared exclusively with Africans, while 2% of the sequence in East Asians is shared exclusively with Africans.[16]
Genomic analysis suggests that there is a global division in Neanderthal introgression between sub-Saharan African populations and other modern human groups (including North Africans) rather than between African and non-African populations.[29] North African groups share a similar excess of derived alleles with Neanderthals as do non-African populations, whereas sub-Saharan African groups are the only modern human populations that generally did not experience Neanderthal admixture.[30] The Neanderthal genetic signal among North African populations was found to vary depending on the relative quantity of North African, European, Near Eastern and sub-Saharan ancestry. Using F4 ancestry ratio statistical analysis, the Neanderthal inferred admixture was observed to be highest among the North African populations with highest North African ancestry such as Tunisian Berbers, where it was at the same level or even higher than that of Eurasian populations (100–138%); high among North African populations carrying greater European or Near Eastern admixture, such as groups in North Morocco and Egypt (~60–70%); and lowest among North African populations with greater Sub-Saharan admixture, such as in South Morocco (20%).[31] Quinto et al. (2012) therefore postulate that the presence of this Neanderthal genetic signal in Africa is not due to recent gene flow from Near Eastern or European populations since it is higher among populations bearing indigenous pre-Neolithic North African ancestry.[32] Low but significant rates of Neanderthal admixture has also been observed for the Maasai of East Africa.[33] After identifying African and non-African ancestry among the Maasai, it can be concluded that recent non-African modern human (post-Neanderthal) gene flow was the source of the contribution since around an estimated 30% of the Maasai genome can be traced to non-African introgression from about 100 generations ago.[24]
Presenting a high-quality genome sequence of a female Altai Neanderthal, it has been found that the Neanderthal component in non-African modern humans is more related to the Mezmaiskaya Neanderthal (North Caucasus) than to the Altai Neanderthal (Siberia) or the Vindija Neanderthals (Croatia).[13] By high-coverage sequencing the genome of a 50,000-year-old female Vindija Neanderthal fragment, it was later found that the Vindija and Mezmaiskaya Neanderthals did not seem to differ in the extent of their allele-sharing with modern humans.[15] In this case, it was also found that the Neanderthal component in non-African modern humans is more closely related to the Vindija and Mezmaiskaya Neanderthals than to the Altai Neanderthal.[15] These results suggest that a majority of the admixture into modern humans came from Neanderthal populations that had diverged (about 80–100kya) from the Vindija and Mezmaiskaya Neanderthal lineages before the latter two diverged from each other.[15]
Analysis of chromosome 21 of the Altai, El Sidrón (Spain), and Vindija Neanderthals indicates that of these three lineages, only the El Sidrón and Vindija Neanderthals display significant rates of gene flow (0.3–2.6%) into modern humans, suggesting that the El Sidrón and Vindija Neanderthals are more closely related than the Altai Neanderthal to the Neanderthals that interbred with modern humans about 47,000–65,000 years ago.[35] Conversely, significant rates of modern human gene flow into Neanderthals occurred—of the three examined lineages—for only the Altai Neanderthal (0.1–2.1%), suggesting that modern human gene flow into Neanderthals mainly took place after the separation of the Altai Neanderthals from the El Sidrón and Vindija Neanderthals that occurred roughly 110,000 years ago.[35] The findings show that the source of modern human gene flow into Neanderthals originated from a population of early modern humans from about 100,000 years ago, predating the out-of-Africa migration of the modern human ancestors of present-day non-Africans.[35]
No evidence of Neanderthal mitochondrial DNA has been found in modern humans.[36][37][38] This suggests that successful Neanderthal admixture happened in pairings with Neanderthal males and modern human females.[39][40] Possible hypotheses are that Neanderthal mitochondrial DNA had detrimental mutations that led to the extinction of carriers, that the hybrid offspring of Neanderthal mothers were raised in Neanderthal groups and became extinct with them, or that female Neanderthals and male Sapiens did not produce fertile offspring.[39] However, the hypothesized incompatibility between Neanderthals and modern humans is contested by findings that suggest that the Y chromosome of Neanderthals was replaced by an extinct lineage of the modern human Y chromosome, which introgressed into Neanderthals between 100,000 and 370,000 years ago.[41] Furthermore, the study concludes that the replacement of the Y chromosomes and mitochondrial DNA in Neanderthals after gene flow from modern humans is highly plausible given the increased genetic load in Neanderthals relative to modern humans.[41]
As shown in an interbreeding model produced by Neves and Serva (2012), the Neanderthal admixture in modern humans may have been caused by a very low rate of interbreeding between modern humans and Neanderthals, with the exchange of one pair of individuals between the two populations in about every 77 generations.[42] This low rate of interbreeding would account for the absence of Neanderthal mitochondrial DNA from the modern human gene pool as found in earlier studies, as the model estimates a probability of only 7% for a Neanderthal origin of both mitochondrial DNA and Y chromosome in modern humans.[42]
There are large genomic regions with strongly reduced Neanderthal contribution in modern humans due to negative selection,[20][25] partly caused by hybrid male infertility.[25] These regions were most-pronounced on the X chromosome, with fivefold lower Neanderthal ancestry compared to autosomes.[5][25] They also contained relatively high numbers of genes specific to testes.[25] This means that modern humans have relatively few Neanderthal genes that are located on the X chromosome or expressed in the testes, suggesting male infertility as a probable cause.[25] It may be partly affected by hemizygosity of X chromosome genes in males.[5]
Deserts of Neanderthal sequences may also be caused by genetic drift involving intense bottlenecks in the modern human population and background selection as a result of strong selection against deleterious Neanderthal alleles.[5] The overlap of many deserts of Neanderthal and Denisovan sequences suggests that repeated loss of archaic DNA occur at specific loci.[5]
It has also been shown that Neanderthal ancestry has been selected against in conserved biological pathways, such as RNA processing.[25]
Consistent with the hypothesis that purifying selection has reduced Neanderthal contribution in present-day modern human genomes, Upper Paleolithic Eurasian modern humans (such as the Tianyuan modern human) carry more Neanderthal DNA (about 4–5%) than present-day Eurasian modern humans (about 1–2%).[43]
Rates of selection against Neanderthal sequences varied for European and Asian populations.[5]
In Eurasia, modern humans have adaptive sequences introgressed from archaic humans, which provided a source of advantageous genetic variants that are adapted to local environments and a reservoir for additional genetic variation.[5] Adaptive introgression from Neanderthals has targeted genes involved with keratin filaments, sugar metabolism, muscle contraction, body fat distribution, enamel thickness, and oocyte meiosis, as well as brain size and functioning.[44] There are signals of positive selection, as the result of adaptation to diverse habitats, in genes involved with variation in skin pigmentation and hair morphology.[44] In the immune system, introgressed variants have heavily contributed to the diversity of immune genes, of which there's an enrichment of introgressed alleles that suggest a strong positive selection.[44]
Genes affecting keratin were found to have been introgressed from Neanderthals into modern humans (shown in East Asians and Europeans), suggesting that these genes gave a morphological adaptation in skin and hair to modern humans to cope with non-African environments.[20][25] This is likewise for several genes involved in medical-relevant phenotypes, such as those affecting systemic lupus erythematosus, primary biliary cirrhosis, Crohn's disease, optic disk size, smoking behavior, interleukin 18 levels, and diabetes mellitus type 2.[25]
Researchers found Neanderthal introgression of 18 genes—several of which are related to UV-light adaptation—within the chromosome 3p21.31 region (HYAL region) of East Asians.[45] The introgressive haplotypes were positively selected in only East Asian populations, rising steadily from 45,000 years BP until a sudden increase of growth rate around 5,000 to 3,500 years BP.[45] They occur at very high frequencies among East Asian populations in contrast to other Eurasian populations (e.g. European and South Asian populations).[45] The findings also suggests that this Neanderthal introgression occurred within the ancestral population shared by East Asians and Native Americans.[45]
Evans et al. (2006) had previously suggested that a group of alleles collectively known as haplogroup D of microcephalin, a critical regulatory gene for brain volume, originated from an archaic human population.[46] The results show that haplogroup D introgressed 37,000 years ago (based on the coalescence age of derived D alleles) into modern humans from an archaic human population that separated 1.1 million years ago (based on the separation time between D and non-D alleles), consistent with the period when Neanderthals and modern humans co-existed and diverged respectively.[46] The high frequency of the D haplogroup (70%) suggests that it was positively selected for in modern humans.[46] The distribution of the D allele of microcephalin is high outside Africa but low in sub-Saharan Africa, which further suggests that the admixture event happened in archaic Eurasian populations.[46] This distribution difference between Africa and Eurasia suggests that the D allele originated from Neanderthals according to Lari et al. (2010), but they found that a Neanderthal individual from the Mezzena Rockshelter (Monti Lessini, Italy) was homozygous for an ancestral allele of microcephalin, thus providing no support that Neanderthals contributed the D allele to modern humans and also not excluding the possibility of a Neanderthal origin of the D allele.[47] Green et al. (2010), having analyzed the Vindija Neanderthals, also could not confirm a Neanderthal origin of haplogroup D of the microcephalin gene.[11]
It has been found that HLA-A*02, A*26/*66, B*07, B*51, C*07:02, and C*16:02 of the immune system were contributed from Neanderthals to modern humans.[48] After migrating out of Africa, modern humans encountered and interbred with archaic humans, which was advantageous for modern humans in rapidly restoring HLA diversity and acquiring new HLA variants that are better adapted to local pathogens.[48]
It is found that introgressed Neanderthal genes exhibit cis-regulatory effects in modern humans, contributing to the genomic complexity and phenotype variation of modern humans.[49] Looking at heterozygous individuals (carrying both Neanderthal and modern human versions of a gene), the allele-specific expression of introgressed Neanderthal alleles was found to be significantly lower in the brain and testes relative to other tissues.[5][49] In the brain, this was most pronounced at the cerebellum and basal ganglia.[49] This downregulation suggests that modern humans and Neanderthals possibly experienced a relative higher rate of divergence in these specific tissues.[49]
Furthermore, correlating the genotypes of introgressed Neanderthal alleles with the expression of nearby genes, it is found that archaic alleles contribute proportionally more to variation in expression than nonarchaic alleles.[5] Neanderthal alleles affect expression of the immune genes OAS1/2/3 and TLR1/6/10, which can be specific to cell type and is influenced by environmental stimuli.[5]
Studying the high-coverage female Vindija Neanderthal genome, Prüfer et al. (2017) identified several Neanderthal-derived gene variants, including those that affect levels of LDL cholesterol and vitamin D, and that influence eating disorders, visceral fat accumulation, rheumatoid arthritis, schizophrenia, as well as responses to antipsychotic drugs.[15]
Examining European modern humans in regards to the Altai Neanderthal genome in high-coverage, results show that Neanderthal admixture is associated with several changes in cranium and underlying brain morphology, suggesting changes in neurological function through Neanderthal-derived genetic variation.[50] Neanderthal admixture is associated with an expansion of the posterolateral area of the modern human skull, extending from the occipital and inferior parietal bones to bilateral temporal locales.[50] In regards to modern human brain morphology, Neanderthal admixture is positively correlated with an increase in sulcal depth for the right intraparietal sulcus and an increase in cortical complexity for the early visual cortex of the left hemisphere.[50] Neanderthal admixture is also positively correlated with an increase in white and gray matter volume localized to the right parietal region adjacent to the right intraparietal sulcus.[50] In the area overlapping the primary visual cortex gyrification in the left hemisphere, Neanderthal admixture is positively correlated with gray matter volume.[50] The results also show evidence for a negative correlation between Neanderthal admixture and white matter volume in the orbitofrontal cortex.[50]
In Papuans, Neanderthal genetic variants are found in highest frequency in genes expressed in the brain, whereas Denisovan DNA has the highest frequency in genes expressed in bones and other tissues.[51]
A Neanderthal allele inherited by modern humans, SNP rs3917862, is associated with hypercoagulability. This can be harmful, but women lacking the allele are 0.1% more likely to die in childbirth.[52]
In December 2023, scientists reported that genes inherited by modern humans from Neanderthals and Denisovans may biologically influence the daily routine of modern humans.[10]
Although less parsimonious than recent gene flow, the observation may have been due to ancient population sub-structure in Africa, causing incomplete genetic homogenization within modern humans when Neanderthals diverged while early ancestors of Eurasians were still more closely related to Neanderthals than those of Africans were to Neanderthals.[11] On the basis of allele frequency spectrum, it was shown that the recent admixture model had the best fit to the results while the ancient population sub-structure model had no fit—demonstrating that the best model was a recent admixture event that was preceded by a bottleneck event among modern humans – thus confirming recent admixture as the most parsimonious and plausible explanation for the observed excess of genetic similarities between modern non-African humans and Neanderthals.[53] On the basis of linkage disequilibrium patterns, a recent admixture event is likewise confirmed by the data.[54] From the extent of linkage disequilibrium, it was estimated that the last Neanderthal gene flow into early ancestors of Europeans occurred 47,000–65,000 years BP.[54] In conjunction with archaeological and fossil evidence, the gene flow is thought likely to have occurred somewhere in Western Eurasia, possibly the Middle East.[54] Through another approach—using one genome each of a Neanderthal, Eurasian, African, and chimpanzee (outgroup), and dividing it into non-recombining short sequence blocks—to estimate genome-wide maximum-likelihood under different models, an ancient population sub-structure in Africa was ruled out and a Neanderthal admixture event was confirmed.[14]
The early Upper Paleolithic burial remains of a modern human child from Abrigo do Lagar Velho (Portugal) features traits that indicate Neanderthal interbreeding with modern humans dispersing into Iberia.[55] Considering the dating of the burial remains (24,500 years BP) and the persistence of Neanderthal traits long after the transitional period from a Neanderthal to a modern human population in Iberia (28,000–30,000 years BP), the child may have been a descendant of an already heavily admixed population.[55]
The remains of an early Upper Paleolithic modern human from Peștera Muierilor (Romania) of 35,000 years BP shows a morphological pattern of European early modern humans, but possesses archaic or Neanderthal features, suggesting European early modern humans interbreeding with Neanderthals.[56] These features include a large interorbital breadth, a relatively flat superciliary arches, a prominent occipital bun, an asymmetrical and shallow mandibular notch shape, a high mandibular coronoid processus, the relative perpendicular mandibular condyle to notch crest position, and a narrow scapular glenoid fossa.[56]
The early modern human Oase 1 mandible from Peștera cu Oase (Romania) of 34,000–36,000 14C years BP presents a mosaic of modern, archaic, and possible Neanderthal features.[58] It displays a lingual bridging of the mandibular foramen, not present in earlier humans except Neanderthals of the late Middle and Late Pleistocene, thus suggesting affinity with Neanderthals.[58] Concluding from the Oase 1 mandible, there was apparently a significant craniofacial change of early modern humans from at least Europe, possibly due to some degree of admixture with Neanderthals.[58]
The earliest (before about 33 ka BP) European modern humans and the subsequent (Middle Upper Paleolithic) Gravettians, falling anatomically largely in line with the earliest (Middle Paleolithic) African modern humans, also show traits that are distinctively Neanderthal, suggesting that a solely Middle Paleolithic modern human ancestry was unlikely for European early modern humans.[59]
Manot 1, a partial calvarium of a modern human that was recently discovered at the Manot Cave (Western Galilee, Israel) and dated to 54.7±5.5 kyr BP, represents the first fossil evidence from the period when modern humans successfully migrated out of Africa and colonized Eurasia.[60] It also provides the first fossil evidence that modern humans inhabited the southern Levant during the Middle to Upper Palaeolithic interface, contemporaneously with the Neanderthals and close to the probable interbreeding event.[60] The morphological features suggest that the Manot population may be closely related to or may have given rise to the first modern humans who later successfully colonized Europe to establish early Upper Palaeolithic populations.[60]
The interbreeding has been discussed ever since the discovery of Neanderthal remains in the 19th century, though earlier writers believed that Neanderthals were a direct ancestor of modern humans. Thomas Huxley suggested that many Europeans bore traces of Neanderthal ancestry, but associated Neanderthal characteristics with primitivism, writing that since they "belong to a stage in the development of the human species, antecedent to the differentiation of any of the existing races, we may expect to find them in the lowest of these races, all over the world, and in the early stages of all races".[61]
Until the early 1950s, most scholars thought Neanderthals were not in the ancestry of living humans.[62][63] Nevertheless, Hans Peder Steensby proposed interbreeding in 1907 in the article Race studies in Denmark. He strongly emphasised that all living humans are of mixed origins.[64] He held that this would best fit observations, and challenged the widespread idea that Neanderthals were ape-like or inferior. Basing his argument primarily on cranial data, he noted that the Danes, like the Frisians and the Dutch, exhibit some Neanderthaloid characteristics, and felt it was reasonable to "assume something was inherited" and that Neanderthals "are among our ancestors".
Carleton Stevens Coon in 1962 found it likely, based upon evidence from cranial data and material culture, that Neanderthal and Upper Paleolithic peoples either interbred or that the newcomers reworked Neanderthal implements "into their own kind of tools".[65]
By the early 2000s, the majority of scholars supported the Out of Africa hypothesis,[66][67] according to which anatomically modern humans left Africa about 50,000 years ago and replaced Neanderthals with little or no interbreeding.
Yet some scholars still argued for hybridisation with Neanderthals. The most vocal proponent of the hybridisation hypothesis was Erik Trinkaus of Washington University in St. Louis.[68] Trinkaus claimed various fossils as products of hybridised populations, including the skeleton of a child found at Lagar Velho in Portugal[69][70][71] and the Peștera Muierii skeletons from Romania.[56]
It has been shown that Melanesians (e.g. Papua New Guinean and Bougainville Islander) share relatively more alleles with Denisovans when compared to other Eurasian-derived populations and Africans.[72] It is estimated that 4% to 6% of the genome in Melanesians derives from Denisovans, while no Eurasians or Africans displayed contributions of the Denisovan genes.[72] It has been observed that Denisovans contributed genes to Melanesians but not to East Asians, indicating that there was interaction between the early ancestors of Melanesians with Denisovans but that this interaction did not take place in the regions near southern Siberia, where as-of-yet the only Denisovan remains have been found.[72] In addition, Aboriginal Australians also show a relative increased allele sharing with Denisovans, compared to Eurasians and African populations, consistent with the hypothesis of increased admixture between Denisovans and Melanesians.[73]
Reich et al. (2011) produced evidence that the highest presence of Denisovan admixture is in Oceanian populations, followed by many Southeast Asian populations, and none in East Asian populations.[74] There is significant Denisovan genetic material in eastern Southeast Asian and Oceanian populations (e.g. Aboriginal Australians, Near Oceanians, Polynesians, Fijians, eastern Indonesians, Philippine Mamanwa and Manobo), but not in certain western and continental Southeast Asian populations (e.g. western Indonesians, Malaysian Jehai, Andaman Onge, and mainland Asians), indicating that the Denisovan admixture event happened in Southeast Asia itself rather than mainland Eurasia.[74] The observation of high Denisovan admixture in Oceania and the lack thereof in mainland Asia suggests that early modern humans and Denisovans had interbred east of the Wallace Line that divides Southeast Asia according to Cooper and Stringer (2013).[75]
Skoglund and Jakobsson (2011) observed that particularly Oceanians, followed by Southeast Asians populations, have a high Denisovans admixture relative to other populations.[76] Furthermore, they found possible low traces of Denisovan admixture in East Asians and no Denisovan admixture in Native Americans.[76] In contrast, Prüfer et al. (2013) found that mainland Asian and Native American populations may have a 0.2% Denisovan contribution, which is about twenty-five times lower than Oceanian populations.[13] The manner of gene flow to these populations remains unknown.[13] However, Wall et al. (2013) stated that they found no evidence for Denisovan admixture in East Asians.[24]
Findings indicate that the Denisovan gene flow event happened to the common ancestors of Aboriginal Filipinos, Aboriginal Australians, and New Guineans.[74][77] New Guineans and Australians have similar rates of Denisovan admixture, indicating that interbreeding took place prior to their common ancestors' entry into Sahul (Pleistocene New Guinea and Australia), at least 44,000 years ago.[74] It has also been observed that the fraction of Near Oceanian ancestry in Southeast Asians is proportional to the Denisovan admixture, except in the Philippines where there is a higher proportional Denisovan admixture to Near Oceanian ancestry.[74] Reich et al. (2011) suggested a possible model of an early eastward migration wave of modern humans, some who were Philippine/New Guinean/Australian common ancestors that interbred with Denisovans, respectively followed by divergence of the Philippine early ancestors, interbreeding between the New Guinean and Australian early ancestors with a part of the same early-migration population that did not experience Denisovan gene flow, and interbreeding between the Philippine early ancestors with a part of the population from a much-later eastward migration wave (the other part of the migrating population would become East Asians).[74]
Finding components of Denisovan introgression with differing relatedness to the sequenced Denisovan, Browning et al. (2018) suggested that at least two separate episodes of Denisovan admixture has occurred.[78] Specifically, introgression from two distinct Denisovan populations is observed in East Asians (e.g. Japanese and Han Chinese), whereas South Asians (e.g. Telugu and Punjabi) and Oceanians (e.g. Papuans) display introgression from one Denisovan population.[78]
Exploring derived alleles from Denisovans, Sankararaman et al. (2016) estimated that the date of Denisovan admixture was 44,000–54,000 years ago.[6] They also determined that the Denisovan admixture was the greatest in Oceanian populations compared to other populations with observed Denisovan ancestry (i.e. America, Central Asia, East Asia, and South Asia).[6] The researchers also made the surprising finding that South Asian populations display an elevated Denisovan admixture (when compared to other non-Oceanian populations with Denisovan ancestry), albeit the highest estimate (which are found in Sherpas) is still ten times lower than in Papuans.[6] They suggest two possible explanations: There was a single Denisovan introgression event that was followed by dilution to different extents or at least three distinct pulses of Denisovan introgressions must have occurred.[6]
A study in 2021 analyzing archaic ancestry in 118 Philippine ethnic groups discovered an independent admixture event into Philippine Negritos from Denisovans. The Ayta Magbukon in particular were found to possess the highest level of Denisovan ancestry in the world, with ~30%–40% more than even that found in Australians and Papuans (Australo-Melanesians), suggesting that distinct Islander Denisovan populations existed in the Philippines which admixed with modern humans after their arrival.[79]
It has been shown that Eurasians have some but significantly lesser archaic-derived genetic material that overlaps with Denisovans, stemming from the fact that Denisovans are related to Neanderthals—who contributed to the Eurasian gene pool—rather than from interbreeding of Denisovans with the early ancestors of those Eurasians.[23][72]
The skeletal remains of an early modern human from the Tianyuan cave (near Zhoukoudian, China) of 40,000 years BP showed a Neanderthal contribution within the range of today's Eurasian modern humans, but it had no discernible Denisovan contribution.[80] It is a distant relative to the ancestors of many Asian and Native American populations, but post-dated the divergence between Asians and Europeans.[80] The lack of a Denisovan component in the Tianyuan individual suggests that the genetic contribution had been always scarce in the mainland.[13]
There are large genomic regions devoid of Denisovan-derived ancestry, partly explained by infertility of male hybrids, as suggested by the lower proportion of Denisovan-derived ancestry on X chromosomes and in genes that are expressed in the testes of modern humans.[6]
Exploring the immune system's HLA alleles, it has been suggested that HLA-B*73 introgressed from Denisovans into modern humans in western Asia due to the distribution pattern and divergence of HLA-B*73 from other HLA alleles.[48] Even though HLA-B*73 is not present in the sequenced Denisovan genome, HLA-B*73 was shown to be closely associated to the Denisovan-derived HLA-C*15:05 from the linkage disequilibrium.[48] From phylogenetic analysis, however, it has been concluded that it is highly likely that HLA-B*73 was ancestral.[44]
The Denisovan's two HLA-A (A*02 and A*11) and two HLA-C (C*15 and C*12:02) allotypes correspond to common alleles in modern humans, whereas one of the Denisovan's HLA-B allotype corresponds to a rare recombinant allele and the other is absent in modern humans.[48] It is thought that these must have been contributed from Denisovans to modern humans, because it is unlikely to have been preserved independently in both for so long due to HLA alleles' high mutation rate.[48]
Tibetan people received an advantageous EGLN1 and EPAS1 gene variant, associated with hemoglobin concentration and response to hypoxia, for life at high altitudes from the Denisovans.[44] The ancestral variant of EPAS1 upregulates hemoglobin levels to compensate for low oxygen levels—such as at high altitudes—but this also has the maladaption of increasing blood viscosity.[81] The Denisovan-derived variant on the other hand limits this increase of hemoglobin levels, thus resulting in a better altitude adaption.[81] The Denisovan-derived EPAS1 gene variant is common in Tibetans and was positively selected in their ancestors after they colonized the Tibetan plateau.[81]
Rapid decay of fossils in Sub-Saharan African environments makes it currently unfeasible to compare modern human admixture with reference samples of archaic Sub-Saharan African hominins.[5][82]
Ancient DNA Data from a ~4,500 BP Ethiopian highland individual,[83] and from Southern (~2,300–1,300 BP), and Eastern and South-Central Africa (~8,100–400 BP) has clarified that some West Africa populations have small amounts of excess alleles best explained by an archaic source in West Africans that is not included in the pre-agricultural Eastern African hunter-gatherers, Southern African hunter-gatherer populations, or the genetic gradation between them. The West African groups carrying the archaic DNA include Yoruba from coastal Nigeria and Mende from Sierra Leone indicating that the ancient DNA was acquired long before the spread of agriculture and likely well before the Holocene (starting 11,600 BP), Such an archaic lineage must have separated before the divergence of San ancestors, which is estimated to have begun on the order of 200–300 thousand years ago.[84][85]
The hypothesis that there has been archaic line in the ancestry of present-day Africans that originated before the San, Pygmies and East African hunter gatherers (and the Eurasians) is supported by a line of evidence independent from the Skoglund findings based on long haplotypes with deep divergences from other human haplotypes including Lachance et al.(2012),[82] Hammer et al., 2011,[86] and Plagnol and Wall (2006).[87]
In the archaic DNA differences found by Hammer, et al., the pygmies (of Central Africa) are grouped with the San (of Southern Africa) in contrast to the Yoruba (of West Africa). Further clarification of the presence of archaic DNA in current West African populations with the extraction and sequencing of DNA from 4 fossils found at Shum Laka in Cameroon dating from 8,000 to 3,000 BP.  These individuals were found to derive most of their DNA from Central African hunter gatherers (Pygmy ancestors) and did not share the archaic DNA found in the Yoruba and Mande.[88] The pattern of differences between Eastern, Central and Southern hunter gatherers when compared to the West African groups which had been found by Hammer was confirmed.  In a second study Lipson et al. (2020) studied DNA extracted from 6 additional Eastern and Southcentral African fossils from the last 18,000 years. It was determined that their genetic origins could be accounted for by DNA contributions from Southern, Central and Eastern hunter gatherers, and that none of them had the archaic DNA found in the Yoruba.[89]
According to a study published in 2020, there are indications that 2% to 19% (or about ≃6.6 and ≃7.0%) of the DNA of four West African populations may have come from an unknown archaic hominin which split from the ancestor of humans and Neanderthals between 360 kya to 1.02 mya. However, in contrast to the studies of Skoglund and Lipson with ancient African DNA, the study also finds that at least part of this proposed archaic admixture is also present in Eurasians/non-Africans, and that the admixture event or events range from 0 to 124 ka B.P, which includes the period before the Out-of-Africa migration and prior to the African/Eurasian split (thus affecting in part the common ancestors of both Africans and Eurasians/non-Africans).[90][91][92] Another recent study, which discovered substantial amounts of previously undescribed human genetic variation, also found ancestral genetic variation in Africans that predates modern humans and was lost in most non-Africans.[8]
Hominins' presence in Eurasia began at least 2 million years BP. Genetic evidence shows that thousands of years later when lineages of Neandertals and Denisovans started to expand into Eurasia, the continent was still inhabited by descendants of these archaic hominins, and their genetic admixture made its way into genome of Neanderthals and Denisovans and later indirectly into modern humans.[93][94]
Genetic studies show two major events of genetic admixture from superarchaics, suggesting that in the late middle Pleistocene, Eurasia was inhabited by at least two separate populations of ancient hominins.[93]
Roger et al. (2020) describes an event of admixture that occurred soon after Neandersovans (common ancestor of Neanderthals and Denisovans) started to expand into Eurasia. They met a lineage of superarchaic hominins that had been separated from African homo lineages since at least 2 Ma ago.[93]
Previous studies identified more recent event of admixture. About 350,000 years ago a genome of an "erectus-like" creature was injected into the Denisovan lineage. With the separation time of about 2 Ma ago and interbreeding that happened 350 ka ago, the two populations involved were more distantly related than any pair of human populations previously known to interbreed.[93][94]
In 2019, scientists discovered evidence, based on genetics studies using artificial intelligence (AI), that suggests the existence of an unknown human ancestor species, not Neanderthal or Denisovan, in the genome of modern humans.[95][96]

Although definitions of music vary wildly throughout the world, every known culture partakes in it, and it is thus considered a cultural universal. The origins of music remain highly contentious; commentators often relate it to the origin of language, with much disagreement surrounding whether music arose before, after or simultaneously with language. Many theories have been proposed by scholars from a wide range of disciplines, though none has achieved broad approval. Most cultures have their own mythical origins concerning the invention of music, generally rooted in their respective mythological, religious or philosophical beliefs.
The music of prehistoric cultures is first firmly dated to c. 40,000 BP of the Upper Paleolithic by evidence of bone flutes, though it remains unclear whether or not the actual origins lie in the earlier Middle Paleolithic period (300,000 to 50,000 BP). There is little known about prehistoric music, with traces mainly limited to some simple flutes and percussion instruments. However, such evidence indicates that music existed to some extent in prehistoric societies such as the Xia dynasty and the Indus Valley civilisation. Upon the development of writing, the music of literate civilizations—ancient music—was present in the major Chinese, Egyptian, Greek, Indian, Persian, Mesopotamian, and Middle Eastern societies. It is difficult to make many generalizations about ancient music as a whole, but from what is known it was often characterized by monophony and improvisation. In ancient song forms, the texts were closely aligned with music, and though the oldest extant musical notation survives from this period, many texts survive without their accompanying music, such as the Rigveda and the Shijing Classic of Poetry. The eventual emergence of the Silk Road and increasing contact between cultures led to the transmission and exchange of musical ideas, practices, and instruments. Such interaction led to the Tang dynasty's music being heavily influenced by Central Asian traditions, while the Tang dynasty's music, the Japanese gagaku and Korean court music each influenced each other.
Historically, religions have often been catalysts for music. The Vedas of Hinduism immensely influenced Indian classical music, and the Five Classics of Confucianism laid the basis for subsequent Chinese music. Following the rapid spread of Islam in the 7th century, Islamic music dominated Persia and the Arab world, and the Islamic Golden Age saw the presence of numerous important music theorists. Music written for and by the early Christian Church properly inaugurates the Western classical music tradition,[1] which continues into medieval music where polyphony, staff notation and nascent forms of many modern instruments developed. In addition to religion or the lack thereof, a society's music is influenced by all other aspects of its culture, including social and economic organization and experience, climate, and access to technology. Many cultures have coupled music with other art forms, such as the Chinese four arts and the medieval quadrivium. The emotions and ideas that music expresses, the situations in which music is played and listened to, and the attitudes toward musicians and composers all vary between regions and periods. Many cultures have or continue to distinguish between art music (or 'classical music'), folk music, and popular music.
"But that music is a language by whose means messages are elaborated, that such messages can be understood by the many but sent out only by the few, and that it alone among all language unites the contradictory character of being at once intelligible and untranslatable—these facts make the creator of music a being like the gods and make music itself the supreme mystery of human knowledge."
Music is regarded as a cultural universal,[3][4] though definitions of it vary depending on culture and throughout history.[5] As with many aspects of human cognition, it remains debated as to what extent the origins of music will ever be understood, with scholars often taking polarizing positions.[6][7] The origin of music is often discussed alongside the origin of language, with the nature of their connection being the subject of much debate.[8] However, before the mid-late 20th century, both topics were seldom given substantial attention by academics.[9][10][n 1] Since the topic's resurgence, the principal source of contention is divided into three perspectives: whether music began as a kind of proto-language (a result of adaptation) that led to language; if music is a spandrel (a phenotypic by-product of evolution) that was the result of language; or if music and language both derived from a common antecedent.[12][13][n 2][n 3]
There is little consensus on any particular theory for the origin of music, which have included contributions from archaeologists, cognitive scientists, ethnomusicologists, evolutionary biologists, linguists, neuroscientists, paleoanthropologists, philosophers, and psychologists (developmental and social).[22][n 4] Some of the most prominent theories are as follows:
Many cultures have their own mythical origins on the creation of music.[34][35] Specific figures are sometimes credited with inventing music, such as Jubal in Christian mythology,[26] the legendary Shah Jamshid in Persian/Iranian mythology,[36] the goddess Saraswati in Hinduism,[37] and the muses in Ancient Greek mythology.[35] Some cultures credit multiple originators of music; ancient Egyptian mythology associates it with numerous deities, including Amun, Hathor, Isis and Osiris, but especially Ihy.[38] There are many stories relating to music's origins in Chinese mythology,[39][n 5] but the most prominent is that of the musician Ling Lun, who—on the orders of the Yellow Emperor (Huangdi)—invented bamboo flute by imitating the song of the mythical fenghuang birds.[40]
In the broadest sense, prehistoric music—more commonly termed primitive music in the past[43][44][n 6]—encompasses all music produced in preliterate cultures (prehistory), beginning at least 6 million years ago when humans and chimpanzees last had a common ancestor.[45] Music first arose in the Paleolithic period,[46] though it remains unclear as to whether this was the Middle (300,000 to 50,000 BP) or Upper Paleolithic (50,000 to 12,000 BP).[47] The vast majority of Paleolithic instruments have been found in Europe and date to the Upper Paleolithic.[48] It is certainly possible that singing emerged far before this time, though this is essentially impossible to confirm.[49] The potentially oldest instrument is the Divje Babe Flute from the Divje Babe cave in Slovenia, dated to 43,000 and 82,000 and made from a young cave bear femur.[50] Purportedly used by Neanderthals, the Divje Babe Flute has received extensive scholarly attention, and whether it is truly a musical instrument or an object formed by animals is the subject of intense debate.[41] If the former, it would be the oldest known musical instrument and evidence of a musical culture in the Middle Paleolithic.[51] Other than the Divje Babe Flute and three other doubtful flutes,[n 7] there is virtually no surviving Middle Paleolithic musical evidence of any certainty, similar to the situation in regards to visual art.[46] The earliest objects whose designations as musical instruments are widely accepted are bone flutes from the Swabian Jura, Germany, namely from the Geissenklösterle, Hohle Fels and Vogelherd caves.[52] Dated to the Aurignacian (of the Upper Paleolithic) and used by Early European modern humans, from all three caves there are eight examples, four made from the wing bones of birds and four from mammoth ivory; three of these are near complete.[52] Three flutes from the Geissenklösterle are dated as the oldest, c. 43,150–39,370 BP.[53][n 8]
Considering the relative complexity of flutes, it is likely earlier instruments existed, akin to objects that are common in later hunter and gatherer societies, such as rattles, shakers, and drums.[49] The absence of other instruments from and before this time may be due to their use of weaker—and thus more biodegradable—materials,[46] such as reeds, gourds, skins, and bark.[54] A painting in the Cave of the Trois-Frères dating to c. 15,000 BCE is thought to depict a shaman playing a musical bow.[55]
Prehistoric cultures are thought to have had a wide variety of uses for music, with little unification between different societies.[56] Music was likely of particular value when food and other basic needs were scarce.[56] It is also probable that prehistoric cultures viewed music as intrinsically connected with nature, and may have believed its use influenced the natural world directly.[56]
The earliest instruments found in prehistoric China are 12 gudi bone flutes in the modern-day Jiahu, Wuyang, Henan Province from c. 6000 BCE.[57][58][n 9][n 10] The only instruments dated to the prehistoric Xia dynasty (c. 2070–1600) are two qing, two small bells (one earthenware, one bronze), and a xun.[60] Due to this extreme scarcity of surviving instruments and the general uncertainty surrounding most of the Xia, creating a musical narrative of the period is impractical.[60] In the Indian subcontinent, the prehistoric Indus Valley civilisation (from c. 2500–2000 BCE in its mature state) has archeological evidence that indicates simple rattles and vessel flutes were used, while iconographical evidence suggests early harps and drums also existed.[61] An ideogram in the later IVC contains the earliest known depiction of an arched harp, dated sometime before 1800 BCE.[62]
Following the advent of writing, literate civilizations are termed part of the ancient world, the first of which is Sumerian literature of Abu Salabikh (now Southern Iraq) of c. 2600 BCE.[63] Though the music of Ancient societies was extremely diverse, some fundamental concepts arise prominently in virtually all of them, namely monophony, improvisation and the dominance of text in musical settings.[64] Varying song forms were present in Ancient cultures, including China, Egypt, Greece, India, Mesopotamia, Rome, and the Middle East.[65] The text, rhythm, and melodies of these songs were closely aligned, as was music in general, with magic, science, and religion.[65] Complex song forms developed in later ancient societies, particularly the national festivals of China, Greece, and India.[65] Later Ancient societies also saw increased trade and transmission of musical ideas and instruments, often shepherded by the Silk Road.[66][67] For example, a tuning key for a qin-zither from 4th–5th centuries BCE China includes considerable Persian iconography.[68] In general, not enough information exists to make many other generalizations about ancient music between cultures.[65]
The few actual examples of ancient music notation that survive usually exist on papyrus or clay tablets.[65] Information on musical practices, genres, and thought is mainly available through literature, visual depictions, and increasingly as the period progresses, instruments.[65] The oldest surviving written music is the Hurrian songs from Ugarit, Syria. Of these, the oldest is the Hymn to Nikkal (hymn no. 6; h. 6), which is somewhat complete and dated to c. 1400 BCE.[69] However, the Seikilos epitaph is the earliest entirely complete noted musical composition. Dated to the 2nd Century CE or later, it is an epitaph, perhaps for the wife of the unknown Seikilos.[70]
They strike the bells, kin, kin,They play the se-zither, play the qin-zither,The mouth organ and chime stones sound together;They sing the Ya and Nan Odes,And perform flawlessly upon their flutes.
By the mid-13th century BCE, the Late Shang dynasty had developed writing, which mostly exists as divinatory inscriptions on the ritualistic oracle bones but also as bronze inscriptions.[73][74] As many as 11 oracle script characters may refer to music to some extent, some of which could be iconographical representations of instruments themselves.[75] The stone bells qing appears to have been particularly popular with the Shang ruling class,[n 11] and while no surviving flutes have been dated to the Shang,[77] oracle script evidence suggests they used ocarinas (xun), transverse flute (xiao and dizi), douple pipes, the mouthorgan (sheng), and maybe the pan flute (paixiao).[78][n 12] Due to the advent of the bronze in 2000 BCE,[80] the Shang used the material for bells—the ling [zh] (鈴), nao [zh] (鐃) and zhong (鐘)[59]—that can be differentiated in two ways: those with or without a clapper and those struck on the inside or outside.[81] Drums, which are not found from before the Shang,[82] sometimes used bronze, though they were more often wooden (bangu).[83][59][n 13] The aforementioned wind instruments certainly existed by the Zhou dynasty (1046–256 BCE), as did the first Chinese string instruments: the qin (or guqin) and se zithers.[59][n 14] The Zhou saw the emergence of major court ensembles and the well known Tomb of Marquis Yi of Zeng (after 433 BCE) contains a variety of complex and decorated instruments.[59] Of the tomb, the by-far most notable instrument is the monumental set of 65 tuned bianzhong bells, which range five octaves requiring at least five players; they are still playable and include rare inscriptions on music.[86]
Ancient Chinese instruments served both practical and ceremonial means. People used them to appeal to supernatural forces for survival needs,[87] while pan flutes may have been used to attract birds while hunting,[88] and drums were common in sacrifices and military ceremonies.[82] Chinese music has always been closely associated with dance, literature and fine arts;[89] many early Chinese thinkers also equated music with proper morality and governance of society.[90][n 15] Throughout the Shang and Zhou music was a symbol of power for the Imperial court,[93] being used in religious services as well as the celebration of ancestors and heroes.[87][94] Confucius (c. 551–479) formally designated the music concerned with ritual and ideal morality as the superior yayue (雅樂; "proper music"), in opposition to suyue (俗樂; "vernacular/popular music"),[87][95] which included virtually all non-ceremonial music, but particularly any that was considered excessive or lascivious.[95] During the Warring States Period when of Confucius's lifetime, officials often ignored this distinction, preferring more lively suyue music and using the older yayue traditional solely for political means.[96] Confucius and disciples such as Mencius considered this preference virtueless and saw ill of the leaders' ignorance of ganying,[97] a theory that held music was intrinsically connected to the universe.[98][99][n 16] Thus, many aspects of Ancient Chinese music were aligned with cosmology: the 12 pitch shí-èr-lǜ system corresponded equally with certain weights and measurements; the pentatonic scale with the five wuxing;[99] and the eight tone classification of Chinese instruments of bayin with the eight symbols of bagua.[100] No actual music or texts on the performance practices of Ancient Chinese musicians survive.[101] The Five Classics of the Zhou dynasty include musical commentary; the I Ching and Chunqiu Spring and Autumn Annals make references, while the Liji Book of Rites contains a substantial discussion (see the chapter Yue Ji Record of Music).[40] While the Yue Jing Classic of Music is lost,[80] the Shijing Classic of Poetry contains 160 texts to now lost songs from the Western Zhou period (1045–771).[102]
The Qin dynasty (221–206 BCE), established by Qin Shi Huang, lasted for only 15 years, but the purported burning of books resulted in a substantial loss of previous musical literature.[67] The Qin saw the guzheng become a particularly popular instrument; as a more portable and louder zither, it meet the needs of an emerging popular music scene.[103][n 17] During the Han dynasty (202 BCE – 220 CE), there were attempts to reconstruct the music of the Shang and Zhou, as it was now "idealized as perfect".[100][67] A Music Bureau, the Yuefu, was founded or at its height by at least 120 BCE under Emperor Wu of Han,[104][n 18] and was responsible for collecting folksongs. The purpose of this was twofold; it allowed the Imperial Court to properly understand the thoughts of the common people,[89] and it was also an opportunity for the Imperial Court to adapt and manipulate the songs to suit propaganda and political purposes.[100][67][n 19] Employing ceremonial, entertainment-oriented and military musicians,[106] the Bureau also performed at a variety of venues, wrote new music, and set music to commissioned poetry by noted figures such as Sima Xiangru.[107] The Han dynasty had officially adopted Confucianism as the state philosophy,[67] and the ganying theories became a dominant philosophy.[108] In practice, however, many officials ignored or downplayed Confucius's high regard for yayue over suyue music, preferring to engage in the more lively and informal later.[109] By 7 BCE the Bureau employed 829 musicians; that year Emperor Ai either disbanded or downsized the department,[67][107] due to financial limitations,[67] and the Bureau's increasingly prominent suyue music which conflicted with Confucianism.[106] The Han dynasty saw a preponderance of foreign musical influences from the Middle East and Central Asia: the emerging Silk Road led to the exchange of musical instruments,[66] and allowed travelers such as Zhang Qian to relay with new musical genres and techniques.[67] Instruments from said cultural transmission include metal trumpets and instruments similar to the modern oboe and oud lute, the latter which became the pipa.[66] Other preexisting instruments greatly increased in popularity, such as the qing,[88] panpipes,[110] and particularly the qin-zither (or guqin), which was from then on the most revered instrument, associated with good character and morality.[85]
Greek written history extends far back into Ancient Greece, and was a major part of ancient Greek theatre. In ancient Greece, mixed-gender choruses performed for entertainment, celebration and spiritual reasons.[citation needed] Instruments included the most important wind instrument, the double-reed aulos,[111] as well as the plucked string instrument, the lyre,[112] especially the special kind called a kithara.
The principal sources on the music of ancient India are textual and iconographical; specifically, some theoretical treatises in Sanskrit survive, there are brief mentions in general literature and many sculptures of Ancient Indian musicians and their instruments exist.[113] Ancient Sanskrit, Pali, and Prakrit literature frequently contains musical references, from the Vedas to the works of Kalidasa and the Ilango Adigal's epic Silappatikaram.[114] Despite this, little is known on the actual musical practices of ancient India and the information available forces a somewhat homogeneous perspective on the music of the time, even though evidence indicates that in reality, it was far more diverse.[113]
The monumental arts treatise Natya Shastra is among the earliest and chief sources for Ancient Indian music; the music portions alone are likely from the Gupta period (4th century to 6th century CE).[115]
In general, it is impossible to create a thorough outline of the earliest music in Persia due to a paucity of surviving records.[116] Evidenced by c. 3300–3100 BCE Elam depictions, arched harps are the first affirmation of Persian music, though it is probable that they existed well before their artistic depictions.[117] Elamite bull lyres from c. 2450 have been found in Susa, while more than 40 small Oxus trumpets have been found in Bactria and Margiana, dated to the c. 2200–1750 Bactria–Margiana Archaeological Complex.[118][n 20] The oxus trumpets seem to have had a close association with both religion and animals; a Zoroastrian myth in which Jamshid attract animals with the trumpet suggests that the Elamites used them for hunting.[119] In many ways the earliest known musical cultures of Iran are strongly connected with those of Mesopotamia. Ancient arched harps (c. 3000) also exist in the latter and the scarcity of instruments makes it unclear as to which culture the harp originated.[117] Far more bull lyres survive in Ur of Mesopotamia, notably the Bull Headed Lyre of Ur, though they are nearly identical to their contemporary Elamite counterparts.[120] From the evidence in terracotta plaques, by the 2nd-century BCE the arched harp was displaced by the angular harps, which existed in 20-string vertical and nine-string horizontal variants.[121] Lutes were purportedly used in Mesopotamia by at least 2300 BCE, but not until c. 1300 BCE do they appear in Iran, where they became the dominant string instruments of Western Iran, though the available evidence suggests its popularity was outside of the elite.[122] The rock reliefs of Kul-e Farah show that sophisticated Persian court ensembles emerged in the 1st-century BCE, in the which the central instrument was the arched harp.[123] The prominence of musicians in these certain rock reliefs suggests they were essential in religious ceremonies.[124]
Like earlier periods, extremely little contemporary information on the music of the Achaemenid Empire (550–330 BCE) exists.[68][125] Most knowledge on the Achaemenid musical culture comes from Greek historians.[125] In his Histories, Herodotus noted that Achaemenid priests did not use aulos music in their ceremonies, while Xenophon reflected on his visit to Persia in the Cyropaedia, mentioning the presence of many female singers at court.[68] Athenaeus also mentions female singers when noting that 329 of them had been taken from the King of Kings Darius III by Macedonian general Parmenion.[68] Later Persian texts assert that gōsān poet-musician minstrels were prominent and of considerable status in court.[126]
The Parthian Empire (247 BCE to 224 CE) saw an increase in textual and iconographical depictions of musical activity and instruments. 2nd century BCE Parthian rhuta (drinking horns) found in the ancient capital of Nisa include some of the most vivid depictions of musicians from the time. Pictorial evidence such as terracotta plaques show female harpists, while plaques from Babylon show panpipes, as well as string (harps, lutes and lyres) and percussion instruments (tambourines and clappers). Bronze statues from Dura-Europos depict larger panpipes and double aulos. Music was evidently used in ceremonies and celebrations; a Parthian-era stone frieze in Hatra shows a wedding where musicians are included, playing trumpets, tambourines, and a variety of flutes. Other textual and iconographical evidence indicates the continued prominence of gōsān minstrels. However, like the Achaemenid period, Greek writers continue to be a major source for information on Parthian music. Strabo recorded that the gōsān learned songs telling tales of gods and noblemen, while Plutarch similarly records the gōsān lauding Parthian heroes and mocking Roman ones. Plutarch also records, much to his bafflement, that rhoptra (large drums) were used by the Parthian army to prepare for war.[127]
The Sasanian period (226–651 CE), however, has left ample evidence of music. This influx of Sasanian records suggests a prominent musical culture in the Empire,[36] especially in the areas dominated by Zoroastrianism.[128] Many Sassanian Shahanshahs were ardent supporters of music, including the founder of the empire Ardashir I and Bahram V.[128] Khosrow II (r. 590–628) was the most outstanding patron, his reign being regarded as a golden age of Persian music.[128] Musicians in Khosrow's service include Āzādvar-e Changi (or Āzād),[125] Bamshad, the harpist Nagisa (Nakisa), Ramtin, Sarkash and Barbad,[116] who was the most famous.[129] These musicians were usually active as minstrels, which were performers who worked as both court poets and musicians;[130] in the Sassanian Empire there was little distinction between poetry and music.[131]
The Western African Nok culture (modern-day Nigeria) existed from c. 500–200 BCE and left a considerable amount of sculptures.[132] Among these are depictions of music, such as a man who shakes two objects thought to be maracas. Another sculpture includes a man with his mouth opening (possibly singing) while there is also a sculpture of a man playing a drum.[133]
The imperial court of Japan developed gagaku ((雅楽); lit. 'elegant music') music, originating from the Gagakuryō imperial music academy established in 701 CE during the Asuka period.[134] Though the word gagaku derives from the Chinese yayue music, the latter originally referred to Confucian ritual music, while gagaku extends to many genres, styles and instruments.[134][135][n 21] In the tradition's early history, the three main genres were wagaku (native Japanese music), sankangaku (music from the Three Kingdoms of Korea) and tōgaku (music from China's Tang dynasty), as well as more minor genres such as toragaku, gigaku, and rin’yūgaku. Uniquely among Asian music of this time, there are numerous extant scores of gagaku music from the 8th to 11th centuries.[135]
A major shift in gagaku music occurred in the 9th century, namely the development of a distinction between tōgaku and komagaku music. Tōgaku was a Chinese-influenced style, which combined with the rin’yūgaku tradition, referred to as "Music of the Left" (sahō). Komagaku was then referred to as "Music of the Right" (uhō), encompassing music influenced by both Korea (sankangaku) and Balhae (bokkaigaku). Though this division was prominent, it was not strict and the tōgaku and komagaku styles nonetheless interlaced and influenced each other.[135] The long Heian period (794–1185) saw much patronage of gagaku music from the court, as it accompanied many festivals and celebrations. Numerous new genres emerged at this time, such as the saibara and rōei song forms.[135][136] Gagaku ensembles consist of a wide variety of instruments and are the largest such formations in traditional Japanese music.[137]
Modern scholars generally define 'Medieval music' as the music of Western Europe during the Middle Ages,[138] from approximately the 6th to 15th centuries. Music was certainly prominent in the Early Middle Ages, as attested by artistic depictions of instruments, writings about music, and other records; however, the only repertory of music which has survived from before 800 to the present day is the plainsong liturgical music of the Roman Catholic Church, the largest part of which is called Gregorian chant. Pope Gregory I, who gave his name to the musical repertory and may himself have been a composer, is usually claimed to be the originator of the musical portion of the liturgy in its present form, though the sources giving details on his contribution date from more than a hundred years after his death. Many scholars believe that his reputation has been exaggerated by legend. Most of the chant repertory was composed anonymously in the centuries between the time of Gregory and Charlemagne.
During the 9th century, several important developments took place. First, there was a major effort by the Church to unify the many chant traditions and suppress many of them in favor of the Gregorian liturgy. Second, the earliest polyphonic music was sung, a form of parallel singing known as organum. Third, and of the greatest significance for music history, notation was reinvented after a lapse of about five hundred years, though it would be several more centuries before a system of pitch and rhythm notation evolved having the precision and flexibility that modern musicians take for granted.
Several schools of polyphony flourished in the period after 1100: the St. Martial school of organum, the music of which was often characterized by a swiftly moving part over a single sustained line; the Notre Dame school of polyphony, which included the composers Léonin and Pérotin, and which produced the first music for more than two parts around 1200; the musical melting-pot of Santiago de Compostela in Galicia, a pilgrimage destination and site where musicians from many traditions came together in the late Middle Ages, the music of whom survives in the Codex Calixtinus; and the English school, the music of which survives in the Worcester Fragments and the Old Hall Manuscript. Alongside these schools of sacred music a vibrant tradition of the secular song developed, as exemplified in the music of the troubadours, trouvères, and Minnesänger. Much of the later secular music of the early Renaissance evolved from the forms, ideas, and the musical aesthetic of the troubadours, courtly poets, and itinerant musicians, whose culture was largely exterminated during the Albigensian Crusade in the early 13th century.
Forms of sacred music which developed during the late 13th century included the motet, conductus, discant, and clausulae. One unusual development was the Geisslerlieder, the music of wandering bands of flagellants during two periods: the middle of the 13th century (until they were suppressed by the Church); and the period during and immediately following the Black Death, around 1350, when their activities were vividly recorded and well-documented with notated music. Their music mixed folk song styles with penitential or apocalyptic texts. The 14th century in European music history is dominated by the style of the ars nova, which by convention is grouped with the medieval era in music, even though it had much in common with early Renaissance ideals and aesthetics. Much of the surviving music of the time is secular, and tends to use the formes fixes: the ballade, the virelai, the lai, the rondeau, which correspond to poetic forms of the same names. Most pieces in these forms are for one to three voices, likely with instrumental accompaniment: famous composers include Guillaume de Machaut and Francesco Landini.
Prominent and diverse musical practices were present in the Byzantine Empire, which existed by 395 to 1453.[139] Both sacred and secular music were commonplace, with sacred music frequently used in church services and secular music in many events including, ceremonies dramas, ballets, banquets, festivals and sports games.[140][141] However, despite its popularity, secular Byzantine music was harshly criticized by the Church Fathers, particularly Jerome.[141] Composers of sacred music, especially hymns and chants, are generally well documented throughout the history of Byzantine music. However, those before the reign of Justinian I are virtually unknown; the monks Anthimos, Auxentios and Timokles are said to have written troparia, but only the text to a single one by Auxentios survives.[142] The first major form was the kontakion, of which Romanos the Melodist was the foremost composer.[143] In the late 7th century the kanōn overtook the kontakion in popularity; Andrew of Crete became its first significant composer, and is traditionally credited as the genre's originator,[144] though modern scholars now doubt this attribution.[145] The kañon reached its peak with the music of John of Damascus and Cosmas of Maiuma and later Theodore of Stoudios and Theophanes the Branded in the 8th and 9th centuries respectively.[140] Composers of secular music are considerably less documented. Not until late in the empire's history are composers known by name, with Joannes Koukouzeles, Xenos Korones and Joannes Glykys as the leading figures.[146]
Like their Western counterparts of the same period, Byzantine composers were primarily men.[147] Kassia is a major exception to this; she was a prolific and important composer of sticheron hymns and the only woman whose works entered the Byzantine liturgy.[148] A few other women are known to have been composers, Thekla, Theodosia, Martha and the daughter of John Kladas (her given name is unrecorded).[149][150] Only the latter has any surviving work, a single antiphon.[151] Some Byzantine emperors are known to have been composers, such as Leo VI the Wise and Constantine VII.[152][153]
During the ancient and medieval periods, the classical music of the Indian subcontinent was a largely unified practice. By the 14th century, socio-political turmoil inaugurated by the Delhi Sultanate began to isolate Northern and Southern India, and independent traditions in each region began emerging. By the 16th-century two distinct styles had formed: the Hindustani classical music of the North and the Carnatic classical music of the South.[154] One of the major differences between them is that the Northern Hindustani vein was considerably influenced by the Persian and Arab musical practices of the time.[155] Carnatic music is largely devotional; the majority of the songs are addressed to the Hindu deities.
Indian classical music (marga) is monophonic and based on a single melody line or raga rhythmically organized through talas.
The beginning of the Renaissance in music is not as clearly marked as the beginning of the Renaissance in the other arts, and unlike in the other arts, it did not begin in Italy, but in northern Europe, specifically in the area currently comprising central and northern France, the Netherlands, and Belgium. The style of the Burgundian composers, as the first generation of the Franco-Flemish school is known, was at first a reaction against the excessive complexity and mannered style of the late 14th century ars subtilior, and contained clear, singable melody and balanced polyphony in all voices. The most famous composers of the Burgundian school in the mid-15th century are Guillaume Dufay, Gilles Binchois, and Antoine Busnois.
By the middle of the 15th century, composers and singers from the Low Countries and adjacent areas began to spread across Europe, especially into Italy, where they were employed by the papal chapel and the aristocratic patrons of the arts (such as the Medici, the Este, and the Sforza families). They carried their style with them: smooth polyphony which could be adapted for sacred or secular use as appropriate. Principal forms of sacred musical composition at the time were the mass, the motet, and the laude; secular forms included the chanson, the frottola, and later the madrigal.
The invention of printing had an immense influence on the dissemination of musical styles, and along with the movement of the Franco-Flemish musicians, contributed to the establishment of the first truly international style in European music since the unification of Gregorian chant under Charlemagne.[citation needed] Composers of the middle generation of the Franco-Flemish school included Johannes Ockeghem, who wrote music in a contrapuntally complex style, with varied texture and an elaborate use of canonical devices; Jacob Obrecht, one of the most famous composers of masses in the last decades of the 15th century; and Josquin des Prez, probably the most famous composer in Europe before Palestrina, and who during the 16th century was renowned as one of the greatest artists in any form. Music in the generation after Josquin explored increasing complexity of counterpoint; possibly the most extreme expression is in the music of Nicolas Gombert, whose contrapuntal complexities influenced early instrumental music, such as the canzona and the ricercar, ultimately culminating in Baroque fugal forms.
By the middle of the 16th century, the international style began to break down, and several highly diverse stylistic trends became evident: a trend towards simplicity in sacred music, as directed by the Counter-Reformation Council of Trent, exemplified in the music of Giovanni Pierluigi da Palestrina; a trend towards complexity and chromaticism in the madrigal, which reached its extreme expression in the avant-garde style of the Ferrara School of Luzzaschi and the late century madrigalist Carlo Gesualdo; and the grandiose, sonorous music of the Venetian school, which used the architecture of the Basilica San Marco di Venezia to create antiphonal contrasts. The music of the Venetian school included the development of orchestration, ornamented instrumental parts, and continuo bass parts, all of which occurred within a span of several decades around 1600. Famous composers in Venice included the Gabrielis, Andrea and Giovanni, as well as Claudio Monteverdi, one of the most significant innovators at the end of the era.
Most parts of Europe had active and well-differentiated musical traditions by late in the century. In England, composers such as Thomas Tallis and William Byrd wrote sacred music in a style similar to that written on the continent, while an active group of home-grown madrigalists adapted the Italian form for English tastes: famous composers included Thomas Morley, John Wilbye and Thomas Weelkes. Spain developed instrumental and vocal styles of its own, with Tomás Luis de Victoria writing refined music similar to that of Palestrina, and numerous other composers writing for the new guitar. Germany cultivated polyphonic forms built on the Protestant chorales, which replaced the Roman Catholic Gregorian Chant as a basis for sacred music, and imported the style of the Venetian school (the appearance of which defined the start of the Baroque era there). In addition, Dutch and German composers, particularly Jan Pieterszoon Sweelinck, wrote enormous amounts of organ music, establishing the basis for the later Baroque organ style which culminated in the work of J.S. Bach. France developed a unique style of musical diction known as musique mesurée, used in secular chansons, with composers such as Guillaume Costeley and Claude Le Jeune prominent in the movement.
One of the most revolutionary movements in the era took place in Florence in the 1570s and 1580s, with the work of the Florentine Camerata, who ironically had a reactionary intent: dissatisfied with what they saw as contemporary musical depravities, their goal was to restore the music of the ancient Greeks. Chief among them were Vincenzo Galilei, the father of the astronomer, and Giulio Caccini. The fruits of their labors was a declamatory melodic singing style known as monody, and a corresponding staged dramatic form: a form known today as opera. The first operas, written around 1600, also define the end of the Renaissance and the beginning of the Baroque eras.
Music prior to 1600 was modal rather than tonal. Several theoretical developments late in the 16th century, such as the writings on scales on modes by Gioseffo Zarlino and Franchinus Gaffurius, led directly to the development of common practice tonality. The major and minor scales began to predominate over the old church modes, a feature which was at first most obvious at cadential points in compositions, but gradually became pervasive. Music after 1600, beginning with the tonal music of the Baroque era, is often referred to as belonging to the common practice period.
The Baroque era took place from 1600 to 1750, as the Baroque artistic style flourished across Europe and, during this time, music expanded in its range and complexity. Baroque music began when the first operas (dramatic solo vocal music accompanied by orchestra) were written. During the Baroque era, polyphonic contrapuntal music, in which multiple, simultaneous independent melody lines were used, remained important (counterpoint was important in the vocal music of the medieval era).[clarification needed] German, Italian, French, Dutch, Polish, Spanish, Portuguese, and English Baroque composers wrote for small ensembles including strings, brass, and woodwinds, as well as for choirs and keyboard instruments such as pipe organ, harpsichord, and clavichord. During this period several major music forms were defined that lasted into later periods when they were expanded and evolved further, including the fugue, the invention, the sonata, and the concerto.[156] The late Baroque style was polyphonically complex and richly ornamented. Important composers from the Baroque era include Jan Pieterszoon Sweelinck, Johann Sebastian Bach, Arcangelo Corelli, François Couperin, Girolamo Frescobaldi, George Frideric Handel, Jean-Baptiste Lully, Jean-Philippe Rameau, Claudio Monteverdi, Georg Philipp Telemann, Domenico Scarlatti and Antonio Vivaldi.
The music of the Classical period is characterized by homophonic texture, or an obvious melody with accompaniment. These new melodies tended to be almost voice-like and singable, allowing composers to actually replace singers as the focus of the music. Instrumental music therefore quickly replaced opera and other sung forms (such as oratorio) as the favorite of the musical audience and the epitome of great composition. However, opera did not disappear: during the classical period, several composers began producing operas for the general public in their native languages (previous operas were generally in Italian).
Along with the gradual displacement of the voice in favor of stronger, clearer melodies, counterpoint also typically became a decorative flourish, often used near the end of a work or for a single movement. In its stead, simple patterns, such as arpeggios and, in piano music, Alberti bass (an accompaniment with a repeated pattern typically in the left hand), were used to liven the movement of the piece without creating a confusing additional voice. The now-popular instrumental music was dominated by several well-defined forms: the sonata, the symphony, and the concerto, though none of these were specifically defined or taught at the time as they are now in music theory. All three derive from sonata form, which is both the overlying form of an entire work and the structure of a single movement. Sonata form matured during the Classical era to become the primary form of instrumental compositions throughout the 19th century.
The early Classical period was ushered in by the Mannheim School, which included such composers as Johann Stamitz, Franz Xaver Richter, Carl Stamitz, and Christian Cannabich. It exerted a profound influence on Joseph Haydn and, through him, on all subsequent European music. Wolfgang Amadeus Mozart was the central figure of the Classical period, and his phenomenal and varied output in all genres defines our perception of the period. Ludwig van Beethoven and Franz Schubert were transitional composers, leading into the Romantic period, with their expansion of existing genres, forms, and even functions of music.
In the Romantic period, music became more expressive and emotional, expanding to encompass literature, art, and philosophy. Famous early Romantic composers include Schumann, Chopin, Mendelssohn, Bellini, Donizetti, and Berlioz. The late 19th century saw a dramatic expansion in the size of the orchestra, and in the role of concerts as part of urban society. Famous composers from the second half of the century include Johann Strauss II, Brahms, Liszt, Tchaikovsky, Verdi, and Wagner. Between 1890 and 1910, a third wave of composers including Grieg, Dvořák, Mahler, Richard Strauss, Puccini, and Sibelius built on the work of middle Romantic composers to create even more complex – and often much longer – musical works. A prominent mark of late 19th-century music is its nationalistic fervor, as exemplified by such figures as Dvořák, Sibelius, and Grieg. Other prominent late-century figures include Saint-Saëns, Fauré, Rachmaninoff, Franck, Debussy and Rimsky-Korsakov.
The 20th century saw a revolution in music listening as the radio gained popularity worldwide and new media and technologies were developed to record, edit and distribute music. Music performances became increasingly visual with the broadcast and recording of performances.[157]
20th-century music brought new freedom and wide experimentation with new musical styles and forms that challenged the accepted rules of music of earlier periods.[citation needed] The invention of musical amplification and electronic instruments, especially the synthesizer, in the mid-20th century revolutionized classical and popular music, and accelerated the development of new forms of music.[158]
As for classical music, two fundamental schools determined the course of the century: that of Arnold Schoenberg and that of Igor Stravinsky.[159] However, other composers also had a notable influence. For example: Béla Bartók, Anton Webern, Dmitri Shostakovich, Olivier Messiaen, John Cage, Benjamin Britten, Karlheinz Stockhausen, Sofia Gubaidulina, Krzysztof Penderecki, Brian Ferneyhough, Kaija Saariaho.[160]
The 20th century saw the unprecedented dissemination of popular music, that is, music with a wide appeal.[n 22] The term has its roots in the music of the American Tin Pan Alley, a group of prominent musicians and publishers who began to emerge during the 1880s in New York City. Although popular music is sometimes known as "pop music", the terms are not always interchangeable.[162] Popular music refers to a variety of music genres that appeal to the tastes of a large segment of the population,[163] whereas pop music usually refers to a specific genre within popular music.[164] Popular music songs and pieces typically have easily singable melodies. The song structure of popular music commonly involves repetition of sections, with the verse and chorus or refrain repeating throughout the song and the bridge providing a contrasting and transitional section within a piece.[165]
Global
Dictionaries
Origins and prehistory
Africa
East Asia
Middle East
South Asia
Europe

Southeast Africa,[1][2] or Southeastern Africa,[3][a] is an African region that is intermediate between East Africa[b] and Southern Africa.[c][8] It comprises the countries Botswana, Eswatini, Kenya, Lesotho, Malawi,[9] Mozambique,[10][11] Namibia, Rwanda, South Africa, Tanzania, Uganda,[12] Zambia and Zimbabwe[13] in the mainland, with the island-nations of Madagascar, Mauritius, Comoros, and Seychelles also included.[10]
East and southern Africa are among the earliest regions where modern humans (Homo sapiens) and their predecessors are believed to have lived. In September 2019, scientists reported the computerized determination, based on 260 CT scans, of a virtual skull shape of the last common human ancestor to modern humans/H. sapiens, representative of the earliest modern humans, and suggested that modern humans arose between 350,000 and 260,000 years ago through a merging of populations in South and East Africa.[14][15]
Bantu-speakers traversed from Central Africa into Southeast Africa approximately 3,000 years ago.[10]
In the 19th and 20th centuries, David Livingstone[16] and Frederick Courtney Selous visited Southeast Africa. The latter wrote down his experiences in the book Travel and Adventure in South-East Africa.[17]
People include the San people.[3] The Swahili language is spoken, both as an official language and lingua franca, by millions of people.[18]
Lake Malawi[16][19] and Limpopo River[20] are located in Southeast Africa.
Fauna[17] includes the cheetah, leopard, lion,[21] Nile crocodile, hyena, Lichtenstein's hartebeest and white rhinoceros.

Kent Vaughn Flannery (born 1934)[1] is an American archaeologist who has conducted and published extensive research on the pre-Columbian cultures and civilizations of Mesoamerica, and in particular those of central and southern Mexico. He has also worked in Iran and Peru.
Flannery grew up in Maryland on a farm near the Susquehanna River,[2] and attended the Gilman School in Baltimore.[3] His father was artist Vaughn Flannery.[2] He entered the University of Chicago after his sophomore year of high school, and gained his B.A. degree in zoology in 1954. He began studying for an M.A. in zoology, but shifted to Anthropology following fieldwork in Mexico; he then excavated in Iran with Robert Braidwood in 1960.[1]  His 1961 M.A. differentiated wild and domestic pigs in Near Eastern Neolithic sites. His 1964 Ph.D. examined the Tehuacán formative.[1]
Flannery is known for proposing the Broad Spectrum Revolution in 1961.[4] In the 1960s and 70s, Flannery was a leading proponent of Processual Archaeology and the use of Systems Theory in archaeology.[2] He has published influential work on origins of agriculture and village life in the Near east, pastoralists in the Andes, and cultural evolution, and many critiques of modern trends in archaeological method, theory, and practice.[1][2] From 1966 to 1980 he directed project "Prehistory and Human Ecology of the Valley of Oaxaca, Mexico," dealing with  the origins of agriculture, village life, and social inequality in Mexico.[5]
At the University of Michigan, Flannery is the James B. Griffin Professor in the Department of Anthropology and the Curator of Human Ecology and Archaeobiology at the Museum of Anthropological Archaeology.[5] He has chaired thirteen doctoral dissertation committees. His students include Robert Drennan, Susan H. Lees, and Charles S. Spencer.[6]
Flannery was elected to the National Academy of Sciences in 1978,[7][8] the American Academy of Arts and Sciences in 1996,[9] and the American Philosophical Society in 2005.[10] He was awarded an honorary doctorate by the University of Pennsylvania in 1987,[5] and in 1992 his scholarship was recognized by the American Anthropological Association with the Alfred Vincent Kidder Award for Eminence in the Field of American Archaeology.[11]
In 1973 Flannery married fellow archaeologist and frequent collaborator Joyce Marcus.[2]

The origin of language, its relationship with human evolution, and its consequences have been subjects of study for centuries. Scholars wishing to study the origins of language draw inferences from evidence such as the fossil record, archaeological evidence, and the contemporary language diversity. They can also learn the studies of language acquisition, and comparisons between human language and systems of animal communication (particularly other primates).[1] Many argue for the close relation between the origins of language and the origins of modern human behavior, but there is little agreement about the facts and implications of this connection.
The shortage of direct, empirical evidence has caused many scholars to regard the entire topic as unsuitable for serious study; in 1866, the Linguistic Society of Paris banned any existing or future debates on the subject, a prohibition which remained influential across much of the Western world until the late twentieth century.[2] Various hypotheses have been developed on the emergence of language.[3] While Charles Darwin's theory of evolution by natural selection had provoked a surge of speculation on the origin of language over a century and a half ago, the speculations had not resulted in a scientific consensus by 1996.[4] Despite this, academic interest has returned to the topic in the early 1990s. Linguists, archaeologists, psychologists, and anthropologists have renewed the investigation into the origin of language with modern methods.[5]
Attempts to explain the origin of language take a variety of forms:[6]
Most linguistic scholars as of 2024[update] favor continuity-based theories, but they vary in how they hypothesize language development.[citation needed] Some among those who consider language as mostly innate avoid speculating about specific precursors in nonhuman primates, stressing simply that the language faculty must have evolved gradually.[7]
Those who consider language as learned socially, such as Michael Tomasello, consider it developing from the cognitively controlled aspects of primate communication, mostly gestural rather than vocal.[8][9] Where vocal precursors are concerned, many continuity theorists envisage language as evolving from early human capacities for song.[10][11][12][13]
Noam Chomsky, a proponent of discontinuity theory, argues that a single change occurred in humans before leaving Africa, coincident with the Great Leap approximately 100,000 years ago, in which a common language faculty developed in a group of humans and their descendants. Chomsky bases his argument on the observation that any human baby of any culture can be raised in a different culture and will completely assimilate the language and behavior of the new culture in which they were raised. This implies that no major change to the human language faculty has occurred since they left Africa.[14]
Transcending the continuity-versus-discontinuity divide, some scholars view the emergence of language as the consequence of some kind of social transformation[15] that, by generating unprecedented levels of public trust, liberated a genetic potential for linguistic creativity that had previously lain dormant.[16][17][18] "Ritual/speech coevolution theory" exemplifies this approach.[19][20] Scholars in this intellectual camp point to the fact that even chimpanzees and bonobos have latent symbolic capacities that they rarely—if ever—use in the wild.[21] Objecting to the sudden mutation idea, these authors argue that even if a chance mutation were to install a language organ in an evolving bipedal primate, it would be adaptively useless under all known primate social conditions. A very specific social structure – one capable of upholding unusually high levels of public accountability and trust – must have evolved before or concurrently with language to make reliance on "cheap signals" (e.g. words) an evolutionarily stable strategy.
Since the emergence of language lies so far back in human prehistory, the relevant developments have left no direct historical traces, and comparable processes cannot be observed today. Despite this, the emergence of new sign languages in modern times—Nicaraguan Sign Language, for example—may offer insights into the developmental stages and creative processes necessarily involved.[22] Another approach inspects early human fossils, looking for traces of physical adaptation to language use.[23][24] In some cases, when the DNA of extinct humans can be recovered, the presence or absence of genes considered to be language-relevant—FOXP2, for example—may prove informative.[25] Another approach, this time archaeological, involves invoking symbolic behavior (such as repeated ritual activity) that may leave an archaeological trace—such as mining and modifying ochre pigments for body-painting—while developing theoretical arguments to justify inferences from symbolism in general to language in particular.[26][27][28]
The time range for the evolution of language or its anatomical prerequisites extends, at least in principle, from the phylogenetic divergence of Homo (2.3 to 2.4 million years ago) from Pan (5 to 6 million years ago) to the emergence of full behavioral modernity some 50,000–150,000 years ago. Few dispute that Australopithecus probably lacked vocal communication significantly more sophisticated than that of great apes in general,[29] but scholarly opinions vary as to the developments since the appearance of Homo some 2.5 million years ago. Some scholars assume the development of primitive language-like systems (proto-language) as early as Homo habilis, while others place the development of symbolic communication only with Homo erectus (1.8 million years ago) or with Homo heidelbergensis (0.6 million years ago) and the development of language proper with Homo sapiens, currently estimated at less than 200,000 years ago.
Using statistical methods to estimate the time required to achieve the current spread and diversity in modern languages, Johanna Nichols—a linguist at the University of California, Berkeley—argued in 1998 that vocal languages must have begun diversifying in the human species at least 100,000 years ago.[30] Estimates of this kind are not universally accepted, but jointly considering genetic, archaeological, palaeontological, and much other evidence indicates that language likely emerged somewhere in sub-Saharan Africa during the Middle Stone Age, roughly contemporaneous with the speciation of Homo sapiens.[31]
I cannot doubt that language owes its origin to the imitation and modification, aided by signs and gestures, of various natural sounds, the voices of other animals, and man's own instinctive cries.
In 1861, historical linguist Max Müller published a list of speculative theories concerning the origins of spoken language:[33]
Most scholars today consider all such theories not so much wrong—they occasionally offer peripheral insights—as naïve and irrelevant.[35][36] The problem with these theories is that they rest on the assumption that once early humans had discovered a workable mechanism for linking sounds with meanings, language would automatically have evolved.[citation needed]
Much earlier, medieval Muslim scholars developed theories on the origin of language.[37][38] Their theories were of five general types:[39]
From the perspective of signalling theory, the main obstacle to the evolution of language-like communication in nature is not a mechanistic one. Rather, it is the fact that symbols—arbitrary associations of sounds or other perceptible forms with corresponding meanings—are unreliable and may as well be false.[40][41][42] The problem of reliability was not recognized at all by Darwin, Müller or the other early evolutionary theorists.
Animal vocal signals are, for the most part, intrinsically reliable. When a cat purrs, the signal constitutes direct evidence of the animal's contented state. The signal is trusted, not because the cat is inclined to be honest, but because it just cannot fake that sound. Primate vocal calls may be slightly more manipulable, but they remain reliable for the same reason—because they are hard to fake.[43] Primate social intelligence is "Machiavellian"; that is, self-serving and unconstrained by moral scruples. Monkeys, apes and particularly humans often attempt to deceive each other, while at the same time remaining constantly on guard against falling victim to deception themselves.[44][45] Paradoxically, it is theorized that primates' resistance to deception is what blocks the evolution of their signalling systems along language-like lines. Language is ruled out because the best way to guard against being deceived is to ignore all signals except those that are instantly verifiable. Words automatically fail this test.[19]
Words are easy to fake. Should they turn out to be lies, listeners will adapt by ignoring them in favor of hard-to-fake indices or cues. For language to work, listeners must be confident that those with whom they are on speaking terms are generally likely to be honest.[46] A peculiar feature of language is displaced reference, which means reference to topics outside the currently perceptible situation. This property prevents utterances from being corroborated in the immediate "here" and "now". For this reason, language presupposes relatively high levels of mutual trust in order to become established over time as an evolutionarily stable strategy. This stability is born of a longstanding mutual trust and is what grants language its authority. A theory of the origins of language must therefore explain why humans could begin trusting cheap signals in ways that other animals apparently cannot.
The "mother tongues" hypothesis was proposed in 2004 as a possible solution to this problem.[47] W. Tecumseh Fitch suggested that the Darwinian principle of "kin selection"[48]—the convergence of genetic interests between relatives—might be part of the answer. Fitch suggests that languages were originally "mother tongues". If language evolved initially for communication between mothers and their own biological offspring, extending later to include adult relatives as well, the interests of speakers and listeners would have tended to coincide. Fitch argues that shared genetic interests would have led to sufficient trust and cooperation for intrinsically unreliable signals—words—to become accepted as trustworthy and so begin evolving for the first time.[49]
Critics of this theory point out that kin selection is not unique to humans.[50] So even if one accepts Fitch's initial premises, the extension of the posited "mother tongue" networks from close relatives to more distant relatives remains unexplained.[50] Fitch argues, however, that the extended period of physical immaturity of human infants and the postnatal growth of the human brain give the human-infant relationship a different and more extended period of intergenerational dependency than that found in any other species.[47]
Ib Ulbæk[6] invokes another standard Darwinian principle—"reciprocal altruism"[51]—to explain the unusually high levels of intentional honesty necessary for language to evolve. "Reciprocal altruism" can be expressed as the principle that if you scratch my back, I'll scratch yours. In linguistic terms, it would mean that if you speak truthfully to me, I'll speak truthfully to you. Ordinary Darwinian reciprocal altruism, Ulbæk points out, is a relationship established between frequently interacting individuals. For language to prevail across an entire community, however, the necessary reciprocity would have needed to be enforced universally instead of being left to individual choice. Ulbæk concludes that for language to evolve, society as a whole must have been subject to moral regulation.
Critics point out that this theory fails to explain when, how, why or by whom "obligatory reciprocal altruism" could possibly have been enforced.[20] Various proposals have been offered to remedy this defect.[20] A further criticism is that language does not work on the basis of reciprocal altruism anyway. Humans in conversational groups do not withhold information to all except listeners likely to offer valuable information in return. On the contrary, they seem to want to advertise to the world their access to socially relevant information, broadcasting that information without expectation of reciprocity to anyone who will listen.[52]
Gossip, according to Robin Dunbar in his book Grooming, Gossip and the Evolution of Language, language does for group-living humans what manual grooming does for other primates—it allows individuals to service their relationships and so maintain their alliances on the basis of the principle: if you scratch my back, I'll scratch yours. Dunbar argues that as humans began living in increasingly larger social groups, the task of manually grooming all one's friends and acquaintances became so time-consuming as to be unaffordable.[53] In response to this problem, humans developed "a cheap and ultra-efficient form of grooming"—vocal grooming. To keep allies happy, one now needs only to "groom" them with low-cost vocal sounds, servicing multiple allies simultaneously while keeping both hands free for other tasks. Vocal grooming then evolved gradually into vocal language—initially in the form of "gossip".[53] Dunbar's hypothesis seems to be supported by adaptations, in the structure of language, to the function of narration in general.[54]
Critics of this theory point out that the efficiency of "vocal grooming"—the fact that words are so cheap—would have undermined its capacity to signal commitment of the kind conveyed by time-consuming and costly manual grooming.[55] A further criticism is that the theory does nothing to explain the crucial transition from vocal grooming—the production of pleasing but meaningless sounds—to the cognitive complexities of syntactical speech.
The ritual/speech coevolution theory was originally proposed by social anthropologist Roy Rappaport[56] before being elaborated by anthropologists such as Chris Knight,[57] Jerome Lewis,[58] Nick Enfield,[59] Camilla Power[60] and Ian Watts.[61] Cognitive scientist and robotics engineer Luc Steels[62] is another prominent supporter of this general approach, as is biological anthropologist and neuroscientist Terrence Deacon.[63] A more recent champion of the approach is the Chomskyan specialist in linguistic syntax, Cedric Boeckx.[64]
These scholars argue that there can be no such thing as a "theory of the origins of language". This is because language is not a separate adaptation, but an internal aspect of something much wider—namely, the entire domain known to anthropologists as human symbolic culture.[65] Attempts to explain language independently of this wider context have failed, say these scientists, because they are addressing a problem with no solution. Language would not work outside its necessary environment of confidence-building social mechanisms and institutions. For example, it would not work for a nonhuman ape communicating with others of its kind in the wild. Not even the cleverest nonhuman ape could make language work under such conditions.
Lie and alternative, inherent in language ... pose problems to any society whose structure is founded on language, which is to say all human societies. I have therefore argued that if there are to be words at all it is necessary to establish The Word, and that The Word is established by the invariance of liturgy.
Advocates of this school of thought point out that words are cheap. Should an especially clever nonhuman ape, or even a group of articulate nonhuman apes, try to use words in the wild, they would carry no conviction. The primate vocalizations that do carry conviction—those they actually use—are unlike words, in that they are emotionally expressive, intrinsically meaningful, and reliable because they are relatively costly and hard to fake.
Oral and gestural languages consist of pattern-making whose cost is essentially zero. As pure social conventions, signals of this kind cannot evolve in a Darwinian social world—they are a theoretical impossibility.[67] Being intrinsically unreliable, language works only if one can build up a reputation for trustworthiness within a certain kind of society—namely, one where symbolic cultural facts (sometimes called "institutional facts") can be established and maintained through collective social endorsement.[68] In any hunter-gatherer society, the basic mechanism for establishing trust in symbolic cultural facts is collective ritual.[69] Therefore, the task facing researchers into the origins of language is more multidisciplinary than is usually supposed. It involves addressing the evolutionary emergence of human ritual, kinship, religion and symbolic culture taken as a whole, with language an important but subsidiary component.
In a 2023 article, Cedric Boeckx[64] endorses the Rappaport/Searle/Knight way of capturing the "special" nature of human words. Words are symbols. This means that, from a standpoint in Darwinian signal evolution theory, they are "patently false signals." Words are facts, but "facts whose existence depends entirely on subjective belief".[70] In philosophical terms, they are "institutional facts": fictions that are granted factual status within human social institutions[71] From this standpoint, according to Boeckx, linguistic utterances are symbolic to the extent that they are patent falsehoods serving as guides to communicative intentions. "They are communicatively useful untruths, as it were."[64] The reason why words can survive among humans despite being false is largely down to a matter of trust. The corresponding origins theory is that language can only have begun to evolve from the moment humans started reciprocally faking in communicatively helpful ways, i.e., when they became capable of upholding the levels of trust necessary for linguistic communication to work.
The point here is that an ape or other nonhuman must always carry at least some of the burden of generating the trust necessary for communication to work. That is, in order to be taken seriously, each signal it emits must be a patently reliable one, trusted because it is rooted in some way in the real world. But now imagine what might happen under social conditions where trust could be taken for granted. The signaller could stop worrying about reliability and concentrate instead on perceptual discriminability. Carried to its conclusion, this should permit digital signaling—the cheapest and most efficient kind of communication.
From this philosophical standpoint, animal communication cannot be digital because it does not have the luxury of being patently false. Costly signals of any kind can only be evaluated on an analog scale. Put differently, truly symbolic, digital signals become socially acceptable only under highly unusual conditions—such as those internal to a ritually bonded community whose members are not tempted to lie.[citation needed]
Critics of the speech/ritual co-evolution idea theory include Noam Chomsky, who terms it the "non-existence" hypothesis—a denial of the very existence of language as an object of study for natural science.[72] Chomsky's own theory is that language emerged in an instant and in perfect form,[73] prompting his critics in turn, to retort that only something that does not exist—a theoretical construct or convenient scientific fiction—could possibly emerge in such a miraculous way.[17] The controversy remains unresolved.
Acheulean tool use began during the Lower Paleolithic approximately 1.75 million years ago. Studies focusing on the lateralization of Acheulean tool production and language production have noted similar areas of blood flow when engaging in these activities separately; this theory suggests that the brain functions needed for the production of tools across generations is consistent with the brain systems required for producing language. Researchers used functional transcranial Doppler ultrasonography (fTDC) and had participants perform activities related to the creation of tools using the same methods during the Lower Paleolithic as well as a task designed specifically for word generation.[74] The purpose of this test was to focus on the planning aspect of Acheulean tool making and cued word generation in language (an example of cued word generation would be trying to list all words beginning with a given letter). Theories of language developing alongside tool use has been theorized by multiple individuals;[75][76][77] however, until recently, there has been little empirical data to support these hypotheses. Focusing on the results of the study performed by Uomini et al. evidence for the usage of the same brain areas has been found when looking at cued word generation and Acheulean tool use. The relationship between tool use and language production is found in working and planning memory respectively and was found to be similar across a variety of participants, furthering evidence that these areas of the brain are shared.[74] This evidence lends credibility to the theory that language developed alongside tool use in the Lower Paleolithic.
The humanistic tradition considers language as a human invention. Renaissance philosopher Antoine Arnauld gave a detailed description of his idea of the origin of language in Port-Royal Grammar. According to Arnauld, people are social and rational by nature, and this urged them to create language as a means to communicate their ideas to others. Language construction would have occurred through a slow and gradual process.[78] In later theory, especially in functional linguistics, the primacy of communication
is emphasised over psychological needs.[79]
The exact way language evolved is however not considered as vital to the study of languages. Structural linguist Ferdinand de Saussure abandoned evolutionary linguistics after having come to the firm conclusion that it would not be able to provide any further revolutionary insight after the completion of the major works in historical linguistics by the end of the 19th century. Saussure was particularly sceptical of the attempts of August Schleicher and other Darwinian linguists to access prehistorical languages through series of reconstructions of proto-languages.[80]
Saussure's solution to the problem of language evolution involves dividing theoretical linguistics in two. Evolutionary and historical linguistics are renamed as diachronic linguistics. It is the study of language change, but it has only limited explanatory power due to the inadequacy of all of the reliable research material that could ever be made available. Synchronic linguistics, in contrast, aims to widen scientists' understanding of language through a study of a given contemporary or historical language stage as a system in its own right.[81]
Although Saussure put much focus on diachronic linguistics, later structuralists who equated structuralism with the synchronic analysis were sometimes criticised of ahistoricism. According to structural anthropologist Claude Lévi-Strauss, language and meaning—in opposition to "knowledge, which develops slowly and progressively"—must have appeared in an instant.[82]
Structuralism, as first introduced to sociology by Émile Durkheim, is nonetheless a type of humanistic evolutionary theory which explains diversification as necessitated by growing complexity.[83] There was a shift of focus to functional explanation after Saussure's death. Functional structuralists including the Prague Circle linguists and André Martinet explained the growth and maintenance of structures as being necessitated by their functions.[79] For example, novel technologies make it necessary for people to invent new words, but these may lose their function and be forgotten as the technologies are eventually replaced by more modern ones.
According to Chomsky's single-mutation theory, the emergence of language resembled the formation of a crystal; with digital infinity as the seed crystal in a super-saturated primate brain, on the verge of blossoming into the human mind, by physical law, once evolution added a single small but crucial keystone.[84][85] Thus, in this theory, language appeared rather suddenly within the history of human evolution. Chomsky, writing with computational linguist and computer scientist Robert C. Berwick, suggests that this scenario is completely compatible with modern biology. They note that "none of the recent accounts of human language evolution seem to have completely grasped the shift from conventional Darwinism to its fully stochastic modern version—specifically, that there are stochastic effects not only due to sampling like directionless drift, but also due to directed stochastic variation in fitness, migration, and heritability—indeed, all the "forces" that affect individual or gene frequencies ... All this can affect evolutionary outcomes—outcomes that as far as we can make out are not brought out in recent books on the evolution of language, yet would arise immediately in the case of any new genetic or individual innovation, precisely the kind of scenario likely to be in play when talking about language's emergence."
Citing evolutionary geneticist Svante Pääbo, they concur that a substantial difference must have occurred to differentiate Homo sapiens from Neanderthals to "prompt the relentless spread of our species, who had never crossed open water, up and out of Africa and then on across the entire planet in just a few tens of thousands of years. ... What we do not see is any kind of 'gradualism' in new tool technologies or innovations like fire, shelters, or figurative art." Berwick and Chomsky therefore suggest language emerged approximately between 200,000 years ago and 60,000 years ago (between the appearance of the first anatomically modern humans in southern Africa and the last exodus from Africa respectively). "That leaves us with about 130,000 years, or approximately 5,000–6,000 generations of time for evolutionary change. This is not 'overnight in one generation' as some have (incorrectly) inferred—but neither is it on the scale of geological eons. It's time enough—within the ballpark for what Nilsson and Pelger (1994) estimated as the time required for the full evolution of a vertebrate eye from a single cell, even without the invocation of any 'evo-devo' effects."[86]
The single-mutation theory of language evolution has been directly questioned on different grounds. A formal analysis of the probability of such a mutation taking place and going to fixation in the species has concluded that such a scenario is unlikely, with multiple mutations with more moderate fitness effects being more probable.[87] Another criticism has questioned the logic of the argument for single mutation and puts forward that from the formal simplicity of Merge, the capacity Berwick and Chomsky deem the core property of human language that emerged suddenly, one cannot derive the (number of) evolutionary steps that led to it.[88]
The Romulus and Remus hypothesis, proposed by neuroscientist Andrey Vyshedskiy, seeks to address the question as to why the modern speech apparatus originated over 500,000 years before the earliest signs of modern human imagination. This hypothesis proposes that there were two phases that led to modern recursive language. The phenomenon of recursion occurs across multiple linguistic domains, arguably most prominently in syntax and morphology. Thus, by nesting a structure such as a sentence or a word within themselves, it enables the generation of potentially (countably) infinite new variations of that structure. For example, the base sentence [Peter likes apples.] can be nested in irrealis clauses to produce [Mary said [Peter likes apples.]], [Paul believed [Mary said [Peter likes apples.]]] and so forth.[89]
The first phase includes the slow development of non-recursive language with a large vocabulary along with the modern speech apparatus, which includes changes to the hyoid bone, increased voluntary control of the muscles of the diaphragm, and the evolution of the FOXP2 gene, as well as other changes by 600,000 years ago.[90] Then, the second phase was a rapid Chomskian single step, consisting of three distinct events that happened in quick succession around 70,000 years ago and allowed the shift from non-recursive to recursive language in early hominins.
It is not enough for children to have a modern prefrontal cortex (PFC) to allow the development of PFS; the children must also be mentally stimulated and have recursive elements already in their language to acquire PFS. Since their parents would not have invented these elements yet, the children would have had to do it themselves, which is a common occurrence among young children that live together, in a process called cryptophasia.[92] This means that delayed PFC development would have allowed more time to acquire PFS and develop recursive elements.
Delayed PFC development also comes with negative consequences, such as a longer period of reliance on one's parents to survive and lower survival rates. For modern language to have occurred, PFC delay had to have an immense survival benefit in later life, such as PFS ability. This suggests that the mutation that caused PFC delay and the development of recursive language and PFS occurred simultaneously, which lines up with evidence of a genetic bottleneck around 70,000 years ago.[93] This could have been the result of a few individuals who developed PFS and recursive language which gave them significant competitive advantage over all other humans at the time.[91]
The gestural theory states that human language developed from gestures that were used for simple communication.
Two types of evidence support this theory.
Research has found strong support for the idea that oral communication and sign language depend on similar neural structures. Patients who used sign language, and who suffered from a left-hemisphere lesion, showed the same disorders with their sign language as vocal patients did with their oral language.[96] Other researchers found that the same left-hemisphere brain regions were active during sign language as during the use of vocal or written language.[97]
Primate gesture is at least partially genetic: different nonhuman apes will perform gestures characteristic of their species, even if they have never seen another ape perform that gesture. For example, gorillas beat their breasts. This shows that gestures are an intrinsic and important part of primate communication, which supports the idea that language evolved from gesture.[98]
Further evidence suggests that gesture and language are linked. In humans, manually gesturing has an effect on concurrent vocalizations, thus creating certain natural vocal associations of manual efforts. Chimpanzees move their mouths when performing fine motor tasks. These mechanisms may have played an evolutionary role in enabling the development of intentional vocal communication as a supplement to gestural communication. Voice modulation could have been prompted by preexisting manual actions.[98]
From infancy, gestures both supplement and predict speech.[99][100] This addresses the idea that gestures quickly change in humans from a sole means of communication (from a very young age) to a supplemental and predictive behavior that is used despite the ability to communicate verbally. This too serves as a parallel to the idea that gestures developed first and language subsequently built upon it.
Two possible scenarios have been proposed for the development of language,[101] one of which supports the gestural theory:
The first perspective that language evolved from the calls of human ancestors seems logical because both humans and animals make sounds or cries. One evolutionary reason to refute this is that, anatomically, the centre that controls calls in monkeys and other animals is located in a completely different part of the brain than in humans. In monkeys, this centre is located in the depths of the brain related to emotions. In the human system, it is located in an area unrelated to emotion. Humans can communicate simply to communicate—without emotions. So, anatomically, this scenario does not work.[101] This suggests that language was derived from gesture[102](humans communicated by gesture first and sound was attached later).
The important question for gestural theories is why there was a shift to vocalization. Various explanations have been proposed:
A comparable hypothesis states that in 'articulate' language, gesture and vocalisation are intrinsically linked, as language evolved from equally intrinsically linked dance and song.[13]
Humans still use manual and facial gestures when they speak, especially when people meet who have no language in common.[106] There are also a great number of sign languages still in existence, commonly associated with Deaf communities. These sign languages are equal in complexity, sophistication, and expressive power, to any oral language.[107] The cognitive functions are similar and the parts of the brain used are similar. The main difference is that the "phonemes" are produced on the outside of the body, articulated with hands, body, and facial expression, rather than inside the body articulated with tongue, teeth, lips, and breathing.[108] (Compare the motor theory of speech perception.)
Critics of gestural theory note that it is difficult to name serious reasons why the initial pitch-based vocal communication (which is present in primates) would be abandoned in favor of the much less effective non-vocal, gestural communication.[109] However, Michael Corballis has pointed out that it is supposed that primate vocal communication (such as alarm calls) cannot be controlled consciously, unlike hand movement, and thus it is not credible as precursor to human language; primate vocalization is rather homologous to and continued in involuntary reflexes (connected with basic human emotions) such as screams or laughter (the fact that these can be faked does not disprove the fact that genuine involuntary responses to fear or surprise exist).[102] Also, gesture is not generally less effective, and depending on the situation can even be advantageous, for example in a loud environment or where it is important to be silent, such as on a hunt. Other challenges to the "gesture-first" theory have been presented by researchers in psycholinguistics, including David McNeill.[110]
Proponents of the motor theory of language evolution have primarily focused on the visual domain and communication through observation of movements. The Tool-use sound hypothesis suggests that the production and perception of sound also contributed substantially, particularly incidental sound of locomotion (ISOL) and tool-use sound (TUS).[111] Human bipedalism resulted in rhythmic and more predictable ISOL. That may have stimulated the evolution of musical abilities, auditory working memory, and abilities to produce complex vocalizations, and to mimic natural sounds.[112] Since the human brain proficiently extracts information about objects and events from the sounds they produce, TUS, and mimicry of TUS, might have achieved an iconic function. The prevalence of sound symbolism in many extant languages supports this idea. Self-produced TUS activates multimodal brain processing (motor neurons, hearing, proprioception, touch, vision), and TUS stimulates primate audiovisual mirror neurons, which is likely to stimulate the development of association chains. Tool use and auditory gestures involve motor-processing of the forelimbs, which is associated with the evolution of vertebrate vocal communication. The production, perception, and mimicry of TUS may have resulted in a limited number of vocalizations or protowords that were associated with tool use.[111] A new way to communicate about tools, especially when out of sight, would have had selective advantage. A gradual change in acoustic properties, meaning, or both could have resulted in arbitrariness and an expanded repertoire of words. Humans have been increasingly exposed to TUS over millions of years, coinciding with the period during which spoken language evolved.
In humans, functional MRI studies have reported finding areas homologous to the monkey mirror neuron system in the inferior frontal cortex, close to Broca's area, one of the language regions of the brain. This has led to suggestions that human language evolved from a gesture performance/understanding system implemented in mirror neurons. Mirror neurons have been said to have the potential to provide a mechanism for action-understanding, imitation-learning, and the simulation of other people's behavior.[113] This hypothesis is supported by some cytoarchitectonic homologies between monkey premotor area F5 and human Broca's area.[114]
Rates of vocabulary expansion link to the ability of children to vocally mirror non-words and so to acquire the new word pronunciations. Such speech repetition occurs automatically, quickly[115] and separately in the brain to speech perception.[116][117] Moreover, such vocal imitation can occur without comprehension such as in speech shadowing[118] and echolalia.[114][119] Further evidence for this link comes from a recent study in which the brain activity of two participants was measured using fMRI while they were gesturing words to each other using hand gestures with a game of charades—a modality that some have suggested might represent the evolutionary precursor of human language. Analysis of the data using Granger Causality revealed that the mirror-neuron system of the observer indeed reflects the pattern of activity of in the motor system of the sender, supporting the idea that the motor concept associated with the words is indeed transmitted from one brain to another using the mirror system.[120]
Not all linguists agree with the above arguments, however. In particular, supporters of Noam Chomsky argue against the possibility that the mirror neuron system can play any role in the hierarchical recursive structures essential to syntax.[121]
According to Dean Falk's "putting-down-the-baby" theory, vocal interactions between early hominid mothers and infants began a sequence of events that led, eventually, to human ancestors' earliest words.[122] The basic idea is that evolving human mothers, unlike their counterparts in other primates, could not move around and forage with their infants clinging onto their backs. Loss of fur in the human case left infants with no means of clinging on. Frequently, therefore, mothers had to put their babies down. As a result, these babies needed to be reassured that they were not being abandoned. Mothers responded by developing 'motherese'—an infant-directed communicative system embracing facial expressions, body language, touching, patting, caressing, laughter, tickling, and emotionally expressive contact calls. The argument is that language developed out of this interaction.[122]
In The Mental and Social Life of Babies, psychologist Kenneth Kaye noted that no usable adult language could have evolved without interactive communication between very young children and adults. "No symbolic system could have survived from one generation to the next if it could not have been easily acquired by young children under their normal conditions of social life."[123]
The "from where to what" model is a language evolution model that is derived primarily from the organization of language processing in the brain into two structures: the auditory dorsal stream and the auditory ventral stream.[124][125] It hypothesizes seven stages of language evolution (see illustration). Speech originated for the purpose of exchanging contact calls between mothers and their offspring to find one another in the event they became separated (illustration part 1). The contact calls could be modified with intonations in order to express either a higher or lower level of distress (illustration part 2). The use of two types of contact calls enabled the first question-answer conversation. In this scenario, the child would emit a low-level distress call to express a desire to interact with an object, and the mother would respond with either another low-level distress call (to express approval of the interaction) or a high-level distress call (to express disapproval) (illustration part 3). Over time, the improved use of intonations and vocal control led to the invention of unique calls (phonemes) associated with distinct objects (illustration part 4). At first, children learned the calls (phonemes) from their parents by imitating their lip-movements (illustration part 5). Eventually, infants were able to encode into long-term memory all the calls (phonemes). Consequentially, mimicry via lip-reading was limited to infancy and older children learned new calls through mimicry without lip-reading (illustration part 6). Once individuals became capable of producing a sequence of calls, this allowed multi-syllabic words, which increased the size of their vocabulary (illustration part 7). The use of words, composed of sequences of syllables, provided the infrastructure for communicating with sequences of words (i.e. sentences).
The theory's name is derived from the two auditory streams, which are both found in the brains of humans and other primates. The auditory ventral stream is responsible for sound recognition, and so it is referred to as the auditory what stream.[126][127][128] In primates, the auditory dorsal stream is responsible for sound localization, and thus it is called the auditory where stream. Only in humans (in the left hemisphere) is it also responsible for other processes associated with language use and acquisition, such as speech repetition and production, integration of phonemes with their lip movements, perception and production of intonations, phonological long-term memory (long-term memory storage of the sounds of words), and phonological working memory (the temporary storage of the sounds of words).[129][130][131][132][133][134][135][136] Some evidence also indicates a role in recognizing others by their voices.[137][138] The emergence of each of these functions in the auditory dorsal stream represents an intermediate stage in the evolution of language.
A contact call origin for human language is consistent with animal studies, as like human language, contact call discrimination in monkeys is lateralised to the left hemisphere.[139][140] Mice with knock-out to language related genes (such as FOXP2 and SRPX2) also resulted in the pups no longer emitting contact calls when separated from their mothers.[141][142] Supporting this model is also its ability to explain unique human phenomena, such as the use of intonations when converting words into commands and questions, the tendency of infants to mimic vocalizations during the first year of life (and its disappearance later on) and the protruding and visible human lips, which are not found in other apes. This theory could be considered an elaboration of the putting-down-the-baby theory of language evolution.
"Grammaticalization" is a continuous historical process in which free-standing words develop into grammatical appendages, while these in turn become ever more specialized and grammatical. An initially "incorrect" usage, in becoming accepted, leads to unforeseen consequences, triggering knock-on effects and extended sequences of change. Paradoxically, grammar evolves because, in the final analysis, humans care less about grammatical niceties than about making themselves understood.[143] If this is how grammar evolves today, according to this school of thought, similar principles at work can be legitimately inferred among distant human ancestors, when grammar itself was first being established.[144][145][146]
In order to reconstruct the evolutionary transition from early language to languages with complex grammars, it is necessary to know which hypothetical sequences are plausible and which are not. In order to convey abstract ideas, the first recourse of speakers is to fall back on immediately recognizable concrete imagery, very often deploying metaphors rooted in shared bodily experience.[147] A familiar example is the use of concrete terms such as "belly" or "back" to convey abstract meanings such as "inside" or "behind". Equally metaphorical is the strategy of representing temporal patterns on the model of spatial ones. For example, English speakers might say "It is going to rain", modelled on "I am going to London." This can be abbreviated colloquially to "It's gonna rain." Even when in a hurry, English speakers do not say "I'm gonna London"—the contraction is restricted to the job of specifying tense. From such examples it can be seen why grammaticalisation is consistently unidirectional—from concrete to abstract meaning, not the other way around.[144]
Grammaticalization theorists picture early language as simple, perhaps consisting only of nouns.[146]p. 111 Even under that extreme theoretical assumption, however, it is difficult to imagine what would realistically have prevented people from using, say, "spear" as if it were a verb ("Spear that pig!"). People might have used their nouns as verbs or their verbs as nouns as occasion demanded. In short, while a noun-only language might seem theoretically possible, grammaticalization theory indicates that it cannot have remained fixed in that state for any length of time.[144][148]
Creativity drives grammatical change.[148] This presupposes a certain attitude on the part of listeners. Instead of punishing deviations from accepted usage, listeners must prioritise imaginative mind-reading. Imaginative creativity—emitting a leopard alarm when no leopard was present, for example—is not the kind of behaviour which, say, vervet monkeys would appreciate or reward.[149] Creativity and reliability are incompatible demands; for "Machiavellian" primates as for animals generally, the overriding pressure is to demonstrate reliability.[150] If humans escape these constraints, it is because in their case, listeners are primarily interested in mental states.
To focus on mental states is to accept fictions—inhabitants of the imagination—as potentially informative and interesting. An example is metaphor: a metaphor is, literally, a false statement.[151] In Romeo and Juliet, Romeo declares "Juliet is the sun!". Juliet is a woman, not a ball of plasma in the sky, but human listeners are not (or not usually) pedants insistent on point-by-point factual accuracy. They want to know what the speaker has in mind. Grammaticalisation is essentially based on metaphor. To outlaw its use would be to stop grammar from evolving and, by the same token, to exclude all possibility of expressing abstract thought.[147][152]
A criticism of all this is that while grammaticalization theory might explain language change today, it does not satisfactorily address the really difficult challenge—explaining the initial transition from primate-style communication to language as it is known today. Rather, the theory assumes that language already exists. As Bernd Heine and Tania Kuteva acknowledge: "Grammaticalisation requires a linguistic system that is used regularly and frequently within a community of speakers and is passed on from one group of speakers to another".[146] Outside modern humans, such conditions do not prevail.
Human language is used for self-expression; however, expression displays different stages. The consciousness of self and feelings represents the stage immediately prior to the external, phonetic expression of feelings in the form of sound (i.e. language). Intelligent animals such as dolphins, Eurasian magpies, and chimpanzees live in communities, wherein they assign themselves roles for group survival and show emotions such as sympathy.[153] When such animals view their reflection (mirror test), they recognize themselves and exhibit self-consciousness.[154] Notably, humans evolved in a quite different environment than that of these animals. Human survival became easier with the development of tools, shelter, and fire, thus facilitating further advancement of social interaction, self-expression, and tool-making, as for hunting and gathering.[155] The increasing brain size allowed advanced provisioning and tools and the technological advances during the Palaeolithic era that built upon the previous evolutionary innovations of bipedalism and hand versatility allowed the development of human language.[citation needed]
According to a study investigating the song differences between white-rumped munias and its domesticated counterpart (Bengalese finch), the wild munias use a highly stereotyped song sequence, whereas the domesticated ones sing a highly unconstrained song. In wild finches, song syntax is subject to female preference—sexual selection—and remains relatively fixed. However, in the Bengalese finch, natural selection is replaced by breeding, in this case for colorful plumage, and thus, decoupled from selective pressures, stereotyped song syntax is allowed to drift. It is replaced, supposedly within 1000 generations, by a variable and learned sequence. Wild finches, moreover, are thought incapable of learning song sequences from other finches.[156] In the field of bird vocalization, brains capable of producing only an innate song have very simple neural pathways: the primary forebrain motor centre, called the robust nucleus of arcopallium, connects to midbrain vocal outputs, which in turn project to brainstem motor nuclei. By contrast, in brains capable of learning songs, the arcopallium receives input from numerous additional forebrain regions, including those involved in learning and social experience. Control over song generation has become less constrained, more distributed, and more flexible.[156]
One way to think about human evolution is that humans are self-domesticated apes. Just as domestication relaxed selection for stereotypic songs in the finches—mate choice was supplanted by choices made by the aesthetic sensibilities of bird breeders and their customers—so might human cultural domestication have relaxed selection on many of their primate behavioural traits, allowing old pathways to degenerate and reconfigure. Given the highly indeterminate way that mammalian brains develop—they basically construct themselves "bottom up", with one set of neuronal interactions preparing for the next round of interactions—degraded pathways would tend to seek out and find new opportunities for synaptic hookups. Such inherited de-differentiations of brain pathways might have contributed to the functional complexity that characterises human language. And, as exemplified by the finches, such de-differentiations can occur in very rapid time-frames.[157]
A distinction can be drawn between speech and language. Language is not necessarily spoken: it might alternatively be written or signed. Speech is among a number of different methods of encoding and transmitting linguistic information, albeit arguably[by whom?] the most natural one.[158]
Some scholars, such as Noam Chomsky, view language as an initially cognitive development, its "externalisation" to serve communicative purposes occurring later in human evolution. According to one such school of thought, the key feature distinguishing human language is recursion,[159] (in this context, the iterative embedding of phrases within phrases). Other scholars—notably Daniel Everett—deny that recursion is universal, citing certain languages (e.g. Pirahã) which allegedly[by whom?] lack this feature.[160]
The ability to ask questions is considered by some[like whom?] to distinguish language from non-human systems of communication.[161] Some captive primates (notably bonobos and chimpanzees), having learned to use rudimentary signing to communicate with their human trainers, proved able to respond correctly to complex questions and requests. Yet they failed to ask even the simplest questions themselves.[162] Conversely, human children are able to ask their first questions (using only question intonation) at the babbling period of their development, long before they start using syntactic structures. Although babies from different cultures acquire native languages from their social environment, all languages of the world without exception—tonal, non-tonal, intonational and accented—use similar rising "question intonation" for yes–no questions.[163][164] Except, of course, the ones that don't.[165] [clarification needed] This fact is a strong evidence of the universality of question intonation. In general, according to some authors[like whom?], sentence intonation/pitch is pivotal in spoken grammar and is the basic information used by children to learn the grammar of whatever language.[13]
Language users have high-level reference (or deixis)—the ability to refer to things or states of being that are not in the immediate realm of the speaker. This ability is often related to theory of mind, or an awareness of the other as a being like the self with individual wants and intentions. According to Chomsky, Hauser and Fitch (2002), there are six main aspects of this high-level reference system:
Simon Baron-Cohen (1999) argues that theory of mind must have preceded language use, based on evidence of use of the following characteristics as much as 40,000 years ago: intentional communication, repairing failed communication, teaching, intentional persuasion, intentional deception, building shared plans and goals, intentional sharing of focus or topic, and pretending. Moreover, Baron-Cohen argues that many primates show some, but not all, of these abilities.[citation needed] Call and Tomasello's research on chimpanzees supports this, in that individual chimps seem to understand that other chimps have awareness, knowledge, and intention, but do not seem to understand false beliefs. Many primates show some tendencies toward a theory of mind, but not a full one as humans have.[166]
Ultimately, there is some consensus within the field that a theory of mind is necessary for language use. Thus, the development of a full theory of mind in humans was a necessary precursor to full language use.[167]
In one particular study, rats and pigeons were required to press a button a certain number of times to get food. The animals showed very accurate distinction for numbers less than four, but as the numbers increased, the error rate increased.[159] In another, the primatologist Tetsuro Matsuzawa attempted to teach chimpanzees Arabic numerals.[168] The difference between primates and humans in this regard was very large, as it took the chimps thousands of trials to learn 1–9, with each number requiring a similar amount of training time; yet, after learning the meaning of 1, 2 and 3 (and sometimes 4), children (after the age of 5.5 to 6) easily comprehend the value of greater integers by using a successor function (i.e. 2 is 1 greater than 1, 3 is 1 greater than 2, 4 is 1 greater than 3; once 4 is reached it seems most children suddenly understand that the value of any integer n is 1 greater than the previous integer).[169] Put simply, other primates learn the meaning of numbers one by one, similar to their approach to other referential symbols, while children first learn an arbitrary list of symbols (1, 2, 3, 4...) and then later learn their precise meanings.[170] These results can be seen as evidence for the application of the "open-ended generative property" of language in human numeral cognition.[159]
Hockett (1966) details a list of features regarded as essential to describing human language.[171] In the domain of the lexical-phonological principle, two features of this list are most important:
The sound system of a language is composed of a finite set of simple phonological items. Under the specific phonotactic rules of a given language, these items can be recombined and concatenated, giving rise to morphology and the open-ended lexicon. A key feature of language is that a simple, finite set of phonological items gives rise to an infinite lexical system wherein rules determine the form of each item, and meaning is inextricably linked with form. Phonological syntax, then, is a simple combination of pre-existing phonological units. Related to this is another essential feature of human language: lexical syntax, wherein pre-existing units are combined, giving rise to semantically novel or distinct lexical items.[This paragraph needs citation(s)]
Certain elements of the lexical-phonological principle are known to exist outside of humans. While all (or nearly all) have been documented in some form in the natural world, very few coexist within the same species. Bird-song, singing nonhuman apes, and the songs of whales all display phonological syntax, combining units of sound into larger structures apparently devoid of enhanced or novel meaning. Certain other primate species do have simple phonological systems with units referring to entities in the world. However, in contrast to human systems, the units in these primates' systems normally occur in isolation, betraying a lack of lexical syntax. There is new[when?] evidence to suggest that Campbell's monkeys also display lexical syntax, combining two calls (a predator alarm call with a "boom", the combination of which denotes a lessened threat of danger), however it is still unclear whether this is a lexical or a morphological phenomenon.[172]
Pidgins are significantly simplified languages with only rudimentary grammar and a restricted vocabulary. In their early stage, pidgins mainly consist of nouns, verbs, and adjectives with few or no articles, prepositions, conjunctions or auxiliary verbs. Often the grammar has no fixed word order and the words have no inflection.[173]
If contact is maintained between the groups speaking the pidgin for long periods of time, the pidgins may become more complex over many generations. If the children of one generation adopt the pidgin as their native language it develops into a creole language, which becomes fixed and acquires a more complex grammar, with fixed phonology, syntax, morphology, and syntactic embedding. The syntax and morphology of such languages may often have local innovations not obviously derived from any of the parent languages.
Studies of creole languages around the world have suggested that they display remarkable similarities in grammar[citation needed] and are developed uniformly from pidgins in a single generation. These similarities are apparent even when creoles do not have any common language origins. In addition, creoles are similar, despite being developed in isolation from each other. Syntactic similarities include subject–verb–object word order. Even when creoles are derived from languages with a different word order they often develop the SVO word order. Creoles tend to have similar usage patterns for definite and indefinite articles, and similar movement rules for phrase structures even when the parent languages do not.[173]
Field primatologists can give useful insights into great ape communication in the wild.[29] One notable finding is that nonhuman primates, including the other great apes, produce calls that are graded, as opposed to categorically differentiated, with listeners striving to evaluate subtle gradations in signallers' emotional and bodily states. Nonhuman apes seemingly find it extremely difficult to produce vocalisations in the absence of the corresponding emotional states.[43] In captivity, nonhuman apes have been taught rudimentary forms of sign language or have been persuaded to use lexigrams—symbols that do not graphically resemble the corresponding words—on computer keyboards. Some nonhuman apes, such as Kanzi, have been able to learn and use hundreds of lexigrams.[174][175]
The Broca's and Wernicke's areas in the primate brain are responsible for controlling the muscles of the face, tongue, mouth, and larynx, as well as recognizing sounds. Primates are known to make "vocal calls", and these calls are generated by circuits in the brainstem and limbic system.[176]
In the wild, the communication of vervet monkeys has been the most extensively studied.[173] They are known to make up to ten different vocalizations. Many of these are used to warn other members of the group about approaching predators. They include a "leopard call", a "snake call", and an "eagle call".[177] Each call triggers a different defensive strategy in the monkeys who hear the call and scientists were able to elicit predictable responses from the monkeys using loudspeakers and prerecorded sounds. Other vocalisations may be used for identification. If an infant monkey calls, its mother turns toward it, but other vervet mothers turn instead toward that infant's mother to see what she will do.[178][179]
Similarly, researchers have demonstrated that chimpanzees (in captivity) use different "words" in reference to different foods. They recorded vocalisations that chimps made in reference, for example, to grapes, and then other chimps pointed at pictures of grapes when they heard the recorded sound.[180][181]
A study published in HOMO: Journal of Comparative Human Biology in 2017 claims that Ardipithecus ramidus, a hominin dated at approximately 4.5 Ma, shows the first evidence of an anatomical shift in the hominin lineage suggestive of increased vocal capability.[182] This study compared the skull of A. ramidus with 29 chimpanzee skulls of different ages and found that in numerous features A. ramidus clustered with the infant and juvenile measures as opposed to the adult measures. Such affinity with the shape dimensions of infant and juvenile chimpanzee skull architecture, it was argued, may have resulted in greater vocal capability. This assertion was based on the notion that the chimpanzee vocal tract ratios that prevent speech are a result of growth factors associated with puberty—growth factors absent in A. ramidus ontogeny. A. ramidus was also found to have a degree of cervical lordosis more conducive to vocal modulation when compared with chimpanzees as well as cranial base architecture suggestive of increased vocal capability.
What was significant in this study, according to the authors,[182] was the observation that the changes in skull architecture that correlate with reduced aggression are the same changes necessary for the evolution of early hominin vocal ability. In integrating data on anatomical correlates of primate mating and social systems with studies of skull and vocal tract architecture that facilitate speech production, the authors argue that paleoanthropologists prior to their study have failed to understand the important relationship between early hominin social evolution and the evolution of our species' capacities for language.
While the skull of A. ramidus, according to the authors, lacks the anatomical impediments to speech evident in chimpanzees, it is unclear what the vocal capabilities of this early hominin were. While they suggest A. ramidus—based on similar vocal tract ratios—may have had vocal capabilities equivalent to a modern human infant or very young child, they concede this is a debatable and speculative hypothesis. However, they do claim that changes in skull architecture through processes of social selection were a necessary prerequisite for language evolution. As they write:
We propose that as a result of paedomorphic morphogenesis of the cranial base and craniofacial morphology Ar. ramidus would have not been limited in terms of the mechanical components of speech production as chimpanzees and bonobos are. It is possible that Ar. ramidus had vocal capability approximating that of chimpanzees and bonobos, with its idiosyncratic skull morphology not resulting in any significant advances in speech capability. In this sense the anatomical features analysed in this essay would have been exapted in later more voluble species of hominin. However, given the selective advantages of pro-social vocal synchrony, we suggest the species would have developed significantly more complex vocal abilities than chimpanzees and bonobos.[182]
Anatomically, some scholars believe that features of bipedalism developed in the australopithecines around 3.5 million years ago. Around this time, these structural developments within the skull led to a more prominently L-shaped vocal tract.[183][page needed] In order to generate the sounds modern Homo sapiens are capable of making, such as vowels, it is vital that Early Homo populations must have a specifically shaped voice track and a lower sitting larynx.[184] Opposing research previously suggested that Neanderthals were physically incapable of creating the full range of vocals seen in modern humans due to the differences in larynx placement. Establishing distinct larynx positions through fossil remains of Homo sapiens and Neanderthals would support this theory; however, modern research has revealed that the hyoid bone was indistinguishable in the two populations. Though research has shown a lower sitting larynx is important to producing speech, another theory states it may not be as important as once thought.[185] Cataldo, Migliano, and Vinicius report speech alone appears inadequate for transmitting stone tool-making knowledge, and suggest that speech may have emerged due to an increase in complex social interactions.[186]
Steven Mithen proposed the term Hmmmmm for the pre-linguistic system of communication posited to have been used by archaic Homo, beginning with Homo ergaster and reaching the highest sophistication in the Middle Pleistocene with Homo heidelbergensis and Homo neanderthalensis. Hmmmmm is an acronym for holistic (non-compositional), manipulative (utterances are commands or suggestions, not descriptive statements), multi-modal (acoustic as well as gestural and facial), musical, and mimetic.[187]
Evidence for Homo erectus potentially using language comes in the form of Acheulean tool usage. The use of abstract thought in the formation of Acheulean hand axes coincides with the symbol creation necessary for simple language.[188] Recent language theories present recursion as the unique facet of human language and theory of mind.[189][190] However, breaking down language into its symbolic parts: separating meaning from the requirements of grammar, it becomes possible to see that language does not depend on either recursion or grammar. This can be evidenced by the Pirahã language users in Brazil that have no myth or creation stories, no numbers and no colors within their language.[191] This is to highlight that even though grammar may have been unavailable, use of foresight, planning and symbolic thought can be evidence of language as early as one million years ago with Homo erectus.
Homo heidelbergensis was a close relative (most probably a migratory descendant) of Homo ergaster. Some researchers believe this species to be the first hominin to make controlled vocalisations, possibly mimicking animal vocalisations,[187] and that as Homo heidelbergensis developed more sophisticated culture, proceeded from this point and possibly developed an early form of symbolic language.
The discovery in 1989 of the (Neanderthal) Kebara 2 hyoid bone suggests that Neanderthals may have been anatomically capable of producing sounds similar to modern humans.[192][193] The hypoglossal nerve, which passes through the hypoglossal canal, controls the movements of the tongue, which may have enabled voicing for size exaggeration (see size exaggeration hypothesis below) or may reflect speech abilities.[24][194][195][196][197][198]
However, although Neanderthals may have been anatomically able to speak, Richard G. Klein in 2004 doubted that they possessed a fully modern language. He largely bases his doubts on the fossil record of archaic humans and their stone tool kit. Bart de Boer in 2017 acknowledges this ambiguity of a universally accepted Neanderthal vocal tract; however, he notes the similarities in the thoracic vertebral canal, potential air sacs, and hyoid bones between modern humans and Neanderthals to suggest the presence of complex speech.[199] For two million years following the emergence of Homo habilis, the stone tool technology of hominins changed very little. Klein, who has worked extensively on ancient stone tools, describes the crude stone tool kit of archaic humans as impossible to break down into categories based on their function, and reports that Neanderthals seem to have had little concern for the final aesthetic form of their tools. Klein argues that the Neanderthal brain may have not reached the level of complexity required for modern speech, even if the physical apparatus for speech production was well-developed.[200][201] The issue of the Neanderthal's level of cultural and technological sophistication remains a controversial one.[citation needed]
Based on computer simulations used to evaluate that evolution of language that resulted in showing three stages in the evolution of syntax, Neanderthals are thought to have been in stage 2, showing they had something more evolved than proto-language but not quite as complex as the language of modern humans.[202]
Some researchers, applying auditory bioengineering models to computerised tomography scans of Neanderthal skulls, have asserted that Neanderthals had auditory capacity very similar to that of anatomically modern humans.[203] These researchers claim that this finding implies that "Neanderthals evolved the auditory capacities to support a vocal communication system as efficient as modern human speech."[203]
Anatomically modern humans begin to appear in the fossil record in Ethiopia some 200,000 years ago.[204] Although there is still much debate as to whether behavioural modernity emerged in Africa at around the same time, a growing number of archaeologists nowadays[when?] invoke the southern African Middle Stone Age use of red ochre pigments—for example at Blombos Cave—as evidence that modern anatomy and behaviour co-evolved.[205] These archaeologists argue strongly that if modern humans at this early stage were using red ochre pigments for ritual and symbolic purposes, they probably had symbolic language as well.[26]
According to the recent African origins hypothesis, from around 60,000 – 50,000 years ago[206] a group of humans left Africa and began migrating to occupy the rest of the world, carrying language and symbolic culture with them.[207]
The larynx (or voice box) is an organ in the neck housing the vocal folds, which are responsible for phonation. In humans, the larynx is descended. The human species is not unique in this respect: goats, dogs, pigs and tamarins lower the larynx temporarily, to emit loud calls.[208] Several deer species have a permanently lowered larynx, which may be lowered still further by males during their roaring displays.[209] Lions, jaguars, cheetahs and domestic cats also do this.[210] However, laryngeal descent in nonhumans (according to Philip Lieberman) is not accompanied by descent of the hyoid; hence the tongue remains horizontal in the oral cavity, preventing it from acting as a pharyngeal articulator.[211]
Despite all this, scholars remain divided as to how "special" the human vocal tract really is. It has been shown that the larynx does descend to some extent during development in chimpanzees, followed by hyoidal descent.[212] As against this, Philip Lieberman points out that only humans have evolved permanent and substantial laryngeal descent in association with hyoidal descent, resulting in a curved tongue and two-tube vocal tract with 1:1 proportions. He argues that Neanderthals and early anatomically modern humans could not have possessed supralaryngeal vocal tracts capable of producing "fully human speech".[213] Uniquely in the human case, simple contact between the epiglottis and velum is no longer possible, disrupting the normal mammalian separation of the respiratory and digestive tracts during swallowing. Since this entails substantial costs—increasing the risk of choking while swallowing food—we are forced to ask what benefits might have outweighed those costs. The obvious benefit—so it is claimed—must have been speech. But this idea has been vigorously contested. One objection is that humans are in fact not seriously at risk of choking on food: medical statistics indicate that accidents of this kind are extremely rare.[214] Another objection is that in the view of most scholars, speech as it is known emerged relatively late in human evolution, roughly contemporaneously with the emergence of Homo sapiens.[215] A development as complex as the reconfiguration of the human vocal tract would have required much more time, implying an early date of origin. This discrepancy in timescales undermines the idea that human vocal flexibility was initially driven by selection pressures for speech, thus not excluding that it was selected for e.g. improved singing ability.
To lower the larynx is to increase the length of the vocal tract, in turn lowering formant frequencies so that the voice sounds "deeper"—giving an impression of greater size. John Ohala argues that the function of the lowered larynx in humans, especially males, is probably to enhance threat displays rather than speech itself.[216] Ohala points out that if the lowered larynx were an adaptation for speech, adult human males would be expected to be better adapted in this respect than adult females, whose larynx is considerably less low. However, females outperform males in verbal tests,[217] falsifying this whole line of reasoning.
W. Tecumseh Fitch likewise argues that this was the original selective advantage of laryngeal lowering in the human species. Although (according to Fitch) the initial lowering of the larynx in humans had nothing to do with speech, the increased range of possible formant patterns was subsequently co-opted for speech. Size exaggeration remains the sole function of the extreme laryngeal descent observed in male deer. Consistent with the size exaggeration hypothesis, a second descent of the larynx occurs at puberty in humans, although only in males. In response to the objection that the larynx is descended in human females, Fitch suggests that mothers vocalizing to protect their infants would also have benefited from this ability.[218]
In 2011, Quentin Atkinson published a survey of phonemes from 500 different languages as well as language families and compared their phonemic diversity by region, number of speakers and distance from Africa. The survey revealed that African languages had the largest number of phonemes, and Oceania and South America had the smallest number. After allowing for the number of speakers, the phonemic diversity was compared to over 2000 possible origin locations. Atkinson's "best fit" model is that language originated in western, central, or southern Africa between 80,000 and 160,000 years ago. This predates the hypothesized southern coastal peopling of Arabia, India, southeast Asia, and Australia. It would also mean that the origin of language occurred at the same time as the emergence of symbolic culture.[219]
Numerous linguists[220][221][222] have criticized Atkinson's paper as misrepresenting both the phonemic data and processes of linguistic change, as language complexity does not necessarily correspond to age, and of failing to take into account the borrowing of phonemes from neighbouring languages, as some Bantu languages have done with click consonants.[222] Recreations of his method gave possible origins of language in the Caucasus[220] and Turkmenistan,[221] in addition to southern and eastern Africa.
The search for the origin of language has a long history in mythology. Most mythologies do not credit humans with the invention of language but speak of a divine language predating human language. Mystical languages used to communicate with animals or spirits, such as the language of the birds, are also common, and were of particular interest during the Renaissance.
Vāc is the Hindu goddess of speech, or "speech personified". As Brahman's "sacred utterance", she has a cosmological role as the "Mother of the Vedas". The Aztecs' story maintains that only a man, Coxcox, and a woman, Xochiquetzal, survived a flood, having floated on a piece of bark. They found themselves on land and had many children who were at first born unable to speak, but subsequently, upon the arrival of a dove, were endowed with language, although each one was given a different speech such that they could not understand one another.[223]
In the Old Testament, the Book of Genesis (chapter 11) says that God prevented the Tower of Babel from being completed through a miracle that made its construction workers start speaking different languages. After this, they migrated to other regions, grouped together according to which of the newly created languages they spoke, explaining the origins of languages and nations outside of the Fertile Crescent.[224]
History contains a number of anecdotes about people who attempted to discover the origin of language by experiment. The first such tale was told by Herodotus (Histories 2.2). He relates that Pharaoh Psammetichus (probably Psammetichus I, 7th century BC) had two children raised by a shepherd, with the instructions that no one should speak to them, but that the shepherd should feed and care for them while listening to determine their first words. When one of the children cried "bekos" with outstretched arms the shepherd concluded that the word was Phrygian, because that was the sound of the Phrygian word for 'bread'. From this, Psammetichus concluded that the first language was Phrygian. King James IV of Scotland is said to have tried a similar experiment; his children were supposed to have spoken Hebrew.[225]
Both the medieval monarch Frederick II and Akbar are said to have tried similar experiments; the children involved in these experiments did not speak. The current situation of deaf people also points into this direction.[clarification needed]
Modern linguistics did not begin until the late 18th century, and the Romantic or animist theses of Johann Gottfried Herder and Johann Christoph Adelung remained influential well into the 19th century. The question of language origin seemed inaccessible to methodical approaches, and in 1866 the Linguistic Society of Paris famously banned all discussion of the origin of language, deeming it to be an unanswerable problem. An increasingly systematic approach to historical linguistics developed in the course of the 19th century, reaching its culmination in the Neogrammarian school of Karl Brugmann and others.[citation needed]
However, scholarly interest in the question of the origin of language has only gradually been rekindled[colloquialism] from the 1950s on (and then controversially) with ideas such as universal grammar, mass comparison and glottochronology.[citation needed]
The "origin of language" as a subject in its own right emerged from studies in neurolinguistics, psycholinguistics and human evolution. The Linguistic Bibliography introduced "Origin of language" as a separate heading in 1988, as a sub-topic of psycholinguistics. Dedicated research institutes of evolutionary linguistics are a recent phenomenon, emerging only in the 1990s.[226]

The origin of the Albanians has been the subject of historical, linguistic, archaeological and genetic studies. The first mention of the ethnonym Albanoi occurred in the 2nd century AD by Ptolemy describing an Illyrian tribe who lived around present-day central Albania.[1][2] The first attestation of Albanians as an ethnic group is in the 11th century.[3]
Albanians have a western Paleo-Balkan origin. Besides the Illyrians, theories regarding which specific ancient Paleo-Balkan group had participated in the origin of the Albanians vary between attributing Thracian, Dacian, or another Paleo-Balkan component whose language was unattested. Among those scholars who support an exclusively Illyrian origin, there is a distinction between those who propose a direct continuity from Illyrian times, and those who propose an in-migration of a different Illyrian population. However, these propositions are not mutually exclusive.[4]
Albanian is an Indo-European language[5] and the only surviving representative of its own branch, which belongs to the Paleo-Balkan group, having its formative core in the Balkans after the Indo-European migrations in the region.[6][7] Early Proto-Albanian speakers came into contact with Doric Greek (West Greek) since the 7th century BCE, and with Ancient Macedonian during the 5th–4th centuries BCE. Thereafter they also had contacts with Koine Greek. Proto-Albanian speakers came into contact with Latin after the Roman conquest of the Western Balkans in the 2nd century BCE, but the major Latin influence in Proto-Albanian occurred during the first years of the common era onwards, when the Western Balkans were eventually incorporated into the Roman Empire after the Great Illyrian Revolt (6–9 CE). Latin loanwords were borrowed through the entire period of spoken Latin in the Western Balkans, reflecting different chronological layers and penetrating into almost all semantic fields. Proto-Albanian speakers were Christianized under the Latin sphere of influence, specifically in the 4th century CE.
All aspects of Albanian tribal society have been directed by the Albanian traditional law code, which is of interest to Indo-European studies as it reflects many legal practices of great antiquity that find precise echoes in Vedic India and ancient Greece and Rome.[8][9] The surviving pre-Christian elements of Albanian culture indicate that Albanian mythology and folklore are of pagan Paleo-Balkanic origin.[10]
The two ethnonyms used by Albanians to refer to themselves are Arbënesh(ë)/Arbëresh(ë) and Shqiptar(ë). Arbënesh is the original Albanian endonym and forms that basis for most names of Albanians in foreign languages and the name of Albania as a country. Greek Arvanitai, Alvanitai and Alvanoi, Turkish Arnaut, Serbo-Croatian Arbanasi and others derive from this term.[11] The ethnic name Albanian was used by Latin and Byzantine sources in the forms arb- and alb- since at least the 2nd century A.D,[12][a] and eventually in Old Albanian texts as an endonym. The ancient attestation of the ethnic designation is not considered strong evidence of an Albanian continuity in southern Illyria, since there are many examples in history of an ethnic name shifting from one ethnos to another.[12] Nevertheless, the ancient ethnonym gave rise to the Albanian old endonym, early generalized to all the tribes of Illyria who spoke the same idiom.[11] The process was similar to the spread of the name Illyrians from a small group of people on the Adriatic coast, the Illyrioi.[13]
Albanians gradually replaced their old endonym by the term Shqiptar, a change most likely trigged after the Ottoman conquests of the Balkans in the 15th century.[14] The words Shqipëri and Shqiptar are attested from 14th century onward,[15] but it was only at the end of 17th and beginning of the early 18th centuries that the placename Shqipëria and the ethnic demonym Shqiptarë gradually replaced Arbëria and Arbëreshë amongst Albanian speakers.[16][15] The usage of the old endonym Arbënesh/Arbëresh, however, persisted and was retained by Albanian communities which had migrated from Albania and adjacent areas centuries before the change of the self-designation, namely the Arbëreshë of Italy, the Arvanites of Greece as well as the Arbanasi in Croatia.[17][18][19][20][21] As such, the medieval migrants to Greece and later migrants to Italy during the 15th-century are not aware of the term Shqiptar.[22]
Michael Attaleiates (1022-1080) mentions the term Albanoi twice and the term Arbanitai once. The term Albanoi is used first to describe the groups which rebelled in southern Italy and Sicily against the Byzantines in 1038–40. The second use of the term Albanoi is related to groups which supported the revolt of George Maniakes in 1042 and marched with him throughout the Balkans against the Byzantine capital, Constantinople. The term Arvanitai is used to describe a revolt of Bulgarians (Boulgaroi) and Arbanitai in the theme of Dyrrhachium in 1078–79. It is generally accepted that Arbanitai refers to the ethnonym of medieval Albanians. As such, it is considered to be the first attestation of Albanian as an ethnic group in Byzantine historiography.[38] The use of the term Albanoi in 1038-49 and 1042 as an ethnonym related to Albanians have been a subject of debate. In what has been termed the "Ducellier-Vrannousi" debate, Alain Ducellier proposed that both uses of the term referred to medieval Albanians. Era Vrannousi counter-suggested that the first use referred to Normans, while the second did not have an ethnic connotation necessarily and could be a reference to the Normans as "foreigners" (aubain) in Epirus which Maniakes and his army traversed.[38] The debate has never been resolved.[39] A newer synthesis about the second use of the term Albanoi by Pëllumb Xhufi suggests that the term Albanoi may have referred to Albanians of the specific district of Arbanon, while Arbanitai to Albanians in general regardless of the specific region they inhabited.[40]
Pre-Indo-European sites are found throughout the territory of Albania; such as in Maliq, Vashtëm, Burimas, Barç, Dërsnik in Korçë District, Kamnik in Kolonja, Kolsh in Kukës District, Rashtan in Librazhd and Nezir in Mat District.[49] As in other parts of Europe, these migratory Indo-European tribes entered the Balkans and contributed to the formation of the historical Paleo-Balkan tribes, to which Albanians trace their origin. The previous populations – during the process of assimilation by the immigrating IE tribes – have played an important part in the formation of the various ethnic groups generated by their long symbiosis. Consequently, the IE languages that developed in the Balkan Peninsula, in addition to their natural evolution, have also been impacted by the idioms of the assimilated pre-Indo-European people.[50] In terms of linguistics, the pre-Indo-European substrate language spoken in the southern Balkans has probably influenced pre-Proto-Albanian, the ancestor idiom of Albanian.[51] The extent of this linguistic impact cannot be determined with precision due to the uncertain position of Albanian among Paleo-Balkan languages and their scarce attestation.[52] Some loanwords, however, have been proposed, such as shegë 'pomegranate' and lëpjetë 'orach'; compare with pre-Greek lápathon 'monk's rhubarb'.[53][51] Albanian is also the only language in the Balkans which has retained elements of the vigesimal numeral system – njëzet 'twenty', dyzet 'forty' – which was prevalent in the pre-Indo-European languages of Europe; such as the Basque language, which broadly uses vigesimal numeration.[49]
This pre-Indo-European substratum has also been identified as one of the contributing factors to the customs of Albanians.[54]
The first attested mention of Albanian occurred in 1285 at the Venetian city of Ragusa (present-day Dubrovnik, Croatia) when a crime witness named Matthew testified: "I heard a voice crying in the mountains in Albanian" (Latin: Audivi unam vocem clamantem in monte in lingua albanesca).[55]
The earliest attested written specimens of Albanian are Formula e pagëzimit (1462) and Arnold Ritter von Harff's lexicon (1496). The first Albanian text written with Greek letters is a fragment of the Ungjilli i Pashkëve (Passover Gospel) from the 15 or 16th century. The first printed books in Albanian are Meshari (1555) and Luca Matranga's E mbsuame e krështerë (1592).[56]
However, as Fortson notes, Albanian written works existed before this point; they have simply been lost. The existence of written Albanian is explicitly mentioned in a letter attested from 1332, and the first preserved books, including both those in Gheg and in Tosk, share orthographic features that indicate that some form of common literary language had developed.[57]
In the Balkans and southern Italy, several toponyms, river and mountain names which have been attested since antiquity can be explained etymologically via Albanian or have evolved phonologically through Albanian and later adopted in other languages. Inherited toponyms from a Proto-Albanian language and the date of adoption of non-Albanian toponyms indicate in Albanology the regions were the Albanian language originated, evolved and expanded. Depending on which proposed etymology and phonological development linguists support, different etymologies are usually used to link Albanian to Illyrian, Messapic, Dardanian, Thracian or an unattested Paleo-Balkan language.
Albanian is attested in a written form beginning only in the 15th century AD. In the absence of prior data on the language, scholars have used Albanian linguistic contacts with Ancient Greek, Latin and Slavic for identifying its historical location. The precursor of Albanian can be considered a completely formed independent IE language since at least the first millennium BCE, with the beginning of the early Proto-Albanian phase.[89] Proto-Albanian is reconstructed by way of the comparative method between the Tosk and Gheg dialects and between Albanian and other Indo-European languages, as well as through contact linguistics studying early loanwords from and into Albanian and structural and phonological convergences with other languages. Loanwords into Albanian treated through its phonetic evolution can be traced back as early as the first contacts with Doric Greek (West Greek) since the 7th century BCE, and with Ancient Macedonian during the 5th–4th centuries BCE, but the most important of which are those from Latin (dated to the period 167 BCE to 400 CE) and from Slavic (dated from c. 600 CE onward).[90] The evidence from loanwords allows linguists to construct in great detail the shape of Albanian native words at the points of major influxes of loans from well-attested languages.[91]
That Albanian possesses a rich and "elaborated" pastoral vocabulary which has been taken to suggest Albanian society in post-Roman times was pastoral, with widespread transhumance, and stock-breeding particularly of sheep and goats.[92] Joseph takes interest in the fact that some of the lexemes in question have "exact counterparts" in Romanian.[92] The fact that the Albanian language reflects a clear pastoralist stage does not allow conclusions about the Proto-Albanian speakers' way of life during classical antiquity, as only the speech of the mountain pastoralists managed to survive the Great Migrations.[93]
Albanian-speakers appear to have been cattle breeders given the vastness of preserved native vocabulary pertaining to cow breeding, milking and so forth, while words pertaining to dogs tend to be loaned. Many words concerning horses are preserved, but the word for horse itself is a Latin loan.[94] The original Palaeo-Balkan word for 'horse', preserved in Albanian mëz or mâz 'foal', from *me(n)za- 'horse', underwent a later semantic shift 'horse' > 'foal' after the loan from Latin caballus into Albanian kalë 'horse'.[95] The Albanian name Mazrek(u), which means 'horse breeder' in Albanian, is found throughout all Albanian regions, and notably it was the name used by the Kastrioti noble family to highlight their tribal affiliation (Albanian: farefisní).[96] Also the Palaeo-Balkan word for 'mule' has been preserved in Albanian mushk(ë) 'mule'.[97]
Concerning the inheritance of hydronymic vocabulary, it has been noted that there were no lexemes relating to seamanship in the Proto-Indo-European language. PIE hydronyms reconstructed so far refer to swamps, marshes, lakes, and riverine environments, but not to the sea. For instance, the Greek term thalassa "sea" is Pre-Greek, not an inherited Indo-European word.[98] The Albanian term for "sea" (det [dēt]), which was considered by some Albanologists to be an inherited term from Proto-Albanian *deubeta as a cognate of Proto-Germanic *deupiþō- "depth", is firmly dismissed by present-day historical linguists.[99] Instead, a borrowing from Greek δέλτα delta "river delta" has been proposed recently.[100] At least two other Albanian terms from the same semantic field are early Greek loanwords: pellg "pond, basin, depth" from πέλαγος pelagos "sea", and zall "riverbank, river sand", from αι҆γιαλός "sea-shore",[100] which underwent in Proto-Albanian a semantic shift,[101] indicating for this language a change in location after its contact with Ancient Greek.[citation needed] Also all Albanian words relating to seamanship appear to be loans.[102]
Words referring to large streams and their banks tend to be loans, but lumë ("river") is native, as is rrymë (the flow of river water). Words for smaller streams and stagnant pools of water are more often native, except pellg. Albanian has maintained since Proto-Indo-European a specific term referring to a riverside forest (gjazë), as well as its words for marshes. Albanian has maintained native terms for "whirlpool", "water pit" and (aquatic) "deep place", leading Orel to speculate that Albanian was likely spoken in an area with an excess of dangerous whirlpools and depths.[101] The term mat, meaning "height", "beach", "bank/shore" in Northern Albanian and "beach", "shore" in Arbëresh, is inherited from Proto-Albanian *mata < *mn̥-ti "height" (cf. Latin mŏns "mountain"),[103][104] after which the river Mat (and the region with the same name) in north-central Albania was named, which can be explained as "mountain river". The meaning "bank/shore" hence would have emerged only at a later time (cf. German Berg "mountain" in relation to Slavic *bergъ "bank/shore").[103]
Regarding forests, words for most conifers and shrubs are native, as are the terms for "alder", "elm", "oak", "beech", and "linden", while "ash", "chestnut", "birch", "maple", "poplar", and "willow" are loans.[105]
The original kinship terminology of Indo-European was radically reshaped; changes included a shift from "mother" to "sister", and were so thorough that only three terms retained their original function; the words for "son-in-law", "mother-in-law" and "father-in-law".[106] All the words for second-degree blood kinship, including "aunt", "uncle", "nephew", "niece", and terms for grandchildren, are ancient loans from Latin.[107]
Openness to loans has been called a "characteristic feature" of Albanian. The Albanian original lexical items directly inherited from Proto-Indo-European are far fewer in comparison to the loanwords, though loans are considered to be "perfectly integrated" and not distinguishable from native vocabulary on a synchronic level.[108] Although Albanian is characterized by the absorption of many loans, even, in the case of Latin, reaching deep into the core vocabulary, certain semantic fields nevertheless remained more resistant. Terms pertaining to social organization are often preserved, though not those pertaining to political organization, while those pertaining to trade are all loaned or innovated.[109]
While the words for plants and animals characteristic of mountainous regions are entirely original, the names for fish and for agricultural activities are often assumed to have been borrowed from other languages. However, considering the presence of some preserved old terms related to the sea fauna, some have proposed that this vocabulary might have been lost in the course of time after proto-Albanian tribes were pushed back into the inland during invasions.[110][111] Wilkes holds that the Slavic loans in Albanian suggest that contacts between the two populations took place when Albanians dwelt in forests 600–900 metres above sea level.[112]
Linguistic contact between Albanian and Greek has been securely dated to the Iron Age. Also contacts between the respective post-PIE languages which gave rise to the two languages also occurred in previous times. Common traces of the Mediterranean-Balkan substratum are considered to date to the common Indo-European phase of Albanian and Greek (c.f. Graeco-Albanian).[113] Innovative creations of agricultural terms shared only between Albanian and Greek, such as *h₂(e)lbʰ-it- 'barley' and *spor-eh₂- 'seed', were formed from non-agricultural Proto-Indo-European roots through semantic changes to adapt them for agriculture. Since they are limited only to Albanian and Greek, they could be traced back with certainty only to their last common Indo-European ancestor, and not projected back into Proto-Indo-European.[114] Shortly after they had diverged from one another, Albanian, Greek and Armenian, also underwent a longer period of contact (as can be seen, for example, in the irregular correspondence: Greek σκόρ(ο)δον, Armenian sxtor, xstor, and Albanian hudhër, hurdhë "garlic"). Furthermore, intense Greek–Albanian contacts have certainly occurred thereafter,[115] with ongoing connections between them in the Balkans from the ancient times, continuing up to the present-days.[116]
Ancient Greek loans in Proto-Albanian originated from two distinct geographical and historical groups: borrowings from the Greek colonies on the Adriatic coast from the 7th century BCE, either directly or indirectly through trade communication in the hinterland; direct borrowings from Greek-speaking populations of ancient Macedonia during the 5th–4th centuries BCE, before the replacement of Ancient Macedonian with Koine Greek.[117] Several Proto-Albanian terms have been preserved in the lexicon of Hesychius of Alexandria and other ancient glossaries.[118][119][120][121] Some of the Proto-Albanian glosses in Hesychius are considered to have been loaned to the Dorik Greek as early as the 7th century BCE.[119] Witczak (2016) specifically points to seven words recorded by the Greek grammarian Hesychius of Alexandria (5th century AD), and particularly to the term ἀάνθα 'a kind of earring', which was first attested in the work of the choral lyric poet Alcman (fl. 7th century BCE).[122] This means that the ancestors of the Albanians were in contact with the northwestern part of Ancient Greek civilization and probably borrowed words from Greek cities (Dyrrachium, Apollonia, etc.) in the Illyrian territory, colonies which belonged to the Doric division of Greek, or from contacts in the Epirus area. The earliest Greek loans began to enter Albanian circa 600 BC, and are of Doric provenance, tending to refer to vegetables, fruits, spices, animals and tools. This stratum reflects contacts between Greeks and Proto-Albanians from the 8th century BC onward, with the Greeks being either colonists on the Adriatic coast or Greek merchants inland in the Balkans. The second wave of Greek loans began after the split of the Roman empire in 395 and continued throughout the Byzantine, Ottoman and modern periods.[123]
According to Hermann Ölberg, the modern Albanian lexicon may include 33 words of ancient Greek origin,[102][b] although it can be increased if the Albanian lexicon is properly evaluated.[113] An argument claimed by some scholars as an indication of a location of Albanian further north than present-day Albania in antiquity is the number of loanwords from Ancient Greek, mostly from Doric dialect, which is considered by them relatively small, even though Southern Illyria neighbored the Classical Greek civilization and there were a number of Greek colonies along the Illyrian coastline.[125] For instance, according to Bulgarian linguist Vladimir I. Georgiev there is limited Greek influence in Albanian (See Jireček Line of Roman times), and if Albanians had been inhabiting a homeland situated in modern Albania continuously since ancient times, the number of Greek loanwords in Albanian should be higher.[126] However, the number of surviving loanwords is not a valid argument, as many Greek loans were likely lost through replacement by later Latin and Slavic loans, just as notoriously happened to most native Albanian vocabulary.[124] On the other hand, the specifically Northwestern/Doric affiliations and ancient dating of Greek loans imply a specifically Western Balkan Albanian presence to the north and west of Greeks specifically in antiquity, though Huld cautions that the classical "precursors" of the Albanians would be "'Illyrians' to classical writers", but that the Illyrian label is hardly "enlightening" since classical ethnology was imprecise.[127][128][129]
Evidence of a significant level of early linguistic contact between Albanian and Greek is provided by ancient common structural innovations and phonologic convergence such as:[130][131]
Those innovations are limited only to the Albanian and Greek languages and are not shared with other languages of the Balkan sprachbund.[130] Since they precede the Balkan sprachbund era, those innovations date to a prehistoric phase of the Albanian language, spoken at that time in the same area as Greek and within a social frame of bilingualism among early Albanians having to be able to speak some form of Greek.[131]
Latin loans are dated to the period of 167 BC to 400 AD.[132] 167 BC coincides with the fall of the kingdom ruled by Gentius and reflects the early date of the entry of Latin-based vocabulary in Albanian. It entered Albanian in the Early Proto-Albanian stage and evolved in later stages as a part of the Proto-Albanian vocabulary and within its phonological system. Albanian is one of the oldest languages that came into contact with Latin and adopted Latin vocabulary. It has preserved 270 Latin-based words which are found in all Romance languages, 85 words which are not found in Romance languages, 151 which are found in Albanian but not in Eastern Romance and its descendant Romanian, and 39 words which are found only in Albanian and Romanian.[133] The contact zone between Albanian and Romanian was likely located in eastern and southeastern Serbia.[134] The preservation of Proto-Albanian vocabulary and linguistic features in Romanian highlights that at least partly Balkan Latin emerged as Albanian-speakers shifted to Latin.[135]
The other layer of linguistic contacts of Albanian with Latin involves Old Dalmatian, a western Balkan derivative of Balkan Latin. Albanian maintained links with both coastal western and central inland Balkan Latin formations.[136] Hamp indicates there are words that follow Dalmatian phonetic rules in Albanian, giving as an example the word drejt 'straight' < d(i)rectus matching developments in Old Dalmatian traita < tract.[125] Romanian scholars Vatasescu and Mihaescu, using lexical analysis of Albanian, have concluded that Albanian was also heavily influenced by an extinct Romance language that was distinct from both Romanian and Dalmatian. Because the Latin words common to only Romanian and Albanian are significantly less than those that are common to only Albanian and Western Romance, Mihaescu argues that Albanian evolved in a region with much greater contact with Western Romance regions than with Romanian-speaking regions, and located this region in present-day Albania, Kosovo and Western North Macedonia, spanning east to Bitola and Pristina.[137]
The Christian religious vocabulary of Albanian is mostly Latin as well, including even the basic terms such "to bless", "altar," and "to receive communion". It indicates that Albanians were Christianized under the Latin-based liturgy and ecclesiastical order which would be known as "Roman Catholic" in later centuries.[123]
The contacts began after the South Slavic migrations to Southeastern Europe in the 6th and 7th centuries. The modern Albanian lexicon contains around 250 Slavic borrowings that are shared among all the dialects.[138] Slavic settlement probably shaped the present geographic spread of the Albanians. It is likely that Albanians took refuge in the mountainous areas of northern and central Albania, eastern Montenegro, western North Macedonia, and Kosovo. Long-standing contact between Slavs and Albanians might have been common in mountain passages and agriculture or fishing areas, in particular in the valleys of the White and Black branches of the Drin and around the Shkodër and Ohrid lakes. Such contact with one another in these areas has caused many changes in Slavic and Albanian local dialects.[139] Historical linguist Eric P. Hamp, analyzing the influence of substrates on the Old Serbo-Croatian language, has concluded that the toponymic and Romanian evidence indicate that the South Slavs who became Serbo-Croatian speakers settled in a zone of former Albanoid speech, which reasonably explains why the resultant population was well-predisposed to preserve the richest system of lateral consonant distinctions and alternations among the later Slavic-speaking peoples.[140][141][142]
The evolution of the ancient toponym Lychnidus into Oh(ë)r(id) (city and lake), which is attested in this form from 879 CE, required an early long-standing period of Tosk Albanian–East South Slavic bilingualism, or at least contact, resulting from the Tosk Albanian rhotacism -n- into -r- and Eastern South Slavic l-vocalization ly- into o-.[59]
As Albanian and Slavic have been in contact since the early Middle Ages, toponymical loanwords in both belong to different chronological strata and reveal different periods of acquisition. Old Slavic loanwords into Albanian develop early Slavic *s as sh and *y as u within Albanian phonology of that era. Norbert Jokl defined this older period from the earliest Albanian-Slavic contacts to 1000 AD at the latest, while contemporary linguists like Vladimir Orel define it as between the 6th and the 8th century AD.[143][144] Newer loanwords preserve Slavic /s/ and other features which no longer show phonological development within Albanian. Such toponyms from the earlier period of contact in Albania include Bushtricë (Kukës),[145] Dishnica (Përmet),[146] Dragoshtunjë (Elbasan),[147] Leshnjë (Leshnjë, Berat and other areas),[148] Shelcan (Elbasan), Shishtavec (Kukës/Gora), Shuec (Devoll) and Shtëpëz (Gjirokastër),[149] Shopël (Iballë),[150] Veleshnjë (Skrapar)[151] and others.[152] Similar toponyms in a later period produced different results e.g. Bistricë (Sarandë) instead of Bushtricë or Selcan (Këlcyrë) instead of Shelcan.[153] Part of the toponyms of Slavic origin were acquired in Albanian before undergoing the changes of Slavic liquid metathesis (before ca. the end of the 8th century). They include Ardenicë (Lushnjë), Berzanë (Lezhë), Gërdec and Berzi (Tiranë) and a cluster of toponyms along the route Berat-Tepelenë-Përmet.[154] Labëri, from the Albanian endonym, resulted through the Slavic liquid metathesis, and was reborrowed in that form into Albanian.[155][156]
It has been concluded that the partial Latinization of Roman-era Albania was heavy in coastal areas, in the plains, and along the Via Egnatia, which passed through Albania. In these regions, Madgearu notes that the survival of Illyrian names and the depiction of people with Illyrian dress on gravestones is not enough to prove successful resistance against Romanization, and that in these regions there were many Latin inscriptions and Roman settlements. Madgearu concludes that only the northern mountain regions escaped Romanization. In some regions, Madgearu concludes that it has been shown that in some areas a Latinate population that survived until at least the seventh century passed on local place names that had mixed characteristics of Eastern and Western Romance into Albanian.[137]
The Komani-Kruja culture is an archaeological culture attested from late antiquity to the Middle Ages in central and northern Albania, southern Montenegro and similar sites in the western parts of North Macedonia.[157][158] It consists of settlements usually built below hillforts along the Lezhë (Praevalitana)-Dardania and Via Egnatia road networks which connected the Adriatic coastline with the central Balkan Roman provinces. Its type site is Komani and the nearby Dalmace hill in the Drin river valley. Limited excavations campaigns occurred until the 1990s. Objects from a vast area covering nearby regions the entire Byzantine Empire, the northern Balkans and Hungary and sea routes from Sicily to Crimea were found in Dalmace and other sites coming from many different production centres: local, Byzantine, Sicilian, Avar-Slavic, Hungarian, Crimean and even possibly Merovingian and Carolingian.[159] Within Albanian archaeology, based on the continuity of pre-Roman Illyrian forms in the production of several types of local objects found in graves, the population of Komani-Kruja was framed as a group which descended from the local Illyrians who "re-asserted their independence" from the Roman Empire after many centuries and formed the core of the later historical region of Arbanon.[160][need quotation to verify] As research focused almost entirely on grave contexts and burial sites, settlements and living spaces were often ignored.[161] Yugoslav archaeology proposed an opposite narrative and tried to frame the population as Slavic, especially in the region of western Macedonia.[162] Archaeological research has shown that these sites were not related to regions then inhabited by Slavs and even in regions like Macedonia, no Slavic settlements had been founded in the 7th century.[163]
What was established in this early phase of research was that Komani-Kruja settlements represented a local, non-Slavic population which has been described as Romanized Illyrian, Latin-speaking or Latin-literate.[164][165] This is corroborated by the absence of Slavic toponyms and survival of Latin ones in the Komani-Kruja area. In terms of historiography, the thesis of older Albanian archaeology was an untestable hypothesis as no historical sources exist which can link Komani-Kruja to the first definite attestation of medieval Albanians in the 11th century.[164][165] Archaeologically, while it was considered possible and even likely that Komani-Kruja sites were used continuously from the 7th century onwards, it remained an untested hypothesis as research was still limited.[166] Whether this population represented local continuity or arrived at an earlier period from a more northern location as the Slavs entered the Balkans remained unclear at the time but regardless of their ultimate geographical origins, these groups maintained Justinianic era cultural traditions of the 6th century possibly as a statement of their collective identity and derived their material cultural references to the Justinianic military system.[167] In this context, they may have used burial customs as a means of reference to an "idealized image of the past Roman power".[167]
Research greatly expanded after 2009 and the first survey of Komani's topography was produced in 2014. Until then, except for the area of the cemetery the size of the settlement and its extension remained unknown. In 2014, it was revealed that Komani occupied an area of more than 40 ha, a much larger territory than originally thought. Its oldest settlement phase dates to the Hellenistic era.[168] Proper development began in the late antiquity and continued well into the Middle Ages (13th-14th centuries). It indicates that Komani was a late Roman fort and an important trading node in the networks of Praevalitana and Dardania. In the Avar-Slavic raids, communities from present-day northern Albania and nearby areas clustered around hill sites for better protection as is the case of other areas like Lezha and Sarda. During the 7th century as Byzantine authority was reestablished after the Avar-Slavic raids and the prosperity of the settlements increased, Komani saw increase in population and a new elite began to take shape. Increase in population and wealth was marked by the establishment of new settlements and new churches in their vicinity. Komani formed a local network with Lezha and Kruja and in turn this network was integrated in the wider Byzantine Mediterranean world, maintained contacts with the northern Balkans and engaged in long-distance trade.[169] Tom Winnifrith (2020) says that the Komani-Kruja culture shows that in that area a Latin-Illyrian civilization survived, to emerge later as Albanians and Vlachs. The lack of interest among Slavs for the barren mountains of Northern Albania would explain the survival of Albanian as a language.[170]
The general consensus is that Albanians originate from one or possibly a mixture of Paleo-Balkan peoples but which specific peoples besides Illyrians is a matter of continuing debate.[171][172][173]
Messapic is the only sufficiently attested ancient language via which commonly accepted Illyrian-Albanian connections have been produced. It is unclear whether Messapic was an Illyrian dialect or if it diverged enough to be a separate language, although in general it is treated as a distinct language. Dardanian in the context of a distinct language has gained prominence in the possible genealogy of the Albanian language in recent decades.[12]
Vladimir I. Georgiev, although accepting an Illyrian component in Albanian, and even not excluding an Illyrian origin of Albanian, proposed as the ancestor of Albanian a language called "Daco-Mysian" by him, considering it a separate language from Thracian.[174][175] Georgiev maintained that "Daco-Mysian tribes gradually migrated to the northern-central part of the Balkan Peninsula, approximately to Dardania, probably in the second millennium B.C. (or not later than the first half of the first millennium B.C.), and thence they migrated to the areas of present Albania".[175] Based on shared innovations between Albanian and Messapic, Eric P. Hamp has argued that Albanian is closely related to Illyrian and not to Thracian or Daco-Moesian, maintaining that it descended from a language that was sibling of Illyrian and that was once closer to the Danube and in contact with Daco-Moesian.[176] Due to the paucity of written evidence, what can be said with certainty in current research is that on the one hand a significant group of shared Indo-European non-Romance cognates between Albanian and Romanian indicates at least contact with the 'Daco-Thraco-Moesian complex', and that on the other hand there is some evidence to argue that Albanian is descended from the 'Illyrian complex'.[177] From a "genealogical standpoint", Messapic is the closest at least partially attested language to Albanian. Hyllested & Joseph (2022) label this Albanian-Messapic branch as Illyric and in agreement with recent bibliography identify Greco-Phrygian as the IE branch closest to the Albanian-Messapic one. These two branches form an areal grouping - which is often called "Balkan IE" - with Armenian.[178]
The Illyrian linguistic theory has some consensus, but Illyrian language is too little attested for definite comparisons to be made. Further issues are linked to the definitions of "Illyrian" and "Thracian" which are vague and aren't applied to the same areas which were considered to be part of Illyria and Thrace in antiquity.[179][12] For instance, Martin Huld argues that the classical "precursors" of the Albanians would be "'Illyrians' to classical writers", but that the Illyrian label is hardly "enlightening" because ethnology in classical antiquity was imprecise.[117] It is also uncertain whether Illyrians spoke a homogeneous language or rather a collection of different but related languages that were wrongly considered the same language by ancient writers. In contemporary research, two main onomastic provinces have been defined in which Illyrian personal names occur; the southern Illyrian or south-eastern Dalmatian province (Albania, Montenegro and their hinterland) and the central Illyrian or middle Dalmatian-Pannonian province (parts of Croatia, Bosnia and western Serbia). The region of the Dardani (modern Kosovo, parts of northern North Macedonia, parts of eastern Serbia) saw the overlap of the southern/south-eastern, Dalmatian and local anthroponymy.[180] A third area around modern Slovenia sometimes considered part of Illyria in antiquity is considered to have been closer to Venetic, which is no longer considered to be related to Illyrian.[181][182] The conceptual paucity of the label 'Illyrian' makes its usage uncomfortable to some scholars, for this reason in current research some call the Albanian's ancestor 'Albanoid' in reference to a "specific ethnolinguistically pertinent and historically compact language group", which still remains relatable with Messapic.[183] The term 'Albanoid' for the ancestor of the Albanian was used for the first time by Hamp, who developed the thesis about the Proto-Albanoid dialects, spoken in the central-western Balkans including the historical regions of Dardania, Illyria proper, Paeonia, Upper Moesia, western Dacia and western Thrace.[118]
Albanian shows traces of satemization within the Indo-European language tree, however the majority of Albanologists[184] hold that unlike most satem languages it has preserved the distinction of /kʷ/ and /gʷ/ from /k/ and /g/ before front vowels (merged in satem languages), and there is a debate whether Illyrian was centum or satem. On the other hand, Dacian[185] and Thracian[186] seem to belong to satem. A clear isogloss that distinguishes Albanoid languages and Thracian is the palatilization of the IE labiovelars, which in Albanoid was present well before Roman times, while the IE labiovelars clearly did not palatalize in the pre-Roman period in Thracian or in the area where it was spoken.[187]
The debate is often politically charged, and to be conclusive, more evidence is needed. Such evidence unfortunately may not be easily forthcoming because of a lack of sources.[188]
The very first recorded mention of a connection between Illyrians and Albanians is in 1709, attributed to the German philosopher and mathematician Gottfried Leibniz, most famous for being the co-inventor of calculus along with Isaac Newton. In a series of letters, he first speculated Albanian to be related to the other Slavic languages along the Adriatic, but soon changed his mind and connected the Albanian language to that of the ancient Illyrians.[189]
In terms of linguists or historians, the theory that Albanians were related to the Illyrians was proposed for the first time by the Swedish[190] historian Johann Erich Thunmann in 1774.[191] The scholars who advocate an Illyrian origin are numerous.[192][193][194][195] Those who argue in favour of an Illyrian origin maintain that the indigenous Illyrian tribes dwelling in South Illyria (including today's Albania) went up into the mountains when Slavs occupied the lowlands,[196][197] while another version of this hypothesis states that the Albanians are the descendants of Illyrian tribes located between Dalmatia and the Danube who spilled south.[198]
Some of the arguments for the Illyrian-Albanian connection have been as follows:[195][199]
Messapic is an Iron Age language spoken in Apulia by the Iapygians (Messapians, Peucetians, Daunians), which settled in Italy as part of an Illyrian migration from the Balkans in the transitional period between the Bronze and Iron Ages.[212] As Messapic was attested after over 500 years of development in the Italian peninsula, it's generally treated as distinct linguistically from Illyrian. Both languages are placed in the same branch of Indo-European. Eric Hamp has grouped them under "Messapo-Illyrian" which is further grouped with Albanian under "Adriatic Indo-European".[213] Other schemes group the three languages under "General Illyrian" and "Western Paleo-Balkan".[214] Messapian shares several exclusive lexical correspondences and general features with Albanian. Whether Messapian and Albanian share common features because of a common ancestral Illyrian idiom or whether these are features which developed in convergence among the languages of their grouping in the territory of Illyria. Shared cognates and features indicate a closer link between the two languages.[215] The cognates include Messapic aran and Albanian arë ("field"), biliā and bijë ("daughter"), menza- (in the name Manzanas) and mëz ("foal"), brendion (in Brundisium) and bri (horn) .[216] Some Messapian toponyms like Manduria in Apulia have no etymological forms outside Albanian linguistic sources.[217] Other linguistic elements such as particles, prepositions, suffixes and phonological features of the Messapic language find singular affinities with Albanian.[218]
Aside from an Illyrian origin, Thracian or "Daco-Moesian" origins have also been hypothesized based on linguistic arguments that had been claimed as evidence, although in current historical linguistics the documented Thracian material clearly points to a different language than Albanian or its reconstructed precursor,[219][220][221][222][187] whereas the "Daco-Mysian" hypothetical relation is highly based on speculations that have been thoroughly dismantled by other scholars.[223]
Scholars who support a Dacian origin maintain on their side that Albanians moved southwards between the 3rd and 6th centuries AD from the Moesian area.[224] Others argue instead for a Thracian origin and maintain that the proto-Albanians are to be located in the area between Niš, Skopje, Sofia and Albania[225] or between the Rhodope and Balkan Mountains, from which they moved to present-day Albania before the arrival of the Slavs.[226]
German historian Gottfried Schramm speculated that the Albanians derived from the Christianized Bessi, after their remnants were allegedly pushed by Slavs and Bulgars during the 9th century westwards into today Albania.[227] Archaeologically, there is absolutely no evidence of a 9th-century migration of any population, such as the Bessi, from western Bulgaria to Albania.[228] Also according to historical linguistics the Thracian-Bessian hypothesis of the origin of Albanian should be rejected, since only very little comparative linguistic material is available (the Thracian is attested only marginally, while the Bessian is completely unknown), and at the same time the individual phonetic history of Albanian and Thracian clearly indicates a very different sound development that cannot be considered as the result of one language. Furthermore, the Christian vocabulary of Albanian is mainly Latin, which speaks against the construct of a "Bessian church language".[229] The elite of the Bessi tribe was gradually Hellenized.[230][231] Low level of borrowings from Greek in the Albanian language is a further argument against the identification of Albanian with the Bessi.[232] Also the dialectal division of the Albanian-speaking area in the Early Middle Ages contradicts the alleged migration of Albanians in the hinterland of Dyrrhachium in the first decades of the 9th century AD, especially because the dialectal division of a linguistic space is in general a result of a number of linguistic phenomena occurring during a considerable span of time and requires a very large number of natural speakers.[233]
Cities whose names follow Albanian phonetic laws – such as Shtip (Štip), Shkupi (Skopje) and Nish (Niš) – lie in the areas, believed to historically been inhabited by Thracians, Paionians and Dardani; the latter is most often considered an Illyrian tribe by ancient historians. While there still is no clear picture of where the Illyrian-Thracian border was, Niš is mostly considered Illyrian territory.[234]
There are some close correspondences between Thracian and Albanian words.[235] However, as with Illyrian, most Dacian and Thracian words and names have not been closely linked with Albanian (v. Hamp). Also, many Dacian and Thracian placenames were made out of joined names (such as Dacian Sucidava or Thracian Bessapara; see List of Dacian cities and List of ancient Thracian cities), while modern Albanian does not allow this.[235] Many city names were composed of an initial lexical element affixed to -dava, -daua, -deva, -deba, -daba, or -dova, which meant "city" or "town" Endings on more southern regions are exclusively -bria ("town, city"), -disza, -diza, -dizos ("fortress, walled settlement"), -para, -paron, -pera, -phara ("town, village"). Most Illyrian names are composed of a single unit; many Thracian ones are made of two units joined. Several Thracian place-names end in -para, for example, which is thought to mean 'ford', or -diza, which is thought to mean 'fortress'. Thus in the territory of the Bessi, a well-known Thracian tribe, we have the town of Bessapara, 'ford of the Bessi'. The structure here is the same as in many European languages: thus the 'town of Peter' can be called Peterborough, Petrograd, Petersburg, Pierreville, and so on. But the crucial fact is that this structure is impossible in Albanian, which can only say 'Qytet i Pjetrit', not 'Pjeterqytet'. If para were the Albanian for 'ford', then the place-name would have to be 'Para e Besseve'; this might be reduced in time to something like 'Parabessa', but it could never become 'Bessapara'. And what is at stake here is not some superficial feature of the language, which might easily change over time, but a profound structural principle. This is one of the strongest available arguments to show that Albanian cannot have developed out of Thracian or Dacian.[236]
Bulgarian linguist Vladimir I. Georgiev posits that Albanians descend from a Dacian population from Moesia, now the Morava region of eastern Serbia, and that Illyrian toponyms are found in a far smaller area than the traditional area of Illyrian settlement.[134] According to Georgiev, Latin loanwords into Albanian show East Balkan Latin (proto-Romanian) phonetics, rather than West Balkan (Dalmatian) phonetics.[237] Combined with the fact that the Romanian language contains several hundred words similar only to Albanian, Georgiev proposes that Albanian formed in Dardania, in the Roman province of Moesia Superior, where his "Daco-Mysian" construct had allegedly been spoken probably since the 2nd millennium BCE or not later than circa 500 BCE.[175] He suggests that Romanian is a fully Romanised Dacian language, whereas Albanian a partly Romanized "Daco-Mysian" language.[175] Georgiev's theory however has been challenged and dismantled by other scholars.[236][238] Noel Malcolm suggests Romanian and Aromanian originated in the Southern Balkans from Romanized Illyrians.[236]
Apart from the linguistic theory that Albanian is more akin to East Balkan Romance (i.e. Dacian substrate) than West Balkan Romance (i.e. Illyrian/Dalmatian substrate), Georgiev also notes that marine words in Albanian are borrowed from other languages, suggesting that Albanians were not originally a coastal people.[126] According to Georgiev the scarcity of Greek loan words also supports a "Daco-Mysian" theory – if Albanians originated in the region of Illyria there would surely be a heavy Greek influence.[126] According to historian John Van Antwerp Fine, who does define "Albanians" in his glossary as "an Indo-European people, probably descended from the ancient Illyrians",[239] nevertheless states that "these are serious (non-chauvinistic) arguments that cannot be summarily dismissed."[126] Romanian scholars Vatasescu and Mihaescu, using lexical analysis of Albanian, have concluded that Albanian was also heavily influenced by an extinct Romance language that was distinct from both Romanian and Dalmatian. Because the Latin words common to only Romanian and Albanian are significantly less than those that are common to only Albanian and Western Romance, Mihaescu argues that Albanian evolved in a region with much greater contact to Western Romance regions than to Romanian-speaking regions, and located this region in present-day Albania, Kosovo and Western North Macedonia, spanning east to Bitola and Pristina.[137]
An argument against a Thracian origin (which does not apply to Dacian) is that most Thracian territory was on the Greek half of the Jireček Line, aside from varied Thracian populations stretching from Thrace into Albania, passing through Paionia and Dardania and up into Moesia; it is considered that most Thracians were Hellenized in Thrace (v. Hoddinott) and Macedonia.
The Dacian theory could also be consistent with the known patterns of barbarian incursions. Although there is no documentation of an Albanian migration, "during the fourth to sixth centuries the Rumanian region was heavily affected by large-scale invasion of Goths and Slavs, and the Morava valley (in Serbia) was a possible main invasion route and the site of the earliest known Slavic sites. Thus this would have been a region from which an indigenous population would naturally have fled".[126]
Various genetic studies have been done on the European population, some of them including current Albanian population, Albanian-speaking populations outside Albania, and the Balkan region as a whole. Albanians share similar genetics with neighbouring ethnic populations with close clusters forming primarily with mainland Greeks and southern Italian populations.[240][241][242][243]
The three haplogroups most strongly associated with Albanian people are E-V13, R1b and J2b-L283.
A study by Battaglia et al. in 2008[244] found the following haplogroup distributions among Albanians in Albania itself:
The same study by Battaglia et al. (2008) also found the following distributions among Albanians in North Macedonia:
The same study by Battaglia et al. (2008) also found the following distributions among Albanians in Albania itself and Albanians in North Macedonia:
A study by Peričić et al. in 2005[245] found the following Y-Dna haplogroup frequencies in Albanians from Kosovo with E-V13 subclade of haplogroup E1b1b representing 43.85% of the total (note that Albanians from other regions have slightly lower percentages of E-V13, but similar J2b and R1b):
The same study by Peričić et al. in 2005[245] found the following Y-Dna haplogroup frequencies in Albanians from Kosovo with E-V13 subclade of haplogroup E1b1b representing 43.85% of the total (note that Albanians from other regions have slightly lower percentages of E-V13, but similar J2b and R1b):
Table notes:
A study on the Y chromosome haplotypes DYS19 STR and YAP and on mitochondrial DNA found no significant difference between Albanians and most other Europeans.[260]
Larger samples collected by volunteer-led projects, show the Albanians belong largely to Y-chromosomes J2b2-L283, R1b-Z2103/BY611 and EV-13 from Ancient Balkan populations.[261][262]
In a 2013 study which compared one Albanian sample to other European samples, the authors concluded that it did not differ significantly to other European populations, especially groups such as Greeks, Italians and Macedonians.[263][264][244][245]
Another study of old Balkan populations and their genetic affinities with current European populations was done in 2004, based on mitochondrial DNA on the skeletal remains of some old Thracian populations from SE of Romania, dating from the Bronze and Iron Age.[265] This study was during excavations of some human fossil bones of 20 individuals dating about 3200–4100 years, from the Bronze Age, belonging to some cultures such as Tei, Monteoru and Noua were found in graves from some necropoles SE of Romania, namely in Zimnicea, Smeeni, Candesti, Cioinagi-Balintesti, Gradistea-Coslogeni and Sultana-Malu Rosu; and the human fossil bones and teeth of 27 individuals from the early Iron Age, dating from the 10th to 7th centuries BC from the Hallstatt Era (the Babadag culture), were found extremely SE of Romania near the Black Sea coast, in some settlements from Dobruja, namely: Jurilovca, Satu Nou, Babadag, Niculitel and Enisala-Palanca.[265] After comparing this material with the present-day European population, the authors concluded:
Computing the frequency of common point mutations of the present-day European population with the Thracian population has resulted that the Italian (7.9%), the Albanian (6.3%) and the Greek (5.8%) have shown a bias of closer [mtDna] genetic kinship with the Thracian individuals than the Romanian and Bulgarian individuals (only 4.2%).[265]
Analysis of autosomal DNA, which analyses all genetic components has revealed that few rigid genetic discontinuities exist in European populations, apart from certain outliers such as Saami, Sardinians, Basques, Finns and Kosovar Albanians. They found that Albanians, on the one hand, have a high amount of identity by descent sharing, suggesting that Albanian-speakers derived from a relatively small population that expanded recently and rapidly in the last 1,500 years. On the other hand, they are not wholly isolated or endogamous because Greek and Macedonian samples shared much higher numbers of common ancestors with Albanian speakers than with other neighbors, possibly a result of historical migrations, or else perhaps smaller effects of the Slavic expansion in these populations. At the same time the sampled Italians shared nearly as much IBD with Albanian speakers as with each other.[263]
In Lazaridis et al. (2022) a transect of samples from Albania which date from the EBA to the present day were tested. The population of Albania "appears to be largely made up of the same components in similar proportions" since the MBA. The core part of this profile consists of 50% Anatolian Neolithic Farmers, 20-25% Caucasus Hunter-Gatherers, 10-15% Eastern Hunter-Gatherers.[266] According to this study, the speakers of Albanian, as well as Greek and other Paleo-Balkan languages, go back directly to the migration of Yamnaya steppe pastoralists into the Balkans about 5,000 to 4,500 years ago, whose admixture with the local populations generated a tapestry of various ancestry, which in Albanians resulted in the above-mentioned components.[267]
Another study, from 2023, concludes that "a significant proportion" of the Modern Albanians paternal ancestry comes from West Balkans "including those traditionally known as Illyrians"[268]
Laonikos Chalkokondyles (c. 1423–1490), the Byzantine historian, considered the Albanians to be an extension of the Italians.[173] The theory has its origin in the first mention of the Albanians, disputed whether it refers to Albanians in an ethnic sense,[42] made by Attaliates (11th century): "...For when subsequent commanders made base and shameful plans and decisions, not only was the island lost to Byzantium, but also the greater part of the army. Unfortunately, the people who had once been our allies and who possessed the same rights as citizens and the same religion, i.e. the Albanians and the Latins, who live in the Italian regions of our Empire beyond Western Rome, quite suddenly became enemies when Michael Dokeianos insanely directed his command against their leaders..."[269]
One of the earliest theories on the origins of the Albanians, now considered obsolete, incorrectly identified the proto-Albanians with an area of the eastern Caucasus, separately referred to by classical geographers as Caucasian Albania, located in what roughly corresponds to modern-day southern Dagestan, northern Azerbaijan and bordering Caucasian Iberia to its west. This theory conflated the two Albanias supposing that the ancestors of the Balkan Albanians (Shqiptarët) had migrated westward in the late classical or early medieval period. The Caucasian theory was first proposed by Pope Pius II in his writings. [270] and later by Renaissance humanists who were familiar with the works of classical geographers, and further developed by early 19th-century French consul and writer François Pouqueville. It was soon rendered obsolete in the 19th century when linguists proved Albanian as being an Indo-European, rather than a Caucasian language.[271]
In terms of historical theories, an outdated theory [272][273] is the 19th century theory that Albanians specifically descend from the Pelasgians, a broad term used by classical authors to denote the autochthonous, pre-Indo-European inhabitants of Greece and the southern Balkans in general. However, there is no evidence about the possible language, customs and existence of the Pelasgians as a distinct and homogeneous people and thus any particular connection to this population is unfounded.[51] This theory was developed by the Austrian linguist Johann Georg von Hahn in his work Albanesische Studien in 1854. According to Hahn, the Pelasgians were the original proto-Albanians and the language spoken by the Pelasgians, Illyrians, Epirotes and ancient Macedonians were closely related. In Hahn's theory the term Pelasgians was mostly used as a synonym for Illyrians. This theory quickly attracted support in Albanian circles, as it established a claim of predecence over other Balkan nations, particularly the Greeks. In addition to establishing "historic right" to territory this theory also established that the ancient Greek civilization and its achievements had an "Albanian" origin.[274] The theory gained staunch support among early 20th-century Albanian publicists.[275] This theory is rejected by scholars today.[276] In contemporary times with the Arvanite revival of the Pelasgian theory, it has also been recently borrowed by other Albanian speaking populations within and from Albania in Greece to counter the negative image of their communities.[277]
In the late 19th and early 20th century Romanian linguist Hasdeu speculated the origin of Albanians from the free Dacians (i.e., according to him, the Costoboci, the Carpi and the Bessi), after their alleged migration southwards from outside the Danubian or Carpathian limes during Roman Imperial times. His outdated methods are linguistically unsustainable and his reconstructed narrative is based on no factual evidence.[278] This unfounded narrative was revived in the late 20th century by Romanian historian I. I. Russu[279] persistently and beyond the scientific knowledge that was achieved in the meantime. Despite having a history background, he made claims in the field of philology and comparative linguistics, eager to prove the autochthony of the Romanian people in their present-day heartlands (mainly north of the Danube and in Transylvania).[280] According to him, the pre-Roman lexical element shared with Romanian ("Traco-Dacian"), the massive Roman lexical element in Albanian, and the little Ancient Greek element, indicate an origin from the Thracian Carpi beyond the northeastern borders of the Empire, in the Carpatho-Danubian areas where Romanization could have been averted. Although Russu himself reported Pedersen's argument according to which precisely the large Latin influence and the small Ancient Greek influence speak in favor of the Illyrian origin of Albanian, the question arises why Russu ignored the fact that the large Latin influence actually indicates the location of Albanian within the Roman world and not outside it. Russu's linguistic analysis obviously has errors, and above all his mode of argumentation goes even beyond Hasdeu's romantic narrative.[281]
"The name with the root arb- is mentioned in old Albanian documents, but it went out of use in the main part of Albanian-speaking area and remains in use only in diaspora dialects (It.-Alb. arbëresh, Gr.-Alb. arvanitas). In other areas, it has been replaced by the term with the root shqip-."
"The ethnic name shqiptar has always been discussed together with the ethnic complex: (tosk) arbëresh, arbëror, arbër — (gheg) arbënesh, arbënu(e)r, arbën; i.e. [arbën/r(—)]. [...] Among the neighbouring peoples and elsewhere the denomination of the Albanians is based upon the root arb/alb, cp. Greek 'Αλβανός, 'Αρβανός "Albanian", 'Αρβανίτης "Arbëresh of Greece", Serbian Albanac, Arbanas, Bulg., Mac. албанец, Arom. arbinés (Papahagi 1963 135), Turk. arnaut, Ital. albanese, German Albaner etc. This basis is in use among the Arbëreshs of Italy and Greece as well."
"They called themselves arbënesh, arbëresh, the country Arbëni, Arbëri, and the language arbëneshe, arbëreshe. In the foreign languages, the Middle Ages denominations of these names survived, but for the Albanians they were substituted by shqiptarë, Shqipëri and shqipe... Shqip spread out from the north to the south, and Shqipni/Shqipëri is probably a collective noun, following the common pattern of Arbëni, Arbëri."
"Arbëreshë was the term self-designiation of Albanians before the Ottoman invasion of the 15 century; similar terms are used for Albanian origins populations living in Greece ("Arvanitika," the Greek rendering of Arbëreshë) and Turkey ("Arnaut," Turkish for the Greek term Arvanitika)".
"The Albanians who use the 'Alb-' root are the ones who emigrated to Italy in the fifteenth century, who call themselves 'Arberesh'."
"Today, the common name for Albanians, i.e. Albania, shqiptar, Shqiperia, is more recent. Albanians who settled in Greece in the Middle Ages and those who emigrated to Italy in the 15th century and later do not actually know about this name. The origin of the name Shqiptar is not clearly established. Until recently, the favorite interpretation was that it was derived from the Albanian shqipe "lord, nobility", thus "lord's sons". It is more likely, however, that the modern name given by the Albanians to themselves is derived from shqipon "to speak clearly" or from shqipton "to pronounce" (compared to the Slavic name nemci "dumb; those who do not speak intelligibly")."

This is a list of dates associated with the prehistoric peopling of the world (first known presence of Homo sapiens).
The list is divided into four categories, Middle Paleolithic (before 50,000 years ago),
Upper Paleolithic (50,000 to 12,500 years ago), Holocene (12,500 to 500 years ago) and Modern (Age of Sail and modern exploration).
List entries are identified by region (in the case of genetic evidence spatial resolution is limited) or region, country or island, with the date of the first known or hypothesised modern human presence (or "settlement", although Paleolithic humans were not sedentary).
Human "settlement" does not necessarily have to be continuous; settled areas in some cases become depopulated due to environmental conditions, such as glacial periods or the Toba volcanic eruption.[1] Early Homo sapiens migrated out of Africa from as early as 270,000 years ago, although these early migrations may have died out and permanent Homo sapiens presence outside of Africa may not have been established until about 70–50,000 years ago.
Before Homo sapiens, Homo erectus had already spread throughout Africa and non-Arctic Eurasia by about one million years ago. The oldest known evidence for anatomically modern humans (as of 2017[update]) are fossils found at Jebel Irhoud, Morocco, dated about 360,000 years old.[2]
Evidence from population genetics suggests separation before 110 ka,[9] most likely between 130 and 200 ka.[10]
Genetic evidence suggests first settlement 70–66 kya. Available fossil evidence from Sri Lanka has been dated to 34 kya.

Paleolithic
Epipalaeolithic
Mesolithic
Neolithic
The Later Stone Age (LSA) is a period in African prehistory that follows the Middle Stone Age.
The Later Stone Age is associated with the advent of modern human behavior in Africa, although definitions of this concept and means of studying it are up for debate. The transition from the Middle Stone Age to the Late Stone Age is thought to have occurred first in eastern Africa between 50,000 and 39,000 years ago. It is also thought that Later Stone Age peoples and/or their technologies spread out of Africa over the next several thousand years.[1]
The terms "Early Stone Age", "Middle Stone Age" and "Later Stone Age" in the context of African archaeology are not to be confused with the terms Lower Paleolithic, Middle Paleolithic, and Upper Paleolithic. 
They were introduced in the 1920s, as it became clear that the existing chronological system of Upper, Middle, and Lower Paleolithic was not a suitable correlate to the prehistoric past in Africa. Some scholars, however, continue to view these two chronologies as parallel, arguing that they both represent the development of behavioral modernity.[2]
Originally, the Later Stone Age was defined as several stone industries and/or cultures which included other evidence of human activity, such as ostrich eggshell beads and worked bone implements, and lacked Middle Stone Age stone tools other than those recycled and reworked. LSA peoples were directly linked with biologically and behaviorally modern populations of hunter/gatherers, some being directly identified as San "Bushmen." This definition has changed since its creation with the discovery of ostrich eggshell beads and bone harpoons in contexts which predate the LSA by tens of thousands of years.[3] The Later Stone Age was also long distinguished from the earlier Middle Stone Age as the time in which modern human behavior developed in Africa. This definition has become more tenuous as evidence for such modern human behaviors is found in sites which predate the LSA significantly.
The LSA follows the Middle Stone Age and begins about 50,000 years ago. The LSA is characterized by a wider variety in stone artifacts than in the previous MSA period. These artifacts vary with time and location, unlike Middle Stone Age technology which appeared to have been relatively unchanged for several hundreds of thousands of years. LSA technology is also characterized by the use of bone tools. The LSA was associated with modern human behavior,[4] but this view was modified after discoveries in MSA sites such as Blombos Cave and Pinnacle Point.
LSA sites also greatly outnumber MSA sites in Africa, a trend that could indicate an increase in population numbers. The greater number of LSA sites could also result from bias towards better preservation of younger sites which have had fewer chances to be destroyed.[5]
Differences in stone tool technologies are often used to distinguish between the Middle Stone Age and the Later Stone Age. The larger prepared platform flake-based stone tool industries of the Middle Stone Age, such as Levallois were increasingly replaced with industries that focused on producing blades and bladelets on cores with simple platforms.[6] African stone tool technologies are divided into modes as proposed by Grahame Clark in 1969 and outlined by Lawrence Barham and Peter Mitchell as follows:[7]
The lithic technologies of the Later Stone Age often fall into Modes 4 and 5. They have been further broken into four stages within the LSA.[3]
The end of the Later Stone Age took place when groups adopted technologies such as metallurgy to replace the use of stone tools. This process happened at different rates across the continent, and that the term "LSA" is typically used by archaeologists today to refer primarily to stone tool-using hunter/gatherer populations in southern Africa. The model of the LSA "human revolution" is no longer favored by many archaeologists working in Africa due to the increasing evidence for development of modern human behavior earlier than 40,000-50,000 years ago.

In classical antiquity, several theses were elaborated on the origin of the Etruscans from the 5th century BC, when the Etruscan civilization had been already established for several centuries in its territories, that can be summarized into three main hypotheses. The first is the autochthonous development in situ out of the Villanovan culture, as claimed by the Greek historian Dionysius of Halicarnassus who described the Etruscans autochthonous people who had always lived in Etruria.[1] The second is a migration from the Aegean Sea, as claimed by two Greek historians: Herodotus, who described them as a group of immigrants from Lydia in Anatolia,[2] and Hellanicus of Lesbos who claimed that the Tyrrhenians were the Pelasgians originally from Thessaly, Greece, who entered Italy at the head of the Adriatic Sea in Northern Italy.[3] The third hypothesis was reported by Livy and Pliny the Elder, and puts the Etruscans in the context of the Rhaetian people to the north and other populations living in the Alps.[4]
The first Greek author to mention the Etruscans, whom the Ancient Greeks called Tyrrhenians, was the 8th-century BC poet Hesiod, in his work, the Theogony. He mentioned them as residing in central Italy alongside the Latins.[5] The 7th-century BC Homeric Hymn to Dionysus[6] referred to them as pirates.[7] Unlike later Greek authors, such as Herodotus and Hellanicus, these earlier Greek authors did not suggest that Etruscans had migrated to Italy from elsewhere.
According to prehistoric and protohistoric archaeologists, anthropologists, etruscologists, geneticists, linguists, all the evidence gathered so far points to an autochthonous origin of the Etruscans.[8][9][10][11] Moreover, there is no archeological evidence for a migration of the Lydians or the Pelasgians into Etruria.[12][9][10][11] It was only in the 5th century BC, when the Etruscan civilization had been established for several centuries, that Greek writers started associating the name "Tyrrhenians" with the "Pelasgians" or the "Lydians". There is consensus among modern scholars that these Greek tales are not based on real events.[13] The earliest evidence of a culture that is identifiably Etruscan dates from about 900 BC: this is the period of the Iron Age Villanovan culture, considered to be the earliest phase of Etruscan civilization,[14][15][16][17][18] which itself developed from the previous late Bronze Age Proto-Villanovan culture in the same region, part of the central European Urnfield culture system.[19]
Helmut Rix's classification of the Etruscan language in the Tyrsenian language family reflects the ambiguity of the stories about their origins. Rix finds Etruscan on the one hand genetically related to the Rhaetic language spoken in the Alps north of Etruria, suggesting autochthonous connections, but on the other hand he notes that the Lemnian language found on the "Lemnos stele" is closely related to Etruscan, entailing either Etruscan presence in "Tyrsenian" Lemnos, or "Tyrsenian" expansion westward to Etruria.[20] After more than 90 years of archaeological excavations at Lemnos, nothing has been found that would support a migration from Lemnos to Etruria,[21] the indigenous inhabitants of Lemnos, also called in ancient times Sinteis, were the Sintians, a Thracian population.[21] Some scholars believe the Lemnian language might have arrived in the Aegean Sea from West during the Late Bronze Age, when Mycenaean rulers recruited groups of mercenaries from Sicily, Sardinia and various parts of the Italian peninsula.[22] Other scholars have concluded that the Lemnian inscriptions might be due to an Etruscan commercial settlement on the island that took place before 700 BC, not related to the Sea Peoples.[23][24][25][26][27]
A mtDNA study published in 2013 concluded that the Etruscans' mtDNA appears very similar to that of Neolithic population from Central Europe and to other Tuscan populations.[28][29] This coincides with the Rhaetic language, which was spoken south and north of the Alps in the area of the Urnfield culture of Central Europe. The Villanovan culture, the early period of the Etruscan civilization, derives from the Proto-Villanovan culture that branched from the Urnfield culture around 1200 BC. An autochthonous population that diverged genetically was previously suggested as a possibility by Cavalli-Sforza.[30]
A 2019 genetic study published in the journal Science analyzed the autosomal DNA of 11 Iron Age samples from the areas around Rome, concluding that Etruscans (900-600 BC) and the Latins (900-200 BC) from Latium vetus were genetically similar, and Etruscans also had Steppe-related ancestry despite speaking a pre-Indo-European language.[31]
A 2021 genetic study published in the journal Science Advances analyzed the autosomal DNA of 48 Iron Age individuals from Tuscany and Lazio and confirmed that the Etruscan individuals displayed the ancestral component Steppe in the same percentages as found in the previously analyzed Iron Age Latins, and that the Etruscans' DNA completely lacks a signal of recent admixture with Anatolia or the Eastern Mediterranean, concluding that the Etruscans were autochthonous and they had a genetic profile similar to their Latin neighbors. Both Etruscans and Latins joined firmly the European cluster, 75% of the Etruscan male individuals were found to belong to haplogroup R1b-M269 and its subclades, especially R1b-P312 and its derivative R1b-L2 whose direct ancestor is R1b-U152, while the most common mitochondrial DNA haplogroup among the Etruscans was H.[32]
Dionysius of Halicarnassus asserted:[33]
Indeed, those probably come nearest to the truth who declare that the nation migrated from nowhere else, but was native to the country, since it is found to be a very ancient nation and to agree with no other either in its language or in its manner of living.
With this passage, Dionysius launched the autochthonous theory, that the core element of the Etruscans, who spoke the Etruscan language, were of "Terra (Earth) itself"; that is, on location for so long that they appeared to be the original or native inhabitants. They are therefore the owners of the Villanovan culture.[34]
Picking up this theme, Bonfante (2002) states:[35]
...the history of the Etruscan people extends ... from c. 1200 to c. 100 BC. Many sites of the chief Etruscan cities of historical times were continuously occupied from the Iron Age Villanovan period on. Much confusion would have been avoided if archaeologists had used the name 'Proto-Etruscan' .... For in fact the people ... did not appear suddenly. Nor did they suddenly start to speak Etruscan.
An additional elaboration conjectures that the Etruscans were[36]
...an ethnic island of very ancient peoples isolated by the flood of Indo-European speakers.
In 1942, the Italian historian Massimo Pallottino published a book entitled The Etruscans (which would be released in English in 1955). Pallottino presented various hypotheses that gained wide acceptance in the archeological community. He said "no one would dream of asking where Italians or Frenchmen came from originally; it is the formation of the Italian and French nations that we study." He meant that the formation process for Etruscan civilization took place in Etruria or nearby.[37] Formulating a different point of view on the same evidence, Pallottino says:[38]
... we must consider the concept 'Etruscan' as ... attached to ... a nation that flourished in Etruria between the eighth and first centuries BC... We may discuss the provenance of each of these elements but a more appropriate concept ... would be that of formation... the formative process of the nation can only have taken place on the territories of the Etruscans proper; and we are able to witness the final stages of this process.
J. P. Mallory compares the Etruscans to other remnant non Indo-European central Mediterranean populations, such as the Basques of the Iberian Peninsula and southern France, who absorbed the art styles and alphabet of their Greek neighbors.[39]
The British archaeologists, Graeme Barker and Tom Rasmussen, were also fervent supporters of the "autochthonous theory". In their book, The Etruscans, they state, "There is no evidence for the kind of cultural break at the Villanovan/Etruscan transition envisaged by either of the ‘plantation’ models from the eastern Mediterranean, or for a folk movement of either kind from continental Europe in the Late Bronze Age,".[40] Thus, inferring that the Etruscans were indigenous to Italy and descended from the previous communities of Etruria.
Many supporters of this theory also believed that the Etruscans had foreign influences on their culture. For instance, the historian, Mario Torelli agreed with Dionysius’s claims and believed that the Etruscans inherited elements of their culture from other Italic peoples.[41] Robert Leighton also agreed with the “autochthonous theory”, but he believed the Etruscan's culture was impacted by Greek and Phoenician merchants.[42]
In Greco-Roman mythology, Aeneas (Greek: Αἰνείας, Aineías) was a Trojan hero, the son of prince Anchises and the goddess Venus. His father was also the second cousin of King Priam of Troy. The journey of Aeneas from Troy (led by Venus, his mother), which led to the founding of the city of Rome, is recounted in Virgil's Aeneid, where the historicity of the Aeneas legend is employed to flatter the Emperor Augustus. Romulus and Remus, appearing in Roman mythology as the traditional founders of Rome, were of Eastern origin: their grandfather Numitor and his brother Amulius were alleged to be descendants of fugitives from Troy.
Herodotus reports the Lydians' claim that the Etruscans came from Lydia in Asia Minor (i.e. Anatolia):[43]
This is their story: [...] their king divided the people into two groups, and made them draw lots, so that the one group should remain and the other leave the country; he himself was to be the head of those who drew the lot to remain there, and his son, whose name was Tyrrhenus, of those who departed. [...] they came to the Ombrici, where they founded cities and have lived ever since. They no longer called themselves Lydians, but Tyrrhenians, after the name of the king's son who had led them there.
Since ancient times, doubts have been raised about the accuracy of Herodotus' claims. Xanthus of Lydia, originally from Sardis and a great connoisseur of the history of the Lydians, wasn't aware of a Lydian origin of the Etruscans, as reported by Greek historian Dionysius of Halicarnassus.[33]
Xanthus of Lydia, who was well acquainted with ancient history as any man and who may be regarded as an authority second to none on the history of his own country [and yet he] neither names Tyrrhenus in any part of his history as a ruler of the Lydians nor knows anything of the landing of a colony of Lydians in Italy
The classical scholar Michael Grant commented on this story, writing that it "is based on erroneous etymologies, like many other traditions about the origins of 'fringe' peoples of the Greek world".[44] Grant writes there is evidence that the Etruscans themselves spread it to make their trading easier in Asia Minor when many cities in Asia Minor, and the Etruscans themselves, were at war with the Greeks.[45]
The French scholar Dominique Briquel also disputed the historical validity of Herodotus' account. Briquel demonstrated that "the story of an exodus from Lydia to Italy was a deliberate political fabrication created in the Hellenized milieu of the court at Sardis in the early 6th century BC."[46][47] Briquel also commented that "the traditions handed down from the Greek authors on the origins of the Etruscan people are only the expression of the image that Etruscans' allies or adversaries wanted to divulge. For no reason, stories of this kind should be considered historical documents".[48]
However, the Greek historian Dionysius of Halicarnassus objected that the Tyrrhenian (Etruscan) culture and language shared nothing with the Lydian. He stated:[33]
For this reason, therefore, I am persuaded that the Pelasgians are a different people from the Tyrrhenians. And I do not believe, either, that the Tyrrhenians were a colony of the Lydians; for they do not use the same language as the latter, nor can it be alleged that, though they no longer speak a similar tongue, they still retain some other indications of their mother country. For they neither worship the same gods as the Lydians nor make use of similar laws or institutions, but in these very respects they differ more from the Lydians than from the Pelasgians.
The Etruscans or Tyrrhenians may have been one of the sea peoples of the 14th–13th century BC,[49] if Massimo Pallottino's assimilation of the Teresh of Egyptian inscriptions with Tyrrhenoi is correct.[50] There is no further evidence to connect the Sea Peoples to the Etruscans: the Etruscan autonym Rasna, does not lend itself to the Tyrrhenian derivation.
Neither the Etruscan material culture or language has provided scholars with conclusive evidence regarding the Etruscans' origins. The language, which has been partly deciphered, has variants and representatives in inscriptions on Lemnos, in the Aegean, but these may have been created by travellers or Etruscan colonists, during the period before Rome destroyed Etruscan political and military power.
During the 6th to 5th centuries BC, the word "Tyrrhenians" was referred specifically to the Etruscans, for whom the Tyrrhenian Sea is named, according to Strabo.[51] In Pindar, the Tyrsenoi appear grouped with the Carthaginians as a threat to Magna Graecia:[52]
I entreat you, son of Cronus, grant that the battle-shouts of the Carthaginians and Etruscans stay quietly at home, now that they have seen their arrogance bring lamentation to their ships off Cumae.
Thucydides mentions them together with the Pelasgians and associates them with Lemnian pirates and with the pre-Greek population of Attica. Lemnos remained relatively free of Greek influence up to Hellenistic times, and the Lemnos stele of the 6th century BC is inscribed with a language very similar to Etruscan. This has led to the postulation of a "Tyrrhenian language group" comprising Etruscan, Lemnian and Raetic.[53] There is thus linguistic evidence of a relationship between the Lemnians and the Etruscans. Some scholars ascribe this link to Etruscan expansion between the 8th and 6th centuries BC, putting the homeland of the Etruscans in Italy and the Alps particularly because of their relation to the Alpine Raetic population.[54] Adherents of this latter school of thought point to the legend of Lydian origin of the Etruscans referred to by Herodotus, and the statement of Livy that the Raetians were Etruscans driven into the mountains by the invading Gauls. Critics of this theory point to the very scanty evidence of a linguistic relationship of Etruscan with Indo-European, let alone Anatolian in particular, and to Dionysius of Halicarnassus who decidedly argues against an Etruscan-Lydian relationship. The Indo-European Lydian language is first attested some time after the Tyrrhenian migrants are said to have left for Italy.[55]
The origin of the civilization of Etruria is an ancient debate, because the terms in which historians have opened and contested theories have relied on out-dated conceptions of origin and culture. The last two millennia of raising inconclusive theories towards a definitive location for the origins of Etruria has led modern scholarship to diverge from traditional approaches to national origins and instead focus on the development of concepts, such as national origin and cultural formation, differentiating between cultural influence and cultural origin.
The initial sources of inquiry for historians studying Etruscan origins are the classical sources provided by ancient scholars such as Herodotus and Dionysius. These writers were naturally interested in where such an advanced civilization originated. Herodotus initiated the Lydian theory which told the story of Etruscan origins as a mass migration from Lydia, led by King Tyrsenos, a migration due to the famine experienced shortly after the Trojan War. Larissa Bonfante argues that the traditional concept of origin that classical Greek writers subscribed to "had to be explained as the result of a migration, under the leadership of a mythical founding hero".[56]
The second key hypothesis was launched by the Augustan historian, Dionysius of Halicarnassus. Being aware that his predecessors were "unanimous in stating that the Etruscans came from the East"[57] he expressed an alternate hypothesis that the Etruscans were "native to the country",[58] and by doing so opened the autochthonous theory. Scholarship has questioned why ancient sources appear "unanimous" towards an Anatolian origin. Bonfante suggests that it is the natural response for Greek writers to connect other civilizations accomplishments to "Greek heroes" in an attempt to promote a "glorified national narrative". On the other hand R.S.P. Beekes argues that these ancient writers, especially Herodotus, found the famine in Lydia an obvious connection to the migration to Etruria, rather than a debatable area of discussion. The autochthonous theory that Dionysius instigated was a view held by Etruscans themselves, whom he consulted, though how much these Etruscans knew about their own origins is questionable.
The reason modern scholarship, such as John Bryan Perkins, sceptically uses ancient sources as evidence to support an argument, is because these sources generally promote a national image and harbour political prejudices. He argues that the ancient interpretation of Etruscan origins has derived from a "hostile tradition, of rivals and enemies; the Greeks and Romans". The extent of "classical prejudice" is exemplified in early records of the Etruscans. Classical literature typically portrayed Etruscans as 'pirates' and 'freebooters'. Massimo Pallottino points out that their reputation for piracy took shape between the time of Homer and the image shown in the Homeric Hymns, and was clearly a product of the intense commercial and territorial rivalry between the Etruscans and Greek traders. Consequentially Perkins concludes that ancient "standards of historical criticism were not ours" in which "a great deal of it is seen through a veil of interpretation, misunderstanding, and at times, plain invention".[59] The ancient tendency to invent or apply a fabricated account within their historical record is evident in Herodotus' Histories. His use of fanciful story telling contributes to the overarching glorified narrative of Greece in the Persian wars and exemplifies the greatness of Greek conquest. This agenda is problematic when viewing his 'heroic' understanding of Etruscan origins, because Herodotus' stories tend to contribute to the national narrative rather than an intended historical record. His account is seen through, what Perkins refers to as, antiquity's "distorting mirror".[59]
In the 1950s, Professor Pallottino resurrected the initial autochthonous theory and by doing so contended with traditional scholarship that has "remained fixated on the idea that the origins of the Italic people were to be found in the effects of immigration from outside". The argument has been developed on the basis that the Etruscan culture appears unique to any other known prehistoric culture, therefore must have developed nowhere else but within Italy".[60] He admits to foreign contributions to the cultural development of the Etruscans, however, he maintains that the mixture of culture took place on Italian soil; the "parent stock" was sufficiently homogeneous and therefore of Italian origin. Indigenous arguments are based on the unique attributes of Etruscan culture, believing that it is an "evolutionary sequence" in which Etruria developed its independent culture, a "formative process of the Etruscan which can only take place on the territory of Etruria itself".[60] Nevertheless, to subscribe to this thesis a problem arises; Etruscan culture was "no doubt in itself a unique and developing phenomenon", however, this culture has been compounded of and developed from other earlier cultural strains.[60] The question remains whether these strains were dominant in the finished product; it is difficult to differentiate between a product of a foreign culture and an independent culture with foreign influences. Other historical methodologies, such as linguistics, archaeology and DNA research, have attempted to clarify this distinction and highlight the extent of foreign influence in Etruscan culture.
Linguists have attempted to shed light on the degree of foreign influence on the Etruscan civilization. R.S.P. Beekes places reliance on his linguistic analysis of the Lemnian inscriptions, which he believes "provided the answer to the problem of the origins of the Etruscans".[61] The Lemnos stele is a sixth-century stele in a pre-Hellenic tongue found in Lemnos, a Northern Greek island. The inscription shows distinct similarities to the Etruscan language; both languages apply a similar four vowel system, grammar and vocabulary. Beekes argues that autochthonous theories are merely "a desperate attempt to avoid the evident conclusion from the Lemnian inscription".[61] He does not suggest that the language shaped the Etruscan culture, but rather that the similarities in the two languages proves that the Etruscans migrated from Asia Minor, as Herodotus suggested.
Alison E. Cooley criticises Beekes' assumption that the Eastern features found in the etymological research of the Lemnian inscription "simply settles the question", yet she imposes that the "later Eastern attributes of the Etruscan is often a product of acculturation".[62] Cooley in contrary to Beekes argues that the similarities in the languages are a result of contact with Greek and Lydian civilization due to commercial trade.
Linguists, such as Beekes, are commonly criticised for the assumption that "because they speak a common language, they must belong to the same race".[59] However, recently linguists such as Kari Gibson have argued that language is the predominant factor in the cultural formation of a national identity and therefore cannot be discarded as an independent attribute of a cultural identity, but rather the framework through which such a civilization functions. Gibson suggests that language is inextricably linked to national and cultural identity of the speaker, and as a "powerful symbol of national and ethnic identity" determines an individual's perception of their environment.[63] To place this argument in the linguistic debate of Etruscan origins, modern scholars such as Cooley are perhaps being overly dismissive of the impact of language on the development of the Etruscan identity; "Ethnic identity is twin skin to linguistic identity".[64] It is difficult for scholarship to evaluate the degree of influence the Lydian language would have had on the cultural development of Etruria, though language is undeniably a key ingredient in the development of Etruscan culture.
Archaeology has a prominent role in revealing aspects of Etruscan daily life and the social structure of such a sophisticated civilization, thus exposing foreign influences. The most significant archaeological discoveries of Etruscan civilization are found in the excavation of gravesites. Bonfante emphasises the unique cultural elements the funerary frescoes in these gravesites illustrate. The well preserved frescoes of the funerary chambers found in the necropolis of Monterozzi, situated on a ridge southeast of the ancient city of Tarquinia, are vital to the reconstruction of Etruscan culture. Scholars of the autochthonous theory tend to draw attention to the frescoes' depiction of women. Material evidence for the high social status of Etruscan women can be found on the frescoes in the Tomb of the Leopards, dating to the 5th century BC.[65] The fresco illustrates women and men conversing together and wearing the same crowns of laurel, which implies that symbols of status in Etruscan society were similar for men and women. This advanced status for women is a unique Etruscan element that is not known from any other culture of its time.
Frescoes found in the Tomb of Hunting and Fishing mark the earliest time where men are not depicted dominating their environment.[citation needed] In the fresco of birds flying over a boat of men, the men are shown to be proportionally smaller than the birds. Pallottino points out that this is a unique attribute from Etruscan artworks, because it provides an insight into how the Etruscans viewed themselves in comparison to their environment. Ancient works dated prior to this fresco tended to view men dominating their environment. However, the Tomb of Hunting and Fishing illustrates men in the background of the work, rather than typically the foreground, suggesting to scholars such as Pallottino that Etruria had developed a culture and social understanding unlike any other prehistoric civilization and therefore cannot be a product of any prior culture.
The question of Etruscan origins has long been a subject of interest and debate among historians. In modern times, all the evidence gathered so far by etruscologists points to an indigenous origin of the Etruscans.[10][9][11] Archaeologically there is no evidence for a migration of the Lydians or the Pelasgians into Etruria.[10][9] Modern etruscologists and archeologists, such as Massimo Pallottino (1947), have shown that early historians’ assumptions and assertions on the subject were groundless.[66] The French etruscologist Dominique Briquel, whose numerous writings were devoted to this subject, explained in detail why he believes that ancient Greek historians’ writings on Etruscan origins should not even count as historical documents.[67] He argues that the ancient story of the Etruscans’ 'Lydian origins' was a deliberate, politically motivated fabrication, and that ancient Greeks inferred a connection between the Tyrrhenians and the Pelasgians solely on the basis of certain Greek and local traditions and on the mere fact that there had been trade between the Etruscans and Greeks.[13][47] He noted that, even if these stories include historical facts suggesting contact, such contact is more plausibly traceable to cultural exchange than to migration.[68]
Several archaeologists who have analyzed Bronze Age and Iron Age remains that were excavated in the territory of historical Etruria have pointed out that no evidence has been found, related either to material culture or to social practices, that can support a migration theory from the Aegean Sea.[69] The most marked and radical change that has been archaeologically attested in the area is the adoption, starting in about the 12th century BC, of the funeral rite of incineration in terracotta urns, which is a Continental European practice, derived from the Urnfield culture; there is nothing about it that suggests an ethnic contribution from Asia Minor or the Near East.[69] One of the most common mistakes for a long time, even among some scholars of the past, has been to associate the later Orientalizing period of Etruscan civilization, due, as has been amply demonstrated by archeologists, to contacts with the Greeks and the Eastern Mediterranean and not mass migrations, with the question of their origins.[70] The facial features (the profile, almond-shaped eyes, large nose) in the frescoes and sculptures, and the depiction of reddish-brown men and light-skinned women, influenced by archaic Greek art, followed the artistic traditions from the Eastern Mediterranean, that had spread even among the Greeks themselves, and to a lesser extent also to other several civilizations in the central and western Mediterranean up to the Iberian Peninsula. Actually, many of the tombs of the Late Orientalizing and Archaic periods, such as the Tomb of the Augurs, the Tomb of the Triclinium or the Tomb of the Leopards, as well as other tombs from the archaic period in the Monterozzi necropolis in Tarquinia, were painted by Greek painters or, in any case, foreigner artists. These images have, therefore, a very limited value for a realistic representation of the Etruscan population.[71] It was only from the end of the 4th century B.C. that evidence of physiognomic portraits began to be found in Etruscan art and Etruscan portraiture became more realistic.[72]
A 2012 survey of the previous 30 years’ archaeological findings, based on excavations of the major Etruscan cities, showed a continuity of culture from the last phase of the Bronze Age (12th–10th century BC) to the Iron Age (9th–8th century BC). This is evidence that the Etruscan civilization, which emerged around 900 BC, was built by people whose ancestors had inhabited that region for at least the previous 200 years,[73] as has also been confirmed by anthropological and genetic studies.[32][74] Based on this cultural continuity, there is now a consensus among archeologists that Proto-Etruscan culture developed, during the last phase of the Bronze Age, from the indigenous Proto-Villanovan culture, and that the subsequent Iron Age Villanovan culture is most accurately described as an early phase of the Etruscan civilization.[19] It is possible that there were contacts between northern-central Italy and the Mycenaean world at the end of the Bronze Age. However, contacts between the inhabitants of Etruria and inhabitants of Greece, Aegean Sea Islands, Asia Minor, and the Near East are attested only centuries later, as well as those with the Celtic world, when Etruscan civilization was already flourishing and Etruscan ethnogenesis was well established. The first of these attested contacts relate to the Greek colonies in Southern Italy and the Nuragics and Sardo-Punics in Sardinia, and the consequent orientalizing period.[75]
There have been numerous biological studies on the Etruscan origins, the oldest of which dates back to the 1950s when research was still based on blood tests of modern samples, and DNA analysis (including the analysis of ancient samples) was not yet possible.[76][77][78] It is only in very recent years, starting in 2019, with the development of archaeogenetics, that comprehensive studies containing the whole genome sequencing (WGS) of Etruscan samples have been published, including autosomal DNA and Y-DNA, autosomal DNA being the "most valuable to understand what really happened in an individual's history", as stated by geneticist David Reich, whereas previously studies were based only on mitochondrial DNA analysis, which contains less and limited information.[79] The direct testing of ancient Etruscan DNA supports a deep, local origin, while the testing of modern samples as a proxy for Etruscans has proven to be rather inconclusive and inconsistent.[80][81]
A 2019 genetic study published in the journal Science analyzed the autosomal DNA of 11 Iron Age samples from the areas around Rome, including for the first time the whole genome sequencing (WGS) of some samples from Etruscan tombs, and concluded that Etruscans (900-600 BC) and the Latins (900-200 BC) from Latium vetus were genetically similar, and Etruscans also had Steppe-related ancestry despite speaking a pre-Indo-European language.[31] A 2021 genetic study published in the journal Science Advances analyzed the autosomal DNA of 48 Iron Age individuals from Tuscany and Lazio and confirmed that the Etruscan individuals displayed the ancestral component Steppe in the same percentages as found in the previously analyzed Iron Age Latins, and that the Etruscans' DNA completely lacks a signal of recent admixture with Anatolia or the Eastern Mediterranean, concluding that the Etruscans were autochthonous and they had a genetic profile similar to their Latin neighbors. Both Etruscans and Latins joined firmly the European cluster, 75% of the Etruscan male individuals were found to belong to haplogroup R1b-M269 and its subclades, especially R1b-P312 and its derivative R1b-L2 whose direct ancestor is R1b-U152, while the most common mitochondrial DNA haplogroup among the Etruscans was H.[32] Iron Age Etruscans from central Italy could be modelled as deriving 50% of their ancestry from Central European Bell Beakers, represented by Germany Bell Beaker, with around 25-30% Steppe-related ancestry, and the rest of their ancestry from a local Chalcolithic population. The conclusions of these studies have been confirmed by later ones.[82]
In the collective volume Etruscology published in 2017, British archeologist Phil Perkins provides an analysis of the state of DNA studies and writes that "none of the DNA studies to date conclusively prove that Etruscans were an intrusive population in Italy that originated in the Eastern Mediterranean or Anatolia" and "there are indications that the evidence of DNA can support the theory that Etruscan people are autochthonous in central Italy".[81]
In his book A Short History of Humanity published in 2021, German geneticist Johannes Krause, co-director of the Max Planck Institute for Evolutionary Anthropology in Jena, concludes that it is likely that the Etruscan language (as well as Basque, Paleo-Sardinian and Minoan) "developed on the continent in the course of the Neolithic Revolution".[83]
A 2021 study by the Max Planck Institute, the Universities of Tübingen, Florence, and Harvard, published in the journal Science Advances and focused entirely on the question of the origins of the Etruscans, analyzed the Y-chromosome, mitochondrial DNA, and autosomal DNA of 82 ancient samples from Etruria (Tuscany and Latium) and southern Italy (Basilicata) spanning from 800 BC to 1000 AD, including 48 Iron Age individuals. The study observed that in the samples of Etruscan individuals from Tuscany and Lazio the ancestral component Steppe was present in the same percentages found in the previously analyzed samples of Iron Age Latins, and added that in the DNA of the Etruscans was completely absent a signal of recent admixture with Anatolia or the Eastern Mediterranean. The study concluded that the Etruscans were autochthonous and they had a genetic profile similar to that of their early Iron Age Latin neighbors. Both Etruscans and Latins belonged firmly to the European cluster: 75% of the samples of Etruscan male individuals were found to belong to haplogroup R1b, especially R1b-P312 and its derivative R1b-L2 whose direct ancestor is R1b-U152. Regarding mitochondrial DNA haplogroups, the most prevalent was largely H, followed by J and T. Uniparental marker data and autosomal DNA data from samples of Iron Age Etruscan individuals suggest that Etruria received migrations with a large ancestral Steppe component during the 2nd millennium BC, related to the spread of Indo-European languages, starting with the Bell Beaker culture, and that these migrations merged with populations of the oldest pre-Indo-European layer present since at least the Neolithic period, but it was the latter's language that survived, a situation similar to what happened in the Basque region of northern Spain. The study also concluded that the samples analyzed show that the Etruscans kept their genetic profile unchanged for almost 1000 years, indicating the sparse presence in Etruria of foreigners, and that a demographic change in Etruria occurred only from the Roman imperial period, in which there is the intermixture into the local population of ancestral components from the Eastern Mediterranean Sea.
The conclusions of the 2021 study are in line with those of an earlier 2019 study, the first to publish analyses of whole genome sequencing of Etruscan samples, although the 2019 is more focused on ancient Rome than the question of Etruscan origins. The 2019 genetic study, published in the journal Science, analyzed the remains of eleven Iron Age individuals from the areas around Rome, of which four were Etruscan individuals, one buried in Veio Grotta Gramiccia from the Villanovan period (900-800 BC) and three buried in La Mattonara Necropolis near Civitavecchia from the Orientalizing period (700-600 BC). The study concluded that Etruscans (900–600 BC) and the Latins (900–500 BC) from Latium vetus were genetically similar,[31] genetic differences between the examined Etruscans and Latins were found to be insignificant.[84] The Etruscan individuals and contemporary Latins were distinguished from preceding populations of Italy by the presence of 30.7% steppe ancestry.[85] Their DNA was a mixture of two-thirds Copper Age ancestry (EEF + WHG; Etruscans ~66–72%, Latins ~62–75%) and one-third Steppe-related ancestry (Etruscans ~27–33%, Latins ~24–37%) (with the EEF component mainly deriving from Neolithic-era migrants to Europe from Anatolia and the WHG being local Western European hunter-gatherers, with both components, along with that from the steppe, being found in virtually all European populations).[31] The only sample of Y-DNA extracted belonged to haplogroup J-M12 (J2b-L283), found in an individual dated 700-600 BC, and carried exactly the M314 derived allele also found in a Middle Bronze Age individual from Croatia (1631-1531 calBCE). While the four samples of mtDNA extracted belonged to haplogroups U5a1, H, T2b32, K1a4.[86] Therefore, Etruscans had also Steppe-related ancestry despite speaking a pre-Indo-European language.
In 2024, 6 individuals of Etruscan remains from Tarquinia, Lazio, dated the 9th-7th Century BC, were studied and confirmed the previous finds, Etruscans were an indigenous population. The admixture model showed that they were 84-92% Italy Bell Beaker and 8-26% additional Yamnaya Samara (Steppe-related) ancestry, but with one individual being more similar to Iron Age populations from Scandinavia, and north-west Europe. The two male individuals studied for Y-Chromosome belonged to the J2b/J-M12 lineage, and the five studied mitochondrial haplogroups were typical of post-Neolithic Europe. Phenotypic traits showed blue-eyes, light/dark brown hair, and pale to intermediate skin tones.[82]
A very large mtDNA study from 2013 indicates, based on maternally-inherited DNA from 30 bone samples taken from tombs dating from the eight century to the first century BC from Tuscany and Lazio, that the Etruscans were a native population.[28][29] The study extracted and typed the hypervariable region of mitochondrial DNA of 14 individuals buried in two Etruscan necropoleis, analyzing them along with previously analyzed Etruscan mtDNA, other ancient European mtDNA, modern and Medieval samples from Tuscany, and 4,910 modern individuals from the Mediterranean basin. The ancient (30 Etruscans, 27 Medieval Tuscans) and modern DNA sequences (370 Tuscans) were subjected to several million computer simulation runs, showing that the Etruscans can be considered ancestral to Medieval and, especially in the subpopulations from Casentino and Volterra, of modern Tuscans; modern populations from Murlo and Florence, by contrast, were shown not to continue the Medieval population. By further considering two Anatolian samples (35 and 123 individuals), it was estimated that the genetic links between Tuscany and Anatolia date back to at least 5,000 years ago, and the "most likely separation time between Tuscany and Western Anatolia falls around 7,600 years ago", strongly suggesting that the Etruscan culture developed locally, and not as an immediate consequence of immigration from the Eastern Mediterranean shores. According to the study, ancient Etruscan mtDNA is closest among modern European populations and is not particularly close to Anatolian or other Eastern Mediterranean populations. Among ancient populations based on mtDNA, ancient Etruscans were found to be closest to LBK Neolithic farmers from Central Europe.[28][29]
A mtDNA study, published in 2018 in the journal American Journal of Physical Anthropology, compared both ancient and modern samples from Tuscany, from the Prehistory, Etruscan age, Roman age, Renaissance, and Present-day, and concluded that the Etruscans appear as a local population, intermediate between the prehistoric and the other samples, placing in the temporal network between the Eneolithic Age and the Roman Age.[87]
These results are largely in line with previous mtDNA results from 2004 (in a smaller study also based on ancient DNA), and contradictory to results from 2007 (based on modern DNA). The 2004 study was based on mitochondrial DNA (mtDNA) from 80 bone samples, reduced to 28 bone samples in the analysis phase, taken from tombs dating from the seventh century to the third century BC from Veneto, Tuscany, Lazio and Campania.[88] This study found that the ancient DNA extracted from the Etruscan remains had some affinities with modern European populations including Germans, English people from Cornwall, and Tuscans in Italy. The study was marred by concerns that mtDNA sequences from the archeological samples represented severely damaged or contaminated DNA;[89] however, subsequent investigation showed that the samples passed the most stringent tests of DNA degradation available.[90]
An mtDNA study from 2007, by contrast, earlier suggested a Near Eastern origin.[91] Achilli et al. (2007) found in a modern sample of 86 individuals from Murlo, a small town in southern Tuscany, an unusually high frequency (17.5%) of supposed Near Eastern mtDNA haplogroups, while other Tuscan populations do not show the same striking feature. Based on this result Achilli concluded that "their data support the scenario of a post-Neolithic genetic input from the Near East to the present-day population of Tuscany, a scenario in agreement with the Lydian origin of Etruscans". This research has been much criticized by archeologists, etruscologists and classicists.[92] In the absence of any dating evidence, there is no direct link between this genetic input found in Murlo and the Etruscans. Furthermore, there is no evidence that these mtDNA haplogroups found in Murlo might be proof of an eastern origin of the Etruscans, as some of these mtDNA haplogroups have been found in other studies as early as the Neolithic and Aeneolithic in Italy and Germany.[87] All the mtDNA haplogroups found in the modern sample from Murlo and classified by Achilli et al. as of Near Eastern origin are actually widespread in modern samples from other areas of Italy and Europe with no link with the Etruscans.[93]
A recent Y-DNA study from 2018 on a modern sample of 113 individuals from Volterra, a town of Etruscan origin, Grugni at al. keeps all the possibilities open, although the autochthonous scenario is the most supported by numbers, and concludes that "the presence of J2a-M67* (2.7%) suggests contacts by sea with Anatolian people, the finding of the Central European lineage G2a-L497 (7.1%) at considerable frequency would rather support a Northern European origin of Etruscans, while the high incidence of European R1b lineages (R1b 49.8%, R1b-U152 24.5%) cannot rule out the scenario of an autochthonous process of formation of the Etruscan civilization from the preceding Villanovan society, as suggested by Dionysius of Halicarnassus".[94] In Italy Y-DNA J2a-M67*, not yet found in Etruscan samples, is more widespread on the Adriatic Sea coast between Marche and Abruzzo, and not in those where once lived the Etruscans, and in the study has its peak in the Ionian side of Calabria.[95][96] In 2014, a late Bronze Age Kyjatice culture sample in Hungary was found to be J2a1-M67,[97] a couple of J2a1b were found in Late Neolithic samples from the LBK culture in Austria,[98] a J2a1a was found in a Middle Neolithic Sopot culture sample from Croatia,[98] a J2a was found in a Late Neolithic Lengyel Culture sample from Hungary.[99] In 2019, in a Stanford study published in Science, two ancient samples from the Neolithic settlement of Ripabianca di Monterado in the province of Ancona, in the Marche region of Italy, were found to be Y-DNA J-L26 and J-M304.[31] In 2021, two more ancient samples from the Chalcolitich settlement of Grotta La Sassa, in the province of Latina in southern Lazio, were found to be Y-DNA J2a7-Z2397.[100] Therefore, Y-DNA J2a-M67 may be likely in Italy since the Neolithic and can't be the proof of recent contacts with Anatolia. In any case, J2a-M67 was not found among the Etruscan samples, unlike G2a-L497 and R1b-U152 who were actually found in the Etruscan samples in really significant percentages.[32]
Recent studies on the population structure of modern-day Italians have shown that in Italy there is a north–south cline for Y-chromosome lineages and autosomal loci, with a clear differentiation of peninsular Italians from Sardinians, and that modern Tuscans are the population of central Italy closest genetically to the inhabitants of northern Italy.[101] A 2019 study, based on autosomal DNA of 1616 individuals from all 20 Italian administrative regions, concludes that Tuscans join the northern Italian cluster, close to the inhabitants of Liguria and Emilia-Romagna.[102] A 2013 study, based on uniparental markers of 884 unrelated individuals from 23 Italian locations, had shown that the structure observed for the paternal lineages in continental Italy and Sicily suggests a shared genetic background between people from Tuscany and Northern Italy from one side, and people from Southern Italy and the Adriatic coast from the other side. The most frequent Y-DNA haplogroups in the group represented by populations from North-Western Italy, including Tuscany and most of the Padana plain, are four R1b-lineages (R-U152*, R-M269*, R-P312* and R-L2*).[96]

In archaeogenetics, Ancient Southern East Asian (ASEA), also known as Southern East Asian (sEA), is an ancestral lineage that is represented by individuals from Qihe Cave in Fujian (c. 12–8 kya) and Liangdao Island in the Taiwan Strait (c. 8 kya) as well as Guangxi (c. 9 kya). Ancient Southern East Asian ancestry significantly contributed to the genetic makeup of modern populations in East Asia, Mainland Southeast Asia, Insular Southeast Asia, and Oceania, and is commonly associated with the Neolithic expansion of early Austronesian and Austroasiatic speakers that occurred more than 4,000 years ago.
Until the early Holocene, Ancient Southern East Asians from Fujian were genetically clearly distinct from Ancient Northern East Asians (ANEA) who were distributed in an area stretching from the Yellow River to the Amur. The exact origins of both lineages are still only partially understood, but together they formed a distinct clade vis-a-vis all other known ancient East Eurasian lineages in eastern Asia, viz. the Tianyuan, Hoabinhian, Jomon, and Guangxi/Longlin ancestries.[1][2] The split between the ASEA and ANEA lineages must have occurred at least 19,000 years ago, as evidenced by an 19ky-old Upper Pleistocene individual from the Amur river with a clear ANEA affinity.[3]
In the mid-Holocene, southward migrations of millet farmers from the Yellow River harboring ANEA ancestry (and also to lesser degree a reverse geneflow of ASEA rice farmers from the Yangtse River to the north) resulted in the coastal East Asian ancestry cline that exists to this day. Northern Han Chinese mostly carry ANEA ancestry with a moderate degree of ASEA admixture, whereas southern Han Chinese as well as non-Han ethnic groups of southern East Asia (viz. speakers of Kra-Dai and Hmong-Mien languages) still carry significantly higher levels of ASEA ancestry.[4][5]
Starting from the third millennium BCE, rice farming-based agriculture spread from southern East Asia into Mainland and Insular Southeast Asia. This technological spread was a result of the migration of southern East Asian agriculturalists that carried ASEA ancestry. These Neolithic farmers took two routes: an inland route into Mainland Southeast Asia, and a maritime route that originated from Taiwan.[6][5][7]
Ancient DNA of the first farmer individuals from Mainland Southeast Asia  dated to c. 8–4kya, derive most of their ancestry from the ASEA lineage, with significant admixture from a local hunter-gatherer population.[a] A 9,000 year old specimen from the Dushan Cave could be modeled to have derived most of its ancestry from an ASEA-like source (c. 83%), with around 17% of its ancestry from the deeply branching 'Longlin lineage'. This type of 'Dushan ancestry' was also observed in 8,300 to 6,400 year old individuals from Mainland Southeast Asia (c. 72%) with around 28% additionally deeply branching East Asian admixture associated with the Hoabinhian cultural complex.[11]
This Neolithic Mainland Southeast Asian ancestry peaks among modern populations in Austroasiatic-speaking groups of Southeast Asia (most notably in the Mlabri and Htin peoples in northern Laos and Thailand) and parts of East and South Asia. Hence, the first spread of farming in Mainland Southeast Asia is widely assumed to be linked to the expansion of the Austroasiatic languages.[8][6] From Mainland Southeast Asia, this Austroasiatic-related ancestry spread into Insular Southeast Asia to the Sunda Islands,[8] adjacent areas (viz. Palawan, Mindanao) of the Philippines,[12] and western Wallacea,[13][14] although there are no remaining Austroasiatic languages spoken in this area, having been supplanted by incoming Austronesian languages.
The rapid maritime expansion of the early Austronesians starting c. 5,000–4,000 years ago brought ASEA-like ancestry from Taiwan to the Philippines, the Indonesian archipelago and Oceania, initially with little admixture from local populations, as can be seen from 2,900–2,500 year-old Lapita-related individuals from Vanuatu and Tonga,[15] and from ancient 2,800–2,200 year-old DNA of the first settlers of Guam.[16] In western Indonesia, Austronesian settlers admixed with people from the Austroasiatic-related settlement stream with Neolithic Mainland Southeast Asian ancestry,[5] while in eastern Indonesia and Oceania, all Austronesian-speaking groups have Papuan-related geneflow at various levels.[15][13][16] Later migrations of Austronesian speakers brought ASEA ancestry as far as to Madagascar and eastern Polynesia.[6] The Austronesian-affilated ancestry is best described as a sister lineage to Late Neolithic groups in Fujian (Fujian_LN), which points to a geographic origin of proto-Austronesians somewhere along the southern coast of China and Taiwan. Yet, the specific population of proto-Austronesian speakers, that is directly ancestral to later Austronesian groups, has not been found.[11]

Extant
Extinct
Reconstructed
Hypothetical
Grammar
Other
Mainstream
Alternative and fringe
Pontic Steppe
Caucasus
East Asia
Eastern Europe
Northern Europe
Bronze Age
Pontic Steppe
Northern/Eastern Steppe
Europe
South Asia
Iron Age
Steppe
Europe
Caucasus
Central Asia
India
Iron Age
Indo-Aryans
Iranians
Nuristanis
East Asia
Europe
Middle Ages
East Asia
Europe
Indo-Aryan
Iranian
Historical
Indo-Aryan
Iranian
Others
European
Practices
Institutes
Publications
The Proto-Indo-European homeland was the prehistoric homeland of the Proto-Indo-European language (PIE), meaning it was the region where the proto-language was spoken before it split into the dialects from which the earliest Indo-European language later evolved.
The most widely accepted proposal about the location of the Proto-Indo-European homeland is called the steppe hypothesis.[note 1] it puts the archaic, early, and late PIE homeland in the Pontic–Caspian steppe around 4000 BCE.[1][2][3][4][5] A notable second possibility, which has gained renewed attention during the 2010s and 2020s due to aDNA research, is the Armenian hypothesis, which situates the homeland for archaic PIE ('Indo-Hittite') south of the Caucasus mountains.[6][7][8][9][10][11] A third contender is the Anatolian hypothesis, which puts it in Anatolia c. 8000 BCE.[1][12][13][14]  Several other explanations have been proposed, including the outdated but historically prominent North European hypothesis, the Neolithic creolisation hypothesis, the Paleolithic continuity paradigm, the Arctic theory, and the "indigenous Aryans" (or "out of India") hypothesis. These are not widely accepted, and are considered to be fringe theories.[15][2][16]
The search for the homeland of the Indo-Europeans began during the late 18th century with the discovery of the Indo-European language family.[17] The methods used to establish the homeland have been drawn from the disciplines of historical linguistics, archaeology, physical anthropology and, more recently, human population genetics.
The steppe model, the Anatolian model, and the Near Eastern (or Armenian) model are the three main solutions for the Indo-European homeland.[note 2] The steppe model, placing the Proto-Indo-European (PIE) homeland in the Pontic-Caspian steppe about 4000 BCE,[5] is the theory supported by most scholars.[note 1]
According to linguist Allan R. Bomhard (2019), the steppe hypothesis proposed by archeologists Marija Gimbutas and David W. Anthony "is supported not only by linguistic evidence, but also by a growing body of archeological and genetic evidence. The Indo-Europeans have been identified with several cultural complexes existing in that area between 4500 and 3500 BCE. The literature supporting such a homeland is both extensive and persuasive [...]. Consequently, other scenarios regarding the possible Indo-European homeland, such as Anatolia, have now been mostly abandoned,"[18] although critical issues such as the way the proto-Greek,[19] proto-Armenian,[20][21] proto-Albanian,[citation needed] proto-Celtic,[22] and proto-Anatolian[23] languages became spoken in their attested homeland are still debated within the context of the steppe model.[24]
A notable second possibility, which has gained renewed attention since the 2010s, is the "Near Eastern model",[24] also known as the Armenian hypothesis. It was proposed by linguists Tamaz V. Gamkrelidze and Vyacheslav Ivanov in the early 1980s, postulating relationships between Indo-European and Caucasian languages based on the disputed glottalic theory and related to archaeological findings by Grogoriev.[24] Some recent DNA research has resulted in renewed suggestions of the possibility of a Caucasian or northwest Iranian homeland for archaic or 'proto-proto-Indo-European' (also termed 'Indo-Anatolian' or 'Indo-Hittite' in the literature),[25][note 3] the common ancestor of both Anatolian languages and early proto-IE (from which Tocharian and all other early branches divided).[5][7][26][27][10][note 4] These suggestions are disputed in other recent publications, which still locate the origin of the ancestor of proto-Indo-European in the Eastern European/Eurasian steppe[28][29][30] or from a hybridization of both steppe and Northwest-Caucasian languages,[30][note 5][note 6] while "[a]mong comparative linguists, a Balkan route for the introduction of Anatolian IE is generally considered more likely than a passage through the Caucasus, due, for example, to greater Anatolian IE presence and language diversity in the west."[26]
The Anatolian hypothesis proposed by archeologist Colin Renfrew places the pre-PIE homeland in Anatolia about 8000 BCE,[13] and the homeland of Proto-Indo-European proper in the Balkans around 5000 BCE, with waves of linguistic expansion following the progression of agriculture in Europe. Although it has attracted substantive attention and discussions, the datings it proposes are at odds with the linguistic timeframe for Proto-Indo-European,[2] and with genetic data which do not find evidence for Anatolian origins in the Indian gene pool.[31]
Apart from DNA evidence (see below), Anthony and Ringe (2015) give a number of arguments against the Anatolian hypothesis.[32] First, cognate words for "axle", "wheel", "wagon-pole", and "convey by vehicle" can be found in a number of Indo-European languages ranging from Irish to Tocharian, but not Anatolian. This suggests that Proto-European speakers, after the split with Anatolian, had wheeled vehicles, which the neolithic farmers did not. For various reasons, such as the regular sound-changes which the words exhibit, the suggestion that the words might have spread later by borrowing or have been introduced by parallel innovation in the different branches of Indo-European can be ruled out. Secondly, the words borrowed at an early date by Proto-Uralic, as well as those borrowed from Caucasian languages, indicate a homeland geographically between the Caucasus and the Urals. Thirdly, if the Indo-European languages had spread westwards from Anatolia, it might be expected that Greek would be closest to Anatolian, whereas in fact it is much closer to Indo-Aryan. In addition, the culture described in early poems such as Homer's – praise of warriors, feasting, reciprocal guest-friendship, and so on – more closely match what is known of the burial practices of the steppe peoples than the neolithic farmers.
The most recent DNA findings from ancient bones as well as modern people show that farmers whose ancestors originated in Anatolia did indeed spread across Europe from 6500 BCE onwards, eventually mixing with the existing hunter-gatherer population. However, about 2500 BCE, a massive influx of pastoralists from the steppe north of the Black Sea, associated with Corded Ware culture, spread from the east. Northern Europeans (especially Norwegians, Lithuanians, and Estonians) get nearly half their ancestry from this group; Spanish and Italians about a quarter, and Sardinians almost none. It is thought that this influx of pastoralists brought the Indo-European languages with them. Steppe ancestry is also found in the DNA of speakers of Indo-European languages in India, especially in the Y chromosome, which is inherited in the male line.[33]
In general, the prestige associated with a specific language or dialect and its progressive dominance over others can be explained by the access to a natural resource unknown or unexploited until then by its speakers, which is thought to be horse-based pastoralism for Indo-European speakers rather than crop cultivation.[note 7][34][18]
A number of other theories have been proposed, most of which have little or no academic credence presently (see discussion below):
Traditionally, homelands of linguistic families are proposed based on evidence from comparative linguistics coupled with evidence of historical populations and migrations from archaeology. Presently, genetics via DNA samples is increasingly used for the study of ancient population movements.
Using comparative linguistics it is possible to reconstruct the vocabulary found in the proto-language, and in this way achieve some knowledge of the cultural, technological and ecological context that the speakers inhabited. Such a context can then be compared with archaeological evidence. This vocabulary includes, in the case of (late) PIE, which is based on the post-Anatolian and post-Tocharian IE-languages:
Zsolt Simon notes that, although it can be useful to determine the period when the Proto-Indo-European language was spoken, using the reconstructed vocabulary to locate the homeland may be flawed, since we do not know whether Proto-Indo-European speakers knew a specific concept because it was part of their environment or because they had heard of it from other peoples they were interacting with.[44]
Proto-Finno-Ugric and PIE have a lexicon in common, generally related to trade, such as words for "price" and "draw, lead". Similarly, "sell" and "wash" were borrowed in Proto-Ugric. Although some have proposed a common ancestor (the hypothetical Indo-Uralic language family), this is generally regarded as the result of intensive borrowing, which suggests that their homelands were located near each other. Proto-Indo-European also exhibits lexical loans to or from Caucasian languages, particularly Proto-Northwest Caucasian and Proto-Kartvelian, which suggests a location close to the Caucasus mountains.[18][4]
Gamkrelidze and Ivanov, using the now largely unsupported glottalic theory of Indo-European phonology, also proposed Semitic borrowings into Proto-Indo-European, suggesting a more southern homeland to explain these borrowings. According to Mallory and Adams, some of these borrowings may be too speculative or from a later date, but they consider the proposed Semitic loans *táwros 'bull' and *wéyh₁on- 'wine; vine' to be more likely.[43]
Anthony notes that the small number of Semitic loanwords in Proto-Indo-European that are generally accepted by linguists, such as words for bull and silver, could have been borrowed via trade and migration routes rather than through direct contact with the Semitic linguistic homeland.[45]
According to Anthony, the following terminology may be used:[2]
The Anatolian languages are the first Indo-European language family to have been separated from the main group. Due to the archaic elements preserved in the Anatolian languages, they may be a "cousin" of Proto-Indo-European, instead of a "child", but Anatolian is generally regarded as an early offshoot of the Indo-European language group.[2]
The Indo-Hittite hypothesis postulates a common predecessor for both the Anatolian languages and the other Indo-European languages, termed Indo-Hittite or Indo-Anatolian.[2] Although PIE had predecessors,[4] the Indo-Hittite hypothesis is not widely accepted, and there is little to suggest that it is possible to reconstruct a proto-Indo-Hittite stage that differs substantially from what is already reconstructed for PIE.[46]
Anthony (2019) suggests a derivation of the proto-Indo-European language mainly from a base of languages spoken by Eastern European Hunter-Gatherers living in the Volga steppes, with influences from languages spoken by northern Caucasus hunter-gatherers who migrated from the Caucasus to the lower Volga basin, in addition to a possible later and lesser influence from the language of the Maikop culture to the south (which is hypothesized to have belonged to the North Caucasian family) during the later Neolithic or Bronze Age involving little genetic effect.[28]
Lexico-statistical studies intended to show the relationship between the various branches of Indo-European languages began during the late 20th century with work by Dyen et al. (1992) and Ringe et al. (2002).[47] Subsequently, a number of authors performed a Bayesian phylogenetic analysis of the IE languages (a mathematical method used for evolutionary biology to establish relationships between species).[48] A secondary intent of these studies was to attempt to estimate the approximate dates at which the various branches separated from each other.
The earlier studies tended to estimate a relatively long time-frame for the development of the different branches. In particular the study by Bouckaert and colleagues (which included a geographical element) was "decisively" in favour of Anatolia as the geographical origin, and assisted Colin Renfrew's hypothesis that Indo-European spread from Anatolia along with agriculture from 7500 to 6000 BCE onwards. According to their analysis, the five major Indo-European subfamilies – Celtic, Germanic, Italic, Balto-Slavic and Indo-Iranian – all emerged as distinct lineages between 4000 and 2000 BCE. The authors stated that this time-scale is consistent with secondary movements such as the expansion of the steppe peoples after 3000 BCE, which they suggest also played a role in the spread of Indo-European languages.
The steppe hypothesis seeks to identify the source of the Indo-European language expansion as a succession of migrations from the Pontic–Caspian steppe between the 5th and 3rd millennia BCE.[49] During the early 1980s,[50] a mainstream consensus had emerged among Indo-Europeanists in favour of the "Kurgan hypothesis" (named after the kurgans, burial mounds, of the Eurasian steppes) placing the Indo-European homeland in the Pontic–Caspian steppe of the Chalcolithic.[51][2]
According to the Kurgan hypothesis as formulated by Gimbutas, Indo-European speaking nomads from Eastern Ukraine and Southern Russia expanded on horseback in several waves during the 3rd millennium BCE, invading and subjugating supposedly peaceful European Neolithic farmers of Gimbutas's Old Europe.[note 8] Later versions of Gimbutas's hypothesis increasingly emphasized the patriarchal and patrilineal nature of the invading culture, in contrast with the supposedly egalitarian and matrilineal culture of the invaded.
J. P. Mallory, dating the migrations to c. 4000 BCE, and having less insistence on their violent or quasi-military nature, essentially modified Gimbutas's theory making it compatible with a less gender-political narrative. David Anthony, emphasizing mostly the evidence for the domestication of horses and the presence of wheeled vehicles, came to regard specifically the Yamna culture, which replaced the Sredny Stog culture about 3500 BCE, as the most likely candidate for the Proto-Indo-European speech community.[2]
Anthony describes the spread of cattle-raising from early farmers in the Danube Valley into the Ukrainian steppes in the 6th–5th millennium BCE, forming a cultural border with the hunter-gatherers[2] whose languages may have included archaic PIE.[2] Anthony notes that domesticated cattle and sheep probably didn't enter the steppes from the Transcaucasia, since the early farming communities there were not widespread, and separated from the steppes by the glaciated Caucasus.[2] Subsequent cultures developed in this area which adopted cattle, most notably the Cucuteni-Trypillian culture.[2]
Asko Parpola regards the Cucuteni-Trypillian culture as the birthplace of wheeled vehicles, and therefore as the homeland for Late PIE, assuming that Early PIE was spoken by Skelya pastoralists (early Sredny Stog culture[2]) who took over the Tripillia culture at c. 4300–4000 BCE.[52] On its eastern border lay the Sredny Stog culture (4400–3400 BCE),[2] whose origins are related to "people from the east, perhaps from the Volga steppes".[2] It plays the main role in Gimbutas's Kurgan hypothesis,[2] and coincides with the spread of early PIE across the steppes[2] and into the Danube valley (c. 4000 BCE),[2] resulting in the end of Old Europe.[2] Hereafter the Maykop culture suddenly began, Tripillia towns grew strongly, and eastern steppe people migrated to the Altai mountains, founding the Afanasevo culture (3300 to 2500 BCE).[2]
The core element of the steppe hypothesis is the identification of the proto-Indo-European culture as a nomadic pastoralist society that did not practice intensive agriculture. This identification rests on the fact that vocabulary related to cows, to horses and horsemanship, and to wheeled vehicles can be reconstructed for all branches of the family, whereas only a few agricultural vocabulary items are reconstructable, suggesting a gradual adoption of agriculture through contact with non-Indo-Europeans. If this evidence and reasoning is accepted, the search for the Indo-European proto-culture has to involve searching for the earliest introduction of domesticated horses and wagons into Europe.[4]
Responding to these arguments, proponents of the Anatolian hypothesis Russell Gray and Quentin Atkinson have argued that the different branches could have independently developed similar vocabulary based on the same roots, creating the false appearance of shared inheritance – or alternatively, that the words related to wheeled vehicle might have been borrowed across Europe at a later date. Proponents of the Steppe hypothesis have argued this to be unlikely, and to violate the established principles for reasonable assumptions when explaining linguistic comparative data.[4]
Another source of evidence for the steppe hypothesis is the presence of what appears to be many shared loanwords between Uralic languages and proto-Indo-European, suggesting that these languages were spoken in adjacent areas. This would have had to occur much further north than the Anatolian or Near Eastern scenarios would allow.[4] According to Kortlandt, Indo-Uralic is the common ancestor of the Indo-European and Uralic language families.[53] Kortlandt argues that "Indo-European is a branch of Indo-Uralic which was radically transformed under the influence of a North Caucasian substratum when its speakers moved from the area north of the Caspian Sea to the area north of the Black Sea."[53][note 9][note 6] Anthony notes that the validity of such deep relationships cannot be reliably demonstrated due to the time-depth involved, and also notes that the similarities may be explained by borrowings from PIE into proto-Uralic.[4] Yet, Anthony also notes that the North Caucasian communities "were southern participants in the steppe world".[2]
Kloekhorst argues that the Anatolian languages have preserved archaisms which are also found in proto-Uralic, providing strong evidence for a steppe-origin of PIE.[54]
The subclade R1a1a (R-M17 or R-M198) is the R1a subclade associated most commonly with Indo-European speakers. In 2000, Ornella Semino et al. proposed a postglacial (Holocene) period spread of the R1a1a haplogroup from north of the Black Sea during the time of the Late Glacial Maximum, which was subsequently magnified by the expansion of the Kurgan culture into Europe and eastward.[55][obsolete source]
In 2015, a large-scale ancient DNA study by Haak et al. published in Nature found evidence of a "massive migration" from the Pontic-Caspian steppe to central Europe that occurred about 4,500 years ago.[5] It found that individuals from the central European Corded Ware culture (3rd millennium BCE) were closely related genetically to individuals from the Yamnaya culture.[5] The authors concluded that their "results provide support for the theory of a steppe origin of at least some of the Indo-European languages of Europe".[31][56]
Two other genetic studies in 2015 gave support to the steppe hypothesis regarding the Indo-European Urheimat. According to those studies, specific subclades of Y chromosome haplogroups R1b and R1a, which are found in Yamnaya and other proposed early Indo-European cultures such as Sredny Stog and Khvalynsk,[57][58] and are now the most common in Europe (R1a is also common in South Asia) would have expanded from the Ukrainian and Russian steppes, along with the Indo-European languages; these studies also detected an autosomal component present in modern Europeans that was not present in Neolithic Europeans, which would have been introduced with paternal lineages R1b and R1a, as well as Indo-European languages.[5][59][60]
However, the folk-migration model cannot be the only diffusion theory for all linguistic families, as the Yamnaya ancestry component is particularly concentrated in Europe in the northwestern parts of the continent. Other models for languages like Proto-Greek are still debated. The steppe genetic component is more diffuse in studied Mycenaean populations: if they came from elsewhere, Proto-Greek speakers were certainly a minority in a sea of populations that had been familiar with agriculture for 4,000 years.[19] Some propose that they gained progressive prominence through a cultural expansion by elite influence.[18] But if high correlations can be proven in ethnolinguistic or remote communities, genetics does not always equate with language,[61] and archaeologists have argued that although such a migration might have occurred it does not necessarily explain either the distribution of archaeological cultures or the spread of the Indo-European languages.[62]
Russian archaeologist Leo Klejn (2017) noted that in the Yamnaya population, R1b-L23 is predominant, whereas Corded Ware males belong mostly to R1a, as well as far-removed R1b clades not found in Yamnaya. In his opinion, this does not support a Yamnaya origin for the Corded Ware culture.[63] British archaeologist Barry Cunliffe describes this inconsistency as "disconcerting for the model as a whole".[64] Klejn has also suggested that the autosomal evidence does not support a proposed Yamnaya migration, as Western Steppe Herder ancestry is lesser in the area from which the Yamnaya were proposed to have expanded, in both contemporary populations and Bronze Age specimens.[65]
Furthermore, Balanovsy et al.[66] (2017) found that the majority of the Yamnaya genomes studied by Haak and Mathieson belonged to the "eastern" R-GG400 subclade of R1b-L23, which is not common in western Europe, and none belonged to the "western" R1b-L51 branch. The authors conclude that the Yamnaya could not have been an important source of modern western European male haplogroups.
An analysis by David Anthony (2019) suggested a genetic origin of Proto-Indo-Europeans (associated with the Yamnaya culture) in the Eastern European steppe north of the Caucasus, deriving from a mixture of Eastern European hunter-gatherers (EHG) and hunter-gatherers from the Caucasus (CHG). Anthony also suggested that the Proto-Indo-European language formed mainly from a base of languages spoken by Eastern European hunter-gathers with influences from languages of northern Caucasus hunter-gatherers, in addition to a possible later and more minor influence from the language of the Maykop culture to the south (which is hypothesized to have belonged to the North Caucasian languages) during the later Neolithic or Bronze Age, involving little genetic effect.[28]
In 2020, David Anthony offered a new hypothesis, with the intent of resolving the questions concerning the apparent absence of haplogroup R1a in Yamnaya. He speculates that haplogroup R1a must have been present in the Yamnaya, but that it was initially extremely rare, and that the Corded Ware culture are the descendants of this wayward population that migrated north from the Pontic steppe and greatly expanded in size and influence, later returning to dominate the Pontic-Caspian steppe.[67]
The main competitor of the Kurgan hypothesis is the Anatolian hypothesis advanced by Colin Renfrew in 1987. It couples the spread of the Indo-European languages to the demonstrated fact of the Neolithic spread of farming from the Near East, stating that the Indo-European languages began to spread peacefully into Europe from Asia Minor from around 7000 BCE with the Neolithic advance of farming (wave of advance). The expansion of agriculture from the Middle East would have diffused three language families: Indo-European toward Europe, Dravidian toward Pakistan and India, and Afro-Asiatic toward Arabia and North Africa.
According to Renfrew (2004) harvp error: no target: CITEREFRenfrew2004 (help)[full citation needed], the spread of Indo-European proceeded in the following phases:[citation needed]
Reacting to criticism, Renfrew revised his proposal to the effect of taking a pronounced Indo-Hittite position. Renfrew's revised opinion places only Pre-Proto-Indo-European in 7th millennium BCE Anatolia, proposing as the homeland of Proto-Indo-European proper the Balkans about 5000 BCE, explicitly identified as the "Old European culture" proposed by Marija Gimbutas. He thus still situates the original source of the Indo-European language family in Anatolia c. 7000 BCE. Reconstructions of a Bronze Age PIE society based on vocabulary items like "wheel" do not necessarily hold for the Anatolian branch, which appears to have separated from PIE at an early stage, prior to the invention of wheeled vehicles.[68]
After the publication of several studies on ancient DNA in 2015, Colin Renfrew has accepted the reality of migrations of populations speaking one or several Indo-European languages from the Pontic steppe towards Northwestern Europe.[69][34]
The main objection to this theory is that it requires an unrealistically early date.[4] According to linguistic analysis, the Proto-Indo-European lexicon seems to include words for a range of inventions and practices related to the Secondary Products Revolution, which post-dates the early spread of farming. On lexico-cultural dating, Proto-Indo-European cannot be earlier than 4000 BCE.[70] Furthermore, it has been objected, on impressionistic grounds, that it seems unlikely that close equivalences such as Hittite [eːsmi, eːsi, eːst͜si] = Sanskrit [ásmi, ási, ásti] ("I am, you are, he is") could have survived over such a long timescale as the Anatolian hypothesis requires.[71]
The idea that farming was spread from Anatolia in a single wave has been revised. Instead, it appears to have spread in several waves by several routes, primarily from the Levant.[72] The trail of plant domesticates indicates an initial foray from the Levant by sea.[73] The overland route via Anatolia seems to have been most significant in spreading farming into south-east Europe.[74]
According to Lazaridis et al. (2016), farming developed independently both in the Levant and in the eastern Fertile Crescent.[31] After this initial development, the two regions and the Caucasus interacted, and the chalcolithic north-west Iranian population appears to be a mixture of Iranian Neolithic, Levant, and Caucasus hunter-gatherers.[31] According to Lazaridis et al. (2016), "farmers related to those from Iran spread northward into the Eurasian steppe; and people related to both the early farmers of Iran and to the pastoralists of the Eurasian steppe spread eastward into South Asia".[75] They further note that ANI (Ancestral North Indian) "can be modelled as a mix of ancestry related to both early farmers of western Iran and to people of the Bronze Age Eurasian steppe",[75] which makes it unlikely that the Indo-European languages in India are derived from Anatolia.[31]
According to Alberto Piazza "[i]t is clear that, genetically speaking, peoples of the Kurgan steppe descended at least in part from people of the Middle Eastern Neolithic who immigrated there from Anatolia."[76] According to Piazza and Cavalli-Sforza, the Yamna culture may have been derived from Middle Eastern Neolithic farmers who migrated to the Pontic steppe and developed pastoral nomadism:
... if the expansions began at 9,500 years ago from Anatolia and at 6,000 years ago from the Yamnaya culture region, then a 3,500-year period elapsed during their migration to the Volga-Don region from Anatolia, probably through the Balkans. There a completely new, mostly pastoral culture developed under the stimulus of an environment unfavorable to standard agriculture, but offering new attractive possibilities. Our hypothesis is, therefore, that Indo-European languages derived from a secondary expansion from the Yamnaya culture region after the Neolithic farmers, possibly coming from Anatolia and settled there, developing pastoral nomadism.[77]
Wells agrees with Cavalli-Sforza that there is "some genetic evidence for migration from the Middle East":
... while we see substantial genetic and archaeological evidence for an Indo-European migration originating in the southern Russian steppes, there is little evidence for a similarly massive Indo-European migration from the Middle East to Europe. One possibility is that, as a much earlier migration (8,000 years old, as opposed to 4,000), the genetic signals carried by Indo-European-speaking farmers may simply have dispersed over the years. There is clearly some genetic evidence for migration from the Middle East, as Cavalli-Sforza and his colleagues showed, but the signal is not strong enough for us to trace the distribution of Neolithic languages throughout the entirety of Indo-European-speaking Europe.[78]
Varying ideas have been proposed regarding the location of archaic PIE, including the Eurasian/Eastern European steppe, the Caucasus to the south, or a mixed origin derived from both regions.
Gamkrelidze and Ivanov claimed that the Urheimat was south of the Caucasus, specifically, "within eastern Anatolia, the southern Caucasus and northern Mesopotamia" during the 5th to 4th millennia BCE.[79] Their proposal was based on a disputed theory of glottal consonants in PIE. According to Gamkrelidze and Ivanov, PIE words for material culture objects imply contact with more advanced peoples to the south, the existence of Semitic loan-words in PIE, Kartvelian borrowings from PIE, some contact with Sumerian, Elamite and others. However, given that the glottalic theory never became accepted very strongly and there was little archaeological evidence, the Gamkrelidze and Ivanov theory did not gain credence until Renfrew's Anatolian theory revived aspects of their proposal.[4]
Gamkrelidze and Ivanov proposed that the Greeks moved west across Anatolia to their present location, a northward movement of some IE speakers that brought them into contact with the Finno-Ugric languages, and suggested that the Kurgan area, or better "Black Sea and Volga steppe", was a secondary homeland from which the western IE languages emerged.[80]
Recent DNA research which shows that the steppe-people derived from a mix of Eastern Hunter-Gatherers (EHG) and Caucasus Hunter-Gatherers,[note 10][note 11] has resulted in renewed suggestions of the possibility of a Caucasian, or even Iranian, homeland for an archaic proto-Indo-European, the common ancestor of both Anatolian languages and all other Indo-European languages.[83][note 3] It is argued that this may support the Indo-Hittite hypothesis, according to which both proto-Anatolian and proto-Indo-European separated from a common language "no later than the 4th millennium BCE."[23] Suggestions in this regard have been made by Haak et al. (2015, p. 138, Supplementary Information), Reich (2018, p. 120), Damgaard (2018, p. 7), Wang et al. (2019, p. 19), Grolle (2018, p. 108), Krause & Trappe (2021, pp. 122, 186), Lazaridis et al. (2022); see also 
[note 4]
Damgaard et al. (2018) found that sampled Copper Age and Bronze Age Anatolians all had similar levels of CHG ancestry, but no EHG ancestry. They conclude that Early and Middle Bronze Age Anatolia did not receive ancestry from steppe populations, indicating that Indo-European language spread into Anatolia was not associated with large migrations from the steppe. The authors assert that their data is consistent with a scenario in which Indo-European languages were introduced to Anatolia in association with CHG admixture before c. 3700 BCE, in contrast to the standard steppe model, and despite the association of CHG ancestry with several non-Indo-European languages. Nevertheless, a second possibility, that Indo-European languages came to Anatolia along with small scale population movements and commerce, is described by them as also consistent with the data. They note that "Among comparative linguists, a Balkan route for the introduction of Anatolian IE is generally considered more likely than a passage through the Caucasus, due, for example, to greater Anatolian IE presence and language diversity in the west."[8][note 12]
Wang et al. (2019) note that the Caucasus and the steppes were genetically separated in the 4th millennium BCE,[87] but that the Caucasus served as a corridor for gene flow between cultures south of the Caucasus and the Maykop culture during the Copper and the Bronze Age, speculating that this "opens up the possibility of a homeland of PIE south of the Caucasus,"[88] which "could offer a parsimonious explanation for an early branching off of Anatolian languages, as shown on many PIE tree topologies."[88] According to Wang et al. (2019), the typical steppe-ancestry, as an even mix between EHG and CHG, may result from "an existing natural genetic gradient running from EHG far to the north to CHG/Iran in the south," or it may be explained as "the result of Iranian/CHG-related ancestry reaching the steppe zone independently and prior to a stream of AF [Anatolian Farmer] ancestry."[note 11] Wang et al. argue that evidence for gene flow to the steppe allows for a possible Indo-European homeland south of the Caucasus mountains. According to this model, Indo-European languages could have been brought north together with CHG ancestry, a scenario which could also explain the early separation from Anatolian. They note that "the spread of some or all of the PIE branches would have been possible via the North Pontic/Caucasus region and from there, along with pastoralist expansions, to the heart of Europe."[93] However, Wang et al. also acknowledge that "the spread of some or all of the PIE branches would have been possible via the North Pontic/Caucasus region," as explained in the steppe hypothesis.[88][note 13]
Lazaridis et al. (2022) state that the genetic evidence is consistent with an origin of Proto-Indo-European either in the EHGs of the steppe, or in the south (the southern arc), but argue that their evidence points to the latter. They argue that genetic evidence from the 'Southern Arc', an area which includes Anatolia, North Mesopotamia, Western Iran, Armenia, Azerbaijan, and the Caucasus, allows the possibility of a West Asian homeland for the Proto-Indo-European language.[note 14] In this opinion, Proto-Indo-European emerged in the southern arc, and was brought to Anatolia when Caucasus/Levantine-related ancestry flowed into Anatolia after the Neolithic, separating the Proto-Anatolian language from the rest of the Indo-European languages. They propose that subsequent migrations from the southern arc brought Proto-Indo-European to the steppes.[note 15] According to Lazaridis et al., the spread of all other (non-Anatolian) ancient Indo-European languages is associated with the migrations of Yamnaya pastoralists or genetically related populations. The study argues that Anatolian languages cannot be linked to steppe migrations due to the absence of EHG ancestry in ancient Anatolians, despite what the study describes as extensive sampling, including  possible entry points into Anatolia by land or sea. The authors caution that they cannot yet identify the ultimate sources of population movements from the Southern Arc without further sampling of the possible source populations.[94]
Bomhard's Caucasian substrate hypothesis (2017, 2019) proposes an origin (Urheimat) in a Central Asian or North Caspian region of the steppe for Indo-Uralic (a proposed common ancestor of Indo-European and Uralic).[95][96] Bomhard elaborates on Johanna Nichols "Sogdiana hypothesis", and Kortlandt's ideas of an Indo-Uralic proto-language, proposing an Urheimat north or east of the Caspian Sea, of a Eurasiatic language which was imposed on a population which spoke a Northwest Caucasian language, with this mixture producing proto-Indo-European.[96][95][note 5][note 6]
Indo-European specialist and anthropologist David Anthony (2019) criticizes the Southern/Caucasian homeland hypothesis (including the suggestions of those such as Reich, Kristiansen, and Wang).[28][29] Instead, Anthony argues that the roots of the proto-Indo-European language formed mainly from a base of languages spoken by Eastern European hunter-gatherers, with some influences from the languages of Caucasus hunter-gatherers. Anthony rejects the possibility that the Bronze Age Maykop people of the Caucasus were a southern source of language and genetics of Indo-European.[28][29] Referring to Wang et al. (2019), he notes that the Anatolian Farmer component in the Yamnaya-ancestry came from European farmers, not from the Maykop, which had too much Anatolian farmer ancestry to be ancestral to the Yamnaya-population.[107] Anthony also notes that the paternal lineages of the Yamnaya, which were rich in R1b, were related to those of earlier Eastern European hunter-gatherers, rather than those of southern or Caucasus peoples such as the Maykop.[108] Anthony rejects the possibility that the Bronze Age Maykop people of the Caucasus were a southern source of language and genetics of Indo-European. According to Anthony, referring to Wang et al. (2019),[note 16] the Maykop culture had little genetic effect on the Yamnaya, whose paternal lineages were found to differ from those found in Maykop remains, but were instead related to those of earlier Eastern European hunter-gatherers. Also, the Maykop (and other contemporary Caucasus samples), along with CHG from this date, had significant Anatolian Farmer ancestry "which had spread into the Caucasus from the west after about 5000 BC", while the Yamnaya had a lower percentage which does not fit with a Maykop origin. Partly for these reasons, Anthony concludes that Bronze Age Caucasus groups such as the Maykop "played only a minor role, if any, in the formation of Yamnaya ancestry." According to Anthony, the roots of Proto-Indo-European (archaic or proto-proto-Indo-European) were mainly in the steppe rather than the south. Anthony considers it likely that the Maykop spoke a Northern Caucasian language not ancestral to Indo-European.[28][29][28]
Anthony proposes that the Yamnaya derived mainly from Eastern European hunter-gatherers (EHG) from the steppes, and undiluted Caucasus hunter-gatherers (CHG) from northwestern Iran or Azerbaijan, similar to the Hotu cave population, who mixed in the Eastern European steppe north of the Caucasus. According to Anthony, hunting-fishing camps from the lower Volga, dated 6200–4500 BCE, could be the remains of people who contributed the CHG-component, migrating westwards along the coast of the Caspian Sea, from an area south-east of the Caspian Sea. They mixed with EHG-people from the north Volga steppes, and the resulting culture contributed to the Sredny Stog culture, a predecessor of the Yamnaya culture.[28]
Anthony (2024), addressing Lazaridis (2022), differentiates Early PIE (EPIE), prior to the Anatolian separation, from Late PIE (LPIE), also known as Core or Nuclear PIE, the ancestor of all other IE branches and evidencing the hypothesis that the LPIE dialects were spoken in the Pontic-Caspian steppes 3500-2500 BCE. 
He states that a homeland for early PIE in the Caucasus or the Pontic-Caspian steppe are both possibilities but that the second is the position supported. He also argues for the possibility of a steppe origin for the Anatolian branch, proposing that, "the Anatolian split could have been caused by a migration from the steppes into the Balkans associated with the Csongrad grave...and other Eneolithic steppe derived graves in the lower Danube valley", and that, in that area, steppe autosomal DNA could have been " lost a millennium later through local admixture before they moved to Anatolia", accounting for its absence in  Anatolia (citing a similar case in Armenia).[109]
Lothar Kilian and Marek Zvelebil have proposed a 6th millennium BCE or later origin of the IE-languages in Northern Europe, as a creolisation of migrating Neolithic farmers settling in northern Europe, and mixing with indigenous Mesolithic hunter-gatherer communities.[36] The steppe theory is compatible with the argument that the PIE homeland must have been larger,[49] because the "Neolithic creolisation hypothesis" allows the Pontic-Caspian region to have been part of PIE territory.
The Paleolithic continuity theory (also labeled "Paleolithic Continuity Paradigm" by Mario Alinei, its main proponent) is a hypothesis suggesting that the Proto-Indo-European language (PIE) can be traced back to the Upper Paleolithic, several millennia earlier than the Chalcolithic or at the most Neolithic estimates in other scenarios of Proto-Indo-European origins. Its claims are linguistically very improbable and depend on the assumption that there is no genetic and archaeological evidence for major population turnovers in Europe since the Last Glacial Maximum.[110]
It was not listed by Mallory in 1997 among the proposals for the origins of the Indo-European languages that are widely discussed and considered credible by academia.[111]
Soviet Indologist Natalia R. Guseva[112] and Soviet ethnographer S. V. Zharnikova,[113] influenced by Bal Gangadhar Tilak's 1903 work The Arctic Home in the Vedas, argued for a northern Urals Arctic homeland of the Indo-Aryan and Slavic people;[114] their ideas were popularized by Russian nationalists.[115]
The Indigenous Aryans theory, also known as the "out of India" theory, proposes an Indian origin for the Indo-European languages. The languages of northern India and Pakistan, including Hindi and the historically and culturally significant liturgical language Sanskrit, belong to the Indo-Aryan branch of the Indo-European language family.[116] The Steppe model, rhetorically presented as an "Aryan invasion", has been opposed by Hindu revivalists and Hindu nationalists,[117][118] who argue that the Aryans were indigenous to India, and some, such as B. B. Lal,[119] Koenraad Elst[120][121] and Shrikant Talageri,[122] have proposed that Proto-Indo-European itself originated in northern India, either with or shortly before the Indus Valley civilisation.[118][123] This "out of India" theory is not regarded as plausible by mainstream scholarship.[123][124][125]

Phasianus gallus Linnaeus, 1758
The red junglefowl (Gallus gallus), also known as the Indian red junglefowl (and formerly the bankiva or bankiva-fowl), is a species of tropical, predominantly terrestrial bird in the fowl and pheasant family, Phasianidae, found across much of Southeast and parts of South Asia. The red junglefowl was the primary species to give rise to today's many breeds of domesticated chicken (G. g. domesticus); additionally, the related grey junglefowl (G. sonneratii), Sri Lankan junglefowl (G. lafayettii) and the Javanese green junglefowl (G. varius) have also contributed genetic material to the gene pool of the modern chicken.[2][3]
Molecular evidence, derived from whole-genome sequencing, has revealed that the chicken was first domesticated from red junglefowl ca. 8,000 years ago,[2] with this domestication-event involving multiple maternal origins.[2][4] Since then, the domestic form has spread around the world, and they are bred by humans in their millions for meat, eggs, colourful plumage and companionship.[5] Outside of their native range, mainly in the Americas and Europe, the wild form of Gallus gallus is sometimes used in zoos, parks or botanical gardens as a free-ranging form of beneficial "pest control", similarly to—and often kept with—the Indian blue peafowl (Pavo cristatus) or the helmeted guineafowl (Numida meleagris); hybridisation has been documented between Gallus and Numida.[6]
The red junglefowl was formally described in 1758 by the Swedish naturalist Carl Linnaeus in the tenth edition of his Systema Naturae under the binomial name Phasianus gallus.[7] Linnaeus specified the type locality as "India orientali" but this has been restricted to the island of Pulo Condor Côn Đảo off the coast of Vietnam.[8] The red junglefowl is now one of the four species placed in the genus Gallus that was introduced in 1760 by Mathurin Jacques Brisson.[9] The word gallu is Latin for a farmyard cockerel.[10]
Five subspecies are recognised:[9]
Green junglefowl–Gallus varius (Shaw, 1798)
Red junglefowl–Gallus gallus (Linnaeus, 1758)
Sri Lankan junglefowl–Gallus lafayettii (Lesson, 1831)
Grey junglefowl–Gallus sonneratii (Temminck, 1813)
The nominate race of red junglefowl has a mix of feather colours, with orange, brown, red, gold, grey, white, olive, and even metallic green plumage. The tail of the male rooster can grow up to 28 centimetres (11 in), and the whole bird may be as long as 70 centimetres (28 in). There are 14 tail feathers. A moult in June changes the bird's plumage to an eclipse pattern, which lasts through October. The male eclipse pattern includes a black feather in the middle of the back and small red-orange plumes spread across the body. Female eclipse plumage is generally indistinguishable from the plumage at other seasons, but the moulting schedule is the same as that of males.[12]
Compared to the more familiar domestic chicken, the red junglefowl has a much smaller body mass (around 2+1⁄4 lbs (1 kg) in females and 3+1⁄4 lbs (1.5 kg) in males) and is brighter in coloration.[12] Junglefowl are also behaviourally different from domestic chickens, being naturally very shy of humans compared to the much tamer domesticated subspecies.
Male junglefowl are significantly larger than females and have brightly coloured decorative feathers. The male's tail is composed of long, arching feathers that initially look black, but shimmer with blue, purple, and green in direct light. He also has long, golden hackle feathers on his neck and his back. The female's plumage is typical of this family of birds in being cryptic and adapted for camouflage. She alone looks after the eggs and chicks. She also has a very small comb and wattles (fleshy ornaments on the head that signal good health to rivals and potential mates) compared to the males.
During their mating season, the male birds announce their presence with the well-known "cock-a-doodle-doo" call or crowing.[13] Within flocks, only dominant males crow.[14] Male red junglefowl have a shorter crowing sound than domestic roosters; the call cuts off abruptly at the end.[13] This serves both to attract potential mates and to make other male birds in the area aware of the risk of fighting a breeding competitor. A spur on the lower leg just behind and above the foot serves in such fighting. Their call structure is complex and they have distinctive alarm calls for aerial and ground predators to which others react appropriately.[15][16]
G. gallus has three transferrins, all of which cluster closely with other vertebrates' orthologs.[17]
The range of the wild form stretches from Pakistan,[18][19] India, Nepal and Bangladesh in the west, and eastwards across southern China, to Indochina; south/southeast into Malaysia, Singapore, the Philippines, and Indonesia. Junglefowl/Chickens were one of three main animals (along with domesticated pigs and dogs) carried by early Austronesian peoples from Island Southeast Asia in their voyages to the islands of Oceania in prehistory, starting around 5,000 years BP [citation needed]. Today, their modern descendants are found throughout Micronesia, Melanesia, and Polynesia.[20]
Red junglefowl prefer disturbed habitats and edges, both natural and human-created. The forage[14][21][22] and thick cover in these sorts of areas are attractive to junglefowl, especially nesting females.[23] Junglefowl use both deforested and regenerating forests,[24] and often are found near human settlements[25] or areas of regrowth from slash-and-burn cultivation.[14] Areas burned to promote bamboo growth also attract junglefowl, with edible bamboo seeds more available.[22][23] In some areas, red junglefowl are absent from silvicultural[24] and rubber[26] plantations; elsewhere, they will occur in both tea plants and palm oil plantations.[26] In the state of Selangor, Malaysia, palm foliage provides suitable cover; palm nut fruit provides adequate food, as well as insects (and their larvae) within, and adjacent to, the trees.[27]  The palms also offer an array of roost sites, from the low perches (~4 m) favored by females with chicks to the higher perches (up to 12 m) used by other adults.[28]
Red junglefowl drink surface water when it is available, but they do not require it.  Birds in North-Central India visit water holes frequently during the dry season, although not all junglefowl on the subcontinent live close enough to water to do so;[23] population densities may thus be lower, where surface water is limited.[22]
Red junglefowl regularly bathe in dust to keep the right balance of oil in their plumage. The dust absorbs extra oil and subsequently falls off.[29]
Flight in these birds is almost purely confined to reaching their roosting areas at sunset in trees or any other high and relatively safe places free from ground predators, and for escape from immediate danger through the day.[30]
Dominant male junglefowl appear to defend a territory against other dominant males, and the size of the territories has been inferred based on the proximity of roosts. Beebe[25] concluded that territories were rather small, especially as compared to some of the pheasants with which he was familiar. This was supported by Collias and Collias,[23] who reported that adjacent roost sites can be as close as 100 metres (330 ft). Within flocks, male red junglefowl exhibit dominance hierarchies, and dominant males tend to have larger combs than subordinate males.[31] Red junglefowl typically live in flocks of one to a few males and several females. Males are more likely to occur alone than females.[14][22][23][32][33][34]
Males make a food-related display called "tidbitting", performed upon finding food in the presence of a female.[35] The display is composed of coaxing, cluck-like calls, and eye-catching bobbing and twitching motions of the head and neck. During the performance, the male repeatedly picks up and drops the food item with his beak. The display usually ends when the hen takes the food item either from the ground or directly from the male's beak. Eventually, they sometimes mate.[36]
In many areas, red junglefowl breed during the dry portion of the year, typically winter or spring. This is true in parts of India, Nepal, Thailand, Vietnam, and Laos.[14][22][23][32][33][34] However, year-round breeding by red junglefowl has been documented in palm oil plantations in Malaysia[27] and also may occur elsewhere.[33] During the laying period, red junglefowl females lay an egg every day. Eggs take 21 days to develop. Chicks fledge in about 4 to 5 weeks, and at 12 weeks old they are chased out of the group by their mother — at which point they start a new group or join an existing one. Sexual maturity is reached at 5 months, with females taking slightly longer than males to reach maturity.[12]
Dominant males attempt to maintain exclusive reproductive access to females, though females choose to mate with subordinate males about 40% of the time in a free-ranging feral flock in San Diego, California.[37][38]
Red junglefowl are attracted to areas with ripe fruit or seeds,[23] including fruit plantations,[26] fields of domestic grain,[25] and stands of bamboo.[14] Although junglefowl typically eat fallen fruits and seeds on the ground, they occasionally forage in trees by perching on branches and feeding on hanging fruit.[14] Fruits and seeds of scores of plant species have been identified from junglefowl crops, along with grasses, leaves, roots, and tubers.[14][39] In addition, red junglefowl capture a wide variety of arthropods, other invertebrates, and vertebrates such as small lizards. Even mammalian faeces may be consumed.[14] Many of these items are taken opportunistically as the birds forage, although some arthropods, such as termites, are taken in large quantities; about 1,000 individual termites have been found in a single crop.[14][23] Plant materials constitute a higher proportion of the diet of adult red junglefowl than do arthropods and other animals. In contrast, chicks eat mostly adult and larval insects, earthworms, and only occasional plant material.[14]
Chickens were created when red junglefowl were domesticated for human use around 8,000 years ago[2] as subspecies Gallus gallus domesticus. They are now a major source of food for humans. However, undomesticated red junglefowl still represent an important source of meat and eggs in their endemic range. The undomesticated form is sometimes used in cock-fighting.[12]
In 2012, a study examined mitochondrial DNA recovered from ancient bones from Europe, Thailand, the Pacific, and Chile, and from Spanish colonial sites in Florida and the Dominican Republic, in directly dated samples originating in Europe at 1,000 BP and in the Pacific at 3,000 BP. The chicken was primarily domesticated from red junglefowl, with subsequent genetic contributions from grey junglefowl, Sri Lankan junglefowl, and green junglefowl.[2] Domestication occurred about 8,000 years ago, as based on molecular evidence[2] from a common ancestor flock in the bird's natural range, and then proceeded in waves both east and west.[40][41] Zoogeography and evolutionary biology point to the original domestication site of chickens as somewhere in Mainland Southeast Asia and southern China in the Neolithic. Chickens were one of the ancestral domesticated animals of the Austronesian peoples. They were transported to Taiwan and the Philippines around 5,500 to 4,500 years ago. From there, they spread outwards with the Austronesian migrations to the rest of Island Southeast Asia, Micronesia, Island Melanesia, and Polynesia in prehistoric times.[42]
Other archaeological evidence suggests domestication dates around 7,400 BP from the Chishan site, in the Hebei province of China. However, the domestication event in China has now been disputed by several studies citing unfavourable weather conditions at the time.[40][41] In the Ganges region of India, wild red junglefowl were being used by humans as early as 7,000 years ago. No domestic chicken remains older than 4,000 years have been identified in the Indus Valley, and the antiquity of chickens recovered from excavations at Mohenjodaro is still debated.[5]
The other three members of the genus — Sri Lanka junglefowl (G. lafayetii), grey junglefowl (G. sonneratii), and  green junglefowl (G. varius) — do not usually produce fertile hybrids with the red junglefowl. However, supporting the hypothesis of a hybrid origin, research published in 2008 found that the gene responsible for the yellow skin of the domestic chicken most likely originated in the closely related grey junglefowl and not from the red junglefowl.[3] Similarly, a 2020 study that analysed the whole genomes of Sri Lanka junglefowl, grey junglefowl, and green junglefowl found strong introgressive hybridisation events in different populations of indigenous village chickens.[2] The study also shows that 71–79% of red junglefowl DNA is shared with the domestic chicken.[2] A culturally significant hybrid between the red junglefowl and the green junglefowl in Indonesia is known as the bekisar.
Wild-type red junglefowl are thought to be facing threats due to hybridisation at forest edges, where domesticated free-ranging chickens are common.[43][44][45][46][47][48][49][50][51][52][53] The red junglefowl is considered near threatened in Singapore.[54] Nevertheless, they are classified by the IUCN as a species of least concern.

Glynn Llywelyn Isaac (19 November 1937 – 5 October 1985) was a South African archaeologist who specialised in the very early prehistory of Africa, and was one of twin sons born to botanists William Edwyn Isaac and Frances Margaret Leighton. He has been called the most influential Africanist of the last half century, and his papers on human movement and behavior are still cited in studies a quarter of a century later.[1]
He took his first degree from the University of Cape Town in 1958 before studying for his PhD at Peterhouse, Cambridge which he completed in 1969. He was also Warden for Prehistoric Sites in Kenya between 1961 and 1962 and deputy director of the Centre for Prehistory and Palaeontology at the National Museums of Kenya from 1963 to 1965. Working with Richard Leakey, he was co-director of the East African Koobi Fora project.
In 1966 he joined the anthropology department at the University of California, Berkeley and in 1983 he was appointed Professor of Anthropology at Harvard University where he was developing new research projects at the time of his death. He was survived by his twin brother, Rhys Isaac, an historian, based at La Trobe University.
He died in 1985 in Yokosuka, Japan due to illness, at the age of 47.[2]
Glynn Isaac is best remembered for a series of papers and ideas which attempted to combine the available archeological record with models of both human behavior and a human activity from the standpoint of evolution.[1] In the early 1970s Isaac published on the effect of social networks, gathering, meat eating and other factors on human evolution, and proposed a series of models to examine how groups of humans in the Paleolithic would have engaged in acquiring the necessities of life, and interacting with each other. Isaac's models focused on a "home base" and the importance of sexual division of labor on hominid social organization.

Paul Gerard Bahn, FSA (born 29 July 1953)[1] is a British archaeologist, translator, writer and broadcaster who has published extensively on a range of archaeological topics, with particular attention to prehistoric art. He is a contributing editor to Archaeology magazine.[2] With Colin Renfrew, he wrote the popular archaeology textbook Archaeology: Theories, Methods and Practice.
Born and raised in Kingston-upon-Hull,[2] Bahn was educated at the Marist College in the city and at Gonville and Caius College, Cambridge, where he studied archaeology and graduated with a BA in 1974.[1][3] He completed his Ph.D thesis on the prehistory of the French Pyrenees at Cambridge in 1979.
After receiving his doctorate, Bahn held several post-doctoral fellowships, at Liverpool and London, as well as a Getty Foundation postdoctoral fellowship in the History of Art and the Humanities. He went freelance in the mid-80s, and since then has devoted himself to writing, editing and translating books on archaeology, plus occasional journalism. His main research interest is prehistoric art, especially rock art of the world, and most notably Palaeolithic art, as well as the archaeology of Easter Island. He led the team which discovered the first Ice Age cave art in Britain in 2003 and 2004.
He is a contributing editor to Archaeology magazine published by the Archaeological Institute of America (AIA), and has lectured on many archaeological study tours sponsored by the AIA and others across Europe, Africa, North America and Polynesia. Bahn has also been an active consultant expert to a number of archaeological documentaries, including the BBC production "The Making of Mankind" and the trilogy of programmes "Human Origins" for the Nova series produced by WGBH-TV, Boston.[2]
Bahn's 2012 memoir, The Cambridge Rapist - Unmasking The Beast of Bedsitland recalls his student days at a time when serial rapist Peter Samuel Cook was at large.[4][5]
On 9 January 1986, Bahn was elected a Fellow of the Society of Antiquaries of London (FSA).[6]

Traditional
The history of Hinduism covers a wide variety of related religious traditions native to the Indian subcontinent.[1] It overlaps or coincides with the development of religion in the Indian subcontinent since the Iron Age, with some of its traditions tracing back to prehistoric religions such as those of the Bronze Age Indus Valley Civilisation. Hinduism has been called the "oldest religion" in the world,[a] but scholars regard Hinduism as a relatively recent synthesis[2][3][4] of various Indian cultures and traditions,[2][3][5] with diverse roots[6] and no single founder,[7][b] which emerged around the beginning of the Common Era.[8][c]
The history of Hinduism is often divided into periods of development. The first period is the pre-Vedic period, which includes the Indus Valley Civilization and local pre-historic religions. Northern India had the Vedic period with the introduction of the historical Vedic religion (sometimes called Vedic Hinduism or ancient Hinduism[d]) by the Indo-Aryan migrations, starting somewhere between 1900 BCE and 1400 BCE.[9][note 1] The subsequent period of the second urbanisation (600-200 BCE) is a formative period for Hinduism, Jainism and Buddhism followed by "a turning point between the Vedic religion and Hindu religions,"[12] during the Epic and Early Puranic period (c. 200 BCE to 500 CE), when the Epics and the first Purānas were composed.[3][13] This was followed by the classical "Golden Age" of Hinduism (c. 320–650 CE), which coincides with the Gupta Empire. In this period the six branches of Hindu philosophy evolved, namely, Samkhya, Yoga, Nyaya, Vaisheshika, Mīmāṃsā, and Vedānta. Monotheistic sects like Shaivism and Vaishnavism developed during this same period through the Bhakti movement. It flourished in the  medieval period  from roughly 650 to 1100 CE, which forms the late Classical period[14] or early Middle Ages, 
with the decline of Buddhism in India[15] and the establishment of classical Puranic Hinduism is established.
Hinduism under both Hindu and Islamic rulers from c. 1200 to 1750 CE[16][17] saw the increasing prominence of the Bhakti movement, which remains influential today. Adi Shankara became glorified as the main proponent of Advaita Vedanta, in response to the success of Vaishnavite bhakti.
The colonial period saw the emergence of various Hindu reform movements partly inspired by western movements, such as Unitarianism and Theosophy. The Partition of India in 1947 was along religious lines, with the Republic of India emerging with a Hindu majority. During the 20th century, due to the Indian diaspora, Hindu minorities have formed in all continents, with the largest communities in absolute numbers in the United States and the United Kingdom.
While the Puranic chronology presents a genealogy of thousands of years, scholars regard Hinduism as a fusion[2][note 2] or synthesis[3][note 3] of various Indian cultures and traditions.[3][note 4]
Among its roots are the historical Vedic religion,[5][20] itself already the product of "a composite of the Indo-Aryan and Harappan cultures and civilizations",[21][note 5] which evolved into the Brahmanical religion and ideology of the Kuru Kingdom of Iron Age northern India; but also the Śramaṇa[22] or renouncer traditions[5] of northeast India,[22] and mesolithic[23] and neolithic[24] cultures of India, such as the religions of the Indus Valley Civilisation,[25] Dravidian traditions,[26] and the local traditions[5] and tribal religions.[27]
This Hindu synthesis emerged after the Vedic period, between 500[3]–200[13] BCE and c. 300 CE,[3] in the period of the second urbanisation and the early classical period of Hinduism, when the Epics and the first Puranas were composed.[3][13] This Brahmanical synthesis incorporated śramaṇic[13][28] and Buddhist influences[13][29] and the emerging bhakti tradition into the Brahmanical fold via the smriti literature.[13][30] This synthesis emerged under the pressure of the success of Buddhism and Jainism,[31] starting with the conquest of the Vedic heartland by the Nanda and Maurya rulers, which deprived the Brahmins of their patrons, threatening the survival of the Vedic ritual tradition.[32] In response, Brahmins broadened their services,[33] eventually resulting in the Hindu synthesis of Brahmanical orthodoxy with local religious traditions,[3][31][34] which, centuries later, came to dominate India.[34] During the Gupta reign the first Puranas were written,[35][note 6] which were used to disseminate "mainstream religious ideology amongst pre-literate and tribal groups undergoing acculturation."[35] The resulting Puranic Hinduism differed markedly from the earlier Brahmanism of the Dharmasutras and the smritis.[35][note 7] Hinduism co-existed for several centuries with Buddhism,[36] to finally gain the upper hand at all levels in the 8th century.[37][web 1][note 8]
From northern India this "Hindu synthesis", and its societal divisions, spread to southern India and parts of Southeast Asia, as courts and rulers adopted the Brahmanical culture.[38][note 9][note 10][note 11] It was aided by the settlement of Brahmins on land granted by local rulers,[39][40] the incorporation and assimilation of popular non-Vedic gods,[web 2][41][note 12] and the process of Sanskritization, in which "people from many strata of society throughout the subcontinent tended to adapt their religious and social life to Brahmanic norms".[web 2][note 13][42] This process of assimilation explains the wide diversity of local cultures in India "half shrouded in a taddered cloak of conceptual unity".[43]
According to Eliot Deutsch, Brahmins played an essential role in the development of this synthesis. They were bilingual and bicultural, speaking both their local language, and popular Sanskrit, which transcended regional differences in culture and language. They were able to "translate the mainstream of the large culture in terms of the village and the culture of the village in terms of the mainstream", thereby integrating the local culture into a larger whole.[44] While vaidikas and, to a lesser degree, smartas, remained faithful to the traditional Vedic lore, a new brahminism arose which composed litanies for the local and regional gods, and became the ministers of these local traditions.[44]
Authors on Hinduism and its history have used various periodisations, elaborating on influential periodisations like Mill's, and also describing some of the constituting traditions preceding the Hindu-synthesis.[45][14][46][47] An elaborate periodisation may be as follows:[14]
Notes
Smart[E] and Michaels[I] seem to follow Mill's periodisation (Michaels mentions Flood 1996 as a source for "Prevedic Religions".[O]), while Flood[P] and Muesse[Q][G] follow the "ancient, classical, mediaeval and modern periods" periodisation.[R]
Different periods are designated as "classical Hinduism":
References
Sources
According to Doniger, Hinduism may have roots in pre-historic (pre-textual, pre-Vedic) Mesolithic prehistoric religion, such as evidenced in the rock paintings of Bhimbetka rock shelters,[note 15] which are about 10,000 years old (c. 8,000 BCE),[48][49][50][51][52] as well as Neolithic times. At least some of these shelters were occupied over 100,000 years ago.[53][note 16] Several tribal religions still  exist, though their practices may not resemble those of prehistoric religions.[web 3]
Some Indus valley seals show swastikas, which are found in other religions worldwide. Phallic symbols interpreted as the much later Hindu linga have been found in the Harappan remains.[54][55] Many Indus valley seals show animals. One seal showing a horned figure seated in a posture reminiscent of the Lotus position and surrounded by animals was named by early excavators "Pashupati", an epithet of the later Hindu gods Shiva and Rudra.[56][57][58] Writing in 1997, Doris Meth Srinivasan said, "Not too many recent studies continue to call the seal's figure a 'Proto-Siva', rejecting thereby Marshall's package of proto-Shiva features, including that of three heads. She interprets what John Marshall interpreted as facial as not human but more bovine, possibly a divine buffalo-man."[59][verification needed] According to Iravatham Mahadevan, symbols 47 and 48 of his Indus script glossary The Indus Script: Texts, Concordance and Tables (1977), representing seated human-like figures, could describe the South Indian deity Murugan.[60]
In view of the large number of figurines found in the Indus valley, some scholars believe that the Harappan people worshipped a mother goddess symbolizing fertility, a common practice among rural Hindus even today.[61] However, this view has been disputed by S. Clark who sees it as an inadequate explanation of the function and construction of many of the figurines.[62]
There are no religious buildings or evidence of elaborate burials. If there were temples, they have not been identified.[attribution needed][63] However, House – 1 in HR-A area in Mohenjo Daro's Lower Town has been identified as a possible temple.[64]
The historical Vedic religion, also known as Vedicism and Vedism, sometimes referred  to as an early phase of Hinduism called Vedic Hinduism and Ancient Hinduism,[d] was the sacrificial religion of the early Indo-Aryans, speakers of early Old Indic dialects, ultimately deriving from the Proto-Indo-Iranian peoples of the Bronze Age who lived on the Central Asian steppes.[note 17]
The Vedic period, named after the Vedic religion of the Indo-Aryans of the Kuru Kingdom 1200 BCE – 525 BCE,[71][note 18] lasted from c. 1750 to 500 BCE.[72][note 19] The Indo-Aryans were a branch of the Indo-European language family, which many scholars believe originated in Kurgan culture of the Central Asian steppes.[73][74][note 20][note 21] Indeed, the ancient Vedic religion, including the names of certain deities, was in essence a branch of the same religious tradition as the ancient Greeks, Romans, Persians, and Germanic peoples. For example, the Vedic god Dyaus is a variant of the Proto-Indo-European god *Dyēus ph2ter (or simply *Dyēus), from which also derive the Greek Zeus and the Roman Jupiter. Similarly the Vedic Manu and Yama derive from the Proto-Indo-European *Manu and *Yemo, from which also derive the Germanic Mannus and Ymir.
According to the Indo-European migration theory, the Indo-Iranians were the common ancestor of the Indo-Aryans and the Proto-Iranians. The Indo-Iranians split into the Indo-Aryans and Iranians around 1800–1600 BCE.[75]
The Indo-Aryans were pastoralists[76] who migrated into north-western India after the collapse of the Indus Valley Civilization.[77][78][79][note 22] The Indo-Aryans were a branch of the Indo-Iranians, which originated in the Andronovo culture[80] in the Bactria-Margiana era, in present northern Afghanistan.[81] The roots of this culture go back further to the Sintashta culture, with funeral sacrifices which show close parallels to the sacrificial funeral rites of the Rigveda.[82]
Although some early depictions of deities seem to appear in the art of the Indus Valley Civilisation, very few religious artifacts remain from the period corresponding to the Indo-Aryan migration during the Vedic period.[83] It has been suggested that the early Vedic religion focused exclusively on the worship of purely "elementary forces of nature by means of elaborate sacrifices", which did not lend themselves easily to anthropomorphological representations.[83][84] Various artefacts may belong to the Copper Hoard culture (2nd millennium CE), some of them suggesting anthropomorphological characteristics.[85] Interpretations vary as to the exact signification of these artifacts, or even the culture and the periodization to which they belonged.[85]
During the Early Vedic period (c. 1500–1100 BCE[76]) Indo-Aryan tribes were pastoralists in north-west India.[86] After 1100 BCE, with the introduction of iron, the Indo-Aryan tribes moved into the western Ganges Plain, adopting an agrarian lifestyle.[76][87][88] Rudimentary state-forms appeared, of which the Kuru-tribe and realm was the most influential.[76][89] It was a tribal union, which developed into the first recorded state-level society in South Asia around 1000 BCE.[76] It decisively changed their religious heritage of the early Vedic period, collecting their ritual hymns into the Veda-collections, and developing new rituals which gained their position in Indian civilization as the orthodox Śrauta rituals,[76] which contributed to the so-called "classical synthesis"[90] or "Hindu synthesis".[3]
Who really knows?
Who will here proclaim it?
Whence was it produced? Whence is this creation?
The gods came afterwards, with the creation of this universe.
Who then knows whence it has arisen?
The Indo-Aryans brought with them their language[94] and religion.[95][96] The Indo-Aryan and Vedic beliefs and practices of the pre-classical era were closely related to the hypothesised Proto-Indo-European religion,[97] and the Indo-Iranian religion.[98] According to Anthony, the Old Indic religion probably emerged among Indo-European immigrants in the contact zone between the Zeravshan River (present-day Uzbekistan) and (present-day) Iran.[99] It was "a syncretic mixture of old Central Asian and new Indo-European elements",[99] which borrowed "distinctive religious beliefs and practices"[98] from the Bactria–Margiana culture.[98] At least 383 non-Indo-European words were borrowed from this culture, including the god Indra and the ritual drink Soma.[100] According to Anthony,
Many of the qualities of Indo-Iranian god of might/victory, Verethragna, were transferred to the adopted god Indra, who became the central deity of the developing Old Indic culture. Indra was the subject of 250 hymns, a quarter of the Rig Veda. He was associated more than any other deity with Soma, a stimulant drug (perhaps derived from Ephedra) probably borrowed from the BMAC religion. His rise to prominence was a peculiar trait of the Old Indic speakers.[81]
The oldest inscriptions in Old Indic, the language of the Rig Veda, are found not in northwestern India and Pakistan, but in northern Syria, the location of the Mitanni kingdom.[101] The Mitanni kings took Old Indic throne names, and Old Indic technical terms were used for horse-riding and chariot-driving.[101] The Old Indic term r'ta, meaning "cosmic order and truth", the central concept of the Rig Veda, was also employed in the Mitanni kingdom.[101] And Old Indic gods, including Indra, were also known in the Mitanni kingdom.[102][103][104]
Their religion was further developed when they migrated into the Ganges Plain after c. 1100 BCE and became settled farmers,[76][105][106] further syncretising with the native cultures of northern India.[90] The Brahmanical culture of the later Vedic period co-existed with local religions, such as the Yaksha cults,[90][107][web 5] and was itself the product of "a composite of the Indo-Aryan and Harappan cultures and civilizations".[21][note 5] David Gordon White cites three other mainstream scholars who "have emphatically demonstrated" that Vedic religion is partially derived from the Indus Valley Civilisation.[108][note 5]
Its liturgy is preserved in the three Vedic Samhitas: the Rigveda, Samaveda and the Yajurveda. The Vedic texts were the texts of the elite, and do not necessarily represent popular ideas or practices.[111] Of these, the Rig-Veda is the oldest, a collection of hymns composed between c. 1500 and 1200 BCE.[112][113][81] The other two add ceremonial detail for the performance of the actual sacrifice. The Atharvaveda may also contain compositions dating to before 1000 BCE. It contains material pertinent to domestic ritual and folk magic of the period.
These texts, as well as the voluminous commentary on orthopraxy collected in the Brahmanas compiled during the early 1st millennium BCE, were transmitted by oral tradition alone until the advent, in the 4th century CE, of the Pallava and Gupta period and by a combination of written and oral tradition since then.
The Hindu samskaras
go back to a hoary antiquity. The Vedas, the Brahmanas, the Grhyasutras, the Dharmasutras, the Smritis and other treatises describe the rites, ceremonies and customs.[114]
The earliest text of the Vedas is the Rigveda,[115] a collection of poetic hymns used in the sacrificial rites of Vedic priesthood. Many Rigvedic hymns concern the fire ritual (Agnihotra) and especially the offering of Soma to the gods (Somayajna). Soma is both an intoxicant and a god itself, as is the sacrificial fire, Agni. The royal horse sacrifice (Ashvamedha) is a central rite in the Yajurveda.
The gods in the Rig-Veda are mostly personified concepts, who fall into two categories: the devas – who were gods of nature – such as the weather deity Indra (who is also the King of the gods), Agni ("fire"), Usha ("dawn"), Surya ("sun") and Apas ("waters") on the one hand, and on the other hand the asuras – gods of moral concepts – such as Mitra ("contract"), Aryaman (guardian of guest, friendship and marriage), Bhaga ("share") or Varuna, the supreme Asura (or Aditya). While Rigvedic deva is variously applied to most gods, including many of the Asuras, the Devas are characterised as Younger Gods while Asuras are the Older Gods (pūrve devāḥ). In later Vedic texts, "Asura" comes to mean demon.
The Rigveda has 10 mandalas ('books'). There is significant variation in the language and style between the family books (RV books 2–7), book 8, the "Soma Mandala" (RV 9), and the more recent books 1 and 10. The older books share many aspects of common Indo-Iranian religion, and is an important source for the reconstruction of earlier common Indo-European traditions. Especially RV 8 has striking similarity to the Avesta,[116] containing allusions to Afghan flora and fauna,[117] e.g. to camels (úṣṭra- = Avestan uštra). Many of the central religious terms in Vedic Sanskrit have cognates in the religious vocabulary of other Indo-European languages (deva: Latin deus; hotar: Germanic god; asura: Germanic ansuz; yajna: Greek hagios; brahman: Norse Bragi or perhaps Latin flamen etc.). In the Avesta, Asura (Ahura) is considered good and Devas (Daevas) are considered evil entities, quite the opposite of the Rig Veda.
Ethics in the Vedas are based on the concepts of Satya and Ṛta. Satya is the principle of integration rooted in the Absolute.[118] Ṛta is the expression of Satya, which regulates and coordinates the operation of the universe and everything within it.[119] Conformity with Ṛta would enable progress whereas its violation would lead to punishment. Panikkar remarks:
Ṛta is the ultimate foundation of everything; it is "the supreme", although this is not to be understood in a static sense. ... It is the expression of the primordial dynamism that is inherent in everything....[120]
The term "dharma" was already used in Brahmanical thought, where it was conceived as an aspect of Rta.[121] The term rta is also known from the Proto-Indo-Iranian religion, the religion of the Indo-Iranian peoples prior to the earliest Vedic (Indo-Aryan) and Zoroastrian (Iranian) scriptures. Asha[pronunciation?] (aša) is the Avestan language term corresponding to Vedic language ṛta.[122]
The 9th and 8th centuries BCE witnessed the composition of the earliest Upanishads.[123] Upanishads form the theoretical basis of classical Hinduism and are known as Vedanta (conclusion of the Veda).[124] The older Upanishads launched attacks of increasing intensity on the rituals, however, a philosophical and allegorical meaning is also given to these rituals. In some later Upanishads there is a spirit of accommodation towards rituals. The tendency which appears in the philosophical hymns of the Vedas to reduce the number of gods to one principle becomes prominent in the Upanishads.[125]
Although it is sometimes assumed that the Upanishads propound a monistic framework, scholars like Brian Black and Andrew Nicholson have argued that this is an unfair assumption, referring to the presence of philosophically diverse themes in early Upanishads like the Brihadaranyaka Upanishad and Chandogya Upanishad.[126][127] The ideas of the Upanishads were synthesised into a theistic framework in the Bhagavad Gita.[128]
Brahmanism, also called Brahminism or Brahmanical Hinduism, developed out of the Vedic religion, incorporating non-Vedic religious ideas, and expanding to a region stretching from the northwest Indian subcontinent to the Ganges valley.[129] Brahmanism included the Vedic corpus, but also post-Vedic texts such as the Dharmasutras and Dharmasastras, which gave prominence to the priestly (Brahmin) class of the society.[129] The emphasis on ritual and the dominant position of Brahmans developed as an ideology developed in the Kuru-Pancala realm, and expanded into a wider realm after the demise of the Kuru-Pancala realm.[76] It co-existed with local religions, such as the Yaksha cults.[90][107][web 5]
In Iron Age India, during a period roughly spanning the 10th to 6th centuries BCE, the Mahajanapadas arise from the earlier kingdoms of the various Indo-Aryan tribes, and the remnants of the Late Harappan culture. In this period the mantra portions of the Vedas are largely completed, and a flowering industry of Vedic priesthood organised in numerous schools (shakha) develops exegetical literature, viz. the Brahmanas. These schools also edited the Vedic mantra portions into fixed recensions, that were to be preserved purely by oral tradition over the following two millennia.
Brahmanism, with its orthodox rituals, may have been challenged as a consequence of the increasing urbanisation of India in the 7th and 6th centuries BCE, and the influx of foreign stimuli initiated with the Achaemenid conquest of the Indus Valley (circa 535 BCE).[83][130] New ascetic or sramana movements arose, such as Buddhism, Jainism and local popular cults, which challenged the established religious orthodoxy.[83][130] The anthropomorphic depiction of various deities apparently resumed in the middle of the 1st millennium BCE, also as the consequence of the reduced authority of Vedism.[83]
Mahavira (c. 549–477 BCE), proponent of Jainism, and Buddha (c. 563–483 BCE), founder of Buddhism, were the most prominent icons of this movement.[131] According to Heinrich Zimmer, Jainism and Buddhism are part of the pre-Vedic heritage, which also includes Samkhya and Yoga:
[Jainism] does not derive from Brahman-Aryan sources, but reflects the cosmology and anthropology of a much older pre-Aryan upper class of northeastern India – being rooted in the same subsoil of archaic metaphysical speculation as Yoga, Sankhya, and Buddhism, the other non-Vedic Indian systems.[132][note 23]
The Sramana tradition in part created the concept of the cycle of birth and death, the concept of Saṃsāra, and the concept of liberation, which became characteristic for Hinduism.[note 24]
Pratt notes that Oldenberg (1854–1920), Neumann (1865–1915) and Radhakrishnan (1888–1975) believed that the Buddhist canon had been influenced by Upanishads, while la Vallee Poussin thinks the influence was nil, and "Eliot and several others insist that on some points the Buddha was directly antithetical to the Upanishads".[134][note 25]
The Mauryan period saw an early flowering of classical Sanskrit Sutra and Shastra literature and the scholarly exposition of the "circum-Vedic" fields of the Vedanga. However, during this time Buddhism was patronised by Ashoka, who ruled large parts of India, and Buddhism was also the mainstream religion until the Gupta period.
The post-Vedic period of the Second Urbanisation saw a decline of Brahmanism.[136][137][note 26] At the end of the Vedic period, the meaning of the words of the Vedas had become obscure, and was perceived as "a fixed sequence of sounds"[138][note 27] with a magical power, "means to an end."[note 28] With the growth of cities, which threatened the income and patronage of the rural Brahmins; the rise of Buddhism; and the Indian campaign of Alexander the Great (327–325 BCE), the expansion of the Maurya Empire (322–185 BCE) with its embrace of Buddhism, and the Saka invasions and rule of northwestern India (2nd c. BCE – 4th c. CE), Brahmanism faced a grave threat to its existence.[139] In some later texts, Northwest-India (which earlier texts consider as part of "Aryavarta") is even seen as "impure", probably due to invasions.
Vedism as the religious tradition of a priestly elite was marginalised by other traditions such as Jainism and Buddhism in the later Iron Age, but in the Middle Ages would rise to renewed prestige with the Mimamsa school, which as well as all other astika traditions of Hinduism, considered them authorless (apaurusheyatva) and eternal. A last surviving elements of the Historical Vedic religion or Vedism is Śrauta tradition, following many major elements of the ancient Vedic religion and is prominent in South India, with communities in Tamil Nadu, Kerala, Karnataka, Andhra Pradesh, but also in some pockets of Uttar Pradesh, Maharashtra and other states; the best known of these groups are the Nambudiri of Kerala, whose traditions were notably documented by Frits Staal.[140][141][142]
The decline of Brahmanism was overcome by providing new services[148] and incorporating the non-Vedic Indo-Aryan religious heritage of the eastern Ganges plain and local religious traditions, giving rise to contemporary Hinduism.[139][web 6][90][149][76][129] Between about 500 BCE and c. 400 CE[3] or starting from 200 BCE[13] the "Hindu synthesis" developed,[3][13] which incorporated Sramanic and Buddhist influences[13][29] and the emerging Bhakti tradition into the Brahmanical fold via the smriti literature.[30][13] This synthesis emerged under the pressure of the success of Buddhism and Jainism.[31]
According to Embree, several other religious traditions had existed side by side with the Vedic religion. These indigenous religions "eventually found a place under the broad mantle of the Vedic religion".[150] When Brahmanism was declining[note 26] and had to compete with Buddhism and Jainism,[note 29] the popular religions had the opportunity to assert themselves.[150] According to Embree,
[T]he Brahmanists themselves seem to have encouraged this development to some extent as a means of meeting the challenge of the heterodox movements. At the same time, among the indigenous religions, a common allegiance to the authority of the Vedas provided a thin, but nonetheless significant, thread of unity amid their variety of gods and religious practices.[150]
This "new Brahmanism" appealed to rulers, who were attracted to the supernatural powers and the practical advice Brahmins could provide,[148] and resulted in a resurgence of Brahmanical influence, dominating Indian society since the classical Age of Hinduism in the early centuries CE.[139] It is reflected in the process of Sanskritization, a process in which "people from many strata of society throughout the subcontinent tended to adapt their religious and social life to Brahmanic norms".[web 2] It is reflected in the tendency to identify local deities with the gods of the Sanskrit texts.[web 2]
The Brahmins response of assimilation and consolidation is reflected in the smriti literature which took shape in this period.[151] The smriti texts of the period between 200 BCE and 100 CE proclaim the authority of the Vedas, and acceptance of the Vedas became a central criterion for defining Hinduism over and against the heterodoxies, which rejected the Vedas.[152] Most of the basic ideas and practices of classical Hinduism derive from the new smriti literature.[note 30]
Of the six Hindu darsanas, the Mimamsa and the Vedanta "are rooted primarily in the Vedic sruti tradition and are sometimes called smarta schools in the sense that they develop smarta orthodox current of thoughts that are based, like smriti, directly on sruti".[153][verify] According to Hiltebeitel, "the consolidation of Hinduism takes place under the sign of bhakti".[153] It is the Bhagavadgita that seals this achievement.[153] The result is a "universal achievement" that may be called smarta.[153] It views Shiva and Vishnu as "complementary in their functions but ontologically identical".[153]
The major Sanskrit epics, Ramayana and Mahabharata, which belong to the smriti, were compiled over a protracted period during the late centuries BCE and the early centuries CE.[web 7] They contain mythological stories about the rulers and wars of ancient India, and are interspersed with religious and philosophical treatises. The later Puranas recount tales about devas and devis, their interactions with humans and their battles against rakshasa. The Bhagavad Gita "seals the achievement"[154] of the "consolidation of Hinduism",[154] integrating Brahmanic and sramanic ideas with theistic devotion.[154][155][156][web 8]
In early centuries CE several schools of Hindu philosophy were formally codified, including Samkhya, Yoga, Nyaya, Vaisheshika, Purva-Mimamsa and Vedanta.[157]
The Sangam literature (300 BCE – 400 CE), written in the Sangam period, is a mostly secular body of classical literature in the Tamil language. Nonetheless, there are some works, significantly Pattuppāṭṭu and Paripāṭal, wherein the personal devotion to God was written in the form of devotional poems. Vishnu, Shiva and Murugan were mentioned gods. These works are therefore the earliest evidence of monotheistic Bhakti traditions, preceding the large bhakti movement, which was given great attention in later times.
During the time of the Roman Empire, trade took place between India and east Africa, and there is archaeological evidence of small Indian presence in Zanzibar, Zimbabwe, Madagascar, and the coastal parts of Kenya along with the Swahili coast,[158][159] but no conversion to Hinduism took place.[159][160]
Armenian historian Zenob Glak (300–350 CE) said "there was an Indian colony in the canton of Taron on the upper Euphrates, to the west of Lake Van, as early as the second century B.C. The Indians had built there two temples containing images of gods about 18 and 22 feet high."[161]
During this period, power was centralised, along with a growth of near distance trade, standardization of legal procedures, and general spread of literacy.[162] Mahayana Buddhism flourished, but orthodox Brahmana culture began to be rejuvenated by the patronage of the Gupta Dynasty,[163] who were Vaishnavas.[164] The position of the Brahmans was reinforced,[162] the first Hindu temples dedicated to the gods of the Hindu deities, emerged during the late Gupta age.[162][note 31] During the Gupta reign the first Puranas were written,[35][note 6] which were used to disseminate "mainstream religious ideology amongst pre-literate and tribal groups undergoing acculturation".[35] The Guptas patronised the newly emerging Puranic religion, seeking legitimacy for their dynasty.[164] The resulting Puranic Hinduism, differed markedly from the earlier Brahmanism of the Dharmasastras and the smritis.[35]
According to P. S. Sharma, "the Gupta and Harsha periods form really, from the strictly intellectual standpoint, the most brilliant epocha in the development of Indian philosophy", as Hindu and Buddhist philosophies flourished side by side.[165] Charvaka, the atheistic materialist school, came to the fore in North India before the 8th century CE.[166]
The Gupta period (4th to 6th centuries) saw a flowering of scholarship, the codification of the classical schools of Hindu philosophy, and of classical Sanskrit literature in general on topics ranging from medicine, veterinary science, mathematics, to astrology and astronomy and astrophysics. The famous Aryabhata and Varāhamihira belong to this age. The Gupta established a strong central government which also allowed a degree of local control. Gupta society was ordered in accordance with Brahmanical beliefs. This included a strict caste system, or class system. The peace and prosperity created under Gupta leadership enabled the pursuit of scientific and artistic endeavors.
The Pallavas (4th to 9th centuries) were, alongside the Guptas of the North, patronisers of Sanskrit in the South of the Indian subcontinent. The Pallava reign saw the first Sanskrit inscriptions in a script called Grantha. The Pallavas used Dravidian architecture to build some very important Hindu temples and academies in Mahabalipuram, Kanchipuram and other places; their rule saw the rise of great poets, who are as famous as Kalidasa.
During early Pallavas period, there are different connections to Southeast Asian and other countries. Due to it, in the Middle Ages, Hinduism became the state religion in many kingdoms of Asia, the so-called Greater India—from Afghanistan (Kabul) in the West and including almost all of Southeast Asia in the East (Cambodia, Vietnam, Indonesia, Philippines)—and only by the 15th century was near everywhere supplanted by Buddhism and Islam.[167][168][169]
The practice of dedicating temples to different deities came into vogue followed by fine artistic temple architecture and sculpture (see Vastu shastra).
This period saw the emergence of the Bhakti movement. The Bhakti movement was a rapid growth of bhakti beginning in Tamil Nadu in Southern India with the Vaisnava Alvars (3rd to 9th centuries CE)[170] and Saiva Nayanars (4th to 10th centuries CE)[171] who spread bhakti poetry and devotion throughout India by the 12th to 18th centuries CE.[172][171][173]
Hindu influences reached the Indonesian Archipelago as early as the first century.[174] At this time, India started to strongly influence Southeast Asian countries. Trade routes linked India with southern Burma, central and southern Siam, lower Cambodia and southern Vietnam and numerous urbanised coastal settlements were established there.
For more than a thousand years, Indian Hindu/Buddhist influence was, therefore, the major factor that brought a certain level of cultural unity to the various countries of the region. The Pali and Sanskrit languages and the Indian script, together with Theravada and Mahayana Buddhism, Brahmanism and Hinduism, were transmitted from direct contact as well as through sacred texts and Indian literature, such as the Ramayana and the Mahabharata epics.
From the 5th to the 13th century, South-East Asia had very powerful Indian colonial empires and became extremely active in Hindu and Buddhist architectural and artistic creation. The Sri Vijaya Empire to the south and the Khmer Empire to the north competed for influence.
Langkasuka (-langkha Sanskrit for "resplendent land" -sukkha of "bliss") was an ancient Hindu kingdom located in the Malay Peninsula. The kingdom, along with Old Kedah settlement, are probably the earliest territorial footholds founded on the Malay Peninsula. According to tradition, the founding of the kingdom happened in the 2nd century; Malay legends claim that Langkasuka was founded at Kedah, and later moved to Pattani.
From the 5th to 15th centuries Sri Vijayan empire, a maritime empire centred on the island of Sumatra in Indonesia, had adopted Mahayana and Vajrayana Buddhism under a line of rulers named the Sailendras. The Empire of Sri Vijaya declined due to conflicts with the Chola rulers of India. The Majapahit Empire succeeded the Singhasari empire. It was one of the last and greatest Hindu empires in maritime Southeast Asia.
Funan was a pre-Angkor Cambodian kingdom, located around the Mekong delta, probably established by Mon-Khmer settlers speaking an Austroasiatic language. According to reports by two Chinese envoys, K'ang T'ai and Chu Ying, the state was established by an Indian Brahmin named Kaundinya, who in the 1st century CE was given instruction in a dream to take a magic bow from a temple and defeat a Khmer queen, Soma. Soma, the daughter of the king of the Nagas, married Kaundinya and their lineage became the royal dynasty of Funan. The myth had the advantage of providing the legitimacy of both an Indian Brahmin and the divinity of the cobras, who at that time were held in religious regard by the inhabitants of the region.
The kingdom of Champa (or Lin-yi in Chinese records) controlled what is now south and central Vietnam from approximately 192 through 1697. The dominant religion of the Cham people was Hinduism and the culture was heavily influenced by India.
Later, from the 9th to the 13th century, the Mahayana Buddhist and Hindu Khmer Empire dominated much of the South-East Asian peninsula. Under the Khmer, more than 900 temples were built in Cambodia and in neighboring Thailand. Angkor was at the centre of this development, with a temple complex and urban organisation able to support around one million urban dwellers. The largest temple complex of the world, Angkor Wat, stands here; built by the king Vishnuvardhan.
After the end of the Gupta Empire and the collapse of the Harsha Empire, power became decentralised in India. Several larger kingdoms emerged, with "countless vasal states".[176][note 32] The kingdoms were ruled via a feudal system. Smaller kingdoms were dependent on the protection of the larger kingdoms. "The great king was remote, was exalted and deified",[176] as reflected in the Tantric Mandala, which could also depict the king as the centre of the mandala.[177]
The disintegration of central power also lead to regionalisation of religiosity, and religious rivalry.[178][note 33] Local cults and languages were enhanced, and the influence of "Brahmanic ritualistic Hinduism"[178] was diminished.[178] Rural and devotional movements arose, along with Shaivism, Vaisnavism, Bhakti and Tantra,[178] though "sectarian groupings were only at the beginning of their development".[178] Religious movements had to compete for recognition by the local lords.[178] Buddhism lost its position after the 8th century, due to the loss of financial support from royal donors and the lack of appeal among the rural masses, and began to disappear in India.[178] This was reflected in the change of puja-ceremonies at the courts in the 8th century, where Hindu gods replaced the Buddha as the "supreme, imperial deity".[note 34]
The Brahmanism of the Dharmaśāstra and the smritis underwent a radical transformation at the hands of the Purana composers, resulting in the rise of Puranic Hinduism,[35] "which like a colossus striding across the religious firmanent soon came to overshadow all existing religions".[181] Puranic Hinduism was a "multiplex belief-system which grew and expanded as it absorbed and synthesised polaristic ideas and cultic traditions".[181] It was distinguished from its Vedic Smarta roots by its popular base, its theological and sectarian pluralism, its Tantric veneer, and the central place of bhakti.[181][note 7]
The early mediaeval Puranas were composed to disseminate religious mainstream ideology among the pre-literate tribal societies undergoing acculturation.[35] With the breakdown of the Gupta empire, gifts of virgin waste-land were heaped on brahmanas,[40][182] to ensure profitable agrarian exploitation of land owned by the kings,[40] but also to provide status to the new ruling classes.[40] Brahmanas spread further over India, interacting with local clans with different religions and ideologies.[40] The Brahmanas used the Puranas to incorporate those clans into the agrarian society and its accompanying religion and ideology.[40] According to Flood, "[t]he Brahmans who followed the puranic religion became known as smarta, those whose worship was based on the smriti, or pauranika, those based on the Puranas."[183] Local chiefs and peasants were absorbed into the varna, which was used to keep "control over the new kshatriyas and shudras."[184]
The Brahmanic group was enlarged by incorporating local subgroups, such as local priests.[40] This also lead to stratification within the Brahmins, with some Brahmins having a lower status than other Brahmins.[40] The use of caste worked better with the new Puranic Hinduism than with the Sramanic sects.[184] The Puranic texts provided extensive genealogies which gave status to the new kshatriyas.[184] Buddhist myths pictured government as a contract between an elected ruler and the people.[184] And the Buddhist chakkavatti[note 35] "was a distinct concept from the models of conquest held up to the kshatriyas and the Rajputs".[184]
Many local religions and traditions were assimilated into puranic Hinduism. Vishnu and Shiva emerged as the main deities, together with Sakti/Deva.[186] Vishnu subsumed the cults of Narayana, Jagannaths, Venkateswara "and many others".[186] Nath:
[S]ome incarnations of Vishnu such as Matsya, Kurma, Varaha and perhaps even Nrsimha helped to incorporate certain popular totem symbols and creation myths, especially those related to wild boar, which commonly permeate preliterate mythology, others such as Krsna and Balarama became instrumental in assimilating local cults and myths centering around two popular pastoral and agricultural gods.[187]
The transformation of Brahmanism into Pauranic Hinduism in post-Gupta India was due to a process of acculturation. The Puranas helped establish a religious mainstream among the pre-literate tribal societies undergoing acculturation. The tenets of Brahmanism and of the Dharmashastras underwent a radical transformation at the hands of the Purana composers, resulting in the rise of a mainstream "Hinduism" that overshadowed all earlier traditions.[40]
Rama and Krishna became the focus of a strong bhakti tradition, which found expression particularly in the Bhagavata Purana. The Krishna tradition subsumed numerous Naga, yaksa and hill and tree-based cults.[188] Shiva absorbed local cults by the suffixing of Isa or Isvara to the name of the local deity, for example, Bhutesvara, Hatakesvara, and Chandesvara.[186] In 8th-century royal circles, the Buddha started to be replaced by Hindu gods in pujas.[note 34] This also was the same period of time the Buddha was made into an avatar of Vishnu.[189]
The first documented Bhakti movement was founded by the first three Vaishnavite Alvars. Traditionally, the Alvars are considered to have lived between 4200 BCE and 2700 BCE,[190][191] while some texts account for range between 4200 BCE and early 5th century. Traditional dates take them to the age of Shuka from the period of the Mahabharata and Bhagavata Purana, the first four (Poigai Alvar, Bhoothath Alvar, Peyalvar and Tirumalisai Alvar) are from the Dvapara Yuga, while Nammalvar, Madhurakavi Alvar and others belong to the Kali Yuga.[192] Shuka is dated minimum around 200 BCE.[193] Hence the first three Alvars are also considered minimum 200 BCE. The twelve Alvars who were Vaishnavite devotees and the sixty-three Nayanars who were Shaivite devotees nurtured the incipient Bhakti movement in Tamil Nadu.
During the 12th century CE in Karnataka, the Bhakti movement took the form of the Virashaiva movement. It was inspired by Basavanna, a Hindu reformer who created the sect of Lingayats or Shiva bhaktas. During this time, a unique and native form of Kannada literature-poetry called Vachanas was born.
The early Advaitin Gaudapada (6th–7th c. CE) was influenced by Buddhism.[195][196][197][198] Gaudapda took over the Buddhist doctrines that ultimate reality is pure consciousness (vijñapti-mātra)[199] and "that the nature of the world is the four-cornered negation".[199] Gaudapada "wove [both doctrines] into a philosophy of the Mandukya Upanishad, which was further developed by Shankara".[196] Gaudapada also took over the Buddhist concept of "ajāta" from Nagarjuna's Madhyamaka philosophy.[197][198] Gaudapada seems to have ignored the Brahma-sutras, and it was Shankara who succeeded in reading Gaudapada's mayavada, a polemic term used by opponents,[200][note 36] into Badarayana's Brahma Sutras, "and give it a locus classicus",[200] against the realistic strain of the Brahma Sutras.[200]
Shankara (8th century CE) was a scholar who synthesized and systematized Advaita Vedanta views which already existed at his lifetime.[201][202][203][web 13] Shankara propounded a unified reality, in which the innermost self of a person (atman) and the supernatural power of the entire world (brahman) are one and the same. Perceiving the changing multiplicity of forms and objects as the final reality is regarded as maya, "illusion", obscuring the unchanging ultimate reality of brahman.[204][205][206][207]
While Shankara has an unparalleled status in the history of Advaita Vedanta, Shankara's early influence in India is doubtful.[208] Until the 11th century, Vedanta itself was a peripheral school of thought,[209] and until the 10th century Shankara himself was overshadowed by his older contemporary Maṇḍana Miśra, who was considered to be the major representative of Advaita.[210][211]
Several scholars suggest that the historical fame and cultural influence of Shankara and Advaita Vedanta grew only centuries later, during the era of the Muslim invasions and consequent devastation of India,[208][212][213] due to the efforts of Vidyaranya (14th c.), who created legends to turn Shankara into a "divine folk-hero who spread his teaching through his digvijaya ("universal conquest") all over India like a victorious conqueror."[214][215]
Shankara's position was further established in the 19th and 20th centuries, when neo-Vedantins and western Orientalists elevated Advaita Vedanta "as the connecting theological thread that united Hinduism into a single religious tradition".[216] Advaita Vedanta has acquired a broad acceptance in Indian culture and beyond as the paradigmatic example of Hindu spirituality,[135] Shankara became "an iconic representation of Hindu religion and culture", despite the fact that most Hindus do not adhere to Advaita Vedanta.[217]
Hindu and also Buddhist religious and secular learning had first reached Persia in an organised manner in the 6th century, when the Sassanid Emperor Khosrow I (531–579) deputed Borzuya the physician as his envoy, to invite Indian and Chinese scholars to the Academy of Gondishapur. Burzoe had translated the Sanskrit Panchatantra. His Pahlavi version was translated into Arabic by Ibn al-Muqaffa' under the title of Kalila and Dimna or The Fables of Bidpai.[218]
Under the Abbasid caliphate, Baghdad had replaced Gundeshapur as the most important centre of learning in the then vast Islamic Empire, wherein the traditions, as well as scholars of the latter, flourished. Hindu scholars were invited to the conferences on sciences and mathematics held in Baghdad.[219]
The Muslim conquests in the Indian subcontinent took place between the 13th and the 18th centuries. The Ghurid ruler Muhammad of Ghor laid the foundation of Muslim rule in India in 1192,[221] expanding up to Bengal by 1202. The Ghurid Empire soon evolved into the Delhi Sultanate in 1206, transitioning to the Mamluk dynasty.[222][223] During this historical period, Buddhism experienced a decline,[224] and there were instances of religious tensions and conflicts in the Indian subcontinent. Some records indicate incidents of raids, property seizures, and the enslavement of some Hindu families.[225][226] Additionally, there were accounts suggesting that some Hindus may have converted to Islam, possibly under various circumstances, including to secure their freedom.[227][228] In between the periods of wars and conquests, there were periods of cooperation and syncretism. There were harmonious Hindu-Muslim relations in most Indian communities.[229] No populations were expelled based on their religion by either the Muslim or Hindu kings, nor were attempts made to annihilate a specific religion.[229]
In the 16th century, the Mughal Empire was established. Under the Mughals, India experienced a period of relative stability and prosperity.[230][231][232] The Mughals were generally known for their religious tolerance,[233][234][235][236] and they actively patronized the arts and literature. There were instances of religious conflicts between the Mughals and the Rajput over control of territories. Aurangzeb in particular was noted for his policies of religious intolerance towards non-Muslims and destruction of temples.[web 14][237]
The impact and consequences of the Muslim conquest of South Asia remain subjects of scrutiny and diverse viewpoints. Will Durant characterizes the Muslim conquest of India as a particularly tumultuous chapter in history. He suggests that it was marked by significant violence and upheaval, which he attributes in part to factors such as internal divisions, the influence of religions like Buddhism and Jainism. Alain Daniélou criticized the Muslim rulers, claiming that the violence was often justified in the name of religious holy wars.[238] Other, like Sir Thomas Arnold and De Lacy O'Leary, criticized the view that Islam was spread by force and sword as 'absurd.'[239] According to Ira Lapidus, while instances of forced conversion in Muslim regions did occur, they were relatively infrequent. Muslim conquerors generally sought to exert control rather than enforce conversion, with the majority of conversions to Islam being voluntary in nature.[240][239]
Teachers such as Ramanuja, Madhva, and Chaitanya aligned the Bhakti movement with the textual tradition of Vedanta, which until the 11th century was only a peripheral school of thought,[209] while rejecting and opposing the abstract notions of Advaita. Instead, they promoted emotional, passionate devotion towards the more accessible Avatars, especially Krishna and Rama.[241][242][page needed]
According to Nicholson, already between the 12th and the 16th century, "certain thinkers began to treat as a single whole the diverse philosophical teachings of the Upanishads, epics, Puranas, and the schools known retrospectively as the 'six systems' (saddarsana) of mainstream Hindu philosophy."[245][note 37] Michaels notes that a historicization emerged which preceded later nationalism, articulating ideas which glorified Hinduism and the past.[246]
Several scholars suggest that the historical fame and cultural influence of Shankara and Advaita Vedanta was intentionally established during this period.[208][212][213] Vidyaranya (14th c.), also known as Madhava and a follower of Shankara, created legends to turn Shankara, whose elevated philosophy had no appeal to gain widespread popularity, into a "divine folk-hero who spread his teaching through his digvijaya ("universal conquest") all over India like a victorious conqueror."[214][215] In his Savadarsanasamgraha ("Summary of all views") Vidyaranya presented Shankara's teachings as the summit of all darsanas, presenting the other darsanas as partial truths which converged in Shankara's teachings.[214] Vidyaranya enjoyed royal support,[247] and his sponsorship and methodical efforts helped establish Shankara as a rallying symbol of values, spread historical and cultural influence of Shankara's Vedānta philosophies, and establish monasteries (mathas) to expand the cultural influence of Shankara and Advaita Vedānta.[208]
Eastern Ganga and Surya were Hindu polities, which ruled much of present-day Odisha (historically known as Kalinga) from the 11th century until the mid-16th century CE. During the 13th and 14th centuries, when large parts of India were under the rule of Muslim powers, an independent Kalinga became a stronghold of Hindu religion, philosophy, art, and architecture. The Eastern Ganga rulers were great patrons of religion and the arts, and the temples they built are considered among the masterpieces of Hindu architecture.[web 15][web 16]
The fall of Vijayanagara Empire to Muslim rulers had marked the end of Hindu imperial defences in the Deccan. But, taking advantage of an over-stretched Mughal Empire (1526–1857), Hinduism once again rose to political prestige, under the Maratha Empire, from 1674 to 1818.
The Vijayanagara Empire was established in 1336 by Harihara I and his brother Bukka Raya I of Sangama dynasty,[248] which originated as a political heir of the Hoysala Empire, Kakatiya Empire,[249] and the Pandyan Empire.[250] The empire rose to prominence as a culmination of attempts by the south Indian powers to ward off Islamic invasions by the end of the 13th century. According to one narrative, the empire's founders Harihara I and Bukka Raya I were two brothers in the service of the Kampili chief. After Kampili fell to the Muslim invasion, they were taken to Delhi and converted to Islam. They were sent back to Kampili as the Delhi Sultan's vassals. After gaining power in the region, they approached Vidyaranya, who converted them back to the Hindu faith.[251]
The Vijayanagara Emperors were tolerant of all religions and sects, as writings by foreign visitors show.[252] The kings used titles such as Gobrahamana Pratipalanacharya (literally, "protector of cows and Brahmins") and Hindurayasuratrana (lit. "upholder of Hindu faith") that testified to their intention of protecting Hinduism and yet were at the same time staunchly Islamicate in their court ceremonials and dress.[253] The empire's founders, Harihara I and Bukka Raya I, were devout Shaivas (worshippers of Shiva), but made grants to the Vaishnava order of Sringeri with Vidyaranya as their patron saint, and designated Varaha (the boar, an avatar of Vishnu) as their emblem.[254] Over one-fourth of the archaeological dig found an "Islamic Quarter" not far from the "Royal Quarter". Nobles from Central Asia's Timurid kingdoms also came to Vijayanagara. The later Saluva and Tuluva kings were Vaishnava by faith, but worshipped at the feet of Virupaksha (Shiva) at Hampi as well as Venkateswara (Vishnu) at Tirupati. A Sanskrit work, Jambavati Kalyanam by King Krishnadevaraya, called Virupaksha Karnata Rajya Raksha Mani ("protective jewel of Karnata Empire").[255] The kings patronised the saints of the dvaita order (philosophy of dualism) of Madhvacharya at Udupi.[256]
The Bhakti (devotional) movement was active during this time, and involved well known Haridasas (devotee saints) of that time. Like the Virashaiva movement of the 12th century, this movement presented another strong current of devotion, pervading the lives of millions. The haridasas represented two groups, the Vyasakuta and Dasakuta, the former being required to be proficient in the Vedas, Upanishads and other Darshanas, while the Dasakuta merely conveyed the message of Madhvacharya through the Kannada language to the people in the form of devotional songs (Devaranamas and Kirthanas). The philosophy of Madhvacharya was spread by eminent disciples such as Naraharitirtha, Jayatirtha, Sripadaraya, Vyasatirtha, Vadirajatirtha and others.[257] Vyasatirtha, the guru (teacher) of Vadirajatirtha, Purandaradasa (Father of Carnatic music[258][note 38]) and Kanakadasa[259] earned the devotion of King Krishnadevaraya.[260][261][262] The king considered the saint his Kuladevata (family deity) and honoured him in his writings.[web 17] During this time, another great composer of early carnatic music, Annamacharya composed hundreds of Kirthanas in Telugu at Tirumala – Tirupati, in present-day Andhra Pradesh.[263]
The Vijayanagara Empire created an epoch in South Indian history that transcended regionalism by promoting Hinduism as a unifying factor. The empire reached its peak during the rule of Sri Krishnadevaraya when Vijayanagara armies were consistently victorious. The empire annexed areas formerly under the Sultanates in the northern Deccan and the territories in the eastern Deccan, including Kalinga, while simultaneously maintaining control over all its subordinates in the south.[264] Many important monuments were either completed or commissioned during the time of Krishna Deva Raya.
Vijayanagara went into decline after the defeat in the Battle of Talikota (1565). After the death of Aliya Rama Raya in the Battle of Talikota, Tirumala Deva Raya started the Aravidu dynasty, moved and founded a new capital of Penukonda to replace the destroyed Hampi, and attempted to reconstitute the remains of Vijayanagara Empire.[265] Tirumala abdicated in 1572, dividing the remains of his kingdom to his three sons, and pursued a religious life until his death in 1578. The Aravidu dynasty successors ruled the region but the empire collapsed in 1614, and the final remains ended in 1646, from continued wars with the Bijapur Sultanate and others.[266][267][268] During this period, more kingdoms in South India became independent and separate from Vijayanagara. These include the Mysore Kingdom, Keladi Nayaka, Nayaks of Madurai, Nayaks of Tanjore, Nayakas of Chitradurga and Nayak Kingdom of Gingee – all of which declared independence and went on to have a significant impact on the history of South India in the coming centuries.[269]
The Vijayanagara Empire renovated many ancient temples of Ancient Tamilakam, they made significant contributions to temples like Srirangam Ranganathaswamy Perumal temple, Madurai Meenakshi Amman Temple, Kallalagar temple, Rajagopalaswamy Temple, Mannargudi and many more.
Srirangam Ranganathaswamy Perumal temple under the Vijayanagara Empire site saw over 200 years of stability, repairs, first round of fortifications, and addition of mandapas.[271] The Maha Vishnu and Mahalakshmi images were reinstalled and the site became a Hindu temple again in 1371 CE under Kumara Kampana, a Vijayanagara commander and the son of Bukka I.[272] In the last decade of the 14th century, a pillared antechamber was gifted by the Vijayanagara rulers. In the 15th century, they coated the apsidal roofs with solid gold sheets, followed by financing the addition of a series of new shrines, mandapas and gopuras to the temple.[272]The Nayakas fortified the temple town and the seven prakaras. Now this temple is the largest temple compound in India and one of the largest religious complexes in the world.[270][273] Some of these structures have been renovated, expanded and rebuilt over the centuries as a living temple. Srirangam temple is often listed as one of the largest functioning Hindu temple in the world.[274][275]
The official state religion of Mughal India was Islam, with the preference to the jurisprudence of the Hanafi Madhhab (Mazhab). Hinduism remained under strain during Babur and Humanyun's reigns. Sher Shah Suri, the Afghan ruler of North India was comparatively non-repressive. Hinduism came to fore during the three-year rule of Hindu ruler Hemu Vikramaditya during 1553–1556 when he had defeated Akbar at Agra and Delhi and had taken up the reign from Delhi as a Hindu 'Vikramaditya' after his 'Rajyabhishake' or coronation at Purana Quila in Delhi. However, during Mughal history, at times, subjects had the freedom to practise any religion of their choice, though kafir able-bodied adult males with income were obliged to pay the jizya, which signified their status as dhimmis.
Akbar, the Mughal emperor Humayun's son and heir from his Sindhi queen Hameeda Banu Begum, had a broad vision of Indian and Islamic traditions. One of Emperor Akbar's most unusual ideas regarding religion was Din-i-Ilahi (Faith of God), which was an eclectic mix of Islam, Zoroastrianism, Hinduism, Jainism and Christianity. It was proclaimed the state religion until his death. These actions, however, met with stiff opposition from the Muslim clergy, especially the Sufi Shaykh Alf Sani Ahmad Sirhindi. Akbar's abolition of poll-tax on non-Muslims, acceptance of ideas from other religious philosophies, toleration of public worship by all religions and his interest in other faiths showed an attitude of considerable religious tolerance, which, in the minds of his orthodox Muslim opponents, were tantamount to apostasy. Akbar's imperial expansion acquired many Hindu states, many of whom were Hindu Rajputs, through vassalage. The Rajput vassals maintained semi-autonomy in running religious affairs. Many Hindu Rajput vassals built monumental Hindu temples during the period, such as Chaturbhuj Temple and Lakshmi Temple at Orchha, by the Mughal vassal, the Hindu Rajput Orchha State.[276]
Akbar's son, Jahangir, half Rajput, was also a religious moderate, his mother being Hindu. The influence of his two Hindu queens (the Maharani Maanbai and Maharani Jagat) kept religious moderation as a centre-piece of state policy which was extended under his son, Emperor Shah Jahan, who was by blood 75% Rajput and less than 25% Moghul.
Religious orthodoxy would only play an important role during the reign of Shah Jahan's son and successor, Aurangzeb, a devout Sunni Muslim. Aurangzeb was comparatively less tolerant of other faiths than his predecessors had been; and has been subject to controversy and criticism for his policies that abandoned his predecessors' legacy of pluralism, citing his introduction of the jizya tax, doubling of custom duties on Hindus while abolishing it for Muslims, destruction of Hindu temples, forbidding construction and repairs of some non-Muslim temples, and the executions of Maratha ruler Sambhaji[277][278] and the ninth Sikh guru, Guru Tegh Bahadur,[237] and his reign saw an increase in the number and importance of Islamic institutions and scholars. He led many military campaigns against the remaining non-Muslim powers of the Indian subcontinent – the Sikh states of Punjab, the last independent Hindu Rajputs and the Maratha rebels – as also against the Shia Muslim kingdoms of the Deccan. He also virtually stamped out, from his empire, open proselytisation of Hindus and Muslims by foreign Christian missionaries, who remained successfully active, however, in the adjoining regions (i.e regions outside of his dominion) namely: present day Kerala, Tamil Nadu and Goa. The Hindus in Konkan were helped by Marathas, Hindus in Punjab, Kashmir and North India were helped by Sikhs and Hindus in Rajasthan and Central India were helped by Rajputs.
The Hindu Marathas had resisted incursions into the region by the Muslim Mughal rulers of northern India. Under their ambitious leader Chhatrapati Shivaji Maharaj, the Maratha freed themselves from the Muslim sultans of Bijapur to the southeast and, becoming much more aggressive, began to frequently raid Mughal territory. The Marathas had spread and conquered much of central India by Shivaji's death in 1680. Subsequently, under the able leadership of Brahmin prime ministers (Peshwas), the Maratha Empire reached its zenith; Pune, the seat of Peshwas, flowered as a centre of Hindu learning and traditions. The empire at its peak stretched from Tamil Nadu[279] in the south, to Peshawar, present day Khyber Pakhtunkhwa[280][note 39] in the north, and Bengal in the east.[web 18]
King Prithvi Narayan Shah, the last Gorkhali monarch, self-proclaimed the newly unified Kingdom of Nepal as Asal Hindustan ("Real Land of Hindus") due to North India being ruled by the Islamic Mughal rulers. The proclamation was done to enforce Hindu social code Dharmaśāstra over his reign and refer to his country as being inhabitable for Hindus. He also referred Northern India as Mughlan (Country of Mughals) and called the region infiltrated by Muslim foreigners.[283]
After the Gorkhali conquest of Kathmandu valley, King Prithvi Narayan Shah expelled the Christian Capuchin missionaries from Patan and revisioned Nepal as Asal Hindustan ("real land of Hindus").[284] The Hindu Tagadharis, a Nepalese Hindu socio-religious group, were given the privileged status in the Nepalese capital thereafter.[285][286] Since then Hinduisation became the significant policy of the Kingdom of Nepal.[284] Professor Harka Gurung speculates that the presence of Islamic Mughal rule and Christian British rule in India had compelled the foundation of Brahmin Orthodoxy in Nepal for the purpose building a haven for Hindus in the Kingdom of Nepal.[284]
The Goa Inquisition was the office of the Christian Inquisition acting in the Indian city of Goa and the rest of the Portuguese empire in Asia. Francis Xavier, in a 1545 letter to John III, requested for an Inquisition to be installed in Goa. It was installed eight years after the death of Francis Xavier in 1552. Established in 1560 and operating until 1774, this highly controversial institution was aimed primarily at Hindus and wayward new converts.[288]
The Battle of Plassey would see the emergence of the British as a political power; their rule later expanded to cover much of India over the next hundred years, conquering all of the Hindu states on the Indian subcontinent,[289] with the exception of the Kingdom of Nepal. While the Maratha Empire remained the preeminent power in India, making it the last remaining Hindu empire,[290] until their defeat in the Third Anglo-Maratha War which left the East India Company in control of most of India; as noted by acting Governor-General Charles Metcalfe, after surveying and analyzing the conditions in India, in 1806 wrote: "India contains no more than two great powers, British and Mahratta."[291][292] During this period, Northeastern India was divided into many kingdoms, most notable being the Kingdom of Manipur, which ruled from their seat of power at Kangla Palace and developed a sophisticated Hindu Gaudiya Vaishnavism culture, later the kingdom became a princely state of the British.[293][294][295] The Kingdom of Mysore was defeated in the Fourth Anglo-Mysore War by the British East India Company, leading to the reinstatement of the Hindu Wadiyar dynasty in Mysore as a princely states.[296] In 1817, the British went to war with the Pindaris, raiders who were based in Maratha territory, which quickly became the Third Anglo-Maratha War, and the British government offered its protection to the mainly Hindu Rajput rulers of Rajputana from the Pindaris and the Marathas.[297] The mainly Hindu Palaiyakkarar states emerged from the fall of the Vijayanagara Empire, and were a bastion of Hindu resistance; and managed to weather invasions and survive till the advent of the British.[web 19] From 1799 to 1849, the Sikh Empire, ruled by members of the Sikh religion, emerged as the last major indigenous power in the Northwest of the Indian subcontinent under the leadership of Maharaja Ranjit Singh.[298][299] After the death of Ranjit Singh, the empire weakened, alienating Hindu vassals and Wazirs, and leading to the conflict with the British East India Company, marked the downfall of the Sikh Empire, making it the last area of the Indian subcontinent to be conquered by the British. The entire subcontinent fell under British rule (partly indirectly, via princely states) following the Indian Rebellion of 1857.
With the onset of the British Raj, the colonization of India by the British, there also started a Hindu Renaissance in the 19th century, which profoundly changed the understanding of Hinduism in both India and the west.[302] Indology as an academic discipline of studying Indian culture from a European perspective was established in the 19th century, led by scholars such as Max Müller and John Woodroffe. They brought Vedic, Puranic and Tantric literature and philosophy to Europe and the United States. Western orientalist searched for the "essence" of the Indian religions, discerning this in the Vedas,[303] and meanwhile creating the notion of "Hinduism" as a unified body of religious praxis[135] and the popular picture of 'mystical India'.[135][302] This idea of a Vedic essence was taken over by Hindu reform movements as the Brahmo Samaj, which was supported for a while by the Unitarian Church,[304] together with the ideas of Universalism and Perennialism, the idea that all religions share a common mystic ground.[305] This "Hindu modernism", with proponents like Vivekananda, Aurobindo, Rabindranath and Radhakrishnan, became central in the popular understanding of Hinduism.[306][307][308][309][135]
During the 19th century, Hinduism developed a large number of new religious movements, partly inspired by the European Romanticism, nationalism, scientific racism and esotericism (Theosophy) popular at the time (while conversely and contemporaneously, India had a similar effect on European culture with Orientalism, "Hindoo style" architecture, reception of Buddhism in the West and similar). According to Paul Hacker, "the ethical values of Neo-Hinduism stem from Western philosophy and Christianity, although they are expressed in Hindu terms".[310]
These reform movements are summarised under Hindu revivalism and continue into the present.
An important development during the British colonial period was the influence Hindu traditions began to form on Western thought and new religious movements. An early champion of Indian-inspired thought in the West was Arthur Schopenhauer who in the 1850s advocated ethics based on an "Aryan-Vedic theme of spiritual self-conquest", as opposed to the ignorant drive toward earthly utopianism of the superficially this-worldly "Jewish" spirit.[317] Helena Blavatsky moved to India in 1879, and her Theosophical Society, founded in New York in 1875, evolved into a peculiar mixture of Western occultism and Hindu mysticism over the last years of her life.
The sojourn of Swami Vivekananda to the World Parliament of Religions in Chicago in 1893 had a lasting effect. Vivekananda founded the Ramakrishna Mission, a Hindu missionary organisation still active today.
In the early 20th century, Western occultists influenced by Hinduism include Maximiani Portaz – an advocate of "Aryan Paganism" – who styled herself Savitri Devi and Jakob Wilhelm Hauer, founder of the German Faith Movement. It was in this period, and until the 1920s, that the swastika became a ubiquitous symbol of good luck in the West before its association with the Nazi Party became dominant in the 1930s.
Hinduism-inspired elements in Theosophy were also inherited by the spin-off movements of Ariosophy and Anthroposophy and ultimately contributed to the renewed New Age boom of the 1960s to 1980s, the term New Age itself deriving from Blavatsky's 1888 The Secret Doctrine.
Influential 20th-century Hindus were Ramana Maharshi, B. K. S. Iyengar, Paramahansa Yogananda, Prabhupada (founder of ISKCON), Sri Chinmoy, Swami Rama and others who translated, reformulated and presented Hinduism's foundational texts for contemporary audiences in new iterations, raising the profiles of Yoga and Vedanta in the West and attracting followers and attention in India and abroad.
Hinduism is followed by around 1.1 billion people in India.[web 20] Other significant populations are found in Nepal (21.5 million), Bangladesh (13.1 million), and the Indonesian island of Bali (3.9 million).[web 21] The majority of the Vietnamese Cham people also follow Hinduism, with the largest proportion in Ninh Thuận province.[web 22]
In modern times Smarta-views have been highly influential in both the Indian[web 23] and western[web 24] understanding of Hinduism via Neo-Vedanta. Vivekananda was an advocate of Smarta-views,[web 24] and Radhakrishnan was himself a Smarta-Brahman.[318][319] According to iskcon.org,
Many Hindus may not strictly identify themselves as Smartas but, by adhering to Advaita Vedanta as a foundation for non-sectarianism, are indirect followers.[web 23]
Influential in spreading Hinduism to a western audience were Swami Vivekananda, Paramahansa Yogananda, A. C. Bhaktivedanta Swami Prabhupada (Hare Krishna movement), Sri Aurobindo, Meher Baba, Maharishi Mahesh Yogi (Transcendental Meditation), Jiddu Krishnamurti, Sathya Sai Baba, Mother Meera, among others.
In the 20th century, Hinduism also gained prominence as a political force and a source for national identity in India. With origins traced back to the establishment of the Hindu Mahasabha in the 1910s, the movement grew with the formulation and development of the Hindutva ideology in the following decades; the establishment of Rashtriya Swayamsevak Sangh (RSS) in 1925; and the entry, and later success, of RSS offshoots Jana Sangha and Bharatiya Janata Party (BJP) in electoral politics in post-independence India.[320] Hindu religiosity plays an important role in the nationalist movement.[321][note 40][note 41]
Besides India, the idea of Hindu nationalism and Hindutva can also be seen in the other areas with good population of Hindus, such as in Nepal, Bangladesh, Sri Lanka and Malaysia.[web 25][322][323] In the modern world, the Hindu identity and nationalism is encouraged by many organisations as per their areas and territories. In India, Sangh Parivar is the umbrella organisation for most of the Hindu nationalist organisations, including that of Rashtriya Swayamsevak Sangh, Bharatiya Janata Party, Vishva Hindu Parishad, etc.[324][325] The other nationalist organisations include Siva Senai (Sri Lanka), Nepal Shivsena, Rastriya Prajatantra Party, Hindu Prajatantrik Party, (Nepal) Bangabhumi (Bangladesh) and HINDRAF (Malaysia).
Samuel (2010, p. 199): "By the first and second centuries CE, the Dravidian-speaking regions of the south were also increasingly being incorporated into the general North and Central Indian cultural pattern, as were parts at least of Southeast Asia. The Pallava kingdom in South India was largely Brahmanical in orientation although it included a substantial Jain and Buddhist population, while Indic states were also beginning to develop in Southeast Asia."
Subnotes

Nakedness and clothing use are characteristics of humans related by evolutionary and social prehistory. The major loss of body hair distinguishes humans from other primates. Current evidence indicates that anatomically modern humans were naked in prehistory for at least 90,000 years before they invented clothing. Today, isolated Indigenous peoples in tropical climates continue to be without clothing in many everyday activities.
The general hairlessness of humans in comparison to related species may be due to loss of functionality in the pseudogene KRT41P (which helps produce keratin) in the human lineage about 240,000 years ago.[1] On an individual basis, mutations in the gene HR can lead to complete hair loss, though this is not typical in humans.[2] Humans may also lose their hair as a result of hormonal imbalance due to drugs or pregnancy.[3]
In order to comprehend why humans have significantly less body hair than other primates, one must understand that mammalian body hair is not merely an aesthetic characteristic; it protects the skin from wounds, bites, heat, cold, and UV radiation.[4] Additionally, it can be used as a communication tool and as a camouflage.[5]
The first member of the genus  Homo to be hairless was Homo erectus, originating about 1.6 million years ago.[6] The dissipation of body heat remains the most widely accepted evolutionary explanation for the loss of body hair in early members of the genus Homo, the surviving member of which is modern humans.[7][8][9] Less hair, and an increase in sweat glands, made it easier for their bodies to cool when they moved from living in shady forest to open savanna. This change in environment also resulted in a change in diet, from largely vegetarian to hunting. Pursuing game on the savanna also increased the need for regulation of body heat.[10][11]
Anthropologist and paleo-biologist Nina Jablonski posits that the ability to dissipate excess body heat through eccrine sweating helped make possible the dramatic enlargement of the brain, the most temperature-sensitive human organ.[12] Thus the loss of fur was also a factor in further adaptations, both physical and behavioral, that differentiated humans from other primates. Some of these changes are thought to be the result of sexual selection. By selecting more hairless mates, humans accelerated changes initiated by natural selection. Sexual selection may also account for the remaining human hair in the pubic area and armpits, which are sites for pheromones, while hair on the head continued to provide protection from the sun.[13] Anatomically modern humans, whose traits include hairlessness, evolved 260,000 to 350,000 years ago.[14]
Humans are the only primate species that have undergone significant hair loss and of the approximately 5000 extant species of mammal, only a handful are effectively hairless. This list includes elephants, rhinoceroses, hippopotamuses, walruses, some species of pigs, whales and other cetaceans, and naked mole rats.[5] Most mammals have light skin that is covered by fur, and biologists believe that early human ancestors started out this way also. Dark skin probably evolved after humans lost their body fur, because the naked skin was vulnerable to the strong UV radiation as explained in the Out of Africa hypothesis. Therefore, evidence of the time when human skin darkened has been used to date the loss of human body hair, assuming that the dark skin was needed after the fur was gone.
With the loss of fur, darker, high-melanin skin evolved as a protection from ultraviolet radiation damage.[15] As humans migrated outside of the tropics, varying degrees of depigmentation evolved in order to permit UVB-induced synthesis of previtamin D3.[16][17] The relative lightness of female compared to male skin in a given population may be due to the greater need for women to produce more vitamin D during lactation.[18]
The sweat glands in humans could have evolved to spread from the hands and feet as the body hair changed, or the hair change could have occurred to facilitate sweating. Horses and humans are two of the few animals capable of sweating on most of their body, yet horses are larger and still have fully developed fur. In humans, the skin hairs lie flat in hot conditions, as the arrector pili muscles relax, preventing heat from being trapped by a layer of still air between the hairs, and increasing heat loss by convection.
Another hypothesis for the thick body hair on humans proposes that Fisherian runaway sexual selection played a role (as well as in the selection of long head hair), (see terminal and vellus hair), as well as a much larger role of testosterone in men. Sexual selection is the only theory thus far that explains the sexual dimorphism seen in the hair patterns of men and women. On average, men have more body hair than women. Males have more terminal hair, especially on the face, chest, abdomen, and back, and females have more vellus hair, which is less visible. The halting of hair development at a juvenile stage, vellus hair, would also be consistent with the neoteny evident in humans, especially in females, and thus they could have occurred at the same time.[19] This theory, however, has significant holdings in today's cultural norms. There is no evidence that sexual selection would proceed to such a drastic extent over a million years ago when a full, lush coat of hair would most likely indicate health and would therefore be more likely to be selected for, not against.
The aquatic ape hypothesis (AAH) includes hair loss as one of several characteristics of modern humans that could indicate adaptations to an aquatic environment. Serious consideration may be given by contemporary anthropologists to some hypotheses related to AAH, but hair loss is not one of them.[20]
A divergent explanation of humans' relative hairlessness holds that ectoparasites (such as ticks) residing in fur became problematic as humans became hunters living in larger groups with a "home base".[21][22] Nakedness would also make the lack of parasites apparent to prospective mates.[23] However, this theory is inconsistent with the abundance of parasites that continue to exist in the remaining patches of human hair.[24]
The "ectoparasite" explanation of modern human nakedness is based on the principle that a hairless primate would harbor fewer parasites. When our ancestors adopted group-dwelling social arrangements roughly 1.8 mya, ectoparasite loads increased dramatically. Early humans became the only one of the 193 primate species to have fleas, which can be attributed to the close living arrangements of large groups of individuals. While primate species have communal sleeping arrangements, these groups are always on the move and thus are less likely to harbor ectoparasites.
It was expected that dating the split of the ancestral human louse into two species, the head louse and the pubic louse, would date the loss of body hair in human ancestors. However, it turned out that the human pubic louse does not descend from the ancestral human louse, but from the gorilla louse, diverging 3.3 million years ago. This suggests that humans had lost body hair (but retained head hair) and developed thick pubic hair prior to this date, were living in or close to the forest where gorillas lived, and acquired pubic lice from butchering gorillas or sleeping in their nests.[25][26] The evolution of the body louse from the head louse, on the other hand, places the date of clothing much later, some 100,000 years ago.[27][28]
Another hypothesis is that humans' use of fire caused or initiated the reduction in human hair.[29]
Another view is proposed by James Giles, who attempts to explain hairlessness as evolved from the relationship between mother and child, and as a consequence of bipedalism. Giles also connects romantic love to hairlessness.[30][31]
The last common ancestor of humans and chimpanzees was only partially bipedal, often using their front legs for locomotion and their rear legs for grasping branches when they climbed.
According to this view, the loss of grasping or prehensile feet--a loss that was necessary for the adaptation to walking--meant that infants could no longer grasp their mothers' fur effectively, something that all other primate infants must do to stay with the mother. But bipedal mothers could compensate for this by carrying their infants with their free hands.[32] Bearing an infant for more than a short distance, however, is hard work. Even today few women choose to carry their infants for long, preferring to use a pram or baby sling. Hairless mothers who had hairless infants had the pleasure of skin-to-skin contact with their infants. This would have given them extra incentive to hold and carry their infants. This 'maternal selection' would have favored the survival of hairless infants.
This explains why women and children are more hairless than adult males--something other accounts have a difficulty doing. Since these hairless individuals had the pleasure of skin-to-skin contact as infants, they would have probably preferred more-or-less hairless sexual partners when they matured. Thus maternal selection for hairlessness would have been reinforced by this sexual selection
The combination of hairlessness and upright posture may also explain the enlargement of the female breasts as a sexual signal.[9] Giles also argues that this explains long head-hair, pubic hair, and distinctive underarm hair, hair that other primates do not have. Nakedness also affects sexual relationships as well, the duration of human intercourse being many times the duration of any other primates.[24]
A 2010 study published in Molecular Biology and Evolution indicates that the habitual wearing of clothing began at some point in time between 83,000 years ago and 170,000 years ago based upon a genetic analysis indicating when clothing lice diverged from their head louse ancestors. This information suggests that the use of clothing likely originated with anatomically modern humans in Africa prior to their migration to colder climates, allowing them to do so.[33]
Some of the technology for what is now called clothing may have originated to make other types of adornment, including jewelry, body paint, tattoos, and other body modifications, "dressing" the naked body without concealing it.[34][35] According to Mark Leary and Nicole R. Buttermore, body adornment is one of the changes that occurred in the late Paleolithic (40,000 to 60,000 years ago) in which humans became not only anatomically modern, but also behaviorally modern and capable of self-reflection and symbolic interaction.[36] More recent studies place the use of adornment at 77,000 years ago in South Africa, and 90,000—100,000 years ago in Israel and Algeria.[37] While modesty may be a factor, often overlooked purposes for body coverings are camouflage used by hunters, body armor, and costumes used to impersonate "spirit-beings".[38]
The origin of complex, fitted clothing required the invention of fine stone knives for cutting skins into pieces, and the eyed needle for sewing. This was done by Cro-Magnons, who migrated to Europe around 35,000 years ago.[39] The Neanderthal occupied the same region, but became extinct in part because they could not make fitted garments, but draped themselves with crudely cut skins—based upon their simple stone tools—which did not provide the warmth needed to survive as the climate grew colder in the Last Glacial Period.[40] In addition to being less functional, the simple wrappings would not have been habitually worn by Neanderthal due to their being more cold-tolerant than Homo sapiens, and would not have acquired the secondary functions of decoration and promoting modesty.[41]
The earliest archeological evidence of fabric clothing is inferred from representations in figurines in the southern Levant dated between 11,700 and 10,500 years ago.[42] The surviving examples of woven cloth are linen from Egypt dated 5,000 BCE, although knotted or twisted flax fibers have been found as early as 7000 BCE.[43]
While adults are rarely completely naked in modern societies, covering at least their genitals, adornments and clothing often emphasize, enhance, or otherwise call attention to the sexuality of the body.[44]

The following tables give an overview of notable finds of hominin fossils and remains relating to human evolution, beginning with the formation of the tribe Hominini (the divergence of the human and chimpanzee lineages) in the late Miocene, roughly 7 to 8 million years ago.
As there are thousands of fossils, mostly fragmentary, often consisting of single bones or isolated teeth with complete skulls and skeletons rare, this overview is not complete, but shows some of the most important findings. The fossils are arranged by approximate age as determined by radiometric dating and/or incremental dating and the species name represents current consensus; if there is no clear scientific consensus the other possible classifications are indicated.
The early fossils shown are not considered ancestors to Homo sapiens but are closely related to ancestors and are therefore important to the study of the lineage. After 1.5 million years ago (extinction of Paranthropus), all fossils shown are human (genus Homo). After 11,500 years ago (11.5 ka, beginning of the Holocene), all fossils shown are Homo sapiens (anatomically modern humans), illustrating recent divergence in the formation of modern human sub-populations.
The chimpanzee–human divergence likely took place around 10 to 7 million years ago.[1] The list of fossils begins with Graecopithecus, dated some 7.2 million years ago, which may or may not still be ancestral to both the human and the chimpanzee lineage. For the earlier history of the human lineage, see Timeline of human evolution#Hominidae, Hominidae#Phylogeny.
(Niño de la Gran Dolina 342)
Homo neanderthalensis
or
Denisovan

Fertile Crescent:
Europe:
Africa:
Siberia:
The Middle Paleolithic (or Middle Palaeolithic) is the second subdivision of the Paleolithic  or Old Stone Age  as it is understood in Europe, Africa and Asia. The term Middle Stone Age is used as an equivalent or a synonym for the Middle Paleolithic in African archeology.[1] The Middle Paleolithic broadly spanned from 300,000 to 50,000 years ago. There are considerable dating differences between regions. The Middle Paleolithic was succeeded by the Upper Paleolithic subdivision which first began between 50,000 and  40,000 years ago.[1] Pettit and White date the Early Middle Paleolithic in Great Britain to about 325,000 to 180,000 years ago (late Marine Isotope Stage 9 to late Marine Isotope Stage 7), and the Late Middle Paleolithic as about 60,000 to 35,000 years ago.[2] The Middle Paleolithic was in the geological Chibanian (Middle Pleistocene) and Late Pleistocene ages.
According to the theory of the recent African origin of modern humans, anatomically modern humans began migrating out of Africa during the Middle Stone Age/Middle Paleolithic around 125,000 years ago and began to replace other Homo species such as the Neanderthals and   Homo erectus.
The earliest evidence of behavioral modernity first appears during the Middle Paleolithic; undisputed evidence of behavioral modernity, however, only becomes common during the following Upper Paleolithic period.[1]
Middle Paleolithic burials at sites such as Krapina in Croatia (dated to c. 130,000 BP) and the Qafzeh and Es Skhul caves in Israel (c. 100,000 BP) have led some anthropologists and archeologists (such as Philip Lieberman) to believe that Middle Paleolithic cultures may have possessed a developing religious ideology which included concepts such as an afterlife; other scholars suggest the bodies were buried for secular reasons.[3][4]
According to recent[when?] archeological findings from Homo heidelbergensis sites in the Atapuerca Mountains, the practice of intentional burial may have begun much earlier during the late Lower Paleolithic, but this theory is widely questioned in the scientific community. Cut-marks on Neandertal bones from various sites – such as Combe Grenal and the Moula rock shelter in France – may imply that Neanderthals, like some contemporary human cultures, may have practiced excarnation for presumably religious reasons (see Neanderthal behavior § Cannibalism or ritual defleshing?).
The earliest undisputed evidence of artistic expression during the Paleolithic period comes from Middle Paleolithic/Middle Stone Age sites such as Blombos Cave in the form of bracelets,[5] beads,[6] art rock,[7] ochre used as body paint and perhaps in ritual,[1][7] though earlier examples of artistic expression such as the Venus of Tan-Tan and the patterns found on elephant bones from Bilzingsleben in Thuringia may have been produced by Acheulean tool-users such as Homo erectus prior to the start of the Middle Paleolithic period.[8] Activities such as catching large fish and hunting large game animals with specialized tools indicate increased group-wide cooperation and more elaborate social organization.[1]
In addition to developing advanced cultural traits, humans also first began to take part in long-distance trade between groups for rare commodities (such as ochre (which was often used for religious purposes such as ritual[7][9])) and raw materials during the Middle Paleolithic as early as 120,000 years ago.[1][10]
Inter-group trade may have appeared during the Middle Paleolithic because trade between bands would have helped ensure their survival by allowing them to exchange resources and commodities such as raw materials during times of relative scarcity (i.e., famine or drought).[10]
Evidence from archeology and comparative ethnography indicates that Middle Paleolithic people lived in small, egalitarian band societies similar to those of Upper Paleolithic societies and some modern hunter-gatherers such as the ǃKung and Mbuti peoples.[1][11] Both Neanderthal and modern human societies took care of the elderly members of their societies during the Middle Paleolithic.[10] Christopher Boehm (1999) has hypothesized that egalitarianism may have arisen in Middle Paleolithic societies because of a need to distribute resources such as food and meat equally to avoid famine and ensure a stable food supply.[12]
It has usually been assumed that women gathered plants and firewood and men hunted and scavenged dead animals through the Paleolithic.[13] However,  Steven L. Kuhn and Mary Stiner from the University of Arizona suggest that this sex-based division of labor did not exist prior to the Upper Paleolithic. The sexual division of labor may have evolved after 45,000 years ago to allow humans to acquire food and other resources more efficiently.[13]
Although gathering and hunting comprised most of the food supply during the Middle Paleolithic, people began to supplement their diet with seafood and began smoking and drying meat to preserve and store it. For instance the Middle Stone Age inhabitants of the region now occupied by the Democratic Republic of the Congo hunted large 1.8-metre (6 ft) long catfish with specialized barbed fishing points as early as 90,000 years ago,[1][14] and Neandertals and Middle Paleolithic Homo sapiens in Africa began to catch shellfish for food as revealed by shellfish cooking in Neanderthal sites in Italy about 110,000 years ago and Middle Paleolithic Homo sapiens sites at Pinnacle Point, in Africa.[1][15]
Anthropologists such as Tim D. White suggest that cannibalism was common in human societies prior to the beginning of the Upper Paleolithic, based on the large amount of "butchered human" bones found in Neandertal and other Middle Paleolithic sites.[16] Cannibalism in the Middle Paleolithic may have occurred because of food shortages.[17]
However it is also possible that Middle Paleolithic cannibalism occurred for religious reasons which would coincide with the development of religious practices thought to have occurred during the Upper Paleolithic.[18][19] Nonetheless it remains possible that Middle Paleolithic societies never practiced cannibalism and that the damage to recovered human bones was either the result of excarnation or predation by carnivores such as saber-toothed cats, lions and hyenas.[19]
Around 200,000 BP Middle Paleolithic Stone tool manufacturing spawned a tool-making technique known as the prepared-core technique, that was more elaborate than previous Acheulean techniques.[20][21] Wallace and Shea split the core artifacts into two different types: formal cores and expedient cores. Formal cores are designed to extract the maximum amount from the raw material while expedient cores are based more upon functional need.[22] This method increased efficiency by permitting the creation of more controlled and consistent flakes.[21] This method allowed Middle Paleolithic humans correspondingly to create stone-tipped spears, which were the earliest composite tools, by hafting sharp, pointy stone flakes onto wooden shafts. Paleolithic groups such as the Neanderthals who possessed a Middle Paleolithic level of technology appear to have hunted large game just as well as Upper Paleolithic modern humans[23] and the Neanderthals in particular may have likewise hunted with projectile weapons.[24]
Nonetheless Neanderthal usage of projectile weapons in hunting occurred very rarely (or perhaps never) and the Neanderthals hunted large game animals mostly by ambushing them and attacking them with mêlée weapons such as thrusting spears rather than attacking them from a distance with projectile weapons.[10][25] An ongoing controversy about the nature of Middle Paleolithic tools is whether there were a series of functionally specific and preconceived tool forms or whether there was a simple continuum of tool morphology that reflect the extent of edge maintenance, as Harold L. Dibble has suggested.[26]
The use of fire became widespread for the first time in human prehistory during the Middle Paleolithic, and humans began to cook their food c. 250,000 years ago.[27][28] Some scientists have hypothesized that hominids began cooking food to defrost frozen meat which would help ensure their survival in cold regions.[28] Robert K. Wayne, a molecular biologist, has controversially claimed, based on a comparison of canine DNA, that dogs may have been first domesticated during the Middle Paleolithic around or even before 100,000 BCE.[29]

Geological Evidences of the Antiquity of Man is a book written by British geologist, Charles Lyell in 1863.  The first three editions appeared in February, April, and November 1863, respectively.  A much-revised fourth edition appeared in 1873.  Antiquity of Man, as it was known to contemporary readers, dealt with three scientific issues that had become prominent in the preceding decade: the age of the human race, the existence of ice ages, and Charles Darwin's theory of evolution by natural selection.  Lyell used the book to reverse or modify his own long-held positions on all three issues.  The book drew sharp criticism from two of Lyell's younger colleagues – paleontologist Hugh Falconer and archaeologist John Lubbock – who felt that Lyell had used their work too freely and acknowledged it too sparingly.  It sold well, however, and (along with Lubbock's 1865 book Prehistoric Times) helped to establish the new science of prehistoric archaeology in Great Britain.
Lyell had been consistently skeptical of evidence for high human antiquity since the early 1830s, and distanced himself from the theory of ice ages after a brief flirtation with it in the early 1840s.  He attacked the evolutionary ideas of Lamarck in detail in his book Principles of Geology.  New developments in all three areas forced him to reconsider these positions in the late 1850s and early 1860s, and became the subject matter for Antiquity of Man.
The section about man summed up the evidence for human antiquity that had been brought to light by British geologists in 1858-59, and integrated it with archaeological evidence from the Paleolithic, Neolithic, and Bronze Age.
The section about glaciation integrated continental ice ages into the larger picture of the  Quaternary Period that Lyell had built up in his earlier works.
The section about evolution recapitulated Darwin's arguments and endorsed them, though not enthusiastically.  It acknowledged that human bodies might have evolved, but left open the possibility of divine intervention in the origins of human intellect and moral sense.
Hugh Falconer, a key player in the establishment of human antiquity, charged that Lyell – a minor player in the process – had misleadingly cast himself in the lead while ignoring the contributions of others.  He raised his charges in the pages of the weekly journal The Athenaeum, and pressed them with a vehemence that some of his colleagues found distasteful.
John Lubbock, a young but rising scientific star and a member of Darwin's inner circle, charged that Lyell had incorporated large amounts of material that Lubbock had published in articles and was then reworked into a book of his own.  His criticism was largely private, but well known in the scientific circles in which both moved.
Lyell gradually changed the text of Antiquity of Man to blunt some of their criticisms, but throughout the process held that he had been wrongly accused. Lyell's response is recorded in his archive, held at the University of Edinburgh's Heritage Collections, who have also created a dedicated website to Lyell's work.
Antiquity of Man had its greatest impact in the years immediately after its publication.  Lyell's presentation and endorsement of the new evidence for human antiquity firmly established the theory as scientific orthodoxy.  His integration of both ice ages and a very old human race into the (geologically) recent history of the Earth was novel for its time, as was his presentation of archaeological data that from continental Europe.  Until the early 1860s, "archaeology" had been synonymous, in England, with the study of antiquity and the Middle Ages through artifacts.  Antiquity of Man expanded it to include the study of prehistory.[1]
The book's three-part structure meant, however, that it was quickly supplanted by more detailed works that followed in its wake.  Lubbock's Prehistoric Times (1865), Darwin's The Descent of Man (1871), James Archibald Geike's The Great Ice Age (1874) and William Boyd Dawkins' Early Man in Britain (1880) became the standard works on the fields to which Lyell had introduced a generation of mid-Victorian readers.
The full text of Geological Evidences of the Antiquity of Man at Wikisource

Alice May Roberts FRSB (born 19 May 1973)[2] is an English academic, TV presenter and author. Since 2012 she has been Professor of Public Engagement in Science at the University of Birmingham. She was president of the charity Humanists UK between January 2019 and May 2022.[3] She is now a vice-president of the organisation.[4]
Roberts was born in Bristol in 1973,[5] the daughter of an aeronautical engineer and an English and arts teacher.[6] She grew up in the Bristol suburb of Westbury-on-Trym where she attended the private Red Maids' School.[5][7][8] In December 1988, she won the BBC1 Blue Peter Young Artists competition, appearing with her picture and the presenters on the front cover of the 10 December 1988 edition of the Radio Times.[9]
Roberts studied medicine at the University of Wales College of Medicine (now part of Cardiff University) and graduated in 1997 with a Bachelor of Medicine, Bachelor of Surgery (MB BCh) degree, having gained an intercalated Bachelor of Science degree in anatomy.[7][10][11]
After graduating, Roberts worked as a junior doctor with the National Health Service in South Wales for 18 months. In 1998, she left clinical medicine and worked as an anatomy demonstrator at the University of Bristol, becoming a lecturer there in 1999.[5][7][12]
She spent seven years working part-time on her PhD in paleopathology, receiving the degree in 2008.[5][7][13] She was a senior teaching fellow at the University of Bristol Centre for Comparative and Clinical Anatomy, where her main roles were teaching clinical anatomy, embryology, and physical anthropology, as well as researching osteoarchaeology and paleopathology.[7][10][14] She stated in 2009 that she was working towards becoming a professor of anatomy.[15]
In 2009, she co-presented modules for the Beating Bipolar programme, the first online treatment package for bipolar depression, trialled by Cardiff University researchers. A clinical trial began in June 2009 involving about 100 patients with bipolar disorder in South Wales.[16]
From August 2009 until January 2012, Roberts was a visiting fellow in both the Department of Archaeology and Anthropology and the Department of Anatomy of the University of Bristol.[10][17][18]
From 2009 to 2016 Roberts was Director of Anatomy at the NHS Severn Deanery School of Surgery[11] and also an honorary fellow at Hull York Medical School.[19][20]
In February 2012, Roberts was appointed the University of Birmingham's first Professor of Public Engagement in Science.[21][22][23]
Roberts has been a member of the advisory board of Cheltenham Science Festival for 10 years and a member of the Advisory Board of the Milner Centre for Evolution at the University of Bath since 2018.[24]
Writing in the i newspaper in 2016, Roberts dismissed the aquatic ape hypothesis (AAH) as a distraction "from the emerging story of human evolution that is more interesting and complex", adding that AAH has become "a theory of everything" that is simultaneously "too extravagant and too simple". She concluded by saying that "science is about evidence, not wishful thinking".[25][26]
Roberts and Aoife McLysaght co-presented the 2018 Royal Institution Christmas Lectures in London.[24][27] She was president of the British Science Association for the year 2019–2020.[28]
In January 2021, Roberts presented a 10-part narrative history series about the human body entitled Bodies on BBC Radio 4.[29]
A presenter of science and history television documentaries, Roberts was one of the regular co-presenters of the BBC geographical and environmental series Coast.[30]
Roberts first appeared on television in the Time Team Live 2001 episode,[31][32] working on Anglo-Saxon burials at Breamore, Hampshire. She served as a bone specialist and general presenter in many episodes, including the spin-off series Extreme Archaeology. In August 2006, a Time Team special episode Big Royal Dig investigated archaeology of Britain's royal palaces; Roberts was one of the main presenters.
Roberts wrote and presented a BBC Two series on anatomy and health entitled Dr Alice Roberts: Don't Die Young, which was broadcast from January 2007.[33] She presented a five-part series on human evolution and early human migrations for that channel entitled The Incredible Human Journey, beginning on 10 May 2009.[34] In September 2009, she co-presented (with Mark Hamilton) A Necessary Evil?, a one-hour documentary about the Burke and Hare murders.[35]
In August 2010, she presented a one-hour documentary on BBC Four, Wild Swimming, inspired by Roger Deakin's book Waterlog.[36] Roberts presented a four-part BBC Two series on archaeology in August–September 2010, Digging for Britain.[37][38] Roberts explained, "We're taking a fresh approach by showing British archaeology as it's happening out in the field, from the excitement of artefacts as they come out of the ground, through to analysing them in the lab and working out what they tell us about human history."[39] The series returned in 2011 and again (on BBC Four) in 2015, 2016, 2017, 2018, 2019, 2020, 2022, 2023 and 2024.[40]
In March 2011, she presented a BBC documentary in the Horizon series entitled Are We Still Evolving?[41] Later in 2011, she presented another BBC documentary called How to Build a Dinosaur,[42] which aired on BBC4 on 21 September 2011.
She presented the series Origins of Us, which aired on BBC Two in October 2011, examining how the human body has adapted through seven million years of evolution.[43] The last part of this series featured Roberts visiting the Rift Valley in East Africa.
In April 2012, Roberts presented Woolly Mammoth: Secrets from the Ice on BBC Two.[44] From 22 to 24 October 2012, she appeared, with co-presenter Dr George McGavin, in the BBC series Prehistoric Autopsy,[45] which discussed the remains of early hominins such as Neanderthals, Homo erectus and Australopithecus afarensis. In May and June 2013 she presented the BBC Two series Ice Age Giants.[46] In September 2014, she was a presenter on the Horizon programme Is Your Brain Male or Female?[47]
In October 2014, she presented Spider House.[48] In 2015, she co-presented a 3-part BBC TV documentary with Neil Oliver entitled The Celts: Blood, Iron and Sacrifice[49] and wrote a book to tie in with the series: The Celts: Search for a Civilisation.[50] In April–May 2016, she co-presented the BBC Two programme Food Detectives which looked at food nutrition and its effects on the body. In August 2016, she presented the BBC Four documentary Britain's Pompeii: A Village Lost in Time, which explored the Must Farm Bronze Age settlement in Cambridgeshire.[51] In May 2017, she was a presenter of the BBC Two documentary The Day The Dinosaurs Died.[52] In April 2018, she presented the six-part Channel 4 series Britain's Most Historic Towns,[53] which examines the history of British towns, which was followed by a second series in May 2019 and a third series in November 2020.
In September 2018, she presented the BBC Two documentary King Arthur's Britain: The Truth Unearthed, which examines new archaeological discoveries that cast light on the political and trading situation in Britain during the Early Middle Ages.[54] In December 2018, she presented a series of three Royal Institution Christmas Lectures, titled Who am I? and broadcast on BBC Four, with guest lecturer Aoife McLysaght.[27]
On 4 August 2020, Roberts was the guest on BBC Radio 4's The Life Scientific.[55] Aired as a three-part series in September 2020, Roberts co-presented the BBC's The Big Dig focusing on the finds at St. James's Park in London and Park Street in Birmingham.
On 12 February 2021, Roberts presented a one-hour BBC Two documentary, Stonehenge: The Lost Circle Revealed,[56] about Mike Parker Pearson's five-year-long quest that filled in a 400-year historical gap in the provenance of the bluestones of Stonehenge and Waun Mawn.[57][58][59]
On 14 March 2022, Curse Of The Ancients with Alice Roberts, a five-part documentary series presented by Roberts premiered on Sky History.[60] In October Roberts presented Royal Autopsy, a two-part documentary series shown on Sky History; a second series was commissioned in November 2023.[61] The series examined the deaths of Queen Elizabeth I and King Charles II, and then Queen Anne, Queen Mary I, King Henry IV and King George IV.[62] Roberts presented the second series of Royal Autopsy that aired during April 2024.[63]
In March and April 2023, Roberts presented the four-part Channel 4 series Fortress Britain with Alice Roberts.[64] In June, Roberts presented the four-part Channel 4 series Ancient Egypt by Train with Alice Roberts,[65] Ottoman Empire by Train with Alice Roberts during autumn 2024.[66] and Ancient Greece by Train with Alice Roberts during spring 2025.[67]
In May 2024, Roberts presented the documentary The Lost Scrolls of Pompeii: New Revelations, which aired on Channel 5.[68]
In 2011, Roberts was elected an honorary fellow of the British Science Association,[22] and a fellow of the Royal Society of Biology.[69] In 2014, she was selected by the Science Council as one of their leading UK practising scientists.[70] During 2014, she was President of The Association for Science Education,[70] and presented the Morgan-Botti lecture.[71]
Roberts has received honorary doctorates (DSc) from Royal Holloway, University of London; Bournemouth University;[69] the Open University and the University of Leeds;[72] honorary Doctor of Medicine (MD) from the University of Sussex;[73] and honorary Doctor of Education from the University of Bath.[74]
In 2019, she was awarded an Honorary Fellowship by Cardiff University.[75]
Roberts was awarded British Humanist of the Year 2015, for work promoting the teaching of evolution in schools.[76]
The Incredible Unlikeliness of Being was shortlisted for the Wellcome Book Prize 2015.[77]
In 2020, Roberts won the Royal Society David Attenborough Award and Lecture.[78]
On 22 May 2022, Roberts unveiled the Statue of Mary Anning at Lyme Regis; the statue was the result of a crowdfunded campaign ("Mary Anning Rocks") to commission and display a statue to the paleontologist Mary Anning in Lyme Regis.[79]
Roberts lives with her husband, David Stevens, and two children, a daughter born in 2010 and a son born in 2013.[80] She met her husband in Cardiff in 1995 when she was a medical student and he was an archaeology student.[81][5] They married in 2009.[82]
She is a pescatarian,[83] "a confirmed atheist"[84] and former president of Humanists UK, beginning her three-and-a-half-year term in January 2019.[85][28] She is now a vice president of the organisation.[4] Her children were assigned a faith school due to over-subscription of her local community schools; she campaigns against state-funded religious schools, citing her story as an example of the problems perpetuated by faith schools.[86]
Roberts enjoys watercolour painting, surfing, wild swimming, cycling, gardening and pub quizzes.[5] Roberts is an organiser of the Cheltenham Science Festival and school outreach programmes within the University of Bristol's Medical Sciences Division.[7] In March 2007, she hosted the Bristol Medical School's charity dance show Clicendales 2007, to raise funds for the charity CLIC Sargent.[87]
Roberts took her baby daughter with her when touring for the six-month filming of the first series of Digging for Britain in 2010.[39]
In March 2024 Roberts was the guest for BBC Radio 4's Desert Island Discs, where her musical choices included "Monkey Gone to Heaven" by the Pixies, "Temple of Love" by The Sisters of Mercy, and "Sugar" by System of a Down.[88]
Roberts is an author.[89][90][91] She has authored or co-authored a number of peer reviewed scientific articles in journals.[1][92][93][94][95] Her published books include:

Harry Truman Simanjuntak (born 1951) is an Indonesian archaeologist and prehistorian.
Born 27 August 1951 in Pematangsiantar, North Sumatra, he first studied law in Medan and Yogyakarta and received a bachelor's degree in law, but later followed his main interest and earned a bachelor's degree in archaeology in 1979 at Gajah Mada University. His post-graduate studies brought him to the Institut de paléontologie humaine [fr] in Paris, where he obtained a master's degree in 1987 and a Ph.D. in 1991.[1][2]
He worked as a researcher at the Yogyakarta Bureau of Archaeology [id] until 1986. Following his return from Paris, he has worked at the National Research Center for Archaeology [id] from 1992 until present.[3] In 2007, he founded the Center for Prehistory and Austronesian Studies.[2] He was awarded in 2015 with the Sarwono Award of the Indonesian Institute of Sciences.[4][5]
Truman Simanjuntak engaged in fieldwork in most parts of Indonesia. He studied archaic human remains from the Paleolithic in Jawa, Sumatera, Kalimatan, Sumba, and the Maluku Islands. He led excavations of the early modern human inhabitants of Indonesia from the Late Paleolithic to the Neolithic, and also studied Megalithic and early Metal Age sites.[5][3] According to Truman Simanjuntak, the Indonesian archipelago has harbored a wide cultural diversity from prehistorical times until now.[4] Based on archaeological evidence from distinct pottery styles, he proposes a two-way entry of the Neolithic populations of western Indonesia: the Eastern Route Migration from the northeast, connected to the Austronesian expansion, and the Western Route Migration from the northwest from Mainland Southeast Asia. This two-fold origin is also corroborated by archaeogenetic evidence.[6][7]

The Etruscan civilization (/ɪˈtrʌskən/ ih-TRUS-kən) was an ancient civilization created by the Etruscans, a people who inhabited Etruria in ancient Italy, with a common language and culture, and formed a federation of city-states.[2] After adjacent lands had been conquered its territory covered, at its greatest extent, roughly what is now Tuscany, western Umbria and northern Lazio,[3][4] as well as what are now the Po Valley, Emilia-Romagna, south-eastern Lombardy, southern Veneto and western Campania.[5][6]
A large body of literature has flourished on the origins of the Etruscans, but the consensus among modern scholars is that the Etruscans were an indigenous population.[7][8][9][10][11] The earliest evidence of a culture that is identifiably Etruscan dates from about 900 BC.[1] This is the period of the Iron Age Villanovan culture, considered to be the earliest phase of Etruscan civilization,[12][13][14][15][16] which itself developed from the previous late Bronze Age Proto-Villanovan culture in the same region,[17] part of the central European Urnfield culture system. Etruscan civilization dominated Italy until it fell to the expanding Rome beginning in the late 4th century BC as a result of the Roman–Etruscan Wars;[18] Etruscans were granted Roman citizenship in 90 BC and in 27 BC the whole Etruscan territory was incorporated into the newly established Roman Empire.[1]
The territorial extent of Etruscan civilization reached its maximum around 500 BC, shortly after the Roman Kingdom became the Roman Republic. Its culture flourished in three confederacies of cities: that of Etruria (Tuscany, Latium and Umbria), that of the Po Valley with the eastern Alps, and that of Campania.[19][20] The league in northern Italy is mentioned in Livy.[21][22][23] The reduction in Etruscan territory was gradual, but after 500 BC the political balance of power on the Italian peninsula shifted away from the Etruscans in favor of the rising Roman Republic.[24]
The earliest-known examples of Etruscan writing are inscriptions found in southern Etruria that date to around 700 BC.[18][25] The Etruscans developed a system of writing derived from the Euboean alphabet, which was used in the Magna Graecia coastal areas in Southern Italy. The Etruscan language remains only partly understood, making modern understanding of their society and culture heavily dependent on much later and generally disapproving Roman and Greek sources. In the Etruscan political system authority resided in its individual small cities and probably in its prominent individual families. At the height of Etruscan power, elite Etruscan families grew very rich through trade with the Celts to the north and the Greeks to the south, and they filled their large family tombs with imported luxuries.[26][27]
According to Dionysius the Etruscans called themselves Rasenna (Greek Ῥασέννα), a stem from the Etruscan Rasna (𐌛𐌀𐌔𐌍𐌀), the people. Evidence of inscriptions as Tular Rasnal (𐌕𐌖𐌋𐌀𐌛 𐌛𐌀𐌔𐌍𐌀𐌋), "boundary of the people", or Mechlum Rasnal (𐌌𐌄𐌙𐌋 𐌛𐌀𐌔𐌍𐌀𐌋). "community of the people", attest to its autonym usage. The Tyrsenian etymology, however, remains unknown.[28][29][30]
In Attic Greek the Etruscans were known as Tyrrhenians (Τυρρηνοί, Tyrrhēnoi, earlier Τυρσηνοί Tyrsēnoi),[31] from which the Romans derived the names Tyrrhēnī, Tyrrhēnia (Etruria),[32] and Mare Tyrrhēnum (Tyrrhenian Sea).[33]
The ancient Romans referred to the Etruscans as the Tuscī or Etruscī (singular Tuscus).[34][35][36] Their Roman name is the origin of the terms Toscana, which refers to their heartland, and Etruria, which can refer to their wider region. The term Tusci is thought by linguists to have been the Umbrian word for Etruscan, based on an inscription on an ancient bronze tablet from a nearby region.[37] The inscription contains the phrase turskum ... nomen, literally ‘the Tuscan name’. Based on a knowledge of Umbrian grammar, linguists can infer that the base form of the word turskum is *Tursci,[38] which would, through metathesis and a word-initial epenthesis, be likely to lead to the form, E-trus-ci.[39]
As for the original meaning of the root, *Turs-, a widely cited hypothesis is that it, like the Latin turris, means ‘tower’ and comes from the ancient Greek word for tower: τύρσις,[40][41] likely a loan into Greek. On this hypothesis, the Tusci were called the ‘people who build towers"[40] or "the tower builders".[42] This proposed etymology is made the more plausible because the Etruscans preferred to build their towns on high precipices reinforced by walls. Alternatively, Giuliano and Larissa Bonfante have speculated that Etruscan houses may have seemed like towers to the simple Latins.[43] The proposed etymology has a long history, Dionysius of Halicarnassus having observed in the first century BC, "[T]here is no reason that the Greeks should not have called [the Etruscans] by this name, both from their living in towers and from the name of one of their rulers."[44] In his recent Etymological Dictionary of Greek, Robert Beekes claims the Greek word is a "loanword from a Mediterranean language", a hypothesis that goes back to an article by Paul Kretschmer in Glotta from 1934.[45][46]
Literary and historical texts in the Etruscan language have not survived, and the language itself is only partially understood by modern scholars. This makes modern understanding of their society and culture heavily dependent on much later and generally disapproving Roman and Greek sources. These ancient writers differed in their theories about the origin of the Etruscan people. Some suggested they were Pelasgians who had migrated there from Greece. Others maintained that they were indigenous to central Italy and were not from Greece.
The first Greek author to mention the Etruscans, whom the Ancient Greeks called Tyrrhenians, was the 8th-century BC poet Hesiod in his work the Theogony. He mentioned them as residing in central Italy alongside the Latins.[48] The 7th-century BC Homeric Hymn to Dionysus[49] referred to them as pirates.[50] Unlike later Greek authors, these authors did not suggest that Etruscans had migrated to Italy from the east and did not associate them with the Pelasgians.
It was only in the 5th century BC, when the Etruscan civilization had been established for several centuries, that Greek writers started associating the name "Tyrrhenians" with the "Pelasgians", and even then some did so in a way that suggests they were meant only as generic, descriptive labels for "non-Greek" and "indigenous ancestors of Greeks" respectively.[51] The 5th-century BC historians Herodotus,[52] and Thucydides[53] and the 1st-century BC historian Strabo,[54] did seem to suggest that the Tyrrhenians were originally Pelasgians who migrated to Italy from Lydia by way of the Greek island of Lemnos. They all described Lemnos as having been settled by Pelasgians, whom Thucydides identified as "belonging to the Tyrrhenians" (τὸ δὲ πλεῖστον Πελασγικόν, τῶν καὶ Λῆμνόν ποτε καὶ Ἀθήνας Τυρσηνῶν). As Strabo and Herodotus told it,[55] the migration to Lemnos was led by Tyrrhenus / Tyrsenos, the son of Atys (who was king of Lydia). Strabo[54] added that the Pelasgians of Lemnos and Imbros then followed Tyrrhenus to the Italian Peninsula. According to the logographer Hellanicus of Lesbos, there was a Pelasgian migration from Thessaly in Greece to the Italian peninsula, as part of which the Pelasgians colonized the area he called Tyrrhenia, and they then came to be called Tyrrhenians.[56]
There is some evidence suggesting a link between the island of Lemnos and the Tyrrhenians. The Lemnos Stele bears inscriptions in a language with strong structural resemblances to the language of the Etruscans.[57] The discovery of these inscriptions in modern times has led to the suggestion of a "Tyrrhenian language group" consisting of Etruscan, Lemnian, and the Raetic spoken in the Alps.
However the 1st-century BC historian Dionysius of Halicarnassus, a Greek living in Rome, dismissed many of the ancient theories of other Greek historians and postulated that the Etruscans were indigenous people who had always lived in Etruria and were different from both the Pelasgians and the Lydians.[58] Dionysius noted that the 5th-century historian Xanthus of Lydia, who was originally from Sardis and was regarded as an important source and authority for the history of Lydia, never suggested a Lydian origin of the Etruscans and never named Tyrrhenus as a ruler of the Lydians.[58]
For this reason, therefore, I am persuaded that the Pelasgians are a different people from the Tyrrhenians. And I do not believe, either, that the Tyrrhenians were a colony of the Lydians; for they do not use the same language as the latter, nor can it be alleged that, though they no longer speak a similar tongue, they still retain some other indications of their mother country. For they neither worship the same gods as the Lydians nor make use of similar laws or institutions, but in these very respects they differ more from the Lydians than from the Pelasgians. Indeed, those probably come nearest to the truth who declare that the nation migrated from nowhere else, but was native to the country, since it is found to be a very ancient nation and to agree with no other either in its language or in its manner of living.
The credibility of Dionysius of Halicarnassus is arguably bolstered by the fact that he was the first ancient writer to report the endonym of the Etruscans: Rasenna.
The Romans, however, give them other names: from the country they once inhabited, named Etruria, they call them Etruscans, and from their knowledge of the ceremonies relating to divine worship, in which they excel others, they now call them, rather inaccurately, Tusci, but formerly, with the same accuracy as the Greeks, they called them Thyrscoï [an earlier form of Tusci]. Their own name for themselves, however, is the same as that of one of their leaders, Rasenna.
Similarly, the 1st-century BC historian Livy, in his Ab Urbe Condita Libri, said that the Rhaetians were Etruscans who had been driven into the mountains by the invading Gauls; and he asserted that the inhabitants of Raetia were of Etruscan origin.[59]
The Alpine tribes have also, no doubt, the same origin (of the Etruscans), especially the Raetians; who have been rendered so savage by the very nature of the country as to retain nothing of their ancient character save the sound of their speech, and even that is corrupted.
The first-century historian Pliny the Elder also put the Etruscans in the context of the Rhaetian people to the north, and wrote in his Natural History (AD 79):[60]
Adjoining these the (Alpine) Noricans are the Raeti and Vindelici. All are divided into a number of states. The Raeti are believed to be people of Tuscan race driven out by the Gauls, their leader was named Raetus.
The question of the origins of the Etruscans has long been a subject of interest and debate among historians. In modern times, all the evidence gathered so far by prehistoric and protohistoric archaeologists, anthropologists, and etruscologists points to an autochthonous origin of the Etruscans.[7][8][9][10][11] There is no archaeological or linguistic evidence of a migration of the Lydians or Pelasgians into Etruria.[61][9][8][10][11] Modern etruscologists and archeologists, such as Massimo Pallottino (1947), have shown that early historians' assumptions and assertions on the subject were groundless.[62] In 2000, the etruscologist Dominique Briquel explained in detail why he believes that ancient Greek narratives on Etruscan origins should not even count as historical documents.[63] He argues that the ancient story of the Etruscans' 'Lydian origins' was a deliberate, politically motivated fabrication, and that ancient Greeks inferred a connection between the Tyrrhenians and the Pelasgians solely on the basis of certain Greek and local traditions and on the mere fact that there had been trade between the Etruscans and Greeks.[64][65] He noted that, even if these stories include historical facts suggesting contact, such contact is more plausibly traceable to cultural exchange than to migration.[66]
Several archaeologists specializing in Prehistory and Protohistory who have analyzed Bronze Age and Iron Age remains that were excavated in the territory of historical Etruria have pointed out that no evidence has been found, related either to material culture or to social practices, that can support a migration theory.[67] The most marked and radical change that has been archaeologically attested in the area is the adoption, starting in about the 12th century BC, of the funeral rite of incineration in terracotta urns, which is a Continental European practice, derived from the Urnfield culture; there is nothing about it that suggests an ethnic contribution from Asia Minor or the Near East.[67]
A 2012 survey of the previous 30 years' archaeological findings based on excavations of the major Etruscan cities showed a continuity of culture from the last phase of the Bronze Age (13th–11th century BC) to the Iron Age (10th–9th century BC). This is evidence that the Etruscan civilization, which emerged around 900 BC, was built by people whose ancestors had inhabited that region for at least the previous 200 years.[68] Based on this cultural continuity, there is now a consensus among archeologists that Proto-Etruscan culture developed, during the last phase of the Bronze Age, from the indigenous Proto-Villanovan culture and that the subsequent Iron Age Villanovan culture is most accurately described as an early phase of the Etruscan civilization.[17] It is possible that there were contacts between northern-central Italy and the Mycenaean world at the end of the Bronze Age. However contacts between the inhabitants of Etruria and inhabitants of Greece, Aegean Sea Islands, Asia Minor, and the Near East are attested only centuries later, when Etruscan civilization was already flourishing and Etruscan ethnogenesis was well established. The first of these attested contacts relate to the Greek colonies in Southern Italy and Phoenician-Punic colonies in Sardinia, and the consequent orientalizing period.[69]
One of the most common mistakes for a long time, even among some scholars of the past, has been to associate the later Orientalizing period of Etruscan civilization with the question of its origins. Orientalization was an artistic and cultural phenomenon that spread among the Greeks themselves and throughout much of the central and western Mediterranean, not only in Etruria.[70] The Etruscan orientalizing period was due, as has been amply demonstrated by archeologists, to contacts with the Greeks and the Eastern Mediterranean and not to mass migrations.[71] The facial features (the profile, almond-shaped eyes, large nose) in the frescoes and sculptures and the depiction of reddish-brown men and light-skinned women, influenced by archaic Greek art, followed the artistic traditions from the Eastern Mediterranean that had spread even among the Greeks themselves, and to a lesser extent also to several other civilizations in the central and western Mediterranean up to the Iberian Peninsula. Actually, many of the tombs of the Late Orientalizing and Archaic periods, such as the Tomb of the Augurs, the Tomb of the Triclinium and the Tomb of the Leopards, as well as other tombs from the archaic period in the Monterozzi necropolis in Tarquinia, were painted by Greek painters or at least foreign artists. These images have, therefore, a very limited value for a realistic representation of the Etruscan population.[72] It was only from the end of the 4th century BC that evidence of physiognomic portraits began to be found in Etruscan art and Etruscan portraiture became more realistic.[73]
There have been numerous biological studies on the Etruscan origins, the oldest of which dates back to the 1950s when research was still based on blood tests of modern samples and DNA analysis (including the analysis of ancient samples) was not yet possible.[74][75][76] It is only in very recent years, with the development of archaeogenetics, that comprehensive studies containing the whole genome sequencing of Etruscan samples have been published, including autosomal DNA and Y-DNA, autosomal DNA being the "most valuable to understand what really happened in an individual's history", as stated by geneticist David Reich, whereas previously studies were based only on mitochondrial DNA analysis, which contains less and limited information.[77]
An archeogenetic study focusing on the question of Etruscan origins was published in September 2021 in the journal Science Advances and analyzed the autosomal DNA and the uniparental markers (Y-DNA and mtDNA) of 48 Iron Age individuals from Tuscany and Lazio, spanning from 800 to 1 BC and concluding that the Etruscans were autochthonous (locally indigenous) and  had a genetic profile similar to their Latin neighbors. In the Etruscan individuals the ancestral component Steppe was present in the same percentages as those found in the previously analyzed Iron Age Latins, and the Etruscan DNA bore no trace of recent admixture with Anatolia and the Eastern Mediterranean. Both Etruscans and Latins were firmly part of the European cluster, west of modern Italians. The Etruscans were a mixture of WHG, EEF and Steppe ancestry; 75% of the Etruscan male individuals were found to belong to haplogroup R1b (R1b M269), especially its clade R1b-P312 and its derivative R1b-L2, whose direct ancestor is R1b-U152, whilst the most common mitochondrial DNA haplogroup among the Etruscans was H.[78]
The conclusions of the 2021 study are in line with a 2019 study previously published in the journal Science that analyzed the remains of eleven Iron Age individuals from the areas around Rome, of which four were Etruscan individuals, one buried in Veio Grotta Gramiccia from the Villanovan era (900-800 BC) and three buried in La Mattonara Necropolis near Civitavecchia from the Orientalizing period (700-600 BC). The study concluded that Etruscans (900–600 BC) and the Latins (900–500 BC) from Latium vetus were genetically similar,[79] with genetic differences between the examined Etruscans and Latins found to be insignificant.[80] The Etruscan individuals and contemporary Latins were distinguished from preceding populations of Italy by the presence of c. 30% steppe ancestry.[81] Their DNA was a mixture of two-thirds Copper Age ancestry (EEF + WHG; Etruscans ~66–72%, Latins ~62–75%) and one-third Steppe-related ancestry (Etruscans ~27–33%, Latins ~24–37%).[79] The only sample of Y-DNA extracted belonged to haplogroup J-M12 (J2b-L283), found in an individual dated 700-600 BC, and carried the M314 derived allele also found in a Middle Bronze Age individual from Croatia (1631–1531 BC).  The four samples of mtDNA extracted belonged to haplogroups U5a1, H, T2b32, K1a4.[82]
Among the older studies, based only on mitochondrial DNA, a mtDNA study, published in 2018 in the journal American Journal of Physical Anthropology compared both ancient and modern samples from Tuscany, from Prehistory, the Etruscan age, Roman age, Renaissance and the present day and concluded that the Etruscans appear to be a local population, intermediate between the prehistoric and the other samples, placing them in the temporal network between the Eneolithic Age and the Roman Age.[83]
A couple of mitochondrial DNA studies published in 2013 in the journals PLOS One and American Journal of Physical Anthropology, based on Etruscan samples from Tuscany and Latium, concluded that the Etruscans were an indigenous population, showing that Etruscan mtDNA appears to fall very close to a Neolithic population from Central Europe (Germany, Austria, Hungary) and to other Tuscan populations, strongly suggesting that the Etruscan civilization developed locally from the Villanovan culture, as already supported by archaeological evidence and anthropological research,[17][84] and that genetic links between Tuscany and western Anatolia date back to at least 5,000 years ago during the Neolithic and the "most likely separation time between Tuscany and Western Anatolia falls around 7,600 years ago", at the time of the migrations of Early European Farmers (EEF) from Anatolia to Europe in the early Neolithic. The ancient Etruscan samples had mitochondrial DNA haplogroups (mtDNA) JT (subclades of J and T) and U5, with a minority of mtDNA H1b.[85][86]
An earlier mtDNA study published in 2004, based on about 28 samples of individuals who lived from 600 to 100 BC in Veneto, Etruria and Campania, stated that the Etruscans had no significant heterogeneity and that all mitochondrial lineages observed among the Etruscan samples appear typically European or West Asian but only a few haplotypes were shared with modern populations. Allele sharing between the Etruscans and modern populations is highest among Germans (seven haplotypes in common), the Cornish from the South West of Britain (five haplotypes in common), the Turks (four haplotypes in common) and the Tuscans (two haplotypes in common).[87] The modern populations with the shortest genetic distance from the ancient Etruscans, based solely on mtDNA and FST, were Tuscans followed by the Turks, other populations from the Mediterranean and the Cornish after.[87] This study was much criticized by other geneticists, because "data represent severely damaged or partly contaminated mtDNA sequences" and "any comparison with modern population data must be considered quite hazardous",[88][89][90] and by archaeologists, who argued that the study was not clear-cut and had not provided evidence that the Etruscans were an intrusive population to the European context.[76][75]
In the collective volume Etruscology published in 2017, British archeologist Phil Perkins, echoing an earlier article of his from 2009, provides an analysis of the state of DNA studies and writes that "none of the DNA studies to date conclusively prove that [the] Etruscans were an intrusive population in Italy that originated in the Eastern Mediterranean or Anatolia" and "there are indications that the evidence of DNA can support the theory that Etruscan people are autochthonous in central Italy".[75][76]
In his 2021 book A Short History of Humanity, German geneticist Johannes Krause, codirector of the Max Planck Institute for Evolutionary Anthropology in Jena, concludes that it is likely that the Etruscan language (as well as Basque, Paleo-Sardinian and Minoan) "developed on the continent in the course of the Neolithic Revolution".[91]
The Etruscan civilization begins with the early Iron Age Villanovan culture, regarded as the oldest phase, that occupied a large area of northern and central Italy during the Iron Age.[12][13][14][15][16] The Etruscans themselves dated the origin of the Etruscan nation to a date corresponding to the 11th or 10th century BC.[13][92] The Villanovan culture emerges with the phenomenon of regionalization from the late Bronze Age culture called "Proto-Villanovan", part of the central European Urnfield culture system. In the last Villanovan phase, called the recent phase (about 770–730 BC), the Etruscans established relations of a certain consistency with the first Greek immigrants in southern Italy (in Pithecusa and then in Cuma), so much so as to initially absorb techniques and figurative models and soon more properly cultural models, with the introduction, for example, of writing, of a new way of banqueting, of a heroic funerary ideology, that is, a new aristocratic way of life, such as to profoundly change the physiognomy of Etruscan society.[92] Thus, thanks to the growing number of contacts with the Greeks, the Etruscans entered what is called the Orientalizing phase. In this phase, there was a heavy influence in Greece, most of Italy and some areas of Spain, from the most advanced areas of the eastern Mediterranean and the ancient Near East.[93] Also directly Phoenician, or otherwise Near Eastern, craftsmen, merchants and artists contributed to the spread in southern Europe of Near Eastern cultural and artistic motifs. The last three phases of Etruscan civilization are called, respectively, Archaic, Classical and Hellenistic, which roughly correspond to the homonymous phases of the ancient Greek civilization.
Etruscan expansion was focused both to the north beyond the Apennine Mountains and into Campania. Some small towns in the sixth century BC disappeared during this time, ostensibly subsumed by greater, more powerful neighbors. However, it is certain that the political structure of the Etruscan culture was similar to, albeit more aristocratic than, Magna Graecia in the south. The mining and commerce of metal, especially copper and iron, led to an enrichment of the Etruscans and to the expansion of their influence in the Italian peninsula and the western Mediterranean Sea. Here, their interests collided with those of the Greeks, especially in the sixth century BC, when Phocaeans of Italy founded colonies along the coast of Sardinia, Spain and Corsica. This led the Etruscans to ally themselves with Carthage, whose interests also collided with the Greeks.[95][96]
Around 540 BC, the Battle of Alalia led to a new distribution of power in the western Mediterranean. Though the battle had no clear winner, Carthage managed to expand its sphere of influence at the expense of the Greeks, and Etruria saw itself relegated to the northern Tyrrhenian Sea with full ownership of Corsica. From the first half of the 5th century BC, the new political situation meant the beginning of the Etruscan decline after losing their southern provinces. In 480 BC, Etruria's ally Carthage was defeated by a coalition of Magna Graecia cities led by Syracuse, Sicily. A few years later, in 474 BC, Syracuse's tyrant Hiero defeated the Etruscans at the Battle of Cumae. Etruria's influence over the cities of Latium and Campania weakened, and the area was taken over by Romans and Samnites.
In the 4th century BC, Etruria saw a Gallic invasion end its influence over the Po Valley and the Adriatic coast. Meanwhile, Rome had started annexing Etruscan cities. This led to the loss of the northern Etruscan provinces. During the Roman–Etruscan Wars, Etruria was conquered by Rome in the 3rd century BC.[95][96]
According to legend,[97] there was a period between 600 BC and 500 BC in which an alliance was formed among twelve Etruscan settlements, known today as the Etruscan League, Etruscan Federation, or Dodecapolis (Ancient Greek: Δωδεκάπολις). According to a legend, the Etruscan League of twelve cities was founded by Tarchon and his brother Tyrrhenus. Tarchon lent his name to the city of Tarchna, or Tarquinnii, as it was known by the Romans. Tyrrhenus gave his name to the Tyrrhenians, the alternative name for the Etruscans. Although there is no consensus on which cities were in the league, the following list may be close to the mark: Arretium, Caisra, Clevsin, Curtun, Perusna, Pupluna, Veii, Tarchna, Vetluna, Volterra, Velzna, and Velch. Some modern authors include Rusellae.[98] The league was mostly an economic and religious league, or a loose confederation, similar to the Greek states. During the later imperial times, when Etruria was just one of many regions controlled by Rome, the number of cities in the league increased by three. This is noted on many later grave stones from the 2nd century BC onwards. According to Livy, the twelve city-states met once a year at the Fanum Voltumnae at Volsinii, where a leader was chosen to represent the league.[99]
There were two other Etruscan leagues ("Lega dei popoli"): that of Campania, the main city of which was Capua, and the Po Valley city-states in northern Italy, which included Bologna, Spina and Adria.[99]
Those who subscribe to a Latin foundation of Rome followed by an Etruscan invasion typically speak of an Etruscan "influence" on Roman culture – that is, cultural objects which were adopted by Rome from neighboring Etruria. The prevailing view is that Rome was founded by Latins who later merged with Etruscans. In this interpretation, Etruscan cultural objects are considered influences rather than part of a heritage.[100] Rome was probably a small settlement until the arrival of the Etruscans, who constructed the first elements of its urban infrastructure such as the drainage system.[101][102]
The main criterion for deciding whether an object originated at Rome and traveled by influence to the Etruscans, or descended to the Romans from the Etruscans, is date. Many, if not most, of the Etruscan cities were older than Rome. If one finds that a given feature was there first, it cannot have originated at Rome. A second criterion is the opinion of the ancient sources. These would indicate that certain institutions and customs came directly from the Etruscans. Rome is located on the edge of what was Etruscan territory. When Etruscan settlements turned up south of the border, it was presumed that the Etruscans spread there after the foundation of Rome, but the settlements are now known to have preceded Rome.
Etruscan settlements were frequently built on hills – the steeper the better – and surrounded by thick walls. According to Roman mythology, when Romulus and Remus founded Rome, they did so on the Palatine Hill according to Etruscan ritual; that is, they began with a pomerium or sacred ditch. Then, they proceeded to the walls. Romulus was required to kill Remus when the latter jumped over the wall, breaking its magic spell (see also under Pons Sublicius). The name of Rome is attested in Etruscan in the form Ruma-χ meaning 'Roman', a form that mirrors other attested ethnonyms in that language with the same suffix -χ: Velzna-χ '(someone) from Volsinii' and Sveama-χ '(someone) from Sovana'. This in itself, however, is not enough to prove Etruscan origin conclusively. If Tiberius is from θefarie, then Ruma would have been placed on the Thefar (Tiber) river. A heavily discussed topic among scholars is who was the founding population of Rome. In 390 BC, the city of Rome was attacked by the Gauls, and as a result may have lost many – though not all – of its earlier records.
Later history relates that some Etruscans lived in the Vicus Tuscus,[103] the "Etruscan quarter", and that there was an Etruscan line of kings (albeit ones descended from a Greek, Demaratus of Corinth) that succeeded kings of Latin and Sabine origin. Etruscophile historians would argue that this, together with evidence for institutions, religious elements and other cultural elements, proves that Rome was founded by Etruscans.
Under Romulus and Numa Pompilius, the people were said to have been divided into thirty curiae and three tribes. Few Etruscan words entered Latin, but the names of at least two of the tribes – Ramnes and Luceres – seem to be Etruscan. The last kings may have borne the Etruscan title lucumo, while the regalia were traditionally considered of Etruscan origin – the golden crown, the sceptre, the toga palmata (a special robe), the sella curulis (curule chair), and above all the primary symbol of state power: the fasces. The latter was a bundle of whipping rods surrounding a double-bladed axe, carried by the king's lictors. An example of the fasces are the remains of bronze rods and the axe from a tomb in Etruscan Vetulonia. This allowed archaeologists to identify the depiction of a fasces on the grave stele of Avele Feluske, who is shown as a warrior wielding the fasces. The most telling Etruscan feature is the word populus, which appears as an Etruscan deity, Fufluns.
The historical Etruscans had achieved a state system of society, with remnants of the chiefdom and tribal forms. Rome was in a sense the first Italic state, but it began as an Etruscan one. It is believed that the Etruscan government style changed from total monarchy to oligarchic republic (as the Roman Republic) in the 6th century BC.[104]
The government was viewed as being a central authority, ruling over all tribal and clan organizations. It retained the power of life and death; in fact, the gorgon, an ancient symbol of that power, appears as a motif in Etruscan decoration. The adherents to this state power were united by a common religion. Political unity in Etruscan society was the city-state, which was probably the referent of methlum, "district". Etruscan texts name quite a number of magistrates, without much of a hint as to their function: The camthi, the parnich, the purth, the tamera, the macstrev, and so on. The people were the mech.
The princely tombs were not of individuals. The inscription evidence shows that families were interred there over long periods, marking the growth of the aristocratic family as a fixed institution, parallel to the gens at Rome and perhaps even its model. The Etruscans could have used any model of the eastern Mediterranean. That the growth of this class is related to the new acquisition of wealth through trade is unquestioned. The wealthiest cities were located near the coast. At the center of the society was the married couple, tusurthir. The Etruscans were a monogamous society that emphasized pairing.
Similarly, the behavior of some wealthy women is not uniquely Etruscan. The apparent promiscuous revelry has a spiritual explanation. Swaddling and Bonfante (among others) explain that depictions of the nude embrace, or symplegma, "had the power to ward off evil", as did baring the breast, which was adopted by western culture as an apotropaic device, appearing finally on the figureheads of sailing ships as a nude female upper torso. It is also possible that Greek and Roman attitudes to the Etruscans were based on a misunderstanding of the place of women within their society. In both Greece and the earliest Republican Rome, respectable women were confined to the house and mixed-sex socialising did not occur. Thus, the freedom of women within Etruscan society could have been misunderstood as implying their sexual availability.[105] A number of Etruscan tombs carry funerary inscriptions in the form "X son of (father) and (mother)", indicating the importance of the mother's side of the family.[105]
The Etruscans, like the contemporary cultures of Ancient Greece and Ancient Rome, had a significant military tradition. In addition to marking the rank and power of certain individuals, warfare was a considerable economic advantage to Etruscan civilization. Like many ancient societies, the Etruscans conducted campaigns during summer months, raiding neighboring areas, attempting to gain territory and combating piracy as a means of acquiring valuable resources, such as land, prestige, goods, and slaves. It is likely that individuals taken in battle would be ransomed back to their families and clans at high cost. Prisoners could also potentially be sacrificed on tombs to honor fallen leaders of Etruscan society, not unlike the sacrifices made by Achilles for Patrocles.[106][107][108]
The range of Etruscan civilization is marked by its cities. They were entirely assimilated by Italic, Celtic, or Roman ethnic groups, but the names survive from inscriptions and their ruins are of aesthetic and historic interest in most of the cities of central Italy. Etruscan cities flourished over most of Italy during the Roman Iron Age, marking the farthest extent of Etruscan civilization. They were gradually assimilated first by Italics in the south, then by Celts in the north and finally in Etruria itself by the growing Roman Republic.[106]
That many Roman cities were formerly Etruscan was well known to all the Roman authors. Some cities were founded by Etruscans in prehistoric times, and bore entirely Etruscan names. Others were colonized by Etruscans who Etruscanized the name, usually Italic.[107]
The Etruscans were aware of the techniques of water accumulation and conservation in Egypt, Mesopotamia and Greece. They built canals and dams to irrigate the land, and drained and reclaimed swamps. The archaeological remains of this infrastructure are still evident in the maritime southwestern parts of Tuscany.[109]
Vite maritata is a viticulture technique exploiting companion planting named after the Maremma region of Italy which may be relevant to climate change.[110] It was developed around the area by these early predecessors of the Romans who cultivated plant as nearly as possible in their natural habitat. The vines from which wine is made are a kind of liana that naturally intertwine with trees such as maples or willows.[111]
The Etruscan system of belief was an immanent polytheism; that is, all visible phenomena were considered to be a manifestation of divine power and that power was subdivided into deities that acted continually on the world of man and could be dissuaded or persuaded in favor of human affairs. How to understand the will of deities, and how to behave, had been revealed to the Etruscans by two initiators, Tages, a childlike figure born from tilled land and immediately gifted with prescience, and Vegoia, a female figure. Their teachings were kept in a series of sacred books. Three layers of deities are evident in the extensive Etruscan art motifs. One appears to be divinities of an indigenous nature: Catha and Usil, the sun; Tivr, the moon; Selvans, a civil god; Turan, the goddess of love; Laran, the god of war; Leinth, the goddess of death; Maris; Thalna; Turms; and the ever-popular Fufluns, whose name is related in some way to the city of Populonia and the populus Romanus, possibly, the god of the people.[112][113]
Ruling over this pantheon of lesser deities were higher ones that seem to reflect the Indo-European system: Tin or Tinia, the sky, Uni his wife (Juno), and Cel, the earth goddess. In addition, some Greek and Roman gods were inspired by the Etruscan system: Aritimi (Artemis), Menrva (Minerva), Pacha (Dionysus). The Greek heroes taken from Homer also appear extensively in art motifs.[112][113]
Relatively little is known about the architecture of the ancient Etruscans. They adapted the native Italic styles with influence from the external appearance of Greek architecture. In turn, ancient Roman architecture began with Etruscan styles, and then accepted still further Greek influence. Roman temples show many of the same differences in form to Greek ones that Etruscan temples do, but like the Greeks, use stone, in which they closely copy Greek conventions. The houses of the wealthy were evidently often large and comfortable, but the burial chambers of tombs, often filled with grave-goods, are the nearest approach to them to survive. In the southern Etruscan area, tombs have large rock-cut chambers under a tumulus in large necropoleis, and these, together with some city walls, are the only Etruscan constructions to survive. Etruscan architecture is not generally considered as part of the body of Greco-Roman classical architecture.[114]
Etruscan art was produced by the Etruscan civilization between the 9th and 2nd centuries BC. Particularly strong in this tradition were figurative sculpture in terracotta (particularly lifesize on sarcophagi or temples), wall-painting and metalworking (especially engraved bronze mirrors). Etruscan sculpture in cast bronze was famous and widely exported, but few large examples have survived (the material was too valuable, and recycled later). In contrast to terracotta and bronze, there was apparently little Etruscan sculpture in stone, despite the Etruscans controlling fine sources of marble, including Carrara marble, which seems not to have been exploited until the Romans. Most surviving Etruscan art comes from tombs, including all the fresco wall-paintings, a minority of which show scenes of feasting and some narrative mythological subjects.[115]
Bucchero wares in black were the early and native styles of fine Etruscan pottery. There was also a tradition of elaborate Etruscan vase painting, which sprung from its Greek equivalent; the Etruscans were the main export market for Greek vases. Etruscan temples were heavily decorated with colorfully painted terracotta antefixes and other fittings, which survive in large numbers where the wooden superstructure has vanished. Etruscan art was strongly connected to religion; the afterlife was of major importance in Etruscan art.[116]
The Etruscan musical instruments seen in frescoes and bas-reliefs are different types of pipes, such as the plagiaulos (the pipes of Pan or Syrinx), the alabaster pipe and the famous double pipes, accompanied on percussion instruments such as the tintinnabulum, tympanum and crotales, and later by stringed instruments like the lyre and kithara.
Etruscans left around 13,000 inscriptions which have been found so far, only a small minority of which are of significant length. Attested from 700 BC to AD 50, the relation of Etruscan to other languages has been a source of long-running speculation and study. The Etruscans are believed to have spoken a Pre-Indo-European[117][118][119] and Paleo-European language,[120] and the majority consensus is that Etruscan is related only to other members of what is called the Tyrsenian language family, which in itself is an isolate family, that is unrelated directly to other known language groups. Since Rix (1998), it is widely accepted that the Tyrsenian family groups Raetic and Lemnian are related to Etruscan.[18]
Etruscan texts, written in a space of seven centuries, use a form of the Greek alphabet due to close contact between the Etruscans and the Greek colonies at Pithecusae and Cumae in the 8th century BC (until it was no longer used, at the beginning of the 1st century AD). Etruscan inscriptions disappeared from Chiusi, Perugia and Arezzo around this time. Only a few fragments survive, religious and especially funeral texts, most of which are late (from the 4th century BC). In addition to the original texts that have survived to this day, there are a large number of quotations and allusions from classical authors. In the 1st century BC, Diodorus Siculus wrote that literary culture was one of the great achievements of the Etruscans. Little is known of it and even what is known of their language is due to the repetition of the same few words in the many inscriptions found (by way of the modern epitaphs) contrasted in bilingual or trilingual texts with Latin and Punic. Out of the aforementioned genres, is just one such Volnio (Volnius) cited in classical sources mentioned.[121] With a few exceptions, such as the Liber Linteus, the only written records in the Etruscan language that remain are inscriptions, mainly funerary. The language is written in the Etruscan alphabet, a script related to the early Euboean Greek alphabet.[122] Many thousand inscriptions in Etruscan are known, mostly epitaphs, and a few very short texts have survived, which are mainly religious. Etruscan imaginative literature is evidenced only in references by later Roman authors, but it is evident from their visual art that the Greek myths were well known.[123]
With the founding of Pithekussai on Ischia and Kyme (lat. Cumae) in Campania in the course of the Greek colonization, the Etruscans came under the influence of the Greek culture in the 8th century BC. The Etruscans adopted an alphabet from the western Greek colonists that came from their homeland, the Euboean Chalkis. This alphabet from Cumae is therefore also called Euboean[124] or Chalcidian[125] Alphabet. The oldest written records of the Etruscans date from around 700 BC.[126]
One of the oldest Etruscan written documents is found on the tablet of Marsiliana d’Albegna from the hinterland of Vulci, which is now kept in the National Archaeological Museum of Florence. A western Greek model alphabet is engraved on the edge of this wax tablet made of ivory. In accordance with later Etruscan writing habits, the letters in this model alphabet were mirrored and arranged from right to left:
The script with these letters was first used in southern Etruria around 700 BC in the Etruscan Cisra (lat. Caere), today's Cerveteri.[129] The science of writing quickly reached central and northern Etruria. From there, the alphabet spread from Volterra (Etr. Velathri) to Felsina, today's Bologna, and later from Chiusi (Etr. Clevsin) to the Po Valley. In southern Etruria, the writing spread from Tarquinia (Etr. Tarchna) and Veii (Etr. Veia) further south to Campania, which was controlled by the Etruscans at the time.[130] In the following centuries the Etruscans consistently used the letters mentioned, so that the deciphering of the Etruscan inscriptions is not a problem. As in Greek, the characters were subject to regional and temporal changes. Overall, one can distinguish an archaic script from the 7th to 5th centuries from a more recent script from the 4th to 1st centuries BC, in which some characters were no longer used, including the X for a sh sound. In addition, in writing and language, the emphasis on the first syllable meant that internal vowels were not reproduced, e.g. Menrva instead of Menerva.[131] Accordingly, linguists also distinguish between Old and New Etruscan.[132]
Alongside the tablet of Marsiliana d’Albegna, around 70 objects with model alphabets have been preserved from the early period.[133] The most famous of these are:
As all four artifacts date from the 7th century B.C. come from, the alphabets are always written clockwise.[134] The last object has the special feature that, in addition to the letters of the alphabet, almost all consonants are shown in sequence in connection with the vowels I, A, U and E (Syllabary). This syllabic writing system was probably used to practice the written characters.[135]
The most important Etruscan written monuments that contain a large number of words include:
No further Etruscan literature has survived and from the early 1st century AD, inscriptions with Etruscan characters have ceased to exist. All existing ancient Etruscan written documents are systematically collected in the Corpus Inscriptionum Etruscarum.
In the middle of the 7th century BC, the Romans adopted the Etruscan writing system and letters. In particular, they used the three different characters C, K and Q for a K sound. Z was also initially adopted into the Roman alphabet, although the affricate TS did not occur in the Latin language. Later, Z was replaced in the alphabet by the newly formed letter G, which was derived from C, and Z was finally placed at the end of the alphabet.[136] The letters Θ, Φ and Ψ were omitted by the Romans because the corresponding aspirated sounds did not occur in their language.
The Etruscan alphabet spread across the northern and central parts of the Italian peninsula. It is assumed that the formation of the Oscan script, probably in the 6th century BC, was fundamentally influenced by Etruscan. The characters of the Umbrian, Faliscan and Venetic languages can also be traced back to Etruscan alphabets.[137]
Timeline

Prehistoric demography, palaeodemography or archaeological demography is the study of human and hominid demography in prehistory.[1]
More specifically, palaeodemography looks at the changes in pre-modern populations in order to determine something about the influences on the lifespan and health of earlier peoples.[citation needed] Reconstructions of ancient population sizes and dynamics are based on bioarchaeology,[2] ancient DNA, and inference from modern population genetics.[citation needed]
Skeletal analysis can yield information such as an estimation of age at time of death. There are numerous methods that can be used;[3] in addition to age estimation and sex estimation, someone versed in basic osteology can ascertain a minimum number of individuals (or MNI) in cluttered contexts—such as in mass graves or an ossuary. This is important, as it is not always obvious how many bodies compose the bones sitting in a heap as they are excavated.
Occasionally, historical disease prevalence for illnesses such as leprosy can also be determined from bone restructuring and deterioration.  Paleopathology, as these investigations are called, can be useful in accurate estimation of mortality rates.
The increasing availability of DNA sequencing since the late 1990s has allowed estimates on Paleolithic effective population sizes.[4][5][6]
Such models suggest a human effective population size of the order of 10,000 individuals for the Late Pleistocene. This includes only the breeding population that produced descendants over the long term, and the actual population may have been substantially larger (in the six digits).[7]
Sherry et al. (1997) based on Alu elements estimated a roughly constant effective population size of the order of 18,000 individuals for the population of Homo ancestral to modern humans over the past one to two million years.[8] Huff et al. (2010) rejected all models with an ancient effective population size larger than 26,000.[9] For ca. 130,000 years ago, Sjödin et al. (2012) estimate an effective population size of the order of 10,000 to 30,000 individuals, and infer an actual "census population"  of early Homo sapiens of roughly 100,000 to 300,000 individuals.[10]
The authors also note that their model disfavours the assumption of an early (pre-Out-of-Africa) population bottleneck affecting all of Homo sapiens.[11]
According to a 2015 study, the total land area of Africa, Eurasia, and Sahul that was habitable to humans during the Last Glacial Maximum (LGM) was around 76,959,712.4 km2. Based on a dataset of average population density of hunter-gatherer groups collected by Lewis R. Binford, which indicate a mean density of 0.1223 humans per km2 and a median density of 0.0444 humans per km2, the combined human population of Africa and Eurasia at the time of the LGM would have been between 2,998,820 and 8,260,262 people. Alternatively, if a human population density based on that of modern medium to large-bodied carnivores, whose median density is 0.0275 individuals per km2 and whose mean density is 0.0384 individuals per km2, is used, a total Afro-Eurasian human population of 2,120,000 to 2,950,000 is obtained. Sahul's population density was significantly lower than that of Afro-Eurasia, being calculated as only 0.005 humans per km2 during the time just prior to the LGM. As a consequence, assuming Sahul possessed an estimated total habitable land area of 9,418,730.8 km2, its population was at most 47,000 at the time of the LGM, and probably less than that given that its population is believed to have declined by as much as 61% during the LGM,[12] a demographic trend supported by archaeological evidence,[13] and it thus would have possessed an even lower actual population density than the calculated density from just before the LGM.[14]
It is estimated by J. Lawrence Angel [15] that the average life span of hominids on the African savanna between 4,000,000 and 200,000 years ago was 20 years. This means that the population would be completely renewed about five times per century,[citation needed] assuming that infant mortality has already been accounted for[clarification needed]. It is further estimated that the population of hominids in Africa fluctuated between 10,000 and 100,000 individuals, thus averaging about 50,000 individuals[clarification needed]. Multiplying 40,000 centuries by 50,000 to 500,000 individuals per century yields a total of 2 billion to 20 billion hominids that lived during that approximately 4,000,000-year time span.[16]

Human food is food which is fit for human consumption, and which humans willingly eat. Food is a basic necessity of life, and humans typically seek food out as an instinctual response to hunger; however, not all things that are edible constitute as human food.
Humans eat various substances for energy, enjoyment and nutritional support. These are usually of plant, animal, or fungal origin, and contain essential nutrients, such as carbohydrates, fats, proteins, vitamins, and minerals. Humans are highly adaptable omnivores, and have adapted to obtain food in many different ecosystems. Historically, humans secured food through two main methods: hunting and gathering and agriculture. As agricultural technologies improved, humans settled into agriculture lifestyles with diets shaped by the agriculture opportunities in their region of the world. Geographic and cultural differences have led to the creation of numerous cuisines and culinary arts, including a wide array of ingredients, herbs, spices, techniques, and dishes. As cultures have mixed through forces like international trade and globalization, ingredients have become more widely available beyond their geographic and cultural origins, creating a cosmopolitan exchange of different food traditions and practices.
Today, the majority of the food energy required by the ever-increasing population of the world is supplied by the industrial food industry, which produces food with intensive agriculture and distributes it through complex food processing and food distribution systems. This system of conventional agriculture relies heavily on fossil fuels, which means that the food and agricultural system is one of the major contributors to climate change, accountable for as much as 37% of the total greenhouse gas emissions.[1] Addressing the carbon intensity of the food system and food waste are important mitigation measures in the global response to climate change.[2][3]
The food system has significant impacts on a wide range of other social and political issues, including: sustainability, biological diversity, economics, population growth, water supply, and access to food. The right to food is a "human right" derived from the International Covenant on Economic, Social and Cultural Rights (ICESCR), recognizing the "right to an adequate standard of living, including adequate food", as well as the "fundamental right to be free from hunger". Because of these fundamental rights, food security is often a priority international policy activity; for example Sustainable Development Goal 2 "Zero hunger" is meant to eliminate hunger by 2030. Food safety and food security are monitored by international agencies like the International Association for Food Protection, World Resources Institute, World Food Programme, Food and Agriculture Organization, and International Food Information Council, and are often subject to national regulation by institutions, such as the Food and Drug Administration in the United States.
Humans are omnivores finding sustenance in vegetables, fruits, cooked meat, milk, eggs, mushrooms and seaweed.[4] Cereal grain is a staple food that provides more food energy worldwide than any other type of crop.[5] Corn (maize), wheat, and rice account for 87% of all grain production worldwide.[6][7][8] Just over half of the world's crops are used to feed humans (55 percent), with 36 percent grown as animal feed and 9 percent for biofuels.[9] Fungi and bacteria are also used in the preparation of fermented foods like bread, wine, cheese and yogurt.[10]
Humans eat thousands of plant species; there may be as many as 75,000 edible species of angiosperms, of which perhaps 7,000 are often eaten.[12] Most human plant-based food calories come from maize, rice, and wheat.[13] Plants can be processed into bread, pasta, cereals, juices and jams, or raw ingredients such as sugar, herbs, spices and oils can be extracted.[14] Oilseeds are often pressed to produce rich oils: sunflower, flaxseed, rapeseed (including canola oil) and sesame.[15]
Animals may be used as food either directly or indirectly. This includes meat, eggs, shellfish and dairy products like milk and cheese.[16] They are an important source of protein and are considered complete proteins for human consumption, as (unlike plant proteins) they contain all the amino acids essential for the human body.[17] Some cultures and people do not consume meat or animal food products for cultural, dietary, health, ethical, or ideological reasons. Vegetarians choose to forgo food from animal sources to varying degrees. Vegans do not consume any foods that contain ingredients from an animal source.
Fish and other marine animals are harvested from lakes, rivers, wetlands, inland waters, coasts, estuaries, mangroves, near-shore areas, and marine and ocean waters. Although aquatic foods contribute significantly to the health of billions of people around the world, they tend to be undervalued nutritionally, primarily because their diversity is framed in a monolithic way as "seafood or fish." Worldwide, aquatic foods are available every season and are produced in a wide variety. Over 2,370 species are harvested from wild fisheries, and about 624 are farmed in aquaculture. Fish powder for infants, fish wafers for snacks, and fish chutneys have all been developed because marine foods are nutrient-dense.[18]
Some animals, specifically humans, have five different types of tastes: sweet, sour, salty, bitter, and umami. As such animals have evolved, the tastes that provide the most energy (sugar and fats) are the most pleasant to eat while others, such as bitter, are not enjoyable.[19] Water, while important for survival, has no taste.[20] Fats, on the other hand, especially saturated fats, are thicker and rich and are thus considered more enjoyable to eat.
Generally regarded as the most pleasant taste, sweetness is almost always provided by a type of simple sugar such as glucose or fructose, or disaccharides such as sucrose, a molecule combining glucose and fructose.[21] Complex carbohydrates are long chains and do not have a sweet taste.
In the natural settings that human primate ancestors evolved in, sweetness intensity should indicate energy density, while bitterness tends to indicate toxicity.[22][23][24] The high sweetness detection threshold and low bitterness detection threshold would have predisposed our primate ancestors to seek out sweet-tasting (and energy-dense) foods and avoid bitter-tasting foods.[25]
Artificial sweeteners such as sucralose are used to mimic the sugar molecule, creating the sensation of sweetness, without the energy. The stevia plant contains a compound known as steviol which, when extracted, has 300 times the sweetness of sugar while having minimal impact on blood sugar.[26]
Sourness is caused by the taste of acids, such as vinegar in alcoholic beverages. Sour foods include citrus, specifically lemons, limes, and to a lesser degree oranges. Sour is evolutionarily significant as it is a sign of food that may have gone rancid due to bacteria.[27] Many foods, however, are slightly acidic and help stimulate the taste buds and enhance flavour.
Saltiness is the taste of alkali metal ions such as sodium and potassium. It is found in almost every food in low to moderate proportions to enhance flavour, although eating pure salt is regarded as highly unpleasant. There are many different types of salt, with each having a different degree of saltiness, including sea salt, fleur de sel, kosher salt, mined salt, and grey salt.
Other than enhancing flavour, salty foods are significant for body needs and maintaining a delicate electrolyte balance, which is the kidney's function. Salt may be iodized, meaning iodine has been added to it. Iodine is a necessary nutrient for maintenance of thyroid function, and the addition of iodine to salt became a public-health practice in the mid 20th century to prevent iodine deficiency and related diseases such as  endemic goitre.[28][29]
Some canned foods, notably soups or packaged broths, tend to be high in salt as a means of preserving the food longer. Historically salt has long been used as a meat preservative as salt promotes water excretion. Similarly, dried foods also promote food safety.
Bitterness is a sensation often considered unpleasant and characterized by having a sharp, pungent taste. Unsweetened dark chocolate, caffeine, lemon rind, and some types of fruit are known to be bitter.
Umami has been described as savoury and is characteristic of broths and cooked meats.[30][31][32][33]: 35–36  Foods that have a strong umami flavor include meats, shellfish, fish (including fish sauce and preserved fish such as Maldives fish, sardines, and anchovies), tomatoes, mushrooms, hydrolyzed vegetable protein, meat extract, yeast extract, cheeses, and soy sauce.
Many scholars claim that the rhetorical function of food is to represent the culture of a country and that it can be used as a form of communication. According to Goode, Curtis and Theophano, food "is the last aspect of an ethnic culture to be lost".[34]
Many cultures have a recognizable cuisine, a specific set of cooking traditions using various spices or a combination of flavours unique to that culture, which evolves. Other differences include preferences (hot or cold, spicy, etc.) and practices, the study of which is known as gastronomy. Many cultures have diversified their foods by utilizing preparation, cooking methods, and manufacturing. This also includes a complex food trade which helps the cultures to economically survive by way of food, not just by consumption.
Some popular types of ethnic foods include Italian, French, Japanese, Chinese, American, Cajun, Thai, African, Indian and Nepalese. Various cultures throughout the world study the dietary analysis of food habits. While evolutionarily speaking, as opposed to culturally, humans are omnivores, religion and social constructs such as morality, activism, or environmentalism will often affect which foods they will consume. Food is eaten and typically enjoyed through the sense of taste, the perception of flavour from eating and drinking. Certain tastes are more enjoyable than others, for evolutionary purposes.
Aesthetically pleasing and eye-appealing food presentations can encourage people to consume food. A common saying is that people "eat with their eyes". Food presented in a clean and appetizing way will encourage a good flavour, even if unsatisfactory.[35][36]
Texture plays a crucial role in the enjoyment of eating foods. Contrasts in textures, such as something crunchy in an otherwise smooth dish, may increase the appeal of eating it. Common examples include adding granola to yoghurt, adding croutons to a salad or soup, and toasting bread to enhance its crunchiness for a smooth topping, such as jam or butter.[37]
Another universal phenomenon regarding food is the appeal of contrast in taste and presentation. For example, such opposite flavours as sweetness and saltiness tend to go well together, as in kettle corn and nuts.
While many foods can be eaten raw, many also undergo some form of preparation for reasons of safety, palatability, texture, or flavour. At the simplest level, this may involve washing, cutting, trimming, or adding other foods or ingredients, such as spices. It may also involve mixing, heating or cooling, pressure cooking, fermentation, or combination with other food. In a home, most food preparation takes place in a kitchen. Some preparation is done to enhance the taste or aesthetic appeal; other preparation may help to preserve the food; others may be involved in cultural identity. A meal is made up of food which is prepared to be eaten at a specific time and place.[38]
The preparation of animal-based food usually involves slaughter, evisceration, hanging, portioning, and rendering. In developed countries, this is usually done outside the home in slaughterhouses, which are used to process animals en masse for meat production. Many countries regulate their slaughterhouses by law. For example, the United States established the Humane Slaughter Act of 1958, which requires that an animal be stunned before killing. This act, like those in many countries, exempts slaughter following religious law, such as kosher, shechita, and dhabīḥah halal. Strict interpretations of kashrut require the animal to be fully aware when its carotid artery is cut.[39]
On the local level, a butcher may commonly break down larger animal meat into smaller manageable cuts, and pre-wrap them for commercial sale or wrap them to order in butcher paper. In addition, fish and seafood may be fabricated into smaller cuts by a fishmonger. However, fish butchery may be done on board a fishing vessel and quick-frozen for the preservation of the quality.[40]
Certain cultures highlight animal and vegetable foods in a raw state. Salads consisting of raw vegetables or fruits are common in many cuisines. Sashimi in Japanese cuisine consists of raw sliced fish or other meat, and sushi often incorporates raw fish or seafood. Steak tartare and salmon tartare are dishes made from diced or ground raw beef or salmon, mixed with various ingredients and served with baguettes, brioche, or frites.[41] In Italy, carpaccio is a dish of very thinly sliced raw beef, drizzled with a vinaigrette made with olive oil.[42] The health food movement known as raw foodism promotes a mostly vegan diet of raw fruits, vegetables, and grains prepared in various ways, including juicing, food dehydration, sprouting, and other methods of preparation that do not heat the food above 118 °F (47.8 °C).[43] An example of a raw meat dish is ceviche, a Latin American dish made with raw meat that is "cooked" from the highly acidic citric juice from lemons and limes along with other aromatics such as garlic.
The term "cooking" encompasses a vast range of methods, tools, and combinations of ingredients to improve the flavour or digestibility of food. Cooking technique, known as culinary art, generally requires the selection, measurement, and combining of ingredients in an ordered procedure to achieve the desired result. Constraints on success include the variability of ingredients, ambient conditions, tools, and the skill of the individual cook.[44] The diversity of cooking worldwide is a reflection of the myriad nutritional, aesthetic, agricultural, economic, cultural, and religious considerations that affect it.[45]
Cooking requires applying heat to a food which usually, though not always, chemically changes the molecules, thus changing its flavour, texture, appearance, and nutritional properties.[46] Cooking certain proteins, such as egg whites, meats, and fish, denatures the protein, causing it to become firm. There is archaeological evidence of roasted foodstuffs at Homo erectus campsites dating from 420,000 years ago.[47] Boiling as a means of cooking requires a container, and has been practised at least since the 10th millennium BC with the introduction of pottery.[48]
There are many different types of equipment used for cooking.
Ovens are mostly hollow devices that get very hot, up to 500 °F (260 °C), and are used for baking or roasting and offer a dry-heat cooking method. Different cuisines will use different types of ovens. For example, Indian culture uses a tandoor oven, which is a cylindrical clay oven which operates at a single high temperature.[49] Western kitchens use variable temperature convection ovens, conventional ovens, toaster ovens, or non-radiant heat ovens like the microwave oven. Classic Italian cuisine includes the use of a brick oven containing burning wood. Ovens may be wood-fired, coal-fired, gas, electric, or oil-fired.[50]
Various types of cooktops are used as well. They carry the same variations of fuel types as the ovens mentioned above. Cook-tops are used to heat vessels placed on top of the heat source, such as a sauté pan, sauce pot, frying pan, or pressure cooker. These pieces of equipment can use either a moist or dry cooking method and include methods such as steaming, simmering, boiling, and poaching for moist methods, while the dry methods include sautéing, pan frying, and deep-frying.[51]
In addition, many cultures use grills for cooking. A grill operates with a radiant heat source from below, usually covered with a metal grid and sometimes a cover. An open-pit barbecue in the American south is one example along with the American-style outdoor grill fueled by wood, liquid propane, or charcoal along with soaked wood chips for smoking.[52] A Mexican style of barbecue is called barbacoa, which involves the cooking of meats such as whole sheep over an open fire. In Argentina, an asado (Spanish for "grilled") is prepared on a grill held over an open pit or fire made upon the ground, on which a whole animal or smaller cuts are grilled.[53]
Restaurants employ chefs to prepare the food, and waiters to serve customers at the table.[54] The term restaurant comes from an old term for a restorative meat broth; this broth (or bouillon) was served in elegant outlets in Paris from the mid 18th century.[55][56] These refined "restaurants" were a marked change from the usual basic eateries such as inns and taverns,[56] and some had developed from early Parisian cafés, such as Café Procope, by first serving bouillon, then adding other cooked food to their menus.[57]
Commercial eateries existed during the Roman period, with evidence of 150 "thermopolia", a form of fast food restaurant, found in Pompeii,[58] and urban sales of prepared foods may have existed in China during the Song dynasty.[59]
In 2005, the population of the United States spent $496 billion on out-of-home dining. Expenditures by type of out-of-home dining were as follows: 40% in full-service restaurants, 37.2% in limited-service restaurants (fast food), 6.6% in schools or colleges, 5.4% in bars and vending machines, 4.7% in hotels and motels, 4.0% in recreational places, and 2.2% in others, which includes military bases.[60][better source needed][relevant?]
Food systems have complex economic and social value chains that effect many parts of the global economy.
Most food has always been obtained through agriculture. With increasing concern over both the methods and products of modern industrial agriculture, there has been a growing trend toward sustainable agricultural practices. This approach, partly fueled by consumer demand, encourages biodiversity, local self-reliance and organic farming methods.[61] Major influences on food production include international organizations (e.g. the World Trade Organization and Common Agricultural Policy), national government policy (or law), and war.[62]
Several organisations have begun calling for a new kind of agriculture in which agroecosystems provide food but also support vital ecosystem services so that soil fertility and biodiversity are maintained rather than compromised. According to the International Water Management Institute and UNEP, well-managed agroecosystems not only provide food, fibre and animal products, they also provide services such as flood mitigation, groundwater recharge, erosion control and habitats for plants, birds, fish and other animals.[63]
Packaged foods are manufactured outside the home for purchase. This can be as simple as a butcher preparing meat or as complex as a modern international food industry. Early food processing techniques were limited by available food preservation, packaging, and transportation. This mainly involved salting, curing, curdling, drying, pickling, fermenting, and smoking.[64] Food manufacturing arose during the Industrial Revolution in the 19th century.[65] This development took advantage of new mass markets and emerging technology, such as milling, preservation, packaging and labeling, and transportation. It brought the advantages of pre-prepared time-saving food to the bulk of ordinary people who did not employ domestic servants.[66]
At the start of the 21st century, a two-tier structure has arisen, with a few international food processing giants controlling a wide range of well-known food brands. There also exists a wide array of small local or national food processing companies.[67] Advanced technologies have also come to change food manufacturing. Computer-based control systems, sophisticated processing and packaging methods, and logistics and distribution advances can enhance product quality, improve food safety, and reduce costs.[66]
The World Bank reported that the European Union was the top food importer in 2005, followed at a distance by the US and Japan. Britain's need for food was especially well-illustrated in World War II.  Despite the implementation of food rationing, Britain remained dependent on food imports and the result was a long-term engagement in the Battle of the Atlantic.
Food is traded and marketed on a global basis. The variety and availability of food is no longer restricted by the diversity of locally grown food or the limitations of the local growing season.[68] Between 1961 and 1999, there was a 400% increase in worldwide food exports.[69] Some countries are now economically dependent on food exports, which in some cases account for over 80% of all exports.[70]
In 1994, over 100 countries became signatories to the Uruguay Round of the General Agreement on Tariffs and Trade in a dramatic increase in trade liberalization. This included an agreement to reduce subsidies paid to farmers, underpinned by the WTO enforcement of agricultural subsidy, tariffs, import quotas, and settlement of trade disputes that cannot be bilaterally resolved.[71] Where trade barriers are raised on the disputed grounds of public health and safety, the WTO refer the dispute to the Codex Alimentarius Commission, which was founded in 1962 by the United Nations Food and Agriculture Organization and the World Health Organization. Trade liberalization has greatly affected world food trade.[72]
Food marketing brings together the producer and the consumer. The marketing of even a single food product can be a complicated process involving many producers and companies. For example, 56 companies are involved in making one can of chicken noodle soup. These businesses include not only chicken and vegetable processors but also the companies that transport the ingredients and those that print labels and manufacture cans.[73] The food marketing system is the largest direct and indirect non-government employer in the United States.
In the pre-modern era, the sale of surplus food took place once a week when farmers took their wares on market day into the local village marketplace. Here food was sold to grocers for sale in their local shops for purchase by local consumers.[45][66] With the onset of industrialization and the development of the food processing industry, a wider range of food could be sold and distributed in distant locations. Typically early grocery shops would be counter-based shops, in which purchasers told the shopkeeper what they wanted so that the shopkeeper could get it for them.[45][74]
In the 20th century, supermarkets were born. Supermarkets brought with them a selfservice approach to shopping using shopping carts and were able to offer quality food at a lower cost through economies of scale and reduced staffing costs. In the latter part of the 20th century, this has been further revolutionized by the development of vast warehouse-sized, out-of-town supermarkets, selling a wide range of food from around the world.[75]
Unlike food processors, food retailing is a two-tier market in which a small number of very large companies control a large proportion of supermarkets. The supermarket giants wield great purchasing power over farmers and processors, and strong influence over consumers. Nevertheless, less than 10% of consumer spending on food goes to farmers, with larger percentages going to advertising, transportation, and intermediate corporations.[76]
Access to food is an economical and a sociological issue. Disadvantaged people typically live further away from providers of healthy food than the middle class. A study of 94 million visits to food retailers showed that Americans travel a median distance of 5.95 km (3.7 miles) each time they buy food.[77]
Food prices refer to the average price level for food across countries, regions and on a global scale.[78] Food prices affect producers and consumers of food. Price levels depend on the food production process, including food marketing and food distribution. Fluctuation in food prices is determined by a number of compounding factors.[79] Geopolitical events, global demand, exchange rates,[80] government policy, diseases and crop yield, energy costs, availability of natural resources for agriculture,[81] food speculation,[82][83][84] changes in the use of soil and weather events directly affect food prices.[85] To a certain extent, adverse price trends can be counteracted by food politics.
The consequences of food price fluctuation are multiple. Increases in food prices, or agflation, endangers food security, particularly for developing countries, and can cause social unrest.[86][87][88] Increases in food prices is related to disparities in diet quality and health,[89] particularly among vulnerable populations, such as women and children.[90]
Food prices will on average continue to rise due to a variety of reasons. Growing world population will put more pressure on the supply and demand. Climate change will increase extreme weather events, including droughts, storms and heavy rain, and overall increases in temperature will affect food production.[91]
Food speculation refers to the buying and selling of futures contracts or other commodity derivatives by traders with the aim of profiting from changes in food prices. Food speculation can be both positive and negative for food producers and buyers. It is betting on food prices in financial markets.
Food speculation by global players like banks, hedge funds or pension funds is alleged to cause price swings in staple foods such as wheat, maize and soy – even though too large price swings in an idealized economy are theoretically ruled out: Adam Smith in 1776 reasoned that the only way to make money from commodities trading is by buying low and selling high, which has the effect of smoothing out price swings and mitigating shortages.[94][95] For the actors, the apparently random swings are predictable, which means potential huge profits. For the global poor, food speculation and resulting price peaks may result in increased poverty or even famine.[96]
Because of its centrality to human life, problems related to access, quality and production of food effect every aspect of human life.
Between the extremes of optimal health and death from starvation or malnutrition, there is an array of disease states that can be caused or alleviated by changes in diet. Deficiencies, excesses, and imbalances in diet can produce negative impacts on health, which may lead to various health problems such as scurvy, obesity, or osteoporosis, diabetes, cardiovascular diseases as well as psychological and behavioral problems. The science of nutrition attempts to understand how and why specific dietary aspects influence health.
Nutrients in food are grouped into several categories. Macronutrients are fat, protein, and carbohydrates. Micronutrients are the minerals and vitamins. Additionally, food contains water and dietary fiber.
As previously discussed, the body is designed by natural selection to enjoy sweet and fattening foods for evolutionary diets, ideal for hunters and gatherers. Thus, sweet and fattening foods in nature are typically rare and are very pleasurable to eat. In modern times these foods are easily available to consumers, which promotes obesity in adults and children alike.
Food deprivation leads to malnutrition and ultimately starvation. This is often connected with famine, which involves the absence of food in entire communities. This can have a devastating and widespread effect on human health and mortality. Rationing is sometimes used to distribute food in times of shortage, most notably during times of war.[62]
Starvation is a significant international problem. Approximately 815 million people are undernourished, and over 16,000 children die per day from hunger-related causes.[101] Food deprivation is regarded as a deficit need in Maslow's hierarchy of needs and is measured using famine scales.[102]
The causes of food going uneaten are numerous and occur throughout the food system, during production, processing, distribution, retail and food service sales, and consumption. Overall, about one-third of the world's food is thrown away.[104][105] A similar amount is lost on top of that by feeding human-edible food to farm animals (the net effect wastes an estimated 1144 kcal/person/day). A 2021 meta-analysis, that did not include food lost during production, by the United Nations Environment Programme found that food waste was a challenge in all countries at all levels of economic development.[106] The analysis estimated that global food waste was 931 million tonnes of food waste (about 121 kg per capita) across three sectors: 61 percent from households, 26 percent from food service and 13 percent from retail.[106]
Food loss and waste is a major part of the impact of agriculture on climate change (it amounts to 3.3 billion tons of CO2e emissions annually[107][108]) and other environmental issues, such as land use, water use and loss of biodiversity. Prevention of food waste is the highest priority, and when prevention is not possible, the food waste hierarchy ranks the food waste treatment options from preferred to least preferred based on their negative environmental impacts.[109] Reuse pathways of surplus food intended for human consumption, such as food donation, is the next best strategy after prevention, followed by animal feed, recycling of nutrients and energy followed by the least preferred option, landfill, which is a major source of the greenhouse gas methane.[110] Other considerations include unreclaimed phosphorus in food waste leading to further phosphate mining. Moreover, reducing food waste in all parts of the food system is an important part of reducing the environmental impact of agriculture, by reducing the total amount of water, land, and other resources used.
Food policy is the area of public policy concerning how food is produced, processed, distributed, purchased, or provided. Food policies are designed to influence the operation of the food and agriculture system balanced with ensuring human health needs. This often includes decision-making around production and processing techniques, marketing, availability, utilization, and consumption of food, in the interest of meeting or furthering social objectives. Food policy can be promulgated on any level, from local to global, and by a government agency, business, or organization. Food policymakers engage in activities such as regulation of food-related industries, establishing eligibility standards for food assistance programs for the poor, ensuring safety of the food supply, food labeling, and even the qualifications of a product to be considered organic.[114]
Most food policy is initiated at the domestic level for purposes of ensuring a safe and adequate food supply for the citizenry.[115] In a developing nation, there are three main objectives for food policy: to protect the poor from crises, to develop long-run markets that enhance efficient resource use, and to increase food production that will in turn promote an increase in income.[116]
Food policy comprises the mechanisms by which food-related matters are addressed or administered by governments, including international bodies or networks, and by public institutions or private organizations. Agricultural producers often bear the burden of governments' desire to keep food prices sufficiently low for growing urban populations. Low prices for consumers can be a disincentive for farmers to produce more food, often resulting in hunger, poor trade prospects, and an increased need for food imports.[115]
Some countries list a legal definition of food, often referring them with the word foodstuff. These countries list food as any item that is to be processed, partially processed, or unprocessed for consumption. The listing of items included as food includes any substance intended to be, or reasonably expected to be, ingested by humans. In addition to these foodstuffs, drink, chewing gum, water, or other items processed into said food items are part of the legal definition of food. Items not included in the legal definition of food include animal feed, live animals (unless being prepared for sale in a market), plants before harvesting, medicinal products, cosmetics, tobacco and tobacco products, narcotic or psychotropic substances, and residues and contaminants.[118]
The right to food, and its variations, is a human right protecting the right of people to feed themselves in dignity, implying that sufficient food is available, that people have the means to access it, and that it adequately meets the individual's dietary needs. The right to food protects the right of all human beings to be free from hunger, food insecurity, and malnutrition.[122] The right to food implies that governments only have an obligation to hand out enough free food to starving recipients to ensure subsistence, it does not imply a universal right to be fed. Also, if people are deprived of access to food for reasons beyond their control, for example, because they are in detention, in times of war or after natural disasters, the right requires the government to provide food directly.[123]
The right is derived from the International Covenant on Economic, Social and Cultural Rights[123] which has 170 state parties as of April 2020.[120] States that sign the covenant agree to take steps to the maximum of their available resources to achieve progressively the full realization of the right to adequate food, both nationally and internationally.[124][122] In a total of 106 countries the right to food is applicable either via constitutional arrangements of various forms or via direct applicability in law of various international treaties in which the right to food is protected.[125]
At the 1996 World Food Summit, governments reaffirmed the right to food and committed themselves to halve the number of hungry and malnourished from 840 to 420 million by 2015. However, the number has increased over the past years, reaching an infamous record in 2009 of more than 1 billion undernourished people worldwide.[122] Furthermore, the number who suffer from hidden hunger – micronutrient deficiences that may cause stunted bodily and intellectual growth in children – amounts to over 2 billion people worldwide.[126]
Whilst under international law, states are obliged to respect, protect and fulfill the right to food, the practical difficulties in achieving this human right are demonstrated by prevalent food insecurity across the world, and ongoing litigation in countries such as India.[127][128] In the continents with the biggest food-related problems – Africa, Asia and South America – not only is there shortage of food and lack of infrastructure but also maldistribution and inadequate access to food.[129]
Food security is the state of having reliable access to a sufficient quantity of affordable, nutritious food. The availability of food for people of any class and state, gender or religion is another element of food security. Similarly, household food security is considered to exist when all the members of a family, at all times, have access to enough food for an active, healthy life.[132] Individuals who are food-secure do not live in hunger or fear of starvation.[133] Food security includes resilience to future disruptions of food supply. Such a disruption could occur due to various risk factors such as droughts and floods, shipping disruptions, fuel shortages, economic instability, and wars.[134] Food insecurity is the opposite of food security: a state where there is only limited or uncertain availability of suitable food.
The concept of food security has evolved over time. The four pillars of food security include availability, access, utilization, and stability.[135] In addition, there are two more dimensions that are important: agency and sustainability. These six dimensions of food security are reinforced in conceptual and legal understandings of the right to food.[136][137] The World Food Summit in 1996 declared that "food should not be used as an instrument for political and economic pressure."[138][139]
There are many causes of food insecurity. The most important ones are high food prices and disruptions in global food supplies for example due to war. There is also climate change, water scarcity, land degradation, agricultural diseases, pandemics and disease outbreaks that can all lead to food insecurity.
Food aid can benefit people suffering from a shortage of food. It can be used to improve peoples' lives in the short term, so that a society can increase its standard of living to the point that food aid is no longer required.[143] Conversely, badly managed food aid can create problems by disrupting local markets, depressing crop prices, and discouraging food production. Sometimes a cycle of food aid dependence can develop.[144] Its provision, or threatened withdrawal, is sometimes used as a political tool to influence the policies of the destination country, a strategy known as food politics. Sometimes, food aid provisions will require certain types of food be purchased from certain sellers, and food aid can be misused to enhance the markets of donor countries.[145] International efforts to distribute food to the neediest countries are often coordinated by the World Food Programme.[146]
Foodborne illness, commonly called "food poisoning", is caused by bacteria, toxins, viruses, parasites, and prions. Roughly 7 million people die of food poisoning each year, with about 10 times as many suffering from a non-fatal version.[147] The two most common factors leading to cases of bacterial foodborne illness are cross-contamination of ready-to-eat food from other uncooked foods and improper temperature control. Less commonly, acute adverse reactions can also occur if chemical contamination of food occurs, for example from improper storage, or use of non-food grade soaps and disinfectants. Food can also be adulterated by a very wide range of articles (known as "foreign bodies") during farming, manufacture, cooking, packaging, distribution, or sale. These foreign bodies can include pests or their droppings, hairs, cigarette butts, wood chips, and all manner of other contaminants. Certain types of food can become contaminated if stored or presented in an unsafe container, such as a ceramic pot with lead-based glaze.[147]
Food poisoning has been recognized as a disease since as early as Hippocrates.[148] The sale of rancid, contaminated, or adulterated food was commonplace until the introduction of hygiene, refrigeration, and vermin controls in the 19th century. Discovery of techniques for killing bacteria using heat, and other microbiological studies by scientists such as Louis Pasteur, contributed to the modern sanitation standards that are ubiquitous in developed nations today. This was further underpinned by the work of Justus von Liebig, which led to the development of modern food storage and food preservation methods.[149] In more recent years, a greater understanding of the causes of food-borne illnesses has led to the development of more systematic approaches such as the Hazard Analysis and Critical Control Points (HACCP), which can identify and eliminate many risks.[150]
Recommended measures for ensuring food safety include maintaining a clean preparation area with foods of different types kept separate, ensuring an adequate cooking temperature, and refrigerating foods promptly after cooking.[151]
Foods that spoil easily, such as meats, dairy, and seafood, must be prepared a certain way to avoid contaminating the people for whom they are prepared. As such, the rule of thumb is that cold foods (such as dairy products) should be kept cold and hot foods (such as soup) should be kept hot until storage. Cold meats, such as chicken, that are to be cooked should not be placed at room temperature for thawing, at the risk of dangerous bacterial growth, such as Salmonella or E. coli.[152]
Some people have allergies or sensitivities to foods that are not problematic to most people. This occurs when a person's immune system mistakes a certain food protein for a harmful foreign agent and attacks it. About 2% of adults and 8% of children have a food allergy.[153] The amount of the food substance required to provoke a reaction in a particularly susceptible individual can be quite small. In some instances, traces of food in the air, too minute to be perceived through smell, have been known to provoke lethal reactions in extremely sensitive individuals. Common food allergens are gluten, corn, shellfish (mollusks), peanuts, and soy.[153] Allergens frequently produce symptoms such as diarrhea, rashes, bloating, vomiting, and regurgitation. The digestive complaints usually develop within half an hour of ingesting the allergen.[153]
Rarely, food allergies can lead to a medical emergency, such as anaphylactic shock, hypotension (low blood pressure), and loss of consciousness. An allergen associated with this type of reaction is peanut, although latex products can induce similar reactions.[153] Initial treatment is with epinephrine (adrenaline), often carried by known patients in the form of an Epi-pen or Twinject.[154][155]
Human diet was estimated to cause perhaps around 35% of cancers in a human epidemiological analysis by Richard Doll and Richard Peto in 1981.[156]  These cancers may be caused by carcinogens that are present in food naturally or as contaminants. Food contaminated with fungal growth may contain mycotoxins such as aflatoxins which may be found in contaminated corn and peanuts.  Other carcinogens identified in food include heterocyclic amines generated in meat when cooked at high temperature, polyaromatic hydrocarbons in charred meat and smoked fish, and nitrosamines generated from nitrites used as food preservatives in cured meat such as bacon.[157]
Anticarcinogens that may help prevent cancer can also be found in many foods especially fruit and vegetables. Antioxidants are important groups of compounds that may help remove potentially harmful chemicals. It is however often difficult to identify the specific components in diet that serve to increase or decrease cancer risk since many foods, such as beef steak and broccoli, contain low concentrations of both carcinogens and anticarcinogens.[157]
There are many international certifications in the cooking field, such as Monde Selection, A.A. Certification, iTQi. They use high-quality evaluation methods to make the food safer.
Many cultures hold some food preferences and some food taboos. Dietary choices can also define cultures and play a role in religion. For example, only kosher foods are permitted by Judaism, halal foods by Islam, and in Hinduism beef is restricted.[161] In addition, the dietary choices of different countries or regions have different characteristics. This is highly related to a culture's cuisine.
Dietary habits play a significant role in the health and mortality of all humans. Imbalances between the consumed fuels and expended energy results in either starvation or excessive reserves of adipose tissue, known as body fat.[162] Poor intake of various vitamins and minerals can lead to diseases that can have far-reaching effects on health. For instance, 30% of the world's population either has, or is at risk for developing, iodine deficiency.[163] It is estimated that at least 3 million children are blind due to vitamin A deficiency.[164] Vitamin C deficiency results in scurvy.[165] Calcium, vitamin D, and phosphorus are inter-related; the consumption of each may affect the absorption of the others. Kwashiorkor and marasmus are childhood disorders caused by lack of dietary protein.[166]
Many individuals limit what foods they eat for reasons of morality or other habits. For instance, vegetarians choose to forgo food from animal sources to varying degrees due to concerns about animal welfare and negative impacts on humans due to environmental impacts of animal agriculture among other reasons. Others choose a healthier diet, avoiding sugars or animal fats and increasing consumption of dietary fiber and antioxidants like various polyphenols.[167] Obesity, a serious problem in the world,[168][169][170] leads to higher chances of developing heart disease, diabetes, cancer and many other diseases.[171] More recently, dietary habits have been influenced by the concerns that some people have about possible impacts on health or the environment from genetically modified food.[172] Further concerns about the impact of industrial farming on animal welfare, human health, and the environment are also affecting contemporary human dietary habits. This has in part led to the emergence of movements with a preference for organic and local food.[173] There is increasing interest and research about optimizing diets for life extension and improved healthspans.[174][175][176][177]

John D. Bengtson (1948-2024) was an American historical and anthropological linguist.[1][2] He had been president and vice-president of the Association for the Study of Language in Prehistory, and had served as editor (or co-editor) of the journal Mother Tongue (1996–2003, 2007–2024).[3] Since 2001 he had been a member/researcher of Evolution of Human Languages,[4] an international project on the linguistic prehistory of humanity coordinated by the Santa Fe Institute. His areas of specialization included Scandinavian languages and linguistics, Indo-European linguistics, Dené–Caucasian (Sino-Caucasian) languages, and paleolinguistics (the study of human prehistory through linguistic evidence).[5]

The multiregional hypothesis, multiregional evolution (MRE), or polycentric hypothesis, is a scientific model that provides an alternative explanation to the more widely accepted "Out of Africa" model of monogenesis for the pattern of human evolution.
Multiregional evolution holds that the human species first arose around two million years ago and subsequent human evolution has been within a single, continuous human species. This species encompasses all archaic human forms such as Homo erectus, Denisovans, and Neanderthals as well as modern forms, and evolved worldwide to the diverse populations of anatomically modern humans (Homo sapiens).
The hypothesis contends that the mechanism of clinal variation through a model of "centre and edge" allowed for the necessary balance between genetic drift, gene flow, and selection throughout the Pleistocene, as well as overall evolution as a global species, but while retaining regional differences in certain morphological features.[1] Proponents of multiregionalism point to fossil and genomic data and continuity of archaeological cultures as support for their hypothesis.
The multiregional hypothesis was first proposed in 1984, and then revised in 2003. In its revised form, it is similar to the assimilation model, which holds that modern humans originated in Africa and today share a predominant recent African origin, but have also absorbed small, geographically variable, degrees of admixture from other regional (archaic) hominin species.[2]
The multiregional hypothesis is not currently the most accepted theory of modern human origin among scientists. "The African replacement model has gained the widest acceptance owing mainly to genetic data (particularly mitochondrial DNA) from existing populations. This model is consistent with the realization that modern humans cannot be classified into subspecies or races, and it recognizes that all populations of present-day humans share the same potential."[3] The African replacement model is also known as the "out of Africa" theory, which is currently the most widely accepted model. It proposes that Homo sapiens evolved in Africa before migrating across the world."[4] And: "The primary competing scientific hypothesis is currently recent African origin of modern humans, which proposes that modern humans arose as a new species in Africa around 100-200,000 years ago, moving out of Africa around 50-60,000 years ago to replace existing human species such as Homo erectus and the Neanderthals without interbreeding.[5][6][7][8] This differs from the multiregional hypothesis in that the multiregional model predicts interbreeding with preexisting local human populations in any such migration."[8][9]
The Multiregional hypothesis was proposed in 1984 by Milford H. Wolpoff, Alan Thorne and Xinzhi Wu.[10][11][1] Wolpoff credits Franz Weidenreich's "Polycentric" hypothesis of human origins as a major influence, but cautions that this should not be confused with polygenism, or Carleton Coon's model that minimized gene flow.[11][12][13] According to Wolpoff, multiregionalism was misinterpreted by William W. Howells, who confused Weidenreich's hypothesis with a polygenic "candelabra model" in his publications spanning five decades:
How did Multiregional evolution get stigmatized as polygeny? We believe it comes from the confusion of Weidenreich's ideas, and ultimately of our own, with Coon's. The historic reason for linking Coon's and Weidenreich's ideas came from the mischaracterizations of Weidenreich's Polycentric model as a candelabra (Howells, 1942, 1944, 1959, 1993), that made his Polycentric model appear much more similar to Coon's than it actually was.[14]
Through the influence of Howells, many other anthropologists and biologists have confused multiregionalism with polygenism i.e. separate or multiple origins for different populations. Alan Templeton for example notes that this confusion has led to the error that gene flow between different populations was added to the Multiregional hypothesis as a "special pleading in response to recent difficulties", despite the fact: "parallel evolution was never part of the multiregional model, much less its core, whereas gene flow was not a recent addition, but rather was present in the model from the very beginning"[15] (emphasis in original). Despite this, multiregionalism is still confused with polygenism, or Coon's model of racial origins, from which Wolpoff and his colleagues have distanced themselves.[16][17] Wolpoff has also defended Wiedenreich's Polycentric hypothesis from being labeled polyphyletic. Weidenreich himself in 1949 wrote: "I may run the risk of being misunderstood, namely that I believe in polyphyletic evolution of man".[18]
In 1998, Wu founded a China-specific Multiregional model called "Continuity with [Incidental] Hybridization".[19][20] Wu's variant only applies the Multiregional hypothesis to the East Asian fossil record, and is popular among Chinese scientists.[21] However, James Leibold, a political historian of modern China, has argued the support for Wu's model is largely rooted in Chinese nationalism.[22] Outside of China, the Multiregional hypothesis has limited support, held only by a small number of paleoanthropologists.[23]
Chris Stringer, a leading proponent of the more mainstream recent African origin theory, debated Multiregionalists such as Wolpoff and Thorne in a series of publications throughout the late 1980s and 1990s.[24][25][26][27] Stringer describes how he considers the original Multiregional hypothesis to have been modified over time into a weaker variant that now allows a much greater role for Africa in human evolution, including anatomical modernity (and subsequently less regional continuity than was first proposed).[28]
Stringer distinguishes the original or "classic" Multiregional model as having existed from 1984 (its formulation) until 2003, to a "weak" post-2003 variant that has "shifted close to that of the Assimilation Model".[29][30]
The finding that "Mitochondrial Eve" was relatively recent and African seemed to give the upper hand to the proponents of the Out of Africa hypothesis. But in 2002, Alan Templeton published a genetic analysis involving other loci in the genome as well, and this showed that some variants that are present in modern populations existed already in Asia hundreds of thousands of years ago.[31] This meant that even if our male line (Y chromosome) and our female line (mitochondrial DNA) came out of Africa in the last 100,000 years or so, we have inherited other genes from populations that were already outside of Africa. Since this study other studies have been done using much more data (see Phylogeography).
Proponents of the multiregional hypothesis see regional continuity of certain morphological traits spanning the Pleistocene in different regions across the globe as evidence against a single replacement model from Africa. In general, three major regions are recognized: Europe, China, and Indonesia (often including Australia).[32][33][34] Wolpoff cautions that the continuity in certain skeletal features in these regions should not be seen in a racial context, instead calling them morphological clades; defined as sets of traits that "uniquely characterise a geographic region".[35] According to Wolpoff and Thorne (1981): "We do not regard a morphological clade as a unique lineage, nor do we believe it necessary to imply a particular
taxonomic status for it".[36] Critics of multiregionalism have pointed out that no single human trait is unique to a geographical region (i.e. confined to one population and not found in any other) but Wolpoff et al. (2000) note that regional continuity only recognizes combinations of features, not traits if individually accessed, a point they elsewhere compare to the forensic identification of a human skeleton:
Regional continuity ... is not the claim that such features do not appear elsewhere; the genetic structure of the human species makes such a possibility unlikely to the extreme. There may be uniqueness in combinations of traits, but no single trait is likely to have been unique in a particular part of the world although it might appear to be so because of the incomplete sampling provided by the spotty human fossil record.
Combinations of features are "unique" in the sense of being found in only one region, or more weakly limited to one region at high frequency (very rarely in another). Wolpoff stresses that regional continuity works in conjunction with genetic exchanges between populations. Long-term regional continuity in certain morphological traits is explained by Alan Thorne's "centre and edge"[37] population genetics model which resolves Weidenreich's paradox of "how did populations retain geographical distinctions and yet evolve together?". For example, in 2001 Wolpoff and colleagues published an analysis of character traits of the skulls of early modern human fossils in Australia and central Europe. They concluded that the diversity of these recent humans could not "result exclusively from a single late Pleistocene dispersal", and implied dual ancestry for each region, involving interbreeding with Africans.[38]
Thorne held that there was regional continuity in Indonesia and Australia for a morphological clade.[39][40] This sequence is said to consist of the earliest fossils from Sangiran, Java, that can be traced through Ngandong and found in prehistoric and recent Aboriginal Australians. In 1991, Andrew Kramer tested 17 proposed morphological clade features. He found that: "a plurality (eight) of the seventeen non-metric features link Sangiran to modern Australians" and that these "are suggestive of morphological continuity, which implies the presence of a genetic continuum in Australasia dating back at least one million years"[41] but Colin Groves has criticized Kramer's methodology, pointing out that the polarity of characters was not tested and that the study is actually inconclusive.[42] Phillip Habgood discovered that the characters said to be unique to the Australasian region by Thorne are plesiomorphic:
...it is evident that all of the characters proposed... to be 'clade features' linking Indonesian Homo erectus material with Australian Aboriginal crania are retained primitive features present on Homo erectus and archaic Homo sapiens crania in general. Many are also commonly found on the crania and mandibles of anatomically-modern Homo sapiens from other geographical locations, being especially prevalent on the robust Mesolithic skeletal material from North Africa."[43]
Yet, regardless of these criticisms Habgood (2003) allows for limited regional continuity in Indonesia and Australia, recognizing four plesiomorphic features which do not appear in such a unique combination on fossils in any other region: a sagittally flat frontal bone, with a posterior position of minimum frontal breadth, great facial prognathism, and zygomaxillary tuberosities.[44] This combination, Habgood says, has a "certain Australianness about it".
Wolpoff, initially skeptical of Thorne's claims, became convinced when reconstructing the Sangiran 17 Homo erectus skull from Indonesia, when he was surprised that the skull's face to vault angle matched that of the Australian modern human Kow Swamp 1 skull in excessive prognathism. Durband (2007) in contrast states that "features cited as showing continuity between Sangiran 17 and the Kow Swamp sample disappeared in the new, more orthognathic reconstruction of that fossil that was recently completed".[45] Baba et al. who newly restored the face of Sangiran 17 concluded: "regional continuity in Australasia is far less evident than Thorne and Wolpoff argued".[46]
Xinzhi Wu has argued for a morphological clade in China spanning the Pleistocene, characterized by a combination of 10 features.[47][48] The sequence is said to start with Lantian and Peking Man, traced to Dali, to Late Pleistocene specimens (e.g. Liujiang) and recent Chinese. Habgood in 1992 criticized Wu's list, pointing out that most of the 10 features in combination appear regularly on fossils outside China.[49] He did though note that three combined: a non-depressed nasal root, non-projecting perpendicularly oriented nasal bones and facial flatness are unique to the Chinese region in the fossil record and may be evidence for limited regional continuity. However, according to Chris Stringer, Habgood's study suffered from not including enough fossil samples from North Africa, many of which exhibit the small combination he considered to be region-specific to China.[27]
Facial flatness as a morphological clade feature has been rejected by many anthropologists since it is found on many early African Homo erectus fossils, and is therefore considered plesiomorphic,[50] but Wu has responded that the form of facial flatness in the Chinese fossil record appears distinct to other (i.e. primitive) forms. Toetik Koesbardiati in her PhD thesis "On the Relevance of the Regional Continuity Features of the Face in East Asia" also found that a form of facial flatness is unique to China (i.e. only appears there at high frequency, very rarely elsewhere) but cautions that this is the only available evidence for regional continuity: "Only two features appear to show a tendency as suggested by the Multiregional model: flatness at the upper face expressed by an obtuse nasio-frontal angle and flatness at the middle part of the face expressed by an obtuse zygomaxillay angle".
Shovel-shaped incisors are commonly cited as evidence for regional continuity in China.[51][52] Stringer (1992) however found that shovel-shaped incisors are present on >70% of the early Holocene Wadi Halfa fossil sample from North Africa, and common elsewhere.[53] Frayer, et al. (1993) have criticized Stringer's method of scoring shovel-shaped incisor teeth. They discuss the fact that there are different degrees of "shovelled" e.g. trace (+), semi (++), and marked (+++), but that Stringer misleadingly lumped all these together: "...combining shoveling categories in this manner is biologically meaningless and misleading, as the statistic cannot be validly compared with the very high frequencies for the marked shoveling category reported for East Asians."[33] Palaeoanthropologist Fred H. Smith (2009) also emphasizes that: "It is the pattern of shoveling that identities as an East Asian regional feature, not just the occurrence of shoveling of any sort".[2] Multiregionalists argue that marked (+++) shovel-shaped incisors only appear in China at a high frequency, and have <10% occurrence elsewhere.
Since the early 1990s, David W. Frayer has described what he regards as a morphological clade in Europe.[54][55][56] The sequence starts with the earliest dated Neanderthal specimens (Krapina and Saccopastore skulls) traced through the mid-Late Pleistocene (e.g. La Ferrassie 1) to Vindija Cave, and late Upper Palaeolithic Cro-Magnons or recent Europeans. Although many anthropologists consider Neanderthals and Cro Magnons morphologically distinct,[57][58] Frayer maintains quite the opposite and points to their similarities, which he argues is evidence for regional continuity:
"Contrary to Brauer's recent pronouncement that there is a large and generally recognized morphological gap between the Neanderthals and the early moderns, the actual evidence provided by the extensive fossil record of late Pleistocene Europe shows considerable continuity between Neanderthals and subsequent Europeans."[33]
Frayer et al. (1993) consider there to be at least four features in combination that are unique to the European fossil record: a horizontal-oval shaped mandibular foramen, anterior mastoid tubercle, suprainiac fossa, and narrowing of the nasal breadth associated with tooth-size reduction. Regarding the latter, Frayer observes a sequence of nasal narrowing in Neanderthals, following through to late Upper Palaeolithic and Holocene (Mesolithic) crania. His claims are disputed by others,[59] but have received support from Wolpoff, who regards late Neanderthal specimens to be "transitional" in nasal form between earlier Neanderthals and later Cro Magnons.[60] Based on other cranial similarities, Wolpoff et al. (2004) argue for a sizable Neanderthal contribution to modern Europeans.[61]
More recent claims regarding continuity in skeletal morphology in Europe focus on fossils with both Neanderthal and modern anatomical traits, to provide evidence of interbreeding rather than replacement.[62][63][64] Examples include the Lapedo child found in Portugal[65] and the Oase 1 mandible from Peștera cu Oase, Romania,[66] though the "Lapedo child" is disputed by some.[67]
A 1987 analysis of mitochondrial DNA from 147 people by Cann et al. from around the world indicated that their mitochondrial lineages all coalesced in a common ancestor from Africa between 140,000 and 290,000 years ago.[68] The analysis suggested that this reflected the worldwide expansion of modern humans as a new species, replacing, rather than mixing with, local archaic humans outside of Africa. Such a recent replacement scenario is not compatible with the Multiregional hypothesis and the mtDNA results led to increased popularity for the alternative single replacement theory.[69][70][71] According to Wolpoff and colleagues:[72]
When they were first published, the Mitochondrial Eve results were clearly incongruous with Multiregional evolution, and we wondered how the two could be reconciled.
Multiregionalists have responded to what they see as flaws in the Eve theory,[73] and have offered contrary genetic evidences.[74][75][76] Wu and Thorne have questioned the reliability of the molecular clock used to date Eve.[77][78] Multiregionalists point out that Mitochondrial DNA alone can not rule out interbreeding between early modern and archaic humans, since archaic human mitochondrial strains from such interbreeding could have been lost due to genetic drift or a selective sweep.[79][80] Wolpoff for example states that Eve is "not the most recent common ancestor of all living people" since "Mitochondrial history is not population history".[81]
Neanderthal mitochondrial DNA (mtDNA) sequences from Feldhofer and Vindija Cave are substantially different from modern human mtDNA.[82][83][84] Multiregionalists however have discussed the fact that the average difference between the Feldhofer sequence and living humans is less than that found between chimpanzee subspecies,[85][86] and therefore that while Neanderthals were different subspecies, they were still human and part of the same lineage.
Initial analysis of Y chromosome DNA, which like mitochondrial DNA, is inherited from only one parent, was consistent with a recent African replacement model. However, the mitochondrial and Y chromosome data could not be explained by the same modern human expansion out of Africa; the Y chromosome expansion would have involved genetic mixing that retained regionally local mitochondrial lines. In addition, the Y chromosome data indicated a later expansion back into Africa from Asia, demonstrating that gene flow between regions was not unidirectional.[87]
An early analysis of 15 noncoding sites on the X chromosome found additional inconsistencies with the recent African replacement hypothesis. The analysis found a multimodal distribution of coalescence times to the most recent common ancestor for those sites, contrary to the predictions for recent African replacement; in particular, there were more coalescence times near 2 million years ago (mya) than expected, suggesting an ancient population split around the time humans first emerged from Africa as Homo erectus, rather than more recently as suggested by the mitochondrial data. While most of these X chromosome sites showed greater diversity in Africa, consistent with African origins, a few of the sites showed greater diversity in Asia rather than Africa. For four of the 15 gene sites that did show greater diversity in Africa, the sites' varying diversity by region could not be explained by simple expansion from Africa, as would be required by the recent African replacement hypothesis.[88]
Later analyses of X chromosome and autosomal DNA continued to find sites with deep coalescence times inconsistent with a single origin of modern humans,[89][90][91][92][93] diversity patterns inconsistent with a recent expansion from Africa,[94] or both.[95][96] For example, analyses of a region of RRM2P4 (ribonucleotide reductase M2 subunit pseudogene 4) showed a coalescence time of about 2 Mya, with a clear root in Asia,[97][98] while the MAPT locus at 17q21.31 is split into two deep genetic lineages, one of which is common in and largely confined to the present European population, suggesting inheritance from Neanderthals.[99][100][101][102] In the case of the Microcephalin D allele, evidence for rapid recent expansion indicated introgression from an archaic population.[103][104][105][106] However, later analysis, including of the genomes of Neanderthals, did not find the Microcephalin D allele (in the proposed archaic species), nor evidence that it had introgressed from an archaic lineage as previously suggested.[107][108][109]
In 2001, a DNA study of more than 12,000 men from 163 East Asian regions showed that all of them carry a mutation that originated in Africa about 35,000 to 89,000 years ago and these "data do not support even a minimal in situ hominid contribution in the origin of anatomically modern humans in East Asia".[110]
In a 2005 review and analysis of the genetic lineages of 25 chromosomal regions, Alan Templeton found evidence of more than 34 occurrences of gene flow between Africa and Eurasia. Of these occurrences, 19 were associated with continuous restricted gene exchange through at least 1.46 million years ago; only 5 were associated with a recent expansion from Africa to Eurasia. Three were associated with the original expansion of Homo erectus out of Africa around 2 million years ago, 7 with an intermediate expansion out of Africa at a date consistent with the expansion of Acheulean tool technology, and a few others with other gene flows such as an expansion out of Eurasia and back into Africa subsequent to the most recent expansion out of Africa. Templeton rejected a hypothesis of complete recent African replacement with greater than 99% certainty (p < 10−17).[111]
Recent analyses of DNA taken directly from Neanderthal specimens indicates that they or their ancestors contributed to the genome of all humans outside of Africa, indicating there was some degree of interbreeding with Neanderthals before their replacement.[112] It has also been shown that Denisova hominins contributed to the DNA of Melanesians and Australians through interbreeding.[113]
By 2006, extraction of DNA directly from some archaic human samples was becoming possible. The earliest analyses were of Neanderthal DNA, and indicated that the Neanderthal contribution to modern human genetic diversity was no more than 20%, with a most likely value of 0%.[114] By 2010, however, detailed DNA sequencing of the Neanderthal specimens from Europe indicated that the contribution was nonzero, with Neanderthals sharing 1-4% more genetic variants with living non-Africans than with living humans in sub-Saharan Africa.[115][116] In late 2010, a recently discovered non-Neanderthal archaic human, the Denisova hominin from south-western Siberia, was found to share 4–6% more of its genome with living Melanesian humans than with any other living group, supporting admixture between two regions outside of Africa.[117][118] In August 2011, human leukocyte antigen (HLA) alleles from the archaic Denisovan and Neanderthal genomes were found to show patterns in the modern human population demonstrating origins from these non-African populations; the ancestry from these archaic alleles at the HLA-A site was more than 50% for modern Europeans, 70% for Asians, and 95% for Papua New Guineans.[119] Proponents of the multiregional hypothesis believe the combination of regional continuity inside and outside of Africa and lateral gene transfer between various regions around the world supports the multiregional hypothesis. However, "Out of Africa" Theory proponents also explain this with the fact that genetic changes occur on a regional basis rather than a continental basis, and populations close to each other are likely to share certain specific regional SNPs while sharing most other genes in common.[120][121]
Migration Matrix theory (A=Mt) indicates that dependent upon the potential contribution of Neanderthal ancestry, we would be able to calculate the percentage of Neanderthal mtDNA contribution to the human species. As we do not know the specific migration matrix, we are unable to input the exact data, which would answer these questions irrefutably.[85]

This timeline is an incomplete list of significant events of human migration and exploration by sea. This timeline does not include migration and exploration over land, including migration across land that has subsequently submerged beneath the sea, such as the initial settlement of Great Britain and Ireland.
On 27 June 1542, Cabrillo sails northwest with three ships to explore the Pacific Coast of Mexico and the Californias. Cabrillo reaches the Russian River before turning back. Cabrillo dies in the Channel Islands on the return voyage.
On 1 November 1542, López de Villalobos sails west with six galleons and 400 men across the Pacific Ocean to the East Indies. The expedition explores the Philippine Islands and the eastern Islands of Indonesia, but is captured by Portuguese authorities in 1544. López de Villalobos dies on 4 April 1544, in a Portuguese prison cell on the Island of Amboyna. The Portuguese send the 117 survivors of the expedition to Lisbon.
References are included in the linked articles.

Hasmukh Dhirajlal Sankalia (10 December 1908 – 28 January 1989) was an Indian Sanskrit scholar and archaeologist specialising in proto- and ancient Indian history. He is considered to have pioneered archaeological excavation techniques in India, with several significant discoveries from the prehistoric period to his credit. Sankalia received the Ranjitram Suvarna Chandrak award in 1966.
And also received Padma Bhushan in the year 1974.
Sankalia was born in Mumbai into a family of lawyers hailing from Gujarat. A frail infant, he was not expected to survive.
At fifteen, Sankalia read the Gujarati translation of Lokmanya Tilak's The Arctic Home in the Vedas. Although he understood little of the book (p. 6), he was determined to "do something to know about the Aryans in India" (ibid.).[This quote needs a citation] To this end, Sankalia decided to emulate Tilak and study Sanskrit and mathematics. He received a B.A. degree in Sanskrit, and received the Chimanlal Ranglal Prize. Sankalia made Indian prehistory his life's work, and never lost sight of the origin of the Indo-Aryan peoples.[3] (1962c: 125; 1963a: 279–281; 1974: 553–559; 1978a: 79, etc.). He studied English, which introduced him to textual criticism (p. 7), and wrote an article on Kundamala and the Uttararamacarita in which he convincingly proved that Dinnaga (author of the former) influenced Bhavabhuti (author of the latter).[4] Bengali scholar K. K. Dutt arrived at similar conclusion independently of Sankalia.[5]
Sankalia studied ancient Indian history for his M. A. degree at the new Indian Historical Research Institute (now the Heras Institute),[citation needed] and worked on the ancient university at Nalanda for his M.A. dissertation. His dissertation included chapters on history, art and architecture, iconography and the influence of the Nalanda school of art on Greater India (particularly Java).[6] Sankalia visited a number of sites, and studied Buddhism with B. Bhattacharya (p. 10). These studies led to his later study of Gujarat. He passed the LLB examinations at the request of his father and uncle (who were both lawyers), and was expected to follow them (cf. pp. 10, 13, 28). However, Sankalia decided to go to England for his doctoral degree. He wrote an essay, "Caitya caves in the Bombay Presidency", which earned the Bhagwan Lal Indraji prize.[citation needed]
Sankalia captained a cricket team on College Day.[7] He also enjoyed kite-flying[8] and gardening.
Sankalia left for England, and enrolled at the University of London for his PhD on the archaeology of Gujarat. He studied under Bernard Ashmole (Roman classical archaeology), Sidney Smith (Sumerian language), K. de B. Codrington (museology), F. J. Richards (Indian archaeology) and R. E. M. Wheeler (field archaeology) (p. 18).
From Richards, Sankalia learnt geography, geology, anthropology, ethnography and toponymy. He focused on the latter, applying it to inscriptions in Gujarat[9] and elsewhere (Sankalia 1942a; 1984). Sankalia encouraged his students to pursue toponymy, opening a new field in Indian archaeology.
Wheeler, who was excavating at the site of Maiden Castle, Dorset and had perfected his field techniques (begun in 1921), was a significant influence.[10] He lectured on field techniques, in addition to providing practical training. Sankalia said about Wheeler's training, "The training was brief, lasting just about a month or so, but it was of immense importance for my future career. I learnt here, not only what was stratigraphical digging and drawing a section and three-dimensional recording of finds [...] but was also made aware of the necessity of minute-to-minute supervision of the trench under one's charge for [...] at any moment the layer might change and [which should] be noted as early as possible" (pp. 26–27). Sankalia (cf. pp. 112 ff.; 1938; and his popular articles), influenced by Wheeler, was a proponent of popular archaeology.
After returning to India, Sankalia joined Deccan College in 1939 as a professor of proto- and ancient Indian history and began systematic surveys of the monuments in and around Pune with his students. These yielded papers on the megaliths of Bhavsari[11] and the Yadava-period Temple of Pur.[12] At the request of Archaeological Survey of India director general K. N. Dikshit, Sankalia undertook explorations in Gujarat to test Bruce Foote's hypothesis of a hiatus between the Lower Palaeolithic and Neolithic phases;[13] this made him into a prehistorian.
He also conducted other expeditions in Gujarat. During his second expedition, Sankalia found the first human Stone Age skeleton.[citation needed] The Mesolithic site of Langhnaj, "the first Stone Age site to have been excavated scientifically",[This quote needs a citation] was excavated stratigraphically. F. E. Zeuner, an authority on environmental archaeology, was invited by Wheeler to interpret the palaeoclimate of Gujarat. Sankalia was profoundly influenced by Zeuner,[14] from whom he learnt geochronology, geology, the stratigraphy of geological deposits and pluvial and inter-pluvial mechanics.[15]
Sankalia excavated the Kolhapur site in 1945–46 with M. G. Dikshit (Sankalia and Dikshit 1952). Before the excavation, his detailed surveys of the banks of the Godavari River and its tributaries revealed a flake-tool industry.[16][17] These findings were also observed in a stratigraphical deposit at Gangapur (Gangawadi), near Nasik, where flakes, cleavers and hand axes[18] were discovered. This developed industry, as later research proved, was part of the Middle Palaeolithic. Sankalia's explorations in the Pravara River valley (at Nevasa) yielded palaeolithic industries and animal fossils.[19]
The occurrence of Northern Black Polished Ware at Nasik (mentioned in the Puranas and traditional tales), reported to Sankalia by M. N. Deshpande, made him anxious to unearth evidence correlating to the Early Historical Period and (if possible) unearth pre- and proto-historic cultures.[20] The excavation was successfully carried out.[21]
Sankalia's success at Nasik–Jorwe inspired him to excavate the site at Maheshwar (the Mahishmati of the Haihayas, as described in the Puranas) to prove the tradition's historicity. The excavation was carried out at the site and at Navdatoli in 1952–53 in a joint expedition with the Maharaja Sayajirao University of Baroda. This revealed a developed chalcolithic culture dating to between the decline of the Harappan civilisation and the beginning of the Early Historical Period, largely explaining the hiatus between the periods. The culture was interpreted by Sankalia, mainly on the basis of resemblance of its pottery to that of Iran, as of Aryan origin.[22] The horizontal excavation at Navdatoli was made in 1957–59 to reveal the settlement pattern, reconstruct the socioeconomic life of the chalcolithic people, and corroborate Sankalia's Aryan hypothesis.[23]
Sankalia's excavation at Nevasa, intended to prove (or disprove) the legend of its association with Jnaneshvara, revealed human occupation from the Lower Palaeolithic era to the Muslim-Maratha period.[24]
Sankalia went to Kashmir to study its geological deposits, which had been investigated by De Terra, Paterson, and Wadia without finding early human evidence. When Sankalia was examining a deposit he saw a worked flake with a prominent bulb of percussion, establishing the existence of early humans in Kashmir. He also discovered a hand axe in the same deposit, dating to the ice age or slightly later.[25]
After establishing the cultural sequence of the Chalcolithic cultures in Deccan and Central India, Sankalia wanted to reconstruct the lives of the Chalcolithic people with large-scale horizontal excavations at Nevasa and Navdatoli. The former site was found to be highly disturbed and the deposits overlying the Chalcolithic layers were too thick to be thoroughly removed, and the plan was abandoned. The site of Inamgaon was well-preserved, however, and was excavated over a 12-year period. After Sankalia's retirement in 1973 the excavation was completed by Z. D. Ansari and M. K. Dhavalikar, and its report was published in three volumes.[26]
After his retirement, Sankalia lived on campus and was appointed professor emeritus of the department. At his home, he discovered what he believed were palaeolithic implements.[27][28][29] After publishing his studies on the Ramayana,[30][31] new archaeology[32] and prehistoric art,[33] he died at age 80 on 28 January 1989.
Sankalia received the Narmad Suvarna Chandrak.

Homo floresiensis ( /flɔːrˈɛziːˌɛn.sɪs/), also known as "Flores Man" or "Hobbit" (after the fictional species), is an extinct species of small archaic humans that inhabited the island of Flores, Indonesia, until the arrival of modern humans about 50,000 years ago.
The remains of an individual who would have stood about 1.1 m (3 ft 7 in) in height were discovered in 2003 at Liang Bua cave. As of 2015, partial skeletons of 15 individuals have been recovered; this includes one complete skull, referred to as "LB1".[1][2]
Homo floresiensis is thought to have arrived on Flores around 1.27–1 million years ago.[3][4] There is debate as to whether H. floresiensis represents a descendant of Javanese Homo erectus that reduced its body size as a result of insular dwarfism, or whether it represents an otherwise undetected migration of small, Australopithecus or Homo habilis-grade archaic humans outside of Africa.[5]
This hominin was at first considered remarkable for its survival until relatively recent times, initially thought to be only 12,000 years ago.[6] However, more extensive stratigraphic and chronological work has pushed the dating of the most recent evidence of its existence back to 50,000 years ago.[7][8][9] The Homo floresiensis skeletal material at Liang Bua is now dated from 60,000 to 100,000 years ago; stone tools recovered alongside the skeletal remains were from archaeological horizons ranging from 50,000 to 190,000 years ago.[7] Other earlier remains from Mata Menge date to around 700,000 years ago.[10]
The first specimens were discovered on the Indonesian island of Flores on 2 September 2003 by a joint Australian-Indonesian team of archaeologists looking for evidence of the original human migration of modern humans from Asia to Australia.[1][6] They instead recovered a nearly complete, small-statured skeleton, LB1, in the Liang Bua cave, and subsequent excavations in 2003 and 2004 recovered seven additional skeletons, initially dated from 38,000 to 13,000 years ago.[2]
In 2004, a separate species Homo floresiensis was named and described by Peter Brown et al., with LB1 as the holotype. A tooth, LB2, was referred to the species.[1] LB1 is a fairly complete skeleton, including a nearly complete skull, which belonged to a 30-year-old woman, and has been nicknamed "Little Lady of Flores" or "Flo".[1][11] An arm bone provisionally assigned to H. floresiensis, specimen LB3, is about 74,000 years old. The specimens are not fossilized and have been described as having "the consistency of wet blotting paper". Once exposed, the bones had to be left to dry before they could be dug up.[12][13] The discoverers proposed that a variety of features, both primitive and derived, identify these individuals as belonging to a new species.[1][6] Based on previous date estimates, the discoverers also proposed that H. floresiensis lived contemporaneously with modern humans on Flores.[14] Before publication, the discoverers were considering placing LB1 into her own genus, Sundanthropus floresianus (lit. 'Sunda human from Flores'); however, reviewers of the article recommended that, despite her size, she should be placed in the genus Homo.[15]
In 2009, additional finds were reported, increasing the minimum number of individuals represented by bones to fourteen.[16] In 2015, teeth were referred to a fifteenth individual, LB15.[17][18]
Stone implements of a size considered appropriate to these small humans are also widely present in the cave. The implements are at horizons initially dated to 95,000 to 13,000 years ago.[2] Modern humans reached the region by around 50,000 years ago, by which time H. floresiensis is thought to have gone extinct.[7] Comparisons of the stone artifacts with those made by modern humans in East Timor indicate many technological similarities.[19]
The fossils are property of the Indonesian state. In early December 2004, Indonesian paleoanthropologist Teuku Jacob, formerly chief paleontologist of the Indonesian Gadjah Mada University, removed most of the remains from their repository, Jakarta's National Research Centre of Archaeology, with the permission of one of the institute's directors, Raden Panji Soejono, and kept them for three months.[20][21][22][23] Professor Jacob did not believe the specimens represented a different species, contending that the LB1 find was from a 25–30 year-old omnivorous subspecies of H. sapiens, probably a pygmy, and that the small skull was due to microcephaly, which produces a small brain and skull. Professor Richard Roberts of the University of Wollongong in Australia and other anthropologists expressed the fear that important scientific evidence would be sequestered by a small group of scientists who neither allowed access by other scientists nor published their own research.[21] Jacob returned the remains on 23 February 2005 with portions severely damaged[24] and missing two leg bones.[25]
Press reports thus described the condition of the returned remains: "[including] long, deep cuts marking the lower edge of the Hobbit's jaw on both sides, said to be caused by a knife used to cut away the rubber mould ... the chin of a second Hobbit jaw was snapped off and glued back together. Whoever was responsible misaligned the pieces and put them at an incorrect angle ... The pelvis was smashed, destroying details that reveal body shape, gait and evolutionary history.",[26] causing the discovery team leader Morwood to remark, "It's sickening; Jacob was greedy and acted totally irresponsibly."[24]
Jacob, however, denied any wrongdoing. He stated that the damages occurred during transport from Yogyakarta back to Jakarta[26][27] despite the claimed physical evidence that the jawbone had been broken while making a mould of the bones.[24][28]
In 2005, Indonesian officials forbade access to the cave. Some news media, such as the BBC, expressed the opinion that the restriction was to protect Jacob, who was considered "Indonesia's king of palaeoanthropology", from being proved wrong. Scientists were allowed to return to the cave in 2007, shortly after Jacob's death.[26]
Because of the deep neighbouring Lombok Strait, Flores remained an isolated island during episodes of low sea level. Therefore, the ancestors of H. floresiensis could only have reached the island by oceanic dispersal, most likely by rafting.[29] The oldest stone tools on Flores are around 1 million years old.[3][4] Stone artifacts are absent from sites over 1.27 million years old, suggesting that the ancestors of H. floresiensis arrived after this time.[4]
In 2016, fossil teeth and a partial jaw from hominins assumed to be ancestral to H. floresiensis were discovered at Mata Menge, about 74 km (46 mi) from Liang Bua. They date to about 700,000 years ago. Other remains (including a humerus) were later described from Mata Menge, with the remains subsequently being directly assigned to H. floresiensis. These remains are about the same size or somewhat smaller than the remains from Liang Bua, suggesting the size of the species remained stable for hundreds of thousands of years up until its extinction.[30][31][32][10]
Two hypotheses have been proposed as to the origin of H. floresiensis. The first proposes that H. floresiensis descended from an early migration of very primitive small Australopithecus/Homo habilis-grade archaic humans outside of Africa prior to 1.75 million years ago. This is based on various aspects of H. floresiensis sketetal anatomy, such as its feet bones[33] being considered as more similar to those of very archaic humans such as Australopithecus and Homo habilis than to Homo erectus.[33][34][35][36][37] This position has been supported by several cladistical analyses.[34][35][36][37][38] Other authors have argued that H. floresiensis instead likely represents the descendants of a population of Javanese Homo erectus that became isolated on Flores, with the small body size being the result of insular dwarfism, a well known evolutionary trend found among various island animals.[39][40][41][5][42] These authors alternatively suggest that H. floresiensis has several cranial and dental similarities to H. erectus, particularly to early Javanese Homo erectus.[40][41][5][43][42] These authors also dispute some of the similarities to Australopithecus and Homo habilis-grade archaic humans,[10] and suggest that others may have been the result of evolutionary reversals/convergence.[5]
It has been noted that there is no evidence archaic humans in the adjacent (and likely source) region of Java earlier than 1.3-1.5[44] or 1.8 million[45] years ago, with the earliest human presence on Java being represented by Homo erectus,[10] with there also being no evidence of Australopithecus or Homo habilis-grade archaic humans anywhere outside of Africa, which supporters of the Homo erectus-origin hypothesis suggest makes the descent of H. floresiensis from these more primitive hominins unlikely.[10][5]
In 2006, two teams attempted to extract DNA from a tooth discovered in 2003; however, both teams were unsuccessful. It has been suggested that this happened because the dentine was targeted; new research suggests that the cementum has higher concentrations of DNA. Moreover, the heat generated by the high speed of the drill bit may have denatured the DNA.[46]
The small brain size of H. floresiensis at 417 cc prompted hypotheses that the specimens were simply H. sapiens with a birth defect, rather than the result of neurological reorganisation.[47] These claims have subsequently been widely rejected.[5]
Prior to Jacob's removal of the fossils, American neuroanthropologist Dean Falk and her colleagues performed a CT scan of the LB1 skull and a virtual endocast, and concluded that the brainpan was neither that of a pygmy nor an individual with a malformed skull and brain.[48] In response, American neurologist Jochen Weber and colleagues compared the computer model skull with microcephalic human skulls, and found that the skull size of LB1 falls in the middle of the size range of the human samples, and is not inconsistent with microcephaly.[49][50] A 2006 study stated that LB1 probably descended from a pygmy population of modern humans, but herself shows signs of microcephaly, and other specimens from the cave show small stature but not microcephaly.[51]
In 2005, the original discoverers of H. floresiensis, after unearthing more specimens, countered that the skeptics had mistakenly attributed the height of H. floresiensis to microcephaly.[2] Falk stated that Martin's assertions were unsubstantiated. In 2006, Australian palaeoanthropologist Debbie Argue and colleagues also concluded that the finds are indeed a new species.[52] In 2007, Falk found that H. floresiensis brains were similar in shape to modern humans, and the frontal and temporal lobes were well-developed, which would not have been the case were they microcephalic.[53]
In 2008, Greek palaeontologist George Lyras and colleagues said that LB1 falls outside the range of variation for human microcephalic skulls.[54] However, a 2013 comparison of the LB1 endocast to a set of 100 normocephalic and 17 microcephalic endocasts showed that there is a wide variation in microcephalic brain shape ratios and that in these ratios the group as such is not clearly distinct from normocephalics. The LB1 brain shape nevertheless aligns slightly better with the microcephalic sample, with the shape at the extreme edge of the normocephalic group.[55] A 2016 pathological analysis of LB1's skull revealed no pathologies nor evidence of microcephaly, and concluded that LB1 is a separate species.[56]
A 2007 study postulated that the skeletons were those of humans who suffered from Laron syndrome, which was first reported in 1966, and is most common in inbreeding populations, which may have been the scenario on the small island. It causes a short stature and small skull, and many conditions seen in Laron syndrome patients are also exhibited in H. floresiensis. The estimated height of LB1 is at the lower end of the average for afflicted human women, but the endocranial volume is much smaller than anything exhibited in Laron syndrome patients. DNA analysis would be required to support this theory.[57]
In 2008, Australian researcher Peter Obendorf — who studies congenital iodine deficiency syndrome — and colleagues suggested that LB1 and LB6 suffered from myxoedematous (ME) congenital iodine deficiency syndrome resulting from congenital hypothyroidism (underactive thyroid), and that they were part of an affected population of H. sapiens on the island. Congenital iodine deficiency syndrome, caused by iodine deficiency, is expressed by small bodies and reduced brain size (but ME causes less motor and mental disablement than other forms of congenital iodine deficiency syndrome), and is a form of dwarfism still found in the local Indonesian population. They said that various features of H. floresiensis are diagnostic characteristics, such as enlarged pituitary fossa, unusually straight and untwisted humeral heads, relatively thick limbs, double rooted premolar, and primitive wrist morphology.[58]
However, Falk's scans of LB1's pituitary fossa show that it is not larger than usual.[59] Also, in 2009, anthropologists Colin Groves and Catharine FitzGerald compared the Flores bones with those of ten people who had had cretinism, and found no overlap.[60][61] Obendorf and colleagues rejected Groves and FitzGerald's argument the following year.[62] A 2012 study similar to Groves and FitzGeralds' also found no evidence of congenital iodine deficiency syndrome.[63]
From 2006, physical anthropologist Maciej Henneberg and colleagues have claimed that LB1 suffered from Down syndrome, and that the remains of other individuals at the Flores site were merely normal modern humans.[64] However, there are a number of characteristics shared by both LB1 and LB6 as well as other known early humans and absent in H. sapiens, such as the lack of a chin.[65] In 2016, a comparative study concluded that LB1 did not exhibit a sufficient number of Down syndrome characteristics to support a diagnosis.[66] The noted physical anthropologist Chris Stringer in 2011 wrote of Homo floresiensis deniers generally, "I think they have damaged their own, and palaeoanthropologist's, reputation."[67]
The most important and obvious identifying features of Homo floresiensis are its small body and small cranial capacity. Brown and Morwood also identified a number of additional, less obvious features that might distinguish LB1 from modern H. sapiens, including the form of the teeth, the absence of a chin, and a lesser torsion in the lower end of the humerus (upper arm bone). Each of these putative distinguishing features has been heavily scrutinized by the scientific community, with different research groups reaching differing conclusions as to whether these features support the original designation of a new species,[52] or whether they identify LB1 as a severely pathological H. sapiens.[51]
A 2015 study of the dental morphology of forty teeth of H. floresiensis compared to 450 teeth of living and extinct human species, states that they had "primitive canine-premolar and advanced molar morphologies," which is unique among hominins.[40]
The discovery of additional partial skeletons[2] has verified the existence of some features found in LB1, such as the lack of a chin, but Jacob and other research teams argue that these features do not distinguish LB1 from local modern humans.[51] Lyras et al. have asserted, based on 3D-morphometrics, that the skull of LB1 differs significantly from all H. sapiens skulls, including those of small-bodied individuals and microcephalics, and is more similar to the skull of Homo erectus.[54] Ian Tattersall argues that the species is wrongly classified as Homo floresiensis as it is far too archaic to assign to the genus Homo.[68]
LB1's height is estimated to have been 1.06 m (3 ft 6 in). The height of a second skeleton, LB8, has been estimated at 1.09 m (3 ft 7 in) based on tibial length.[2] These estimates are outside the range of normal modern human height and considerably shorter than the average adult height of even the smallest modern humans, such as the Mbenga and Mbuti at 1.5 m (4 ft 11 in),[69] Twa, Semang at 1.37 m (4 ft 6 in) for adult women of the Malay Peninsula,[70] or the Andamanese at also 1.37 m (4 ft 6 in) for adult women.[71] LB1's body mass is estimated to have been 25 kg (55 lb). LB1 and LB8 are also somewhat smaller than the australopithecines, such as Lucy, from three million years ago, not previously thought to have expanded beyond Africa. Thus, LB1 and LB8 may be the shortest and smallest members of the extended human group discovered thus far.[72]
Their short stature was likely due to insular dwarfism, where size decreases as a response to fewer resources in an island ecosystem.[1][73] In 2006, Indonesian palaeoanthropologist Teuku Jacob and colleagues said that LB1 has a similar stature to the Rampasasa pygmies who inhabit the island, and that size can vary substantially in pygmy populations.[51] A 2018 study refuted the possibility of Rampasasa pygmies descending from H. floresiensis, concluding that "multiple independent instances of hominin insular dwarfism occurred on Flores".[74]
Aside from smaller body size, the specimens seem to otherwise resemble H. erectus, a species known to have been living in Southeast Asia at times coincident with earlier finds purported to be of H. floresiensis.[2]
In addition to a small body size, H. floresiensis had a remarkably small brain size. LB1's brain is estimated to have had a volume of 380 cm3 (23 cu in), placing it at the range of chimpanzees or the extinct australopithecines.[1][48] LB1's brain size is less than half that of its presumed immediate ancestor, H. erectus (980 cm3 (60 cu in)).[48] The brain-to-body mass ratio of LB1 lies between that of H. erectus and the great apes.[75] Such a reduction is likely due to insular dwarfism, and a 2009 study found that the reduction in brain size of extinct pygmy hippopotamuses in Madagascar compared with their living relatives is proportionally greater than the reduction in body size, and similar to the reduction in brain size of H. floresiensis compared with H. erectus.[76]
Smaller size does not appear to have affected mental faculties, as Brodmann area 10 on the prefrontal cortex, which is associated with cognition, is about the same size as that of modern humans.[48] H. floresiensis is also associated with evidence for advanced behaviours, such as the use of fire, butchering, and stone tool manufacturing.[2][6]
The angle of humeral torsion was lesser than in modern humans.[1][2][6] The humeral head of modern humans is twisted between 145 and 165 degrees to the plane of the elbow joint, whereas it is 120 degrees in H. floresiensis. This may have provided an advantage when arm-swinging, and, in tandem with the unusual morphology of the shoulder girdle and short clavicle, would have displaced the shoulders slightly forward into an almost shrugging position. The shrugging position would have compensated for the lower range of motion in the arm, allowing for similar maneuverability in the elbows as in modern humans.[77] The wrist bones are similar to those of apes and Australopithecus. They were significantly smaller and more flexible than the carpals of modern humans, lacking contemporary features which evolved at least 800,000 years ago. [78]
The leg bones were more robust than those of modern humans.[1][2][6] The feet were unusually flat and large in relation with the rest of the body.[79] As a result, when walking, they would have had to bend the knees further back than modern humans do. This caused a high-stepping gait and slow walking speed.[80] The toes were thin, long, and halluces were almost indistinguishable from the other metatarsals.[81]
The cave yielded over ten thousand stone artefacts, mainly lithic flakes, surprising considering H. floresiensis's small brain. This has led some researchers to theorize that H. floresiensis inherited their tool-making skills from H. erectus.[82] Points, perforators, blades, and microblades were associated with remains of the extinct elephant-relative Stegodon. It has therefore been proposed that H. floresiensis hunted juvenile Stegodon. Similar artefacts are found at the Soa Basin 50 km (31 mi) south, associated with Stegodon and Komodo dragon remains, and are attributed to a likely ancestral population of H. erectus.[1][2][6] Other authors have doubted the extent of hunting of Stegodon by H. floresiensis, noting the rarity of cut marks on remains of Stegodon found at Liang Bua, suggesting that they would have faced intense competition for carcasses with other predators, like the Komodo dragon, the giant stork Leptoptilos robustus, and vultures, and that it was possible that their main prey was instead the giant rats like Papagomys endemic to the island, which are found abundantly at Liang Bua. While it was initially suggested that H. floresiensis was capable of using fire, the supporting evidence for this claim was later found to be unreliable.[5]
The youngest H. floresiensis bone remains in the cave date to 60,000 years ago, and the youngest stone tools to 50,000 years ago. The previous estimate of 12,000 BP was due to an undetected unconformity in the cave stratigraphy. The timing of their disappearance from the cave stratigraphy is close to the time that modern humans reached the area, which may suggest the effects of modern humans directly on H. floresiensis or more broadly on the ecosystems of Flores caused or contributed to their extinction.[83] DNA analysis of pygmy modern humans from Flores has found no evidence of any DNA from H. floresiensis.[84]
During the late Early Pleistocene to Late Pleistocene, and before the arrival of Homo sapiens, Flores exhibited a depauperate ecosystem with relatively few terrestrial vertebrate species, including the extinct dwarf proboscidean (elephant relative) Stegodon florensis;[4] and a variety of rats (Murinae) including small-sized forms like Rattus hainaldi, the Polynesian rat, Paulamys, and Komodomys, the medium-sized Hooijeromys, and giant Papagomys and extinct Spelaeomys, the latter two genera being about the size of rabbits, with body masses of 600–2,500 grams (1.3–5.5 lb).[85] Also present were the Komodo dragon and another smaller monitor lizard (Varanus hooijeri),[4] with birds including a giant stork (Leptoptilos robustus) and a vulture (Trigonoceps).[86]
Homo floresiensis was swiftly nicknamed "the hobbit" by the discoverers, after the fictional race popularized in J. R. R. Tolkien's book The Hobbit, and some of the discoverers suggested naming the species H. hobbitus.[15]
In October 2012, a New Zealand scientist due to give a public lecture on Homo floresiensis was told by the Tolkien Estate that he was not allowed to use the word "hobbit" in promoting the lecture.[87]
In 2012, the American film studio The Asylum, which produces low-budget "mockbuster" films,[88] planned to release a movie entitled Age of the Hobbits depicting a "peace-loving" community of H. floresiensis "enslaved by the Java Men, a race of flesh-eating dragon-riders."[89] The film was intended to piggyback on the success of Peter Jackson's film The Hobbit: An Unexpected Journey.[90] The film was blocked from release due to a legal dispute about using the word "hobbit."[90] The Asylum argued that the film did not violate the Tolkien copyright because the film was about H. floresiensis, "uniformly referred to as 'Hobbits' in the scientific community."[89] The film was later retitled Clash of the Empires.

The phylogenetic split of Hominidae into the subfamilies Homininae and Ponginae is dated to the middle Miocene, roughly 18 to 14 million years ago. This split is also referenced as the "orangutan–human last common ancestor" by Jeffrey H. Schwartz, professor of anthropology at the University of Pittsburgh School of Arts and Sciences, and John Grehan, director of science at the Buffalo Museum.
Kenyapithecus (†13 Mya)
Sivapithecus (†9)
Crown Ponginae
Ankarapithecus (†9)
Giganthopithecus (†0.1)
Khoratpithecus (†7)
Pierolapithecus (†11)
Hispanopithecus (†10)
Lufengpithecus (†7)
Khoratpithecus (†9)
Ardipithecus (incl. Homo)
Pan
Graecopithecus (†8)
Ouranopithecus (†7)
Crown Gorillini
Chororapithecus (†)
Nakalipithecus (†9.8)
Samburupithecus (†9)
Hominoidea (commonly known as apes) are thought to have evolved in Africa by about 18 million years ago. Among the genera thought to be in the ape lineage leading up to the emergence of the great apes (Hominidae) about 13 million years ago are Proconsul, Rangwapithecus, Dendropithecus, Nacholapithecus, Equatorius, Afropithecus and Kenyapithecus, all from East Africa. During the early Miocene, Europe and Africa were connected by land bridges over the Tethys Sea. Apes showed up in Europe in the fossil record beginning 17 million years ago. Great apes show up in the fossil record in Europe and Asia beginning about 12 million years ago.[1] Apart from humans, the only living great ape in Asia is the orangutan.[2][3][4][5][6][7][8]
Various genera of dryopithecines have been identified and are classified as an extinct sister clade of the Homininae.[9]: 226  Possible further members of this tribe, indicated within Ponginae in the cladistic tree above, or of Homininae or else third or more tribes yet unnamed include extinct Pierolapithecus, Hispanopithecus, Lufengpithecus and Khoratpithecus.[10]
Dryopithecines' nominate genus Dryopithecus was first uncovered in France, and it had a large frontal sinus which ties it to the African great apes.[2] Orangutans which are only found in Asia do not.[2] They did have thick dental enamel, another ape-like characteristic.[2] Orangutans do not have a large frontal sinus.[11]   The study of Dryopithecini as an outgroup to Hominidae suggests a date earlier than 8 million years ago for the Homininae-Ponginae split. It also suggests that the Homininae group evolved in Africa or Western Eurasia, against the theory that Homininae was originally an Asian line which later recolonized Africa after the Great Apes went extinct in Africa.[12]
During the later Miocene, the climate in Europe started to change as the Himalayas were rising, and Europe became cooler and drier. About 9.5 million years ago, tropical forest in Europe was replaced by woodlands which were less suitable for great apes, and European Homininae (close to the Dryopithecini-Hominini split) appear to have migrated back to Africa where they would have diverged into Gorillini and Hominini.[2]
Plant fossils reveal that forests use to once extend "from southern Europe, through Central Asia, and into China prior to the formation of the Himalayas". This suggests that the ancestral hominoid once lived throughout a vast area and as the Earth's climate and ecosystems changed, the ancestral hominoids ultimately became geographically isolated from one another.[13]
The orangutan–human last common ancestor was tailless and had a broad flat rib cage, a larger body size, larger brain, and in females, the canine teeth had started to shrink like their descendants.[9]: 201  Great apes have sweat glands in the armpits versus in the chest like lesser monkeys.[9]: 195
Orangutans have anterior lingual glands and sparse terminal hair like the hominines.[9]: 193
Terminal hairs are those hairs that are easy to see. As compared to the tiny light-colored hairs called Vellus hairs. Certainly, there is some correlation with the size of the mammal. The larger the mammal, the fewer terminal hairs.[9]: 195
Near the tip of the tongue on the underside are the submandibular glands, which secrete amylase. These are only found in the great apes so it is understood that the last common ancestor would also have these.[9]: 195
Concerning the skull, this is an example where humans share more in common with orangutans than with later great apes. We have two small ridges, one over each eye called superciliary arches.[9]: 229
The great apes other than humans and orangutans have brow ridges.[9]: 229
The orangutan genome has many unique features. Structural evolution of the orangutan genome has proceeded much more slowly than other great apes, evidenced by fewer rearrangements, less segmental duplication, and a lower rate of gene family turnover. Diversity among the orangutan populations may not be maintained with continued habitat loss and population fragmentation.[14] Evolutionary evidence from other species suggests fragmentation will not halt diversity, but their slow reproduction rate and arboreal lifestyle may leave the orangutan species especially vulnerable to rapid dramatic environmental change.[14]
Orangutans represent an older lineage of great apes dating from 12–16 million years ago. Though orangutans are the most phylogenetically distant great apes from humans, they nonetheless share significant similarities: equally large brains, high intelligence, slow lives, hunting, meat-eating, reliance on technology, culture, and language capacity. Experts often argue that orangutans resemble humans the most closely, showing greater bipedalism, subtle intellectual advantages, and the longest childhood growth and period of dependency.[15]

The origin of the Armenians is a topic concerned with the emergence of the Armenian people and the country called Armenia. The earliest universally accepted reference to the people and the country dates back to the 6th century BC Behistun Inscription, followed by several Greek fragments and books.[1] The earliest known reference to a geopolitical entity where Armenians originated from is dated to the 13th century BC as Uruatri in Old Assyrian.[2] Historians and Armenologists have speculated about the earlier origin of the Armenian people, but no consensus has been achieved as of yet. Genetic studies show that Armenian people are indigenous to historical Armenia,[3] showing little to no signs of admixture since around the 13th century BC.[4]
Recent studies have shown that Armenians are indigenous to the Armenian Highlands and form a distinct genetic isolate in the region.[5] Analyses of mitochondrial ancient DNA of skeletons from Armenia spanning 7,800 years, including DNA from Neolithic, Bronze Age, Urartian, classical and medieval Armenian skeletons,[6] have revealed that modern Armenians have the least genetic distance to them compared to neighboring peoples such as Turks and Azerbaijani Turks, but followed closely by Georgians. Armenians are also one of the genetic isolates of the Near East who share affinity with the Neolithic farmers who expanded into Europe beginning around 8,000 years ago. There are signs of considerable genetic admixture in Armenians between 3000 BC and 2000 BC but they subside to insignificant levels since 1200 BC, remaining stable until today.
In a study published in 2017,[6] the complete mitochondrial genomes of 52 ancient skeletons from present-day Armenia spanning 7,800 years were analyzed and combined with 206 mitochondrial genomes of modern Armenians and previously published data of seven neighboring populations (482 people).
Coalescence-based analyses suggest that the population size in the region rapidly increased after the Last Glacial Maximum around 18,000 years ago. During the Bronze and Iron ages, many complex societies emerged from distinctive cultures such as Kura–Araxes, Trialeti–Vanadzor, Sevan–Artsakh, Karmir Berd, Karmir Vank’, Lchashen–Metsamor, and Urartian. No changes in the female gene pool could be documented, supporting a cultural diffusion model in the region (the spread of cultural items—such as ideas, styles, religions, technologies, languages—between individuals, whether within a single culture or from one culture to another).
The study sampled 44 ancient human skeletons according to established aDNA guidelines from a total of 19 archaeological sites in Armenia and Artsakh. Based on contextual dating of artifacts, their ages are estimated to be between 300 and 7,800 years old, which covers seven well-defined cultural transitions.
The study shows that modern Armenians have the lowest genetic distance between the ancient individuals in this dataset—followed closely by Georgians—compared to other populations such as Turks, Persians, and Azerbaijanis.
According to a study published in 2015,[7] in which a genome-wide variation in 173 Armenians was analyzed and compared to 78 other worldwide populations, Armenians form a distinct genetic cluster linking the Near East, Europe, and the Caucasus.
The genetic landscape in the Near East had more affinity to Neolithic Europe than the present populations do. Armenians seem to share a similar affinity to those Neolithic farmers as do other genetic isolates in the Near East, such as Greek Cypriots, Mizrahi Jews, and Middle Eastern Christian communities. Twenty-nine percent (29%) of Armenian ancestry seems to originate from an ancestral population that is best represented by Neolithic Europeans. This suggests that they may derive from a people who inhabited the Near East during the Neolithic expansion of Near Eastern farmers into Europe beginning around 8,000 years ago.
An earlier study from 2011[8] has also shown a prevalence of Neolithic paternal chromosomes associated with the Agricultural Revolution. Collectively, they constitute 77% of the observed paternal lineages in the Armenian Plateau – 58% in Sason and an average of 84% in Ararat Valley, Gardman and Lake Van.
Bronze Age demographic processes had a major impact on the genetics of populations in the Armenian Highlands. Armenians appear to originate from a mixture of diverse populations occurring from 3000 BC to 2000 BC. This period coincides with the Kura-Araxes culture, the appearance of Hittites in Anatolia, major population migrations after the domestication of the horse, and the appearance of chariots. It also coincides with the legendary foundation of the Armenian nation in 2492 BC. According to the A genetic atlas of human admixture history published by Hellenthal et al. in 2014, admixture is not inferred or is uncertain.[9]
Up until recently, it was hypothesized that the Armenian people migrated from the Balkans into the Armenian Highlands, based on a passage by Herodotus in the 5th century BC claiming a kinship between Armenians and Phrygians. However, the results of a 2020 study on Armenian genetics "strongly reject" this long-standing narrative, and shows that Armenians are genetically distinct from the ancient populations of the Balkans.[10]
As was concluded in earlier studies, the 2020 study reaffirms the pattern of genetic affinity between modern Armenians and the ancient inhabitants of the Armenian Highlands since the Chalcolithic. It reveals a "strikingly high" level of regional genetic continuity for over 6,000 years with only one detectable input from a mysterious Sardinian-like people during or just after the middle to late Bronze Age. Modern Sardinians, having the highest genetic affinity to early European farmers who migrated into Europe from Anatolia and introduced farming around 8,000 years ago,[11] have 38–44% of ancestry from an Iranian, Steppe, and North-African-related source. However, no detectable signs of input from sources similar to Anatolian farmers or Iranians were detected that may have altered the gene pool of the population of the Armenian Highlands. The input plausibly came by northwards migrations from the Middle East rather than the isolated island of Sardinia, but no conclusions have been made about the identity of the migrating peoples as of yet, nor whether the cause was cultural or climatic.
Starting from around 1200 BC, during the Late Bronze Age collapse, around the time when the Nairi tribal confederation and Urartu begin appearing in historical records, signs of admixture decrease to insignificant levels until today. It seems that widespread destruction and abandonment of major cities and trade routes caused the Armenians' isolation from their surroundings, and their adoption of a distinct culture and identity early on in their history genetically isolated them from major admixture throughout the following millennia.
Recent genetic and linguistic research has suggested that Armenian, along with Greek and Albanian, are connected to the Yamnaya culture of the Pontic–Caspian steppe and Caucasus, whereas all other existent branches of Indo-European were mediated through Corded Ware culture.[12]
The Near East's genetic landscape appears to have been continuously changing since the Bronze Age. There is a sub-Saharan African gene flow around 850 years ago in Syrians, Palestinians, and Jordanians consistent with previous reports of recent gene flow from Africans to Levantine populations after the Arab expansions. There is also an East Asian ancestry in Turks from admixture occurring around 800 years ago coinciding with the arrival of the Seljuk Turks to Anatolia from their homelands near the Aral sea. The introduction of these populations doesn't seem to have affected Armenians significantly. Around 500 years ago, a genetic structure within the population appears to have developed, which coincides with a period when the Armenian people were divided between the Ottoman Empire and the successive Iranian empires.
Armenia and the Armenians were attested multiple times at the end of the Iron Age and the onset of Classical Antiquity.
Armenians (as "Hai") were possibly mentioned in the 10th century BC Hieroglyphic Luwian inscriptions from Carchemish.[15][16]
The earliest record of what can unambiguously be identified as Armenian dates back to the trilingual Behistun Inscription,[17] authored sometime after c. 522 BC, in reference to a country and the people associated with it. The following table breaks down the attestation in the three languages it was written in:
The inscriptions chronicle Darius the Great's battles and conquests during the first Persian Empire. Multiple Armenian people were mentioned in them:[19]
In the Babylonian Akkadian version, these people are referred to as Urartians.[25]
The earliest known attestation in the Greek language is from a fragment attributed to Greek historian Hecataeus of Miletus,[citation needed] which in some sources is dated to prior to the Behistun Inscription. In it, he mentions the Chalybes people in Pontus, past the Thermōdōn River, with Armenians as their southern neighbors:[26][27]
Χάλυβες, περὶ τὸν Πόντον ἔθνος ἐπὶ τῷ Θερμώδοντι, περὶ ὧν Εὔδοξος ἐν πρώτῳ ... Καὶ Χάλυβοι παρ ̓ Ἑκαταίῳ· «Χαλύβοισι πρὸς νότον Ἀρμένιοι δμουρέουσι.»
Xerxes I was king of Achaemenid Persia following the reign of his father, Darius the Great who authored the Behistun Inscription. Xerxes authored an inscription in the Achaemenid province of Armenia sometime between 486 and 465 BC, located in modern-day Van, Turkey. The inscription is also written three languages – in Old Persian, Elamite, and in the Babylonian dialect of Akkadian – and is the last known encounter of the name Urartu/Urashtu in reference to the Armenia.[28]
Herodotus mentions the Armenian people multiple times in his book The Histories:
Next to the Cilicians, are the Armenians, another people rich in flocks, and after the Armenians, the Matieni, […]
Herodotus also lists the ethnic groups in the Persian army, and claims that Armenians are settlers from Phrygia. However, this is an etiological tag added by the ethnographer responsible for the list who felt an obligation to explain where each of the ethnic groups came from – the ancient Armenians themselves seem to have no knowledge of their ancestors' migration from Phrygia.[29]
The Phrygian equipment was very similar to the Paphlagonian, with only a small difference. As the Macedonians say, these Phrygians were called Briges as long as they dwelt in Europe, where they were neighbors of the Macedonians; but when they changed their home to Asia, they changed their name also and were called Phrygians. The Armenians, who are settlers from Phrygia, were armed like the Phrygians. Both these together had as their commander Artochmes, who had married a daughter of Darius.
This passage has often been cited to explain the origin of the Armenians and the introduction of the Proto-Armenian language into the South Caucasus region. However, the latest studies in linguistics show that the Armenian language is as close to Indo-Iranian as it is to Graeco-Phrygian.[30][31][32] Additionally, archaeological research does not indicate a movement of people from Europe into Armenia, nor do the latest studies in genetics,[7][6] with the latest study rejecting the narrative altogether.[10]
In his book about Cyrus, the first Emperor of Persia, Xenophon writes about a conversation between Cyrus and the King of Armenia regarding a past war between Armenians and the Medes led by Astyages (events prior to the ones mentioned in the Behistun Inscriptions):[33][34]
When everything was in order, he began his examination: “King of Armenia,” said he, “I advise you in the first place in this trial to tell the truth, that you may be guiltless of that offence which is hated more cordially than any other. For let me assure you that being caught in a barefaced lie stands most seriously in the way of a man's receiving any mercy. In the next place,” said he, “your children and your wives here and also the Armenians present are cognizant of everything that you have done; and if they hear you telling anything else than the facts, they will think that you are actually condemning your own self to suffer the extreme penalty, if ever I discover the truth.”
“Well, Cyrus,” said he, “ask what you will, and be assured that I will tell the truth, let happen what will as a result of it.”
“Tell me then,” said the other, “did you ever have a war with Astyages, my mother's father, and with the rest of the Medes?”
“Yes,” he answered, “I did.”
“And when you were conquered by him, did you agree to pay tribute and to join his army, wherever he should command you to go, and to own no forts?”
“Those are the facts.”
In reference to a time ancient to him, Strabo mentions Armenia facing Syria (Ancient Greek for Assyria[35][36][37][38]) and ruling the whole of Asia (probably meaning Western Asia) until its authority was diminished by the time of Astyages of the Median Empire (r. 585–550 BC) at the hand of Cyrus of the Persian Empire (r. 559–530 BC), after which it maintained its "ancient dignity":[39]
In ancient times Greater Armenia ruled the whole of Asia, after it broke up the empire of the Syrians, but later, in the time of Astyages, it was deprived of that great authority by Cyrus and the Persians, although it continued to preserve much of its ancient dignity; and Ecbatana was winter residence for the Persian kings, and likewise for the Macedonians who, after overthrowing the Persians, occupied Syria; and still today it affords the kings of the Parthians the same advantages and security.
Historians and Armenologists have attempted to explain the origin of the Armenian people, but nothing conclusive has been discovered as of yet. The current consensus is that the Armenian people emerged as the result of amalgamation between the various peoples who inhabited the mountainous region known in the Iron Age by various forms of the name Urartu (a.k.a., Uruatri, Urashtu, and Ararat).[40] The process of amalgamation is presumed to have been accelerated by the formation of Urartu and completed in succeeding Armenian realms.[41][42][43][44]
Academics have also considered the etymological roots of the stems Armen- and Hay-, from which derive the modern exonym and endonym of Armenia and Armenians, in order to propose candidates for groups (i.e., Proto-Armenians) who may have contributed to the Armenian ethnogenesis. These propositions are purely speculative and are largely based on geographic proximity, similarity between names, linguistics, and extrapolations made from known historical events of the time.
The following cultures, peoples and polities have all been suggested to have contributed to the ethnogenesis of the Armenian people.[45][46][47][48]
There is evidence of Neolithic, Chalcolithic, and Bronze Age cultures in lands historically and presently inhabited by Armenian people, dating to about 4000 BC. Archaeological surveys in 2010 and 2011 at the Areni-1 cave complex have resulted in the discovery of the world's earliest known leather shoe, skirt, and wine-producing facility.
From 2200 BC to 1600 BC, the Trialeti–Vanadzor culture flourished in Armenia, southern Georgia, and northeastern Turkey. It has been speculated that this was an Indo-European culture.[49][50][51] and that it could have been Proto-Armenian-speaking.[52] Other possibly related cultures were spread throughout the Armenia Highlands during this time, namely in the Aragats and Lake Sevan regions.
Early 20th-century scholars suggested that the name "Armenia" may have possibly been recorded for the first time on an inscription which mentions Armani (or Armânum) together with Ibla, from territories conquered by Naram-Sin (2300 BC) identified with an Akkadian colony in the current region of Diyarbekir; however, the precise locations of both Armani and Ibla are unclear. Some modern researchers have placed Armani (Armi) in the general area of modern Samsat,[53] and have suggested it was populated, at least partially, by an early Indo-European-speaking people.[54] Today, the Modern Assyrians (who traditionally speak Neo-Aramaic, not Akkadian) refer to the Armenians by the name Armani.[55] Thutmose III of Egypt, in the 33rd year of his reign (1446 BCE), mentioned as the people of "Ermenen", claiming that in their land "heaven rests upon its four pillars".[56] Armenia is possibly connected to Mannaea, which may be identical to the region of Minni mentioned in The Bible. However, what all these attestations refer to cannot be determined with certainty.
Hittite inscriptions deciphered in the 1920s by the Swiss scholar Emil Forrer testify to the existence of a mountain country, the Hayasa and/or the Azzi, lying around Lake Van. Several prominent authorities agree in placing Azzi to the north of Ishuwa. Others see Hayasa and Azzi as identical.
Records of the time between Telipinu and Tudhaliya III are sketchy. The Hittites seem to have abandoned their capital at Hattusa and moved to Sapinuwa under one of the earlier Tudhaliya kings. In the early 14th century BC, Sapinuwa was burned as well. Hattusili III records at this time that the Azzi had "made Samuha its frontier." The modern Georgian term somekhi 'Armenian' may ultimately derive from Samuha.
One of the common theories for the introduction of the Armenian language into the Armenian Highlands, originating from Herodotus' claim that Armenians were Phrygian settlers, is that it had arrived via Phrygians and/or a related peoples known as the Mushki, as Paleo-Balkan-speaking settlers retroactively named Armeno-Phrygians, who had already settled in the western parts of the region prior to the establishment of Urartu,[57][58][59] and became the ruling elite under the Median Empire, followed by the Achaemenid Empire.[60] According to Igor Diakonoff, the Mushki were a Thraco-Phrygian group who carried their Proto-Armenian language from the Balkans across Asia Minor, mixing with Hurrians (and Urartians) and Luwians along the way.[61] Diakonoff theorized that the root of the name Mushki was "Mush" (or perhaps "Mus," "Mos," or "Mosh") with the addition of the Armenian plural suffix -k'.[62]
Armen Petrosyan clarifies this, suggesting that -ki was a Proto-Armenian form of the Classical Armenian -k' and etymologizes "Mush" as meaning "worker" or "agriculturalist."[63]
However, despite Diakonoff's claims, the connection between the Mushki and Armenian languages is unknown and some modern scholars have rejected a direct linguistic relationship if the Mushki were Thracians or Phrygians.[64][65][66][67] Additionally, genetic research does not support significant admixture into the Armenian nation after 1200 BCE, making the Mushki, if they indeed migrated from a Balkan or western Anatolian homeland during or after the Bronze Age Collapse, unlikely candidates for the Proto-Armenians.[68][69] However, as others have placed (at least the Eastern) Mushki homeland in the Armenian Highlands and South Caucasus region, it is possible that at least some of the Mushki were Armenian-speakers or speakers of a closely related language.[70]
It has been speculated that the Mushki (and their allies, the Urumu) were connected to the spread of the so-called Transcaucasian ceramic ware, which appeared as far west as modern Elazig, Turkey in the late second millennium BCE.[71] This ceramic ware is believed to have been developed in the South Caucasus region, possibly by the Trialeti-Vanadzor culture originally, which suggests an eastern homeland for the Mushki.[72][73][74]
Pliny in the 1st century AD mentions the Moscheni in southern Armenia ("Armenia" at the time stretching south and west to the Mediterranean, bordering on Cappadocia). In Byzantine historiography, Moschoi was a name equivalent to or considered as the ancestors of "Cappadocians" (Eusebius) with their capital at Mazaca (later Caesarea Mazaca, modern Kayseri). According to Armenian tradition, the city of Mazaca was founded by and named after Mishak (Misak, Moshok), a cousin and general of the legendary patriarch Aram.[75] Scholars have proposed a connection between the name Mishak and Mushki.[76][77]
The Armenian region of Moks and the city of Mush, Turkey may derive their names from the Mushki.[78][79]
According to Professor James R. Russell of Harvard University, the Georgian designation for Armenians Somekhi, preserves the old name of the Mushki. However, there are other theories regarding the origins of this exonym as well.[citation needed]
Urartu (a.k.a. Ararat, Urashtu) is the geographic name used during the Iron Age for the region that would later be known as the Armenian Highlands. The polity that emerged in the region as a confederation of tribes was the Kingdom of Van, which was centered around Lake Van in modern-day Turkey. The kingdom rose to power in the mid-9th century BC, but went into gradual decline and was eventually conquered by the Iranian Medes in the early 6th century BC.[80] The geopolitical region would re-emerge as Armenia shortly after. Being heirs to the Urartian realm, the earliest identifiable ancestors of the Armenians are the peoples of Urartu.[41][42][43][44]
The Urartian confederation united the disparate peoples of the highlands, which began a process of intermingling of the peoples and cultures (including possibly Armenian tribes) and languages (potentially including proto-Armenian) within the highlands. This intermixing would ultimately culminate in the emergence of the Armenians as the dominant polity and culture of the Armenian Highlands, and as the direct successors and inheritors of the Urartian domain.[41][42][43][44]
According to historian M. Chahin:[81]
Urartian history is part of Armenian history, in the same sense that the history of the ancient Britons is part of English history, and that of the Gauls is part of French history. Armenians can legitimately claim, through Urartu, an historical continuity of some 4000 years; their history is among those of the most ancient peoples in the world.
Scholars have found a number of probable Armenian deities, personal names, and toponyms mentioned within Urartian texts, suggesting that perhaps Urartu was at least partially composed of Armenian populations.[82] These include the name of the first king of Urartu, Arame, and that of his second capital, Arzashkun.[83]
According to the Armenian tradition, the Medes helped the Armenians establish the Orontid (Yervanduni) dynasty. This would indicate two scenarios—either Media subsequently conquered Urartu, bringing about its subsequent demise, or Urartu maintained its independence and power, going through a mere dynastic change, as a local Armenian dynasty or dynasties (the Haykazunis and/or the Orontids) overthrew the ruling family with the help of the Median army. Ancient sources support the latter version: Xenophon, for example, states that Armenia, ruled by an Orontid king, was not conquered until the reign of Median king Astyages (585–550 BC) – long after Median invasion of the late 7th century BC.[84] Similarly, Strabo (1st century BC – 1st century AD) wrote that "[i]n ancient times Greater Armenia ruled the whole of Asia, after it broke up the empire of the Syrians, but later, in the time of Astyages, it was deprived of that great authority [...]."[85]
Medieval Armenian chronicles corroborate the Greek and Hebrew sources. In particular, Movses Khorenatsi writes that the Armenian king Skayordi Haykazuni was a political foe of Assyria during the reign of Sennacherib (705-681 BCE), which would have been contemporaneous with the rule of Argishti II. Skayordi's son, Paruyr Haykazuni (also known as Paruyr Skayordi), helped Cyaxares and his allies conquer Assyria, for which Cyaxares recognized him as the king of Armenia. According to Khorenatsi, Media conquered Armenia only much later—under Astyages.[86] It is possible that the last Urartian king, Rusa IV, had connections to the future incoming Armenian Orontids dynasty. [citation needed]
With the region reunified again under Armenia, the disparate peoples of the region mixed and became more homogenous and a unified sense of identity developed, and the Armenian language became the predominant language.
Armenologist Armen Petrosyan proposed that the powerful Etiuni confederation, located in what is now the territory of northeastern Turkey and Armenia, may have been the name the Urartians used to refer to Armenian-speaking tribes. According to both Urartian and Assyrian records, the Etiuni were hostile to Urartian rule. Etiuni toponyms and tribal names such as Uduri, Uelikuni, Išteluani, Abiliani, and Lusa, the river name Ildaruni, the goddess Aniqu, and personal names Diasuni, Murini, Qapurini, Nidini, and Ṣinalbi may have Armenian etymologies.[87][88][89][90]
The written language that the kingdom's political elite used is referred to as Urartian, which appears in cuneiform inscriptions in Armenia and eastern Turkey. It is unknown what language was spoken by the peoples of Urartu at the time of the existence of the kingdom of Van, but there is linguistic evidence of contact between the proto-Armenian language and the Urartian language at an early date (sometime between the 3rd—2nd millennium BC), occurring prior to the formation of Urartu as a kingdom.[91][92][93][41][94]
The presence of a population who spoke proto-Armenian in Urartu prior to its demise is subject to speculation, but the existence of Urartian words in the Armenian language suggests early contact between the two languages and long periods of bilingualism.[95][96] It is generally assumed that proto-Armenian speakers entered Anatolia around 1200 BC,[97][98] during the Bronze Age Collapse, which was three to four centuries before the emergence of the Kingdom of Urartu.  Regardless, the Urartian confederation united the disparate peoples of the highlands, which began a process of intermingling of the peoples and cultures (probably including Armenian tribes) and languages (probably including Proto-Armenian) within the highlands. This intermixing would ultimately culminate in the emergence of the Armenian language as the dominant language within the region.[81]
However, recent genetic research suggests that the Armenian ethnogenesis was completed by 1200 BCE, making the arrival of an Armenian-speaking population as late as the Bronze Age Collapse unlikely.[68][69]
Modern genetic studies show that Armenian diversity can be explained by several mixtures of Eurasian populations that occurred between ~3000 and ~2000 BCE, a period characterized by major population migrations after the domestication of the horse, appearance of the chariot, and the rise of advanced civilizations in the Near East. However, genetic signals of population mixture cease after ~1200 BCE when Bronze Age civilizations in the Eastern Mediterranean world suddenly and violently collapsed.
An alternate theory suggests that Armenians were tribes indigenous to the northern shores of Lake Van or Urartu's northern periphery (possibly as the Hayasans, Etiuni, and/or Diauehi, all of whom are known only from references left by neighboring peoples such Hittites, Urartians and Assyrians).[99] While the Urartian language was used by the royal elite, the population they ruled may have been multi-lingual, and some of these peoples would have spoken Armenian.
An addition to this theory, supported by the official historiography of Armenia and experts in Assyrian and Urartian studies such as Igor M. Diakonoff, Giorgi Melikishvili, Mikhail Nikolsky, and Ivan Mestchaninov, suggests that Urartian was solely the formal written language of the state, while its inhabitants, including the royal family, spoke Armenian.This theory primarily hinges on the fact that the Urartian language used in the cuneiform inscriptions were very repetitive and scant in vocabulary (having as little as 350–400 roots). Furthermore, over 250 years of usage, it shows no development, which is taken to indicate that the language had ceased to be spoken before the time of the inscriptions or was used only for official purposes.[better source needed]
A complimentary theory, suggested by Tamaz V. Gamkrelidze and Ivanov in 1984, places the Proto-Indo-European homeland (the location where Indo-European would have emerged from) in the Armenian Highlands (see: Armenian hypothesis), which would entail the presence of proto-Armenians in the area during the entire lifetime of the Urartian state.[100] The Armenian hypothesis supports the theory that the Urartian language was not spoken, but simply written, and postulates that the Armenian language as an in situ development of a 3rd millennium BC Proto-Indo-European language.[100]
The Orontid dynasty, also known by their native name Eruandid or Yervanduni, was a hereditary Armenian[101] dynasty and the rulers of the successor state to the Iron Age kingdom of Urartu (Ararat).[102][103][104] The Orontids established their supremacy over Armenia around the time of the Scythian and Median invasion in the 6th century BC.
Members of the Orontid dynasty ruled Armenia intermittently during the period spanning the 6th century BC to at least the 2nd century BC, first as client kings or satraps of the Median and Achaemenid empires who established an independent kingdom after the collapse of the Achaemenid empire, and later as kings of Sophene and Commagene who eventually succumbed to the Roman Empire. The Orontids are the first of the three royal dynasties that successively ruled the ancient Kingdom of Armenia (321 BC–428 AD).
Little is known about the origins of the Orontid dynasty.[104][105][106] Some historians believe that the Orontid kings were of Armenian or Urartian origin.[104][107][106] In addition, historians believe the dynasty may have had Iranian origin through a possible relation to the Achaemenids, either through marriage or blood.[104][108]
The name Orontes is the Hellenized form of a masculine name of Iranian origin; Երուանդ Eruand in Old Armenian.[citation needed] The name is only attested in Greek (Gr.:Ὀρόντης). Its Avestan connection is Auruuant (brave, hero) and Middle Persian Arwand (Modern Persian اروند Arvand).[citation needed] Some have suggested a continuity with the Hittite name Arnuwanda. Various Greek transcriptions of the name in Classical sources are spelled as Orontes, Aruandes or Ardoates. The presence of this dynasty is attested from at least 400 BC, and it can be shown to have ruled originally from Armavir and subsequently Yervandashat. Armavir is called the "first capital of the Orontid dynasty" — a few Greek language inscriptions have been found, but the penetration of Hellenistic culture in Armavir seems to have been limited.[109]
The precise date of the foundation of the Orontid dynasty is debated by scholars to this day but there is a consensus that it occurred after the destruction of Urartu by the Scythians and the Medes around 612 BC.
The earliest religious beliefs of Armenians are believed to have been a blend of Indo-European, Mesopotamian, and native Anatolian beliefs. Native gods and goddesses worshiped included Ar (Arev, Areg), Angegh, Astghik, Ayg, Vanatur, and others.
During Median and Persian domination, Iranian religious influences began to mix with native Armenian beliefs, leading to the worship of new, syncretic deities such as Mihr, Aramazd, Vahagn, and Anahit.
Christianity spread into the country as early as AD 40. Tiridates III of Armenia (238–314) made Christianity the state religion in 301,[110][111] partly, in defiance of the Sasanian Empire, it seems,[112] becoming the first officially Christian state, ten years before the Roman Empire granted Christianity an official toleration under Galerius. Prior to this, during the latter part of the Parthian period, Armenia was a predominantly Zoroastrian country.[112]

The Serbs trace their history to the 6th- and 7th-century  migrations of Early Slavs to  south-eastern Europe. Settling in various parts of the Balkans, Early Slavs  assimilated local Byzantine populations (primarily descendants of different paleo-Balkan peoples) and other former Roman citizens. Their descendants later coalesced into different Balkan Slavic  medieval states.[1][2]
Various historical authors mentioned names of Serbs (Serbian: Srbi) and Sorbs (Upper Sorbian: Serbja; Lower Sorbian: Serby) in different variants: as Cervetiis (Servetiis), gentis (S)urbiorum, Suurbi, Sorabi, Soraborum, Sorabos, Surpe, Sorabici, Sorabiet, Sarbin, Swrbjn, Servians, Sorbi, Sirbia, Sribia, Zirbia, Zribia, Suurbelant, Surbia, Serbulia / Sorbulia among others.[3][4][5] These authors used these names to refer to Serbs and Sorbs in areas where their historical (or current) presence was/is not disputed (notably in the Balkans and Lusatia), but there are also sources that mention the same or similar names in other parts of the World (most notably in the Asiatic Sarmatia in the Caucasus). Attempts of various researchers to connect these names with modern Serbs produced various theories about the origin of the Serb people.
According to De Administrando Imperio (DAI), written by the Byzantine emperor Constantine VII (912-959), the Serbs originated from the "White Serbs" who lived on the "other side of Turkey" (name used for Hungary), in the area that they called "Boiki" (Bohemia). White Serbia bordered to the Franks and White Croatia. DAI claims that after two brothers inherited the rule from their father, one of them took half of the people and migrated to the Byzantine Empire (i.e. to the Balkans), which was governed by Emperor Heraclius (610–641).[13][14][15] According to German historian Ludwig Albrecht Gebhardi, the two brothers were sons of Dervan, the dux (duke) of the Surbi (Sorbs).[16] Another part of the White Serbs did not migrate southwards, but remained in the Elbe region. Descendants of these White Serbs with still preserved ethnic identity are the present day Lusatian Serbs (Sorbs), who still live in the Lusatia (Lužica, Lausitz) region of eastern Germany.
Contrary to the general consideration in science, there are also opinions that the data from De administrando imperio that describes Serb migration to the Balkans is not correct and that the Serbs came to the Balkans from Eastern Europe, together with other South Slavs.[17][18]
In the Balkans, Serbs settled first an area near Thessaloniki and then area around rivers Tara, Ibar, Drina and Lim (in the present-day border region of Serbia, Montenegro and Bosnia and Herzegovina), and joined with surrounding South Slavic tribes that came to the Balkans earlier (in the 6th century) and the Byzantine population consisting from different people and tribes. Over time, the South Slavic mixed with the Serbs and also adopted Serb name as their own.[19][20]
The Emperor Constantine III (641) transferred a part of the Slavs from the Balkans (Vardar region) to Asia Minor. There these migrants founded the city of Gordoservon, the name of which gives grounds for supposing that among its founders there were Serbs, and was also known under names Gordoserbon and Servochoria.[21]
Theory about Iranian origin of the Serb ethnonym assumes that ancient Serbi / Serboi from north Caucasus (Asiatic Sarmatia) were a Sarmatian (Alanian) tribe.[22] The theory subsequently assumes that Alanian Serbi were subdued by the Huns in the 4th century and that they, as part of the Hunnic army, migrated to the western edge of the Hunnic Empire (in the area of Central Europe near the river Elbe, later designated as White Serbia in what is now Saxony and Thuringia (eastern Germany), recorded by Vibius Sequester as Cervetiis (Servetiis).[23] After the Hunnic leader Attila died (in 453), Alanian Serbi presumably became independent and ruled in the east of the river Saale (in modern-day Germany) over the local Slavic population.[23][22][24] Over time, they, it is argued, intermarried with the local Slavic population of the region,[22][24] adopted Slavic language, and transferred their name to the Slavs.[23][25] According to Tadeusz Sulimirski, similar event could occur in the Balkans Serbia, settled by Slavs who came from the north and who were ruled by already slavicized Serboi.[23][24]
Deformed human skulls that are connected to the Alans are also discovered in the area that was later designated as "White Serbia".[23][25] According to Indo-European interpretation, different sides of the World are designated with different colors, thus, white color is a designation for the west, black color for the north, blue or green color for the east and red color for the south. According to that view, White Serbia and White Croatia were designated as western Serbia and western Croatia, and were situated in the west from some hypothetical lands that had same names and that presumably existed in the east.[26]
This theory assumes that Serbs are an autochthonic people in the Balkans and Podunavlje, where they presumably lived before historical Slavic and Serb migration to the Balkans in the 6th–7th centuries.[27] Proponents of this theory (for example Jovan I. Deretić, Olga Luković Pjanović [sr], Miloš Milojević) claimed that Serbs either came to the Balkans long before the 7th century or Serb 7th-century migration to the Balkans was only partial and Serbs who, according to De Administrando Imperio, came from the north found in the Balkans other Serbs that already lived there.[27] It is suggested that the ancient city of Serbinum in Pannonia was named after these hypothetical autochthonic Serbs. In mainstream historiography, this is considered to be a fringe theory, and the methods used by its proponents are considered pseudoscientific.[28]

A civilization (also spelled civilisation in British English) is any complex society characterized by the development of the state, social stratification, urbanization, and symbolic systems of communication beyond signed or spoken languages (namely, writing systems).[2][3][4][5][6]
Civilizations are organized around densely-populated settlements, divided into more or less rigid hierarchical social classes of division of labour, often with a ruling elite and a subordinate urban and rural populations, which engage in intensive agriculture, mining, small-scale manufacture and trade. Civilization concentrates power, extending human control over the rest of the nature, including over other human beings.[7] Civilizations are characterized with elaborate agriculture, architecture, infrastructure, technological advancement, currency, taxation, regulation, and specialization of labour.[5][6][8]
Historically, a civilization has often been understood as a larger and "more advanced" culture, in implied contrast to smaller, supposedly less advanced cultures,[9][10][11][12] even societies  within civilizations themselves and within their histories. Generally civilization contrasts with non-centralized tribal societies, including the cultures of nomadic pastoralists, Neolithic societies, or hunter-gatherers.
The word civilization relates to the Latin civitas or 'city'. As the National Geographic Society has explained it: "This is why the most basic definition of the word civilization is 'a society made up of cities.'"[13]
The earliest emergence of civilizations is generally connected with the final stages of the Neolithic Revolution in West Asia, culminating in the relatively rapid process of urban revolution and state formation, a political development associated with the appearance of a governing elite.
The English word civilization comes from the French civilisé ('civilized'), from Latin: civilis ('civil'), related to civis ('citizen') and civitas ('city').[14] The fundamental treatise is Norbert Elias's The Civilizing Process (1939), which traces social mores from medieval courtly society to the early modern period.[a] In The Philosophy of Civilization (1923), Albert Schweitzer outlines two opinions: one purely material and the other material and ethical. He said that the world crisis was from humanity losing the ethical idea of civilization, "the sum total of all progress made by man in every sphere of action and from every point of view in so far as the progress helps towards the spiritual perfecting of individuals as the progress of all progress".[16]
Related words like "civility" developed in the mid-16th century. The abstract noun "civilization", meaning "civilized condition", came in the 1760s, again from French. The first known use in French is in 1757, by Victor de Riqueti, marquis de Mirabeau, and the first use in English is attributed to Adam Ferguson, who in his 1767 Essay on the History of Civil Society wrote, "Not only the individual advances from infancy to manhood but the species itself from rudeness to civilisation".[17] The word was therefore opposed to barbarism or rudeness, in the active pursuit of progress characteristic of the Age of Enlightenment.
In the late 1700s and early 1800s, during the French Revolution, "civilization" was used in the singular, never in the plural, and meant the progress of humanity as a whole. This is still the case in French.[18] The use of "civilizations" as a countable noun was in occasional use in the 19th century,[b] but has become much more common in the later 20th century, sometimes just meaning culture (itself in origin an uncountable noun, made countable in the context of ethnography).[19] Only in this generalized sense does it become possible to speak of a "medieval civilization", which in Elias's sense would have been an oxymoron. Using the terms "civilization" and "culture" as equivalents are controversial and generally rejected so that for example some types of culture are not normally described as civilizations.[20]
Already in the 18th century, civilization was not always seen as an improvement. One historically important distinction between culture and civilization is from the writings of Rousseau, particularly his work about education, Emile. Here, civilization, being more rational and socially driven, is not fully in accord with human nature, and "human wholeness is achievable only through the recovery of or approximation to an original discursive or pre-rational natural unity" (see noble savage). From this, a new approach was developed, especially in Germany, first by Johann Gottfried Herder and later by philosophers such as Kierkegaard and Nietzsche. This sees cultures as natural organisms, not defined by "conscious, rational, deliberative acts", but a kind of pre-rational "folk spirit". Civilization, in contrast, though more rational and more successful in material progress, is unnatural and leads to "vices of social life" such as guile, hypocrisy, envy and avarice.[18] In World War II, Leo Strauss, having fled Germany, argued in New York that this opinion of civilization was behind Nazism and German militarism and nihilism.[21]
Social scientists such as V. Gordon Childe have named a number of traits that distinguish a civilization from other kinds of society.[24][25] Civilizations have been distinguished by their means of subsistence, types of livelihood, settlement patterns, forms of government, social stratification, economic systems, literacy and other cultural traits. Andrew Nikiforuk argues that "civilizations relied on shackled human muscle. It took the energy of slaves to plant crops, clothe emperors, and build cities" and considers slavery to be a common feature of pre-modern civilizations.[26]
All civilizations have depended on agriculture for subsistence, with the possible exception of some early civilizations in Peru which may have depended upon maritime resources.[27][28]
The traditional "surplus model" postulates that cereal farming results in accumulated storage and a surplus of food, particularly when people use intensive agricultural techniques such as artificial fertilization, irrigation and crop rotation. It is possible but more difficult to accumulate horticultural production, and so civilizations based on horticultural gardening have been very rare.[29] Grain surpluses have been especially important because grain can be stored for a long time.
Research from the Journal of Political Economy contradicts the surplus model. It postulates that horticultural gardening was more productive than cereal farming. However, only cereal farming produced civilization because of the appropriability of yearly harvest. Rural populations that could only grow cereals could be taxed allowing for a taxing elite and urban development. This also had a negative effect on rural population, increasing relative agricultural output per farmer. Farming efficiency created food surplus and sustained the food surplus through decreasing rural population growth in favour of urban growth. Suitability of highly productive roots and tubers was in fact a curse of plenty, which prevented the emergence of states and impeded economic development.[30][31]
A surplus of food permits some people to do things besides producing food for a living: early civilizations included soldiers, artisans, priests and priestesses, and other people with specialized careers. A surplus of food results in a division of labour and a more diverse range of human activity, a defining trait of civilizations. However, in some places hunter-gatherers have had access to food surpluses, such as among some of the indigenous peoples of the Pacific Northwest and perhaps during the Mesolithic Natufian culture. It is possible that food surpluses and relatively large scale social organization and division of labour predates plant and animal domestication.[32]
Civilizations have distinctly different settlement patterns from other societies. The word civilization is sometimes defined as "living in cities".[33] Non-farmers tend to gather in cities to work and to trade.
Compared with other societies, civilizations have a more complex political structure, namely the state.[34] State societies are more stratified[35] than other societies; there is a greater difference among the social classes. The ruling class, normally concentrated in the cities, has control over much of the surplus and exercises its will through the actions of a government or bureaucracy. Morton Fried, a conflict theorist and Elman Service, an integration theorist, have classified human cultures based on political systems and social inequality. This system of classification contains four categories.[36]
Economically, civilizations display more complex patterns of ownership and exchange than less organized societies. Living in one place allows people to accumulate more personal possessions than nomadic people. Some people also acquire landed property, or private ownership of the land. Because a percentage of people in civilizations do not grow their own food, they must trade their goods and services for food in a market system, or receive food through the levy of tribute, redistributive taxation, tariffs or tithes from the food producing segment of the population. Early human cultures functioned through a gift economy supplemented by limited barter systems. By the early Iron Age, contemporary civilizations developed money as a medium of exchange for increasingly complex transactions. In a village, the potter makes a pot for the brewer and the brewer compensates the potter by giving him a certain amount of beer. In a city, the potter may need a new roof, the roofer may need new shoes, the cobbler may need new horseshoes, the blacksmith may need a new coat and the tanner may need a new pot. These people may not be personally acquainted with one another and their needs may not occur all at the same time. A monetary system is a way of organizing these obligations to ensure that they are fulfilled. From the days of the earliest monetarized civilizations, monopolistic controls of monetary systems have benefited the social and political elites.
The transition from simpler to more complex economies does not necessarily mean an improvement in the living standards of the populace. For example, although the Middle Ages is often portrayed as an era of decline from the Roman Empire, studies have shown that the average stature of males in the Middle Ages (c. 500 to 1500 CE) was greater than it was for males during the preceding Roman Empire and the succeeding Early Modern Period (c. 1500 to 1800 CE).[39][40] Also, the Plains Indians of North America in the 19th century were taller than their "civilized" American and European counterparts. The average stature of a population is a good measurement of the adequacy of its access to necessities, especially food, and its freedom from disease.[41]
Writing, developed first by people in Sumer, is considered a hallmark of civilization and "appears to accompany the rise of complex administrative bureaucracies or the conquest state".[42] Traders and bureaucrats relied on writing to keep accurate records. Like money, the writing was necessitated by the size of the population of a city and the complexity of its commerce among people who are not all personally acquainted with each other. However, writing is not always necessary for civilization, as shown by the Inca civilization of the Andes, which did not use writing at all but except for a complex recording system consisting of knotted strings of different lengths and colours: the "Quipus", and still functioned as a civilized society.
Aided by their division of labour and central government planning, civilizations have developed many other diverse cultural traits. These include organized religion, development in the arts, and countless new advances in science and technology.
Assessments of what level of civilization a polity has reached are based on comparisons of the relative importance of agricultural as opposed to trading or manufacturing capacities, the territorial extensions of its power, the complexity of its division of labour, and the carrying capacity of its urban centres. Secondary elements include a developed transportation system, writing, standardized measurement, currency, contractual and tort-based legal systems, art, architecture, mathematics, scientific understanding, metallurgy, political structures, and organized religion.
The idea of civilization implies a progression or development from a previous "uncivilized" state. Traditionally, cultures that defined themselves as "civilized" often did so in contrast to other societies or human groupings viewed as less civilized, calling the latter barbarians, savages, and primitives. Indeed, the modern Western idea of civilization developed as a contrast to the indigenous cultures European settlers encountered during the European colonization of the Americas and Australia.[43] The term "primitive," though once used in anthropology, has now been largely condemned by anthropologists because of its derogatory connotations and because it implies that the cultures it refers to are relics of a past time that do not change or progress.[44]
Because of this, societies regarding themselves as "civilized" have sometimes sought to dominate and assimilate "uncivilized" cultures into a "civilized" way of living.[45] In the 19th century, the idea of European culture as "civilized" and superior to "uncivilized" non-European cultures was fully developed, and civilization became a core part of European identity.[46] The idea of civilization can also be used as a justification for dominating another culture and dispossessing a people of their land. For example, in Australia, British settlers justified the displacement of Indigenous Australians by observing that the land appeared uncultivated and wild, which to them reflected that the inhabitants were not civilized enough to "improve" it.[43] The behaviours and modes of subsistence that characterize civilization have been spread by colonization, invasion, religious conversion, the extension of bureaucratic control and trade, and by the introduction of new technologies to cultures that did not previously have them. Though aspects of culture associated with civilization can be freely adopted through contact between cultures, since early modern times Eurocentric ideals of "civilization" have been widely imposed upon cultures through coercion and dominance. These ideals complemented a philosophy that assumed there were innate differences between "civilized" and "uncivilized" peoples.[46]
"Civilization" can also refer to the culture of a complex society, not just the society itself. Every society, civilization or not, has a specific set of ideas and customs, and a certain set of manufactures and arts that make it unique. Civilizations tend to develop intricate cultures, including a state-based decision-making apparatus, a literature, professional art, architecture, organized religion and complex customs of education, coercion and control associated with maintaining the elite.
The intricate culture associated with civilization has a tendency to spread to and influence other cultures, sometimes assimilating them into the civilization, a classic example being Chinese civilization and its influence on nearby civilizations such as Korea, Japan and Vietnam[47] Many civilizations are actually large cultural spheres containing many nations and regions. The civilization in which someone lives is that person's broadest cultural identity.[48][49]
It is precisely the protection of this cultural identity that is becoming increasingly important nationally and internationally. According to international law, the United Nations and UNESCO try to set up and enforce relevant rules. The aim is to preserve the cultural heritage of humanity and also the cultural identity, especially in the case of war and armed conflict. According to Karl von Habsburg, President of Blue Shield International, the destruction of cultural assets is also part of psychological warfare. The target of the attack is often the opponent's cultural identity, which is why symbolic cultural assets become a main target. It is also intended to destroy the particularly sensitive cultural memory (museums, archives, monuments, etc.), the grown cultural diversity, and the economic basis (such as tourism) of a state, region or community.[50][51][52][53][54][55]
Many historians have focused on these broad cultural spheres and have treated civilizations as discrete units. Early twentieth-century philosopher Oswald Spengler,[56] uses the German word Kultur, "culture", for what many call a "civilization". Spengler believed a civilization's coherence is based on a single primary cultural symbol. Cultures experience cycles of birth, life, decline, and death, often supplanted by a potent new culture, formed around a compelling new cultural symbol. Spengler states civilization is the beginning of the decline of a culture as "the most external and artificial states of which a species of developed humanity is capable".[56]
This "unified culture" concept of civilization also influenced the theories of historian Arnold J. Toynbee in the mid-twentieth century. Toynbee explored civilization processes in his multi-volume A Study of History, which traced the rise and, in most cases, the decline of 21 civilizations and five "arrested civilizations". Civilizations generally declined and fell, according to Toynbee, because of the failure of a "creative minority", through moral or religious decline, to meet some important challenge, rather than mere economic or environmental causes.
Samuel P. Huntington defines civilization as "the highest cultural grouping of people and the broadest level of cultural identity people have short of that which distinguishes humans from other species".[48]
Another group of theorists, making use of systems theory, looks at a civilization as a complex system, i.e., a framework by which a group of objects can be analysed that work in concert to produce some result. Civilizations can be seen as networks of cities that emerge from pre-urban cultures and are defined by the economic, political, military, diplomatic, social and cultural interactions among them. Any organization is a complex social system and a civilization is a large organization. Systems theory helps guard against superficial and misleading analogies in the study and description of civilizations.
Systems theorists look at many types of relations between cities, including economic relations, cultural exchanges and political/diplomatic/military relations. These spheres often occur on different scales. For example, trade networks were, until the nineteenth century, much larger than either cultural spheres or political spheres. Extensive trade routes, including the Silk Road through Central Asia and Indian Ocean sea routes linking the Roman Empire, Persian Empire, India and China, were well established 2000 years ago when these civilizations scarcely shared any political, diplomatic, military, or cultural relations. The first evidence of such long-distance trade is in the ancient world. During the Uruk period, Guillermo Algaze has argued that trade relations connected Egypt, Mesopotamia, Iran and Afghanistan.[57] Resin found later in the Royal Cemetery at Ur is suggested was traded northwards from Mozambique.
Many theorists argue that the entire world has already become integrated into a single "world system", a process known as globalization. Different civilizations and societies all over the globe are economically, politically, and even culturally interdependent in many ways. There is debate over when this integration began, and what sort of integration – cultural, technological, economic, political, or military-diplomatic – is the key indicator in determining the extent of a civilization. David Wilkinson has proposed that economic and military-diplomatic integration of the Mesopotamian and Egyptian civilizations resulted in the creation of what he calls the "Central Civilization" around 1500 BCE.[58] Central Civilization later expanded to include the entire Middle East and Europe, and then expanded to a global scale with European colonization, integrating the Americas, Australia, China and Japan by the nineteenth century. According to Wilkinson, civilizations can be culturally heterogeneous, like the Central Civilization, or homogeneous, like the Japanese civilization. What Huntington calls the "clash of civilizations" might be characterized by Wilkinson as a clash of cultural spheres within a single global civilization. Others point to the Crusading movement as the first step in globalization. The more conventional viewpoint is that networks of societies have expanded and shrunk since ancient times, and that the current globalized economy and culture is a product of recent European colonialism.[citation needed]
The notion of human history as a succession of "civilizations" is an entirely modern one. In the European Age of Discovery, emerging Modernity was put into stark contrast with the Neolithic and Mesolithic stage of the cultures of many of the peoples they encountered.[59][obsolete source] Nonetheless, developments in the Neolithic stage, such as agriculture and sedentary settlement, were critical to the development of modern conceptions of civilization.[60][61]
The Natufian culture in the Levantine corridor provides the earliest case of a Neolithic Revolution, with the planting of cereal crops attested from c. 11,000 BCE.[62][63] The earliest neolithic technology and lifestyle were established first in Western Asia (for example at Göbekli Tepe, from about 9,130 BCE), later in the Yellow River and Yangtze basins in China (for example the Peiligang and Pengtoushan cultures), and from these cores spread across Eurasia. Mesopotamia is the site of the earliest civilizations developing from 7,400 years ago. This area has been evaluated by Beverley Milton-Edwards as having "inspired some of the most important developments in human history including the invention of the wheel, the building of the earliest cities and the development of written cursive script".[64] Similar pre-civilized "neolithic revolutions" also began independently from 7,000 BCE in northwestern South America (the Caral-Supe civilization)[65] and in Mesoamerica.[66] The Black Sea area served as a cradle of European civilization. The site of Solnitsata – a prehistoric fortified (walled) stone settlement (prehistoric proto-city) (5500–4200 BCE) – is believed by some archaeologists to be the oldest known town in present-day Europe.[67][68][69][70]
The 8.2 Kiloyear Arid Event and the 5.9 Kiloyear Inter-pluvial saw the drying out of semiarid regions and a major spread of deserts.[71] This climate change shifted the cost-benefit ratio of endemic violence between communities, which saw the abandonment of unwalled village communities and the appearance of walled cities, seen by some as a characteristic of early civilizations.[72]
This "urban revolution"—a term introduced by Childe in the 1930s—from the 4th millennium BCE,[73] marked the beginning of the accumulation of transferable economic surpluses, which helped economies and cities develop. Urban revolutions were associated with the state monopoly of violence, the appearance of a warrior, or soldier, class and endemic warfare (a state of continual or frequent warfare), the rapid development of hierarchies, and the use of human sacrifice.[74][75]
The civilized urban revolution in turn was dependent upon the development of sedentism, the domestication of grains, plants and animals, the permanence of settlements and development of lifestyles that facilitated economies of scale and accumulation of surplus production by particular social sectors. The transition from complex cultures to civilizations, while still disputed, seems to be associated with the development of state structures, in which power was further monopolized by an elite ruling class[76] who practiced human sacrifice.[77]
Towards the end of the Neolithic period, various elitist Chalcolithic civilizations began to rise in various "cradles" from around 3600 BCE beginning with Mesopotamia, expanding into large-scale kingdoms and empires in the course of the Bronze Age (Akkadian Empire, Indus Valley Civilization, Old Kingdom of Egypt, Neo-Sumerian Empire, Middle Assyrian Empire, Babylonian Empire, Hittite Empire, and to some degree the territorial expansions of the Elamites, Hurrians, Amorites and Ebla).
Outside the Old World, development took place independently in the Pre-Columbian Americas. Urbanization in the Caral-Supe civilization in what is now coastal Peru began about 3500 BCE.[78] In North America, the Olmec civilization emerged about 1200 BCE; the oldest known Mayan city, located in what is now Guatemala, dates to about 750 BCE.[79] and Teotihuacan (near the modern Mexico City) was one of the largest cities in the world in 350 CE, with a population of about 125,000.[80]
The Bronze Age collapse was followed by the Iron Age around 1200 BCE, during which a number of new civilizations emerged, culminating in a period from the 8th to the 3rd century BCE which Karl Jaspers termed the Axial Age, presented as a critical transitional phase leading to classical civilization.[81]
A major technological and cultural transition to modernity began approximately 1500 CE in Western Europe, and from this beginning new approaches to science and law spread rapidly around the world, incorporating earlier cultures into the technological and industrial society of the present.[77][82]
Civilizations are traditionally understood as ending in one of two ways; either through incorporation into another expanding civilization (e.g. as Ancient Egypt was incorporated into Hellenistic Greek, and subsequently Roman civilizations), or by collapsing and reverting to a simpler form of living, as happens in so-called Dark Ages.[83]
There have been many explanations put forward for the collapse of civilization. Some focus on historical examples, and others on general theory.
According to political scientist Samuel P. Huntington, the 21st century will be characterized by a clash of civilizations,[48] which he believes will replace the conflicts between nation-states and ideologies that were prominent in the 19th and 20th centuries. However, this viewpoint been strongly challenged by others such as Edward Said, Muhammed Asadi and Amartya Sen.[94] Ronald Inglehart and Pippa Norris have argued that the "true clash of civilizations" between the Muslim world and the West is caused by the Muslim rejection of the West's more liberal sexual values, rather than a difference in political ideology, although they note that this lack of tolerance is likely to lead to an eventual rejection of (true) democracy.[95] In Identity and Violence Sen questions if people should be divided along the lines of a supposed "civilization", defined by religion and culture only. He argues that this ignores the many others identities that make up people and leads to a focus on differences.
Cultural Historian Morris Berman argues in Dark Ages America: the End of Empire that in the corporate consumerist United States, the very factors that once propelled it to greatness―extreme individualism, territorial and economic expansion, and the pursuit of material wealth―have pushed the United States across a critical threshold where collapse is inevitable. Politically associated with over-reach, and as a result of the environmental exhaustion and polarization of wealth between rich and poor, he concludes the current system is fast arriving at a situation where continuation of the existing system saddled with huge deficits and a hollowed-out economy is physically, socially, economically and politically impossible.[96] Although developed in much more depth, Berman's thesis is similar in some ways to that of Urban Planner, Jane Jacobs who argues that the five pillars of United States culture are in serious decay: community and family; higher education; the effective practice of science; taxation and government; and the self-regulation of the learned professions. The corrosion of these pillars, Jacobs argues, is linked to societal ills such as environmental crisis, racism and the growing gulf between rich and poor.[97]
Cultural critic and author Derrick Jensen argues that modern civilization is directed towards the domination of the environment and humanity itself in an intrinsically harmful, unsustainable, and self-destructive fashion.[98] Defending his definition both linguistically and historically, he defines civilization as "a culture... that both leads to and emerges from the growth of cities", with "cities" defined as "people living more or less permanently in one place in densities high enough to require the routine importation of food and other necessities of life".[99] This need for civilizations to import ever more resources, he argues, stems from their over-exploitation and diminution of their own local resources. Therefore, civilizations inherently adopt imperialist and expansionist policies and, to maintain these, highly militarized, hierarchically structured, and coercion-based cultures and lifestyles.
The Kardashev scale classifies civilizations based on their level of technological advancement, specifically measured by the amount of energy a civilization is able to harness. The scale is only hypothetical, but it puts energy consumption in a cosmic perspective. The Kardashev scale makes provisions for civilizations far more technologically advanced than any currently known to exist.
The current scientific consensus is that human beings are the only animal species with the cognitive ability to create civilizations that has emerged on Earth. A recent thought experiment, the silurian hypothesis, however, considers whether it would "be possible to detect an industrial civilization in the geological record" given the paucity of geological information about eras before the quaternary.[100]
Astronomers speculate about the existence of communicating with intelligent civilizations within and beyond the Milky Way galaxy, usually using variants of the Drake equation.[101] They conduct searches for such intelligences – such as for technological traces, called "technosignatures".[102] The proposed proto-scientific field "xenoarchaeology" is concerned with the study of artifact remains of non-human civilizations to reconstruct and interpret past lives of alien societies if such get discovered and confirmed scientifically.[103][104]

Christopher Brian Stringer CBE FRS is a British physical anthropologist noted for his work on human evolution.
Growing up in a working-class family in the East End of London, Stringer first took an interest in anthropology during primary school, when he undertook a project on Neanderthals.[1] Stringer studied anthropology at University College London,[2] holds a PhD in Anatomical Science and a DSc in Anatomical Science (both from Bristol University).[3]
Stringer joined the permanent staff of  the Natural History Museum in 1973. He is currently Research Leader in Human Origins.
Stringer was previously one of the leading proponents of the recent African origin hypothesis or ″Out of Africa″ theory, which hypothesizes that modern humans originated in Africa over 100,000 years ago and replaced, in some way, the world's archaic humans, such as Homo floresiensis and Neanderthals, after migrating within and then out of Africa to the non-African world within the last 50,000 to 100,000 years.[4] He always considered that some interbreeding between the different groups could have occurred, but thought this would have been trivial in the big picture. However, recent genetic data show that the replacement process did include some interbreeding. In the last decade he has proposed a more complex version of events within Africa, which he has termed ″multiregional African origin″.
He also directed the Ancient Human Occupation of Britain project which ran for about 10 years from 2001. This consortium reconstructed and studied the episodic pattern of human colonisation of Britain during the Pleistocene. He is co-director of the follow-up project "Pathways to Ancient Britain".[5]
He is a Fellow of the Royal Society and Honorary Fellow of the Society of Antiquaries. He won the 2008 Frink Medal of the Zoological Society of London and the Rivers Memorial Medal from the Royal Anthropological Institute in 2004[6]
He was elected a Member of the American Philosophical Society in 2019.
Stringer was appointed Commander of the Order of the British Empire (CBE) in the 2023 New Year Honours for services to the understanding of human evolution.[7]

Prehistoric Armenia refers to the history of the region that would eventually be known as Armenia, covering the period of the earliest known human presence in the Armenian Highlands from the Lower Paleolithic more than 1 million years ago until the Iron Age and the emergence of Urartu in the 9th century BC, the end of which in the 6th century BC marks the beginning of Ancient Armenia.
The Armenian Highlands have been settled by human groups from the Lower Paleolithic to modern days. The first human traces are supported by the presence of Acheulean tools, generally close to the obsidian outcrops more than 1 million years ago.[1] Middle and Upper Paleolithic settlements have also been identified such as at the Hovk 1 cave and the Trialetian culture.[2]
The most recent and important excavation is at the Nor Geghi 1 Stone Age site in the Hrazdan river valley.[3] Thousands of 325,000 year-old artifacts may indicate that this stage of human technological innovation occurred intermittently throughout the Old World, rather than spreading from a single point of origin (usually hypothesized to be Africa), as was previously thought.[4]
The sites of Aknashen and Aratashen in the Ararat plain region are believed to belong to the Neolithic period.[5] The Mestamor archaeological site, located to the southwest of Armenian village of Taronik in the Armavir Province, also shows evidence of settlement starting from the Neolithic era.[6]
The name 'Armenia' is written for the first time in history in the 24th-23rd centuries B.C. in the Mesopotamian cuneiform inscrptions in the form 'Armani', while in the text of the same period discovered in Ebla (Syria) Armenia is called 'Armi'.[6]
The Shulaveri-Shomu culture of the central Transcaucasus region is one of the earliest known prehistoric cultures in the area, carbon-dated to roughly 6000 - 4000 BC. The Shulaveri-Shomu culture in the area was succeeded by the Bronze Age Kura-Araxes culture, dated to the period of ca. 3400 - 2000 BC.
An early Bronze-Age culture in the area is the Kura-Araxes culture, assigned to the period between c. 4000 and 2200 BC. The earliest evidence for this culture is found on the Ararat plain; thence it spread to Georgia by 3000 BC (but never reaching Colchis), proceeding westward and to the south-east into an area below the Urmia basin and Lake Van.
From 2200 BC to 1600 BC, the Trialeti-Vanadzor culture flourished in Armenia, southern Georgia, and northeastern Turkey.[7][8] It has been speculated that this was an Indo-European culture.[9][10][11] Other possibly related cultures were spread throughout the Armenia Highlands during this time, namely in the Aragats and Lake Sevan regions.[12][13][14]
Early 20th-century scholars suggested that the name "Armenia" may have possibly been recorded for the first time on an inscription which mentions Armanî (or Armânum) together with Ibla, from territories conquered by Naram-Sin (2300 BC) identified with an Akkadian colony in the current region of Diyarbekir; however, the precise locations of both Armani and Ibla are unclear. Some modern researchers have placed Armani (Armi) in the general area of modern Samsat,[15] and have suggested it was populated, at least partially, by an early Indo-European-speaking people.[16] Today, the Modern Assyrians (who traditionally speak Neo-Aramaic, however, not Akkadian) refer to the Armenians by the name Armani.[17] Thutmose III of Egypt, in the 33rd year of his reign (1446 BC), mentioned as the people of "Ermenen", claiming that in their land "heaven rests upon its four pillars".[18] Armenia is possibly connected to Mannaea, which may be identical to the region of Minni mentioned in The Bible. However, what all these attestations refer to cannot be determined with certainty, and the earliest certain attestation of the name "Armenia" comes from the Behistun Inscription (c. 500 BC).
The earliest form of the word "Hayastan", an endonym for Armenia, might possibly be Hayasa-Azzi, a kingdom in the Armenian Highlands that was recorded in Hittite records dating from 1500 to 1200 BC.
Between 1200 and 800 BC, much of Armenia was united under a confederation of tribes, which Assyrian sources called Nairi ("Land of Rivers" in Assyrian").[19]
The main object of early Assyrian incursions into Armenia was to obtain metals. The iron-working age followed that of bronze everywhere, opening a new epoch of human progress. Its influence is noticeable in Armenia, and the transition period is well marked. Tombs whose metal contents are all of bronze are of an older epoch. In most of the cemeteries explored, both bronze and iron furniture were found, indicating the gradual advance into the Iron Age.

While belief in the sanctity of human life has ancient precedents in many religions of the world, the foundations of modern human rights began during the era of renaissance humanism in the early modern period. The European wars of religion and the civil wars of seventeenth-century Kingdom of England gave rise to the philosophy of liberalism and belief in natural rights became a central concern of European intellectual culture during the eighteenth-century Age of Enlightenment. Ideas of natural rights, which had a basis in natural law, lay at the core of the American and French Revolutions which occurred toward the end of that century, but the idea of human rights came about later. Democratic evolution through the nineteenth century paved the way for the advent of universal suffrage in the twentieth century. Two world wars led to the creation of the Universal Declaration of Human Rights.
The post-war era saw movements arising from specific groups experiencing a shortfall in their rights, such as feminism and the civil rights of African Americans. The human rights movements of members of the Soviet bloc emerged in the 1970s along with workers' rights movements in the West. The movements quickly jelled as social activism and political rhetoric in many nations put human rights high on the world agenda.[1] By the 21st century, historian Samuel Moyn has argued, the human rights movement expanded beyond its original anti-totalitarianism to include numerous causes involving humanitarianism and social and economic development in the Developing World.[2]
The history of human rights has been complex. Many established rights for instance would be replaced by other systems which deviate from their original western design. Stable institutions may be uprooted such as in cases of conflict such as war and terrorism or a change in culture.[3]
Some notions of righteousness present in ancient law and religion are sometimes retrospectively included under the term "human rights". While Enlightenment philosophers suggest a secular social contract between the rulers and the ruled, ancient traditions derived similar conclusions from notions of divine law, and, in Hellenistic philosophy, natural law. Samuel Moyn suggests that the concept of human rights is intertwined with the modern sense of citizenship, which did not emerge until the past few hundred years.[4] Nonetheless, relevant examples exist in the Ancient and pre-modern eras, although Ancient peoples did not have the same modern-day conception of universal human rights.[5]
The reforms of Urukagina of Lagash, the earliest known legal code (c. 2350 BC), is often thought to be an early example of reform. Professor Norman Yoffee wrote that after Igor M. Diakonoff "most interpreters consider that Urukagina, himself not of the ruling dynasty at Lagash, was no reformer at all. Indeed, by attempting to curb the encroachment of a secular authority at the expense of temple prerogatives, he was, if a modern term must be applied, a reactionary."[6]  Author Marilyn French wrote that the discovery of penalties for adultery for women but not for men represents "the first written evidence of the degradation of women".[6][7] The oldest legal code extant today is the Neo-Sumerian Code of Ur-Nammu (c. 2050 BC). Several other sets of laws were also issued in Mesopotamia, including the Code of Hammurabi (c. 1780 BC), one of the most famous examples of this type of document. It shows rules, and punishments if those rules are broken, on a variety of matters, including women's rights, men's rights, children's rights and slave rights.
The Northeast African civilization of Ancient Egypt[8] supported basic human rights.[9] For example, Pharaoh Bocchoris (725–720 BC) promoted individual rights, suppressed imprisonment for debt, and reformed laws relating to the transferral of property.[9]
Many historians suggest that the Achaemenid Persian Empire of ancient Iran established unprecedented principles of human rights in the 6th century BC under Cyrus the Great. After his conquest of the Neo-Babylonian Empire in 539 BC, the king issued the Cyrus cylinder, discovered in 1879 and seen by some today as the first human rights document.[10][11][12] The cylinder has been linked by some commentators to the decrees of Cyrus recorded in the Books of Chronicles, Nehemiah, and Ezra, which state that Cyrus allowed (at least some of) the Jews to return to their homeland from their Babylonian Captivity. Additionally it stated the freedom to practice one's faith without persecution and forced conversions.[13][14] According to art historian Neil MacGregor, the proclamation of full religious freedoms in Babylon and elsewhere in the Persian empire was an important inspiration for human rights by prominent thinkers millennia later, especially in the United States.[15]
In opposition to the above viewpoint, the interpretation of the Cylinder as a "charter of human rights" has been dismissed by other historians and characterized by some others as political propaganda devised by the Pahlavi regime.[16] The German historian Josef Wiesehöfer argues that the image of "Cyrus as a champion of the UN human rights policy ... is just as much a phantom as the humane and enlightened Shah of Persia",[17] while historian Elton L. Daniel has described such an interpretation as "rather anachronistic" and tendentious.[18] The cylinder now lies in the British Museum, and a replica is kept at the United Nations Headquarters.
Many thinkers point to the concept of citizenship beginning in the early poleis of ancient Greece, where all free citizens had the right to speak and vote in the political assembly.[19]
The Twelve Tables Law established the principle "Privilegia ne irroganto", which literally means "privileges shall not be imposed".
The Mauryan Emperor Ashoka, who ruled from 268 to 232 BCE, established the largest empire in South Asia. Following the reportedly destructive Kalinga War, Ashoka adopted Buddhism and abandoned an expansionist policy in favor of humanitarian reforms.  The Edicts of Ashoka were erected throughout his empire, containing the 'Law of Piety'.[20] These laws prohibited religious discrimination, and cruelty against both humans and animals.[21] The Edicts emphasize the importance of tolerance in public policy by the government. The slaughter or capture of prisoners of war was also condemned by Ashoka.[22] Some sources claim that slavery was also non-existent in ancient India.[23] The Greek records say there is absence of slavery during the rule of Sandrocottus.[24]
In ancient Rome an ius gentium or jus gentium was a right which a citizen was due simply by dint of his citizenship.[25] The concept of a Roman ius is a precursor to a right as conceived in the Western European tradition.  The word "justice" is derived from ius. Human rights legislation in the Roman Empire included the introduction of the presumption of innocence by Emperor Antoninus Pius[26] and the Edict of Milan by Emperor Constantine the Great establishing complete freedom of religion.[27]
The coining of the phrase 'Human rights' can be attributed to Tertullian in his letter To Scapula wherein he wrote about the religious freedom in Roman Empire.[28][29] He equated "fundamental human rights" as a "privilege of nature" in this letter.
Historians generally agree that Muhammad preached against what he saw as the social evils of his day,[30] and that Islamic social reforms in areas such as social security, family structure, slavery, and the rights of women and ethnic minorities were intended to improve on what was present in existing Arab society at the time.[31][32][33][34][35][36] For example, according to Bernard Lewis, Islam "from the first denounced aristocratic privilege, rejected hierarchy, and adopted a formula of the career open to the talents."[which?][31] John Esposito sees Muhammad as a reformer who condemned practices of the pagan Arabs such as female infanticide, exploitation of the poor, usury, murder, false contracts, and theft.[37] Bernard Lewis believes that the egalitarian nature of Islam "represented a very considerable advance on the practice of both the Greco-Roman and the ancient Persian world."[31] Muhammed also incorporated Arabic and Mosaic laws and customs of the time into his divine revelations.[38]
The Constitution of Medina, also known as the Charter of Medina, was drafted by Muhammad in 622. It constituted a formal agreement between Muhammad and all of the significant tribes and families of Yathrib (later known as Medina), including Muslims, Jews, and pagans.[39][40] The document was drawn up with the explicit concern of bringing to an end the bitter intertribal fighting between the clans of the Aws (Aus) and Khazraj within Medina. To this effect it instituted a number of rights and responsibilities for the Muslim, Jewish and pagan communities of Medina bringing them within the fold of one community-the Ummah.[41]
If the prisoners were in the custody of a person, then the responsibility was on the individual.[42] Lewis states that Islam brought two major changes to ancient slavery which were to have far-reaching consequences. "One of these was the presumption of freedom; the other, the ban on the enslavement of free persons except in strictly defined circumstances," Lewis continues. The position of the Arabian slave was "enormously improved": the Arabian slave "was now no longer merely a chattel but was also a human being with a certain religious and hence a social status and with certain quasi-legal rights."[43]
Esposito states that reforms in women's rights affected marriage, divorce and inheritance.[37] Women were not accorded with such legal status in other cultures, including the West, until centuries later.[44] The Oxford Dictionary of Islam states that the general improvement of the status of Arab women included prohibition of female infanticide and recognizing women's full personhood.[45] "The dowry, previously regarded as a bride-price paid to the father, became a nuptial gift retained by the wife as part of her personal property."[37][46] Under Islamic law, marriage was no longer viewed as a "status" but rather as a "contract", in which the woman's consent was imperative.[37][45][46] "Women were given inheritance rights in a patriarchal society that had previously restricted inheritance to male relatives."[37] Annemarie Schimmel states that "compared to the pre-Islamic position of women, Islamic legislation meant an enormous progress; the woman has the right, at least according to the letter of the law, to administer the wealth she has brought into the family or has earned by her own work."[47] William Montgomery Watt states that Muhammad, in the historical context of his time, can be seen as a figure who testified on behalf of women's rights and improved things considerably. Watt explains: "At the time Islam began, the conditions of women were terrible—they had no right to own property, were supposed to be the property of the man, and if the man died everything went to his sons." Muhammad, however, by "instituting rights of property ownership, inheritance, education and divorce, gave women certain basic safeguards."[48] Haddad and Esposito state that "Muhammad granted women rights and privileges in the sphere of family life, marriage, education, and economic endeavors, rights that help improve women's status in society."[49] However, other writers have argued that women before Islam were more liberated drawing most often on the first marriage of Muhammad and that of Muhammad's parents, but also on other points such as worship of female idols at Mecca.[50]
Sociologist Robert Bellah (Beyond belief) argues that Islam in its 7th-century origins was, for its time and place, "remarkably modern...in the high degree of commitment, involvement, and participation expected from the rank-and-file members of the community."  This is because, he argues, that Islam emphasized the equality of all Muslims, where leadership positions were open to all. Dale Eickelman writes that Bellah suggests "the early Islamic community placed a particular value on individuals, as opposed to collective or group responsibility."[51]
Early Islamic law's principles concerning military conduct and the treatment of prisoners of war under the early Caliphate are considered precursors to international humanitarian law. The many requirements on how prisoners of war should be treated included, for example, providing shelter, food and clothing, respecting their cultures, and preventing any acts of execution, rape or revenge. Some of these principles were not codified in Western international law until modern times.[52] Islamic law under the early Caliphate institutionalised humanitarian limitations on military conduct, including attempts to limit the severity of war, guidelines for ceasing hostilities, distinguishing between civilians and combatants, preventing unnecessary destruction, and caring for the sick and wounded.[53]
The concept of human rights in the medieval ages built on the natural law tradition. This tradition was heavily influenced by the writings of St Paul's early Christian thinkers such as St Hilary of Poitiers, St Ambrose, and St Augustine.[54] Augustine was among the earliest to examine the legitimacy of the laws of man, and attempt to define the boundaries of what laws and rights occur naturally based on wisdom and conscience, instead of being arbitrarily imposed by mortals, and if people are obligated to obey laws that are unjust.[55]
This medieval tradition became prominent and influenced Magna Carta is an English charter originally issued in 1215 which influenced the development of the common law and many later constitutional documents related to human rights, such as the 1689 English Bill of Rights, the 1789 United States Constitution, and the 1791 United States Bill of Rights.[56]
Magna Carta was originally written because of disagreements between Pope Innocent III, King John and the English barons about the rights of the King. Magna Carta required the King to renounce certain rights, respect certain legal procedures and accept that his will could be bound by the law. It explicitly protected certain rights of the King's subjects, whether free or fettered—most notably the writ of habeas corpus, allowing appeal against unlawful imprisonment.
For modern times, the most enduring legacy of Magna Carta is considered the right of habeas corpus. This right arises from what are now known as clauses 36, 38, 39, and 40 of the 1215 Magna Carta. Magna Carta also included the right to due process:
No Freeman shall be taken or imprisoned, or be disseised of his Freehold, or Liberties, or free Customs, or be outlawed, or exiled, or any other wise destroyed; nor will We not pass upon him, nor condemn him, but by lawful judgment of his Peers, or by the Law of the Land. We will sell to no man, we will not deny or defer to any man either Justice or Right.
The statute of Kalisz (1264), bestowed privileges to the Jewish minority in the Kingdom of Poland such as protection from discrimination and hate speech.[57]
At the Council of Constance (1414–1418), scholar and jurist Pawel Wlodkowic delivered an address from his Tractatus de potestate papae et imperatoris respectu infidelium ("Treatise on the Power of the Pope and the Emperor Respecting Infidels") in which he advocated the peaceful coexistence of Christians and pagans, making him a precursor of religious tolerance in Europe.[58]
The Kouroukan Fouga was the constitution of the Mali Empire. It was composed in the 13th century, and was one of the very first charters on human rights. It included the "right to life and to the preservation of physical integrity" and significant protections for women.[59][60]: 334
The conquest of the Americas in the 15th and 16th centuries by Spain, during the Age of Discovery, resulted in vigorous debate about human rights in Colonial Spanish America.[61]  This led to the issuance of the Laws of Burgos by Ferdinand the Catholic on behalf of his daughter, Joanna of Castile. Friar Antonio de Montesinos, a Friar of the Dominican Order at the Island of Hispaniola, delivered a sermon on December 21, 1511, which was attended by Bartolomé de las Casas. It is believed that reports from the Dominicans in Hispaniola motivated the Spanish Crown to act. The sermon, known as the Christmas Sermon, gave way to further debates from 1550 to 1551 between Las Casas and Juan Ginés de Sepúlveda at Valladolid. Among the provisions of the Laws of Burgos were child labor; women's rights; wages; suitable accommodations; and rest/vacation, among others.
Several 17th- and 18th-century European philosophers, most notably John Locke, developed the concept of natural rights, the notion that people are naturally free and equal.[62] Locke believed natural rights were derived from divinity since humans were creations of God, and his ideas were important in the development of the modern notion of rights. Lockean natural rights did not rely on citizenship nor any law of the state, nor were they necessarily limited to one particular ethnic, cultural or religious group. Around the same time, in 1689, the English Bill of Rights was created which asserted some basic human rights,[63] most famously freedom from cruel and unusual punishment.[64]
In the 1700s, the novel became a popular form of entertainment. Popular novels, such as Julie, or the New Heloise by Jean-Jacques Rousseau and Pamela; or, Virtue Rewarded by Samuel Richardson, laid a foundation for popular acceptance of human rights by making readers empathize with characters unlike themselves.[65][66]
Two major revolutions occurred during the 18th century in the United States (1776) and in France (1789). The Virginia Declaration of Rights of 1776 sets up a number of fundamental rights and freedoms. The later United States Declaration of Independence includes concepts of natural rights and famously states "that all men are created equal, that they are endowed by their Creator with certain unalienable rights, that among these are life, liberty and the pursuit of happiness"; this was followed in 1789 by the United States Bill of Rights, that enumerated specific rights, such as freedom of speech and the right against self-incrimination. Similarly, the French Declaration of the Rights of Man and of the Citizen defines a set of individual and collective rights of the people. These are, in the document, held to be universal—not only to French citizens but to all men without exception.
Philosophers such as Thomas Paine, John Stuart Mill and Hegel expanded on the theme of universality during the 18th and 19th centuries.
In 1831 William Lloyd Garrison wrote in The Liberator newspaper that he was trying to enlist his readers in "the great cause of human rights"[67] so the term human rights may have come into use sometime between Paine's The Rights of Man and Garrison's publication. In 1849, a contemporary, Henry David Thoreau, wrote about human rights in his treatise On the Duty of Civil Disobedience which was later influential on human rights and civil rights thinkers. United States Supreme Court Justice David Davis, in his 1867 opinion for Ex parte Milligan, wrote: "By the protection of the law, human rights are secured; withdraw that protection and they are at the mercy of wicked rulers or the clamor of an excited people."[68]
Many groups and movements have managed to achieve profound social changes over the course of the 20th century in the name of human rights. In Western Europe and North America, labour unions brought about laws granting workers the right to strike, establishing safer work conditions and forbidding or regulating child labor. The women's suffrage movement succeeded in gaining for many women the right to vote. National liberation movements in the Global South succeeded in gaining many countries independence from Western colonialism, one of the most influential being Mahatma Gandhi's leadership of the Indian independence movement. Movements by ethnic and religious minorities for racial and religious equality succeeded in many parts of the world, among them the American civil rights movement, and more recent diverse identity politics movements, on behalf of women and minorities which have occurred around the world.[citation needed]
The foundation of the International Committee of the Red Cross, the 1864 Lieber Code and the first of the Geneva Conventions in 1864 laid the foundations of international humanitarian law, to be further developed following the two World Wars.
Auguries of United Nations human rights law have been located in the late-19th century movement to suppress and abolish slavery across the world as well as in the conventional protection of minorities from religious, racial, and national discrimination within states under the auspices of unilateral, bilateral, and multilateral treaty law, first found in the 1878 Treaty of Berlin.[69][70]
Pope Leo XIII's Apostolic Exhortation Rerum Novarum in 1891 marked the official beginning of Catholic Social Teaching.  The document was principally concerned with discussing workers' rights, property rights, and citizens' rights against State intrusion. From that time forward, popes (and Vatican II) would release apostolic exhortations and encyclicals on topics that touched on human rights more and more frequently.
The proposition that a state's agents could be held criminally responsible for atrocities perpetrated against the state's own nationals was advanced by the British, French, and Russian governments in May 1915 in response to Turkey's genocide of Armenians.[71][72]
The League of Nations was established in 1919 at the negotiations over the Treaty of Versailles following the end of World War I. The League's goals included disarmament, preventing war through collective security, settling disputes between countries through negotiation, diplomacy and improving global welfare. Enshrined in its Charter was a mandate to promote many of the rights which were later included in the Universal Declaration of Human Rights.
The League of Nations had mandates to support many of the former colonies of the Western European colonial powers during their transition from colony to independent state.
Established as an agency of the League of Nations, and now part of United Nations, the International Labour Organization also had a mandate to promote and safeguard certain of the rights later included in the UDHR:
the primary goal of the ILO today is to promote opportunities for women and men to obtain decent and productive work, in conditions of freedom, equity, security and human dignity.
Also of particular note is the ILO's 1919 convention protecting women from pregnancy discrimination in employment, the 1921 Right of Association (Agriculture) Convention, and the 1930 Forced Labour Convention.
The Geneva Conventions came into being between 1864 and 1949 as a result of efforts by Henry Dunant, the founder of the International Committee of the Red Cross. The conventions safeguard the human rights of individuals involved in conflict, and follow on from the 1899 and 1907 Hague Conventions, the international community's first attempt to define laws of war. Despite first being framed before World War II, the conventions were revised as a result of World War II and readopted by the international community in 1949.
The Geneva Conventions are:
In addition, there are three additional amendment protocols to the Geneva Convention:
All four conventions were last revised and ratified in 1949, based on previous revisions and partly on some of the 1907 Hague Conventions. Later, conferences have added provisions prohibiting certain methods of warfare and addressing issues of civil wars.  Nearly all 200 countries of the world are "signatory" nations, in that they have ratified these conventions. The International Committee of the Red Cross is the controlling body of the Geneva conventions.
The Universal Declaration of Human Rights is a non-binding declaration adopted by the United Nations General Assembly[77] in 1948, partly in response to the barbarism of World War II. The Declaration urges member nations to promote a number of human, civil, economic and social rights, asserting these rights are part of the "foundation of freedom, justice and peace in the world". It was declared by the United Nations General Assembly to be a common standard of achievements for all peoples and all nations. It sets forth, for the first time in history, fundamental human rights to be universally protected.[78]
...recognition of the inherent dignity and of the equal and inalienable rights of all members of the human family is the foundation of freedom, justice and peace in the world
The Universal Declaration of Human Rights was framed by members of the Human Rights Commission, with Eleanor Roosevelt as Chair, who began to discuss an "International Bill of Rights" in 1947. The members of the Commission did not immediately agree on the form of such a bill of rights, and whether, or how, it should be enforced. The Commission proceeded to frame the UDHR and accompanying treaties, but the UDHR quickly became the priority.[79] Canadian law professor John Humphrey and French lawyer Rene Cassin were responsible for much of the cross-national research and the structure of the document respectively, where the articles of the declaration were interpretative of the general principle of the preamble. The document was structured by Cassin to include the basic principles of dignity, liberty, equality and brotherhood in the first two articles, followed successively by rights pertaining to individuals; rights of individuals in relation to each other and to groups; spiritual, public and political rights; and economic, social and cultural rights. The final three articles place, according to Cassin, rights in the context of limits, duties and the social and political order in which they are to be realized.[79] Humphrey and Cassin intended the rights in the UDHR to be legally enforceable through some means, as is reflected in the third clause of the preamble:[79]
Whereas it is essential, if man is not to be compelled to have recourse, as a last resort, to rebellion against tyranny and oppression, that human rights should be protected by the rule of law.
Some of the Declaration was researched and written by a committee of international experts on human rights, including representatives from all continents and all major religions, and drawing on consultation with leaders such as Mahatma Gandhi.[80] The inclusion of both civil and political rights and economic, social and cultural rights[79][81] was predicated on the assumption that basic human rights are indivisible and that the different types of rights listed are inextricably linked. Though this principle was not opposed by any member states at the time of adoption (the declaration was adopted unanimously, with the abstention of the Soviet Bloc, Apartheid South Africa and Saudi Arabia), this principle was later subject to significant challenges.[81]
The UN declaration was succeeded by the European Convention on Human Rights, a binding convention drafted by the Council of Europe in 1950 and signed by 47 countries.[82]  The Convention has 18 articles, 13 of which are rights guaranteed under it:[83]
The other five articles address enforcement of the rights enumerated in the convention and special circumstances in which these rights can be restricted. The United Kingdom, one of the signatories of the ECHR, later passed the Human Rights Act 1998 enshrining these rights in UK law and giving the judiciary the ability to enforce them under UK law.
We have already found a high degree of personal liberty, and we are now struggling to enhance equality of opportunity. Our commitment to human rights must be absolute, our laws fair, our natural beauty preserved; the powerful must not persecute the weak, and human dignity must be enhanced.
According to historian Samuel Moyn the next major landmark in human rights happened in the 1970s.[85] Human rights were included in point VII of the Helsinki Accords, which was signed in 1975 by thirty-five states, including the United States, Canada, and all European states except Albania and Andorra.
During his inaugural speech in 1977, the 39th President of the United States Jimmy Carter made human rights a pillar of United States foreign policy.[86] Human rights advocacy organization Amnesty International later won the Nobel Peace Prize also in 1977.[87] Carter, who was instrumental to the Camp David accord peace treaty would himself win the Nobel Peace Prize in 2002 "for his decades of untiring effort to find peaceful solutions to international conflicts, to advance democracy and human rights, and to promote economic and social development".[88]
Karel Vasak proposed a categorization to illustrate, more or less, how human rights evolved throughout the 20th century. He identified three generations of human rights: first-generation civil and political rights (e.g., the right to life and political participation), second-generation economic, social, and cultural rights (e.g., the right to subsistence), and third-generation solidarity rights (e.g., the rights to peace and a clean environment). The third generation remains the most debated, lacking both legal and political recognition.
Human rights advocacy has continued into the early 21st century, centred around achieving greater economic and political freedom.[89] In July 2022, the United Nations General Assembly adopted a resolution in which it is recognized that everyone on the planet has a right to a healthy environment. It called on states to step up efforts to ensure their people have access to a "clean, healthy and sustainable environment."[90]
Several authors advocate in this century for the recognition of new generations of human rights, though its content remains unclear and lacks a unified proposal. These new groups of rights encompasses rights related to technological advancements, digital rights, and broader concepts of humanity's relationship with technology and the environment. Suggested rights include equal access to digital resources, digital self-determination, security, and control over personal data.
Some theorists, particularly in India and Africa (e.g., the Ubuntu philosophy), consider imperative to shift from an individualistic and societal approach to rights toward a more communitarian perspective. Other scholars advocate grounding human rights in the inherent dignity and relational nature of the human person. Finally, some authors seek to integrate these perspectives by rooting human rights in a deeper understanding of the "human person." For example, David Walsh[91] emphasizes that the person is fundamentally a relational being, combining in this way the best of the individualistic, societal, and communitarian approaches.
Drawing on this relational perspective and rooted in  Leonardo Polo's anthropology, Juan Carlos Riofrio calls for a reimagining of human rights that emphasizes the co-existential nature of the person, described as freedom, light, and love. Through this lens, he introduces novel rights, such as the rights to hope,[92][93] friendship, dialogue,[94] feast and festivals,[95] arguing that these fundamental aspects of human life have the potential to transform human rights.

In human genetics, a human Y-chromosome DNA haplogroup is a haplogroup defined by specific mutations in the non-recombining portions of DNA on the male-specific Y chromosome (Y-DNA). Individuals within a haplogroup share similar numbers of short tandem repeats (STRs) and single-nucleotide polymorphisms (SNPs).[2] The Y-chromosome accumulates approximately two mutations per generation,[3] and Y-DNA haplogroups represent significant branches of the Y-chromosome phylogenetic tree, each characterized by hundreds or even thousands of unique mutations.
The Y-chromosomal most recent common ancestor (Y-MRCA), often referred to as Y-chromosomal Adam, is the most recent common ancestor from whom all currently living humans are descended patrilineally. Y-chromosomal Adam is estimated to have lived around 236,000 years ago in Africa[citation needed]. By examining other population bottlenecks, most Eurasian men trace their descent from a man who lived in Africa approximately 69,000 years ago (Haplogroup CT). Although Southeast Asia has been proposed as the origin for all non-African human Y chromosomes,[4] this hypothesis is considered unlikely.[5] Other bottlenecks occurred roughly 50,000 and 5,000 years ago, and the majority of Eurasian men are believed to be descended from four ancestors who lived 50,000 years ago, all of whom were descendants of an African lineage (Haplogroup E-M168).[6][7][8]
Y-DNA haplogroups are defined by the presence of a series of Y-DNA single-nucleotide polymorphisms genetic markers. Subclades are defined by a terminal SNP, the SNP furthest down in the Y-chromosome phylogenetic tree.[9][10] The Y Chromosome Consortium (YCC) developed a system of naming major Y-DNA haplogroups with the capital letters A through T, with further subclades named using numbers and lower case letters (YCC longhand nomenclature). YCC shorthand nomenclature names Y-DNA haplogroups and their subclades with the first letter of the major Y-DNA haplogroup followed by a dash and the name of the defining terminal SNP.[11]
Y-DNA haplogroup nomenclature is changing over time to accommodate the increasing number of SNPs being discovered and tested, and the resulting expansion of the Y-chromosome phylogenetic tree. This change in nomenclature has resulted in inconsistent nomenclature being used in different sources.[2] This inconsistency, and increasingly cumbersome longhand nomenclature, has prompted a move toward using the simpler shorthand nomenclature.[12]
Y-chromosomal Adam
Haplogroup A
Haplogroup B
Haplogroup D
Haplogroup E
Haplogroup C
Haplogroup G
Haplogroup H
Haplogroup I
Haplogroup J
Haplogroup L
Haplogroup T
Haplogroup N
Haplogroup O
Haplogroup S
Haplogroup M
Haplogroup Q
Haplogroup R
Haplogroup A is the NRY (non-recombining Y) macrohaplogroup from which all modern paternal haplogroups descend. It is sparsely distributed in Africa, being concentrated among Khoisan populations in the southwest and Nilotic populations toward the northeast in the Nile Valley. BT is a subclade of haplogroup A, more precisely of the A1b clade (A2-T in Cruciani et al. 2011), as follows:
The defining mutations separating CT (all haplogroups except for A and B) are M168 and M294. The site of origin is likely in Africa. Its age has been estimated at approximately 88,000 years old,[14][15] and more recently at around 100,000[16] or 101,000 years old.[17]
The groups descending from haplogroup F are found in some 90% of the world's population, but almost exclusively outside of sub-Saharan Africa.
F xG,H,I,J,K is rare in modern populations and peaks in South Asia, especially Sri Lanka.[13] It also appears to have long been present in South East Asia; it has been reported at rates of 4–5% in Sulawesi and Lembata. One study, which did not comprehensively screen for other subclades of F-M89 (including some subclades of GHIJK), found that Indonesian men with the SNP P14/PF2704 (which is equivalent to M89), comprise 1.8% of men in West Timor, 1.5% of Flores 5.4% of Lembata 2.3% of Sulawesi and 0.2% in Sumatra.[18][19] F* (F xF1,F2,F3) has been reported among 10% of males in Sri Lanka and South India, 5% in Pakistan, as well as lower levels among the Tamang people (Nepal), and in Iran. F1 (P91), F2 (M427) and F3 (M481; previously F5) are all highly rare and virtually exclusive to regions/ethnic minorities in Sri Lanka, India, Nepal, South China, Thailand, Burma, and Vietnam. In such cases, however, the possibility of misidentification is considered to be relatively high and some may belong to misidentified subclades of Haplogroup GHIJK.[20]
Haplogroup G (M201) originated some 48,000 years ago and its most recent common ancestor likely lived 26,000 years ago in the Middle East. It spread to Europe with the Neolithic Revolution.
It is found in many ethnic groups in Eurasia; most common in the Caucasus, Iran, Anatolia and the Levant. Found in almost all European countries, but most common in Gagauzia, southeastern Romania, Greece, Italy, Spain, Portugal, Tyrol, and Bohemia with highest concentrations on some Mediterranean islands; uncommon in Northern Europe.[21][22]
G-M201 is also found in small numbers in northwestern China and India, Bangladesh, Pakistan, Sri Lanka, Malaysia, and North Africa.
Haplogroup H (M69) probably emerged in Southern Central Asia, South Asia or West Asia, about 48,000 years BP, and remains largely prevalent there in the forms of H1 (M69) and H3 (Z5857). Its sub-clades are also found in lower frequencies in Iran, Central Asia, across the middle-east, and the Arabian peninsula.
However, H2 (P96) is present in Europe since the Neolithic and H1a1 (M82) spread westward in the Medieval era with the migration of the Roma people.
Haplogroup I (M170, M258) is found mainly in Europe and the Caucasus.
Haplogroup J (M304, S6, S34, S35) is found mainly in the Middle East, Caucasus and South-East Europe.
Haplogroup K (M9) is spread all over Eurasia, Oceania and among Native Americans.
K(xLT,K2a,K2b) – that is, K*, K2c, K2d or K2e – is found mainly in Melanesia, Aboriginal Australians, India, Polynesia and Island South East Asia.
Haplogroup L (M20) is found in South Asia, Central Asia, South-West Asia, and the Mediterranean.
Haplogroup T (M184, M70, M193, M272) is found at high levels in the Horn of Africa (mainly Cushitic-speaking peoples), parts of South Asia, the Middle East, and the Mediterranean. T-M184 is also found in significant minorities of Sciaccensi, Stilfser, Egyptians, Omanis, Sephardi Jews,[23] Ibizans (Eivissencs), and Toubou. It is also found at low frequencies in other parts of the Mediterranean and South Asia.
The only living males reported to carry the basal paragroup K2* are indigenous Australians. Major studies published in 2014 and 2015 suggest that up to 27% of Aboriginal Australian males carry K2*, while others carry a subclade of K2.
Haplogroup N (M231) is found in northern Eurasia, especially among speakers of the Uralic languages.
Haplogroup N possibly originated in eastern Asia and spread both northward and westward into Siberia, being the most common group found in some Uralic-speaking peoples.
Haplogroup O (M175) is found with its highest frequency in East Asia and Southeast Asia, with lower frequencies in the South Pacific, Central Asia, South Asia, and islands in the Indian Ocean (e.g. Madagascar, the Comoros).
No examples of the basal paragroup K2b1* have been identified. Males carrying subclades of K2b1 are found primarily among Papuan peoples, Micronesian peoples, indigenous Australians, and Polynesians.
Its primary subclades are two major haplogroups:
Haplogroup P (P295) has two primary branches: P1 (P-M45) and the extremely rare P2 (P-B253).[24]
P*, P1* and P2 are found together only on the island of Luzon in the Philippines.[24] In particular, P* and P1* are found at significant rates among members of the Aeta (or Agta) people of Luzon.[25] While, P1* is now more common among living individuals in Eastern Siberia and Central Asia, it is also found at low levels in mainland South East Asia and South Asia. Considered together, these distributions tend to suggest that P* emerged from K2b in South East Asia.[25][26]
P1 is also the parent node of two primary clades:
Haplogroup Q (MEH2, M242, P36) found in Siberia and the Americas
Haplogroup R (M207, M306): found in Europe, West Asia, Central Asia, and South Asia
Q is defined by the SNP M242. It is believed to have arisen in Central Asia approximately 32,000 years ago.[27][28] The subclades of Haplogroup Q with their defining mutation(s), according to the 2008 ISOGG tree[29] are provided below. ss4 bp, rs41352448, is not represented in the ISOGG 2008 tree because it is a value for an STR. This low frequency value has been found as a novel Q lineage (Q5) in Indian populations[30]
The 2008 ISOGG tree
Haplogroup R is defined by the SNP M207. The bulk of Haplogroup R is represented in the descendant subclade R1 (M173), which originated in Siberia. R1 has two descendant subclades: R1a and R1b.
R1a is associated with the proto-Indo-Iranian and Balto-Slavic peoples, and is now found predominantly in Central Asia, South Asia, and Eastern Europe.
Haplogroup R1b is the dominant haplogroup of Western Europe and is also found sparsely distributed among various peoples of Asia and Africa. Its subclade R1b1a2 (M269) is the haplogroup that is most commonly found among modern Western European populations, and has been associated with the Italo-Celtic and Germanic peoples.

Brigitte Pakendorf (born 1970)[1] is a South African linguist and geneticist at the French National Centre for Scientific Research (CNRS).[2]
After receiving her MA in biological anthropology in 1996, Pakendorf studied for two doctorates: one in biology, one in linguistics. She received the former in 2001 from the University of Hamburg and the latter in 2007 from the University of Leiden.[3] This second thesis brought together her interests in dealing with contact in the prehistory of the Sakha, or Yakuts, from both linguistic and genetic perspectives. It received the 2008 prize for the best dissertation in linguistics defended at a Dutch university, awarded jointly by the Dutch Association for General Linguistics (AVT) and Association for Applied Linguistics (Anéla).[3][4]
From 2006 to 2012 Pakendorf was leader of a Max Planck Society junior research group on comparative population linguistics at the Max Planck Institute for Evolutionary Anthropology in Jena. In 2012 she took up a position as senior researcher (DR2) at the French National Centre for Scientific Research (CNRS) in the laboratory Dynamique du langage (Language dynamics). She was promoted to senior researcher first class (DR1) in 2016, and exceptional class (DRCE) in 2019.[2]
Pakendorf has received numerous honours and awards for her research. In 2016 she was awarded a CNRS Silver Medal.[5] In 2019 she was elected member of the Academia Europaea.
Pakendorf's research is interdisciplinary, bridging biology, anthropology and linguistics. She has worked mainly on languages of Siberia, in particular Even, Negidal and other Tungusic languages, but also on languages of the Kalahari Basin such as the Kxʼa languages.[2][3] The topics that she has investigated include prehistory, language contact, linguistic typology, areal linguistics, molecular anthropology, and language documentation.[2] She has received major grants from the German Research Foundation, Endangered Languages Documentation Programme, Leakey Foundation, Volkswagen Foundation and Wenner-Gren Foundation to investigate and document the languages and history of the groups mentioned above.[3][6][7]

In Japanese history, the Jōmon period (縄文時代, Jōmon jidai) is the time between c. 14,000 and 300 BC,[1][2][3] during which Japan was inhabited by a diverse hunter-gatherer and early agriculturalist population united through a common Jōmon culture, which reached a considerable degree of sedentism and cultural complexity. The name "cord-marked" was first applied by the American zoologist and orientalist Edward S. Morse, who discovered sherds of pottery in 1877 and subsequently translated "straw-rope pattern" into Japanese as Jōmon.[4] The pottery style characteristic of the first phases of Jōmon culture was decorated by impressing cords into the surface of wet clay and is generally accepted to be among the oldest in the world.[5]
The Jōmon period was rich in tools and jewelry made from bone, stone, shell and antler; pottery figurines and vessels; and lacquerware.[6][7][8][9] It is often compared to pre-Columbian cultures of the North American Pacific Northwest and especially to the Valdivia culture in Ecuador because in these settings cultural complexity developed within a primarily hunting-gathering context with limited use of horticulture.[10][11][12][13]
The approximately 14,000-year Jōmon period is conventionally divided into several phases, progressively shorter: Incipient (13,750–8,500 BC), Initial (8,500–5,000), Early (5,000–3,520), Middle (3,520–2,470), Late (2,470–1,250), and Final (1,250–500).[14] The fact that this entire period is given the same name by archaeologists should not be taken to mean that there was not considerable regional and temporal diversity; the time between the earliest Jōmon pottery and that of the more well-known Middle Jōmon period is about twice as long as the span separating the building of the Great Pyramid of Giza from the 21st century.
Dating of the Jōmon sub-phases is based primarily upon ceramic typology, and to a lesser extent radiocarbon dating.
Recent findings have refined the final phase of the Jōmon period to 300 BC.[1][2][3] The Yayoi period started between 500 and 300 BC according to radio-carbon evidence, while Yayoi styled pottery was found in a Jōmon site of northern Kyushu already in 800 BC.[15][16][17]
The Japanese archipelago can be divided in 3 regions for whom the chronology of the Jōmon period or its subsequent period are applied differently: Honshu and Kyushu, Okinawa and the Ryukyu Isles, and Hokkaido and Northern Tohōku.[18] In Okinawa and the Ryukyu isles, the Jōmon period does not apply as the Jōmon people were absent from those places. Instead, common chronology for the area uses the Shellmidden Period[19][20][21]  or the Sakishima Prehistoric Period specifically for the island.[21] As for Hokkaido and Northern Tohoku, the Jōmon people were replaced not by the Yayoi people like in most of Japan but by the closer people of the Zoku-Jomon which ushered the Zoku-Jōmon Period unique to the North.[22][23]
The earliest pottery in Japan was made at or before the start of the Incipient Jōmon period. Small fragments, dated to 14,500 BC, were found at the Odai Yamamoto I site in 1998. Pottery of roughly the same age was subsequently found at other sites such as in Kamikuroiwa and the Fukui cave.[24][25][26]
The first Jōmon pottery is characterized by the cord-marking that gives the period its name and has now been found in large numbers of sites.[27] The pottery of the period has been classified by archaeologists into some 70 styles, with many more local varieties of the styles.[4] The antiquity of Jōmon pottery was first identified after World War II, through radiocarbon dating methods.[7][a] The earliest vessels were mostly smallish round-bottomed bowls 10–50 cm (3.9–20 in) high that are assumed to have been used for boiling food and, perhaps, storing it beforehand. They belonged to hunter-gatherers and the size of the vessels may have been limited by a need for portability. As later bowls increase in size, this is taken to be a sign of an increasingly settled pattern of living. These types continued to develop, with increasingly elaborate patterns of decoration, undulating rims, and flat bottoms so that they could stand on a surface.[28]
The manufacture of pottery typically implies some form of sedentary life because pottery is heavy, bulky, and fragile and thus unsuitable to fully nomadic people.[24] It seems that food sources were so abundant in the natural environment of the Japanese islands that they could support fairly large, semi-sedentary populations. The Jōmon people used chipped stone tools, ground stone tools, traps, and bows, and were evidently skillful coastal and deep-water fishers.
Incipient Jōmon (14,000–7500 BC)[29]
Initial Jōmon (7500–4000 BC)[30][31]
Early Jōmon (5000–3520 BC)[32]
Middle Jōmon (3520–2470 BC)[33]
Late Jōmon (2470–1250 BC)
Final Jōmon (1250–500 BC)[36]
The earliest "Incipient Jōmon" phase began while Japan was still linked to continental Asia as a narrow peninsula.[24] As the glaciers melted following the end of the last glacial period (approximately 12,000 BC), sea levels rose, separating the Japanese archipelago from the Asian mainland; the closest point (in Kyushu) about 190 km (120 mi) from the Korean Peninsula is near enough to be intermittently influenced by continental developments, but far enough removed for the peoples of the Japanese islands to develop independently. The main connection between the Japanese archipelago and Mainland Asia was through the Korean Peninsula to Kyushu and Honshu. In addition, Luzon, Taiwan, Ryukyu, and Kyushu constitute a continuous chain of islands, connecting the Jōmon with Southeast Asia, while Honshu, Hokkaido and Sakhalin connected the Jōmon with Siberia.
Within the archipelago, the vegetation was transformed by the end of the Ice Age. In southwestern Honshu, Shikoku, and Kyushu, broadleaf evergreen trees dominated the forests, whereas broadleaf deciduous trees and conifers were common in northeastern Honshu and southern Hokkaido. Many native tree species, such as beeches, buckeyes, chestnuts, and oaks produced edible nuts and acorns. These provided substantial sources of food for both humans and animals.
In the northeast, the plentiful marine life carried south by the Oyashio Current, especially salmon, was another major food source. Settlements along both the Sea of Japan and the Pacific Ocean subsisted on immense amounts of shellfish, leaving distinctive middens (mounds of discarded shells and other refuse) that are now prized sources of information for archaeologists. Other food sources meriting special mention include Sika deer, wild boar (with possible wild-pig management),[38] wild plants such as yam-like tubers, and freshwater fish. Supported by the highly productive deciduous forests and an abundance of seafood, the population was concentrated in Honshu and Kyushu, but Jōmon sites range from Hokkaido to the Ryukyu Islands.[39]
The Early Jōmon period saw an explosion in population, as indicated by the number of larger aggregated villages from this period.[14] This period occurred during the Holocene climatic optimum, when the local climate became warmer and more humid.[40]
The degree to which horticulture or small-scale agriculture was practiced by Jōmon people is debated. Currently, there is no scientific consensus to support a conceptualization of Jōmon period culture as only hunter-gatherer.[38] There is evidence to suggest that arboriculture was practiced in the form of tending groves of lacquer (Toxicodendron verniciflua) and nut (Castanea crenata and Aesculus turbinata) producing trees,[41][42] as well as soybean, bottle gourd, hemp, Perilla, adzuki, among others. These characteristics place them somewhere in between hunting-gathering and agriculture.[38]
An apparently domesticated variety of peach appeared very early at Jōmon sites in 6700–6400 BP (4700–4400 BC).[43] This was already similar to modern cultivated forms. This domesticated type of peach was apparently brought into Japan from China. Although the domestication of wild peaches started in China way before this period, a variety closest to our modern peaches is currently attested in China itself only at a later date of 5300–4300 BP.[43]
Highly ornate pottery dogū figurines and vessels, such as the so-called "flame style" vessels, and lacquered wood objects remain from that time. Although the ornamentation of pottery increased over time, the ceramic fabric always remained quite coarse. During this time Magatama stone beads make a transition from being a common jewelry item found in homes into serving as a grave good.[44] This is a period where there are large burial mounds and monuments.[14]
This period saw a rise in complexity in the design of pit-houses, the most commonly used method of housing at the time,[citation needed] with some even having paved stone floors.[45] A study in 2015 found that this form of dwelling continued up until the Satsumon culture.[46] Using archaeological data on pollen count, this phase is the warmest of all the phases.[47] By the end of this phase the warm climate starts to enter a cooling trend.[14]
After 1500 BC, the climate cooled entering a stage of neoglaciation, and populations seem to have contracted dramatically.[14] Comparatively few archaeological sites can be found after 1500 BC.
The Japanese chestnut, Castanea crenata, becomes essential, not only as a nut bearing tree, but also because it was extremely durable in wet conditions and became the most used timber for building houses during the Late Jōmon phase.[48]
During the Final Jōmon period, a slow shift was taking place in western Japan: steadily increasing contact with the Korean Peninsula eventually led to the establishment of Korean-type settlements in western Kyushu, beginning around 900 BC. The settlers brought with them new technologies such as wet rice farming and bronze and iron metallurgy, as well as new pottery styles similar to those of the Mumun pottery period. The settlements of these new arrivals seem to have coexisted with those of the Jōmon and Yayoi for around a thousand years.
Outside Hokkaido, the Final Jōmon is succeeded by a new farming culture, the Yayoi (c. 300 BC – AD 300), named after an archaeological site near Tokyo.[7]
Within Hokkaido, the Jōmon is succeeded by the Okhotsk culture and Zoku-Jōmon (post-Jōmon) or Epi-Jōmon culture, which later replaced or merged with the Satsumon culture around the 7th century.
At the end of the Jōmon period the local population declined sharply. Scientists suggest that this was possibly caused by food shortages and other environmental problems. They concluded that not all Jōmon groups suffered under these circumstances but the overall population declined.[49] Examining the remains of the people who lived throughout the Jōmon period, there is evidence that these deaths were not inflicted by warfare or violence on a large enough scale to cause these deaths.[50]
The origin myths of Japanese civilization extend back to periods now regarded as part of the Jōmon period, but they show little or no relation to the current archaeological understanding of Jōmon culture. The traditional founding date of the Japanese nation by Emperor Jimmu is February 11, 660 BC. That version of Japanese history, however, comes from the country's first written records, the Kojiki and the Nihon Shoki, dating from the 6th to the 8th centuries, after Japan had adopted Chinese characters (Go-on/Kan-on).[51]
Some elements of modern Japanese culture may date from the period and reflect the influences of a mingled migration from the northern Asian continent and the southern Pacific areas and the local Jōmon peoples. Among those elements are the precursors to Shinto, marriage customs, architectural styles, and technological developments such as lacquerware, laminated bows called "yumi", and metalworking.
The relationship of Jōmon people to the modern Japanese (Yamato people), Ryukyuans, and Ainu is not clear. Morphological studies of dental variation and genetic studies suggest that the Jōmon people were rather diverse, and mitochondrial DNA studies indicate the Jōmon people were closely related to modern-day East Asians.[52][53] The contemporary Japanese people descended from a mixture of the various ancient hunter-gatherer tribes of the Jōmon period and the Yayoi rice-agriculturalists, and these two major ancestral groups came to Japan over different routes at different times.[54][55][56][57][58][59][60]
The modern-day Japanese population carries approximately 30% paternal ancestry from the Jōmon. This is far higher than the maternal Jōmon contribution of around 15%, and autosomal contribution of 10% to the Japanese population. This imbalanced inheritance has been referred to as the "admixture paradox", and is thought to hold clues as to how the admixture between the Jōmon and Yayoi cultures took place.[61] According to Mitsuru Sakitani the Jōmon people are an admixture of several Paleolithic populations. He suggests that Y-chromosome haplogroups C1a1 and D-M55 are two of the Jōmon lineages.[62] The maternal haplogroups M7a, N9b, and G1b have been identified from ancient Jōmon specimens.[61]
According to study "Jōmon culture and the peopling of the Japanese archipelago" by Schmidt and Seguchi (2014), the prehistoric Jōmon people descended from diverse paleolithic populations with multiple migrations into Jōmon-period Japan. They concluded: "In this respect, the biological identity of the Jōmon is heterogeneous, and it may be indicative of diverse peoples who possibly belonged to a common culture, known as the Jōmon".[63]
A study by Lee and Hasegawa of Waseda University concluded that the Jōmon period population of Hokkaido consisted of two distinctive populations which later merged to form the proto-Ainu in northern Hokkaido. The Ainu language can be connected to an "Okhotsk component" which spread southwards. They further concluded that the "dual structure theory" regarding the population history of Japan must be revised and that the Jōmon people had more diversity than originally suggested.[64]
A 2015 study found specific gene alleles, related to facial structure and features among some Ainu individuals, which largely descended from local Hokkaido Jōmon groups. These alleles are typically associated with Europeans but absent from other East Asians (including Japanese people), which suggests geneflow from a currently unidentified source population into the Jōmon period population of Hokkaido. Although these specific alleles can explain the unusual physical appearance of certain Ainu individuals, compared to other Northeast Asians, the exact origin of these alleles remains unknown.[65][66] Matsumura et. al (2019), however, states that these phenotypes were shared by prehistoric south Chinese and Southeast Asian peoples.[67]
Full genome analyses in 2020 and 2021 revealed further information regarding the origin of the Jōmon peoples. The genetic results suggest early admixture between different groups in Japan already during the Paleolithic, followed by constant geneflow from coastal East Asian groups, resulting in a heterogeneous population which then homogenized until the arrival of the Yayoi people. Geneflow from Northeast Asia during the Jōmon period is associated with the C1a1 and C2 lineages, geneflow from the Tibetan Plateau and Southern China is associated with the D1a2a (previously D1b) and D1a1 (previously D1a) lineages. Geneflow from ancient Siberia into the northern Jōmon people of Hokkaido was also detected, with later geneflow from Hokkaido into parts of northern Honshu (Tohoku). The lineages K and F are suggested to have been presented during the early Jōmon period but got replaced by C and D. The analysis of a Jōmon sample (Ikawazu shell-mound, Tahara, Japan) and an ancient sample from the Tibetan Plateau (Chokhopani, China) found only partially shared ancestry, pointing towards a "positive genetic bottleneck" regarding the spread of haplogroup D from ancient "East Asian Highlanders" (related to modern day Tujia people, Yao people, and Tibetans, as well as Tripuri people). The genetic evidence suggests that an East Asian source population, near the Himalayan mountain range, contributed ancestry to the Jōmon period population of Japan, and less to ancient Southeast Asians. The authors concluded that this points to an inland migration through southern or central China towards Japan during the Paleolithic. Another ancestry component seem to have arrived from Siberia into Hokkaido.[68][69][70] Archeological and biological evidence link the southern Jōmon culture of Kyushu, Shikoku and parts of Honshu to cultures of southern China and Northeast India. A common culture, known as the "broadleafed evergreen forest culture", ranged from southwestern Japan through southern China towards Northeast India and southern Tibet, and was characterized by the cultivation of Azuki beans.[71]
Some linguists suggest that the Japonic languages were already present within the Japanese archipelago and coastal Korea, before the Yayoi period, and can be linked to one of the Jōmon populations of southwestern Japan, rather than the later Yayoi or Kofun period rice-agriculturalists. Japonic-speakers then expanded during the Yayoi period, assimilating the newcomers, adopting rice-agriculture, and fusing mainland Asian technologies with local traditions.[72]
Vovin (2021) presented arguments for the presence of Austronesian peoples within the Japanese archipelago during the Jōmon period, based on previous linguistic research and specific Austronesian vocabulary loaned into the core vocabulary of (Insular) Japanese. He suggests that Austronesian-speakers arrived in Japan during the Jōmon period and prior to the arrival of Yayoi period migrants, associated with the spread of Japonic languages. These Austronesian-speakers were subsequently assimilated into the Japanese ethnicity. Evidence for non-Ainuic, non-Austronesian, and non-Korean loanwords are found among Insular Japonic languages, and probably derived from unknown and extinct Jōmon languages.[73]
Modern public perception of Jōmon has gradually changed from primitive and obsolete to captivating:[74]

Prehistoric Egypt and Predynastic Egypt was the period of time starting at the first human settlement and ending at the First Dynasty of Egypt around 3100 BC.
At the end of prehistory, "Predynastic Egypt" is traditionally defined as the period from the final part of the Neolithic period beginning c. 6210 BC to the end of the Naqada III period c. 3000 BC. The dates of the Predynastic period were first defined before widespread archaeological excavation of Egypt took place, and recent finds indicating a very gradual Predynastic development have led to controversy over when exactly the Predynastic period ended. Thus, various terms such as "Protodynastic period", "Zero Dynasty" or "Dynasty 0"[1] are used to name the part of the period which might be characterized as Predynastic by some and Early Dynastic by others.
The Predynastic period is generally divided into cultural eras, each named after the place where a certain type of Egyptian settlement was first discovered. However, the same gradual development that characterizes the Protodynastic period is present throughout the entire Predynastic period, and individual "cultures" must not be interpreted as separate entities but as largely subjective divisions used to facilitate study of the entire period.
The vast majority of Predynastic archaeological finds have been in Upper Egypt, because the silt of the Nile River was more heavily deposited at the Delta region, completely burying most Delta sites long before modern times.[2]
Egypt has been inhabited by humans (including archaic humans) for over a million (and probably over 2 million) years, though the evidence for early occupation of Egypt is sparse and fragmentary. The oldest archaeological finds in Egypt, stone tools belonging to the Oldowan industry, are poorly dated. These tools are succeeded by those belonging to the Acheulean industry.[3] The youngest Achulean sites in Egypt date to around 400,000–300,000 years ago.[4]
During the Late Pleistocene, when Egypt was occupied by modern humans, several archaeological industries are recognised including the Silsilian, Fakhurian, Afian, Kubbaniyan, Idfuan-Shuwikhatian, and the Isnan industries.[5]
Some of the oldest known structures were discovered in Egypt by archaeologist Waldemar Chmielewski along the southern border near Wadi Halfa, Sudan, at the Arkin 8 site. Chmielewski dated the structures to 100,000 BC.[6] The remains of the structures are oval depressions about 30 cm deep and 2 × 1 meters across. Many are lined with flat sandstone slabs which served as tent rings supporting a dome-like shelter of skins or brush. This type of dwelling provided a place to live, but if necessary, could be taken down easily and transported. They were mobile structures—easily disassembled, moved, and reassembled—providing hunter-gatherers with semi-permanent habitation.[6]
Aterian tool-making reached Egypt c. 40,000 BC.[6]
The Khormusan industry in Egypt began between 42,000 and 32,000 BP.[6] Khormusans developed tools not only from stone but also from animal bones and hematite.[6] They also developed small arrow heads resembling those of Native Americans,[6] but no bows have been found.[6] The end of the Khormusan industry came around 16,000 B.C. with the appearance of other cultures in the region, including the Gemaian.[7]
The Late Paleolithic in Egypt started around 30,000 BC.[6] The Nazlet Khater skeleton was found in 1980 and given an age of 33,000 years in 1982, based on nine samples ranging between 35,100 and 30,360 years old.[8] This specimen is the only complete modern human skeleton so far found from the earliest Late Stone Age in Africa.[9]
The Fakhurian late Paleolithic industry in Upper Egypt, showed that a homogenous population existed in the Nile-Valley during the late Pleistocene. Studies of the skeletal material showed they were in the range of variation found in the Wadi Halfa, Jebel Sahaba and fragments from the Kom Ombo populations.[10]
The Halfan and Kubbaniyan, two closely related industries, flourished along the Upper Nile Valley. Halfan sites are found in the far north of Sudan, whereas Kubbaniyan sites are found in Upper Egypt. For the Halfan, only four radiocarbon dates have been produced. Schild and Wendorf (2014) discard the earliest and latest as erratic and conclude that the Halfan existed c. 22.5-22.0 ka cal BP (22,500-22,000 calibrated years before present).[11] People survived on a diet of large herd animals and the Khormusan tradition of fishing. Greater concentrations of artifacts indicate that they were not bound to seasonal wandering, but settled for longer periods.[citation needed] The Halfan culture was derived in turn from the Khormusan,[a][13][page needed] which depended on specialized hunting, fishing, and collecting techniques for survival. The primary material remains of this culture are stone tools, flakes, and a multitude of rock paintings.
The Sebilian culture began around 13,000 BC and vanished around 10,000 BC.[citation needed] In Egypt, analyses of pollen found at archaeological sites indicate that the people of the Sebilian culture (also known as the Esna culture) were gathering grains,[citation needed] though domesticated seeds were not found.[14] It has been hypothesized that the sedentary lifestyle practiced by these grain gatherers led to increased warfare, which was detrimental to sedentary life and brought this period to an end.[14]
The Qadan culture (13,000–9,000 BC) was a Mesolithic industry that, archaeological evidence suggests, originated in Upper Egypt (present-day south Egypt) approximately 15,000 years ago.[15][16] The Qadan subsistence mode is estimated to have persisted for approximately 4,000 years. It was characterized by hunting, as well as a unique approach to food gathering that incorporated the preparation and consumption of wild grasses and grains.[15][16] Systematic efforts were made by the Qadan people to water, care for, and harvest local plant life, but grains were not planted in ordered rows.[17]
Around twenty archaeological sites in Upper Nubia give evidence for the existence of the Qadan culture's grain-grinding culture. Its makers also practiced wild grain harvesting along the Nile during the beginning of the Sahaba Daru Nile phase, when desiccation in the Sahara caused residents of the Libyan oases to retreat into the Nile valley.[14] Among the Qadan culture sites is the Jebel Sahaba cemetery, which has been dated to the Mesolithic.[18]
Qadan peoples were the first to develop sickles and they also developed grinding stones independently to aid in the collecting and processing of these plant foods prior to consumption.[6] However, there are no indications of the use of these tools after 10,000 BC, when hunter-gatherers replaced them.[6]
Early evidence for Neolithic cultures in the Nile Valley are generally located in the north of Egypt, exhibiting well-developed stages of Neolithic subsistence, including the cultivation of crops and sedentism, as well as pottery production from the late 6th Millennium BC onwards.[19]
The natural scientist Frederick Falkenburger in 1947, based on a sample set of around 1,800 prehistoric Egyptian crania, noted great heterogeneity amongst his samples. Falkenburger categorized them based on the nasal index, overall head and face form, taking into account width, eye socket structure, amongst other given indicators. He divided and characterized the skulls into four types: Cro-Magnon type, "Negroid" type, Mediterranean type, and mixed types resulting from the mixture of the aforementioned groups.[20] Similarly, the craniometrics of early Egyptians were according to the physician and anthropologist Eugene Strouhal in 1971, designated as either Cro-Magnon of North Africa, Mediterranean, "Negroid" of East Africa, and intermediate/mixed.[21]
According to professor Fekhri A. Hassan, the peopling of the Egyptian Nile Valley from archaeological and biological data, was the result of a complex interaction between coastal northern Africans, “neolithic” Saharans, Nilotic hunters, and riverine proto-Nubians with some influence and migration from the Levant (Hassan, 1988).[22]
Faiyum B culture, also called Qarunian due to being of the Lake Qarun or Qaroun area is an Epipalaeolithic (also called Mesolithic) culture and predates Faiyum A culture. No pottery has been found, with blade types being both plain and microlithic blades. A set of gouges and arrow-heads suggests it may have had contact with the Sahara (c. 6500 to - 5190 BC).[23][24]
Maciej Henneberg (1989) documented a remote 8,000 year old female skull from the Qarunian. It showed closest affinity to Wadi Halfa, modern Negroes and Australian aborigines, being quite different from Epipalaeolithic materials of Northern Africa usually labelled as Mechta-Afalou (Paleo-Berber) or the later Proto-Mediterranean types (Capsian). The skull still had an intermediate position, being gracile, but possessing large teeth and a heavy set jaw.[25] Similar results would later be found by a short report from SOY Keita in 2021, showing affinities with the Qarunian skull and the Teita series.[26]
Dating to about 5600-4400 BC of the Faiyum Neolithic,[19][27]
continued expansion of the desert forced the early ancestors of the Egyptians to settle around the Nile more permanently, adopting increasingly sedentary lifestyles. The Faiyum A industry is the earliest farming culture in the Nile Valley.[19] 
Archaeological deposits that have been found are characterized by concave base projectile points and pottery. Around 6210 BC, Neolithic settlements appear all over Egypt.[28] Some studies based on morphological,[29] genetic,[30][31][32][33][34] and archaeological data[35][36][37][38][39] have attributed these settlements to migrants from the Fertile Crescent in the Near East returning during the Egyptian and North African Neolithic, bringing agriculture to the region.
Studies in anthropology and post-cranial data has linked the earliest farming populations at Faiyum, Merimde, and El-Badari, to Near Eastern populations.[40][41][42] The archaeological data also suggests that Near Eastern domesticates were incorporated into a pre-existing foraging strategy and only slowly developed into a full-blown lifestyle.[b][44][45] Finally, the names for the Near Eastern domesticates imported into Egypt were not Sumerian or Proto-Semitic loan words.[46][47]
However, some scholars have disputed this view and cited linguistic,[48] physical anthropological,[49] archaeological[50][51][52] and genetic data[53][54][55][56][57] which does not support the hypothesis of a mass migration from the Levant during the prehistoric period. According to historian William Stiebling and archaeologist Susan N. Helft, this view posits that the ancient Egyptians are the same original population group as Nubians and other Saharan populations, with some genetic input from Arabian, Levantine, North African, and Indo-European groups who have known to have settled in Egypt during its long history. On the other hand, Stiebling and Helft acknowledge that the genetic studies of North African populations generally suggest a big influx of Near Eastern populations during the Neolithic Period or earlier. They also added that there have only been a few studies on ancient Egyptian DNA to clarify these issues.[58]
Egyptologist Ian Shaw (2003) wrote that "anthropological studies suggest that the predynastic population included a mixture of racial types (Negroid, Mediterranean and European)", but it is the skeletal material at the beginning of the pharaonic period that has proven to be most controversial. He said according to some scholars there may have been a much slower period of demographic change, than previously hypothesized rapid conquests of people coming into Egypt from the East. It probably involved the gradual infiltration of a different physical type from Syria-Palestine, via the eastern Delta.[59]
Weaving is evidenced for the first time during the Faiyum A Period. People of this period, unlike later Egyptians, buried their dead very close to, and sometimes inside, their settlements.[60]
Although archaeological sites reveal very little about this time, an examination of the many Egyptian words for "city" provides a hypothetical list of causes of Egyptian sedentarism. In Upper Egypt, terminology indicates trade, protection of livestock, high ground for flood refuge, and sacred sites for deities.[62]
From about 5000 to 4200 BC the Merimde culture, so far only known from Merimde Beni Salama, a large settlement site at the edge of the Western Delta, flourished in Lower Egypt. The culture has strong connections to the Faiyum A culture as well as the Levant. People lived in small huts, produced a simple undecorated pottery and had stone tools. Cattle, sheep, goats and pigs were held. Wheat, sorghum and barley were planted. The Merimde people buried their dead within the settlement and produced clay figurines. The first life-sized Egyptian head made of clay comes from Merimde.[63]
The El Omari culture is known from a small settlement near modern Cairo. People seem to have lived in huts, but only postholes and pits survive. The pottery is undecorated. Stone tools include small flakes, axes and sickles. Metal was not yet known.[64] Their sites were occupied from 4000 BC to the Archaic Period (3,100 BC).[65]
The Maadi culture (also called Buto Maadi culture) is the most important Lower Egyptian prehistoric culture dated about 4000–3500 BC,[67] and contemporary with Naqada I and II phases in Upper Egypt. The culture is best known from the site Maadi near Cairo, as well as the site of Buto,[68] but is also attested in many other places in the Delta to the Faiyum region. This culture was marked by development in architecture and technology. It also followed its predecessor cultures when it comes to undecorated ceramics.[69]
Copper was known, and some copper adzes have been found. The pottery is hand-made; it is simple and undecorated. Presence of black-topped red pots indicate contact with the Naqada sites in the south. Many imported vessels from Palestine have also been found. Black basalt stone vessels were also used.[67]
People lived in small huts, partly dug into the ground. The dead were buried in cemeteries, but with few burial goods. The Maadi culture was replaced by the Naqada III culture; whether this happened by conquest or infiltration is still an open question.[70]
The developments in Lower Egypt in the times previous to the unification of the country have been the subject of considerable disputes over the years. The recent excavations at Tell el-Farkha [de], Sais, and Tell el-Iswid have clarified this picture to some extent. As a result, the Chalcolithic Lower Egyptian culture is now emerging as an important subject of study.[71]
The Tasian culture appeared around 4500 BC in Upper Egypt. This culture group is named for the burials found at Der Tasa, on the east bank of the Nile between Asyut and Akhmim. The Tasian culture group is notable for producing the earliest blacktop-ware, a type of red and brown pottery that is colored black on the top portion and interior.[60] This pottery is vital to the dating of Predynastic Egypt. Because all dates for the Predynastic period are tenuous at best, WMF Petrie developed a system called sequence dating by which the relative date, if not the absolute date, of any given Predynastic site can be ascertained by examining its pottery.
As the Predynastic period progressed, the handles on pottery evolved from functional to ornamental. The degree to which any given archaeological site has functional or ornamental pottery can also be used to determine the relative date of the site. Since there is little difference between Tasian ceramics and Badarian pottery, the Tasian Culture overlaps the Badarian range significantly.[73] From the Tasian period onward, it appears that Upper Egypt was influenced strongly by the culture of Lower Egypt.[74] Archaeological evidence has suggested that "the Tasian and Badarian Nile Valley sites were a peripheral network of earlier African cultures of around which Badarian, Saharan, Nubian, and Nilotic peoples regularly circulated."[75] Bruce Williams, Egyptologist, has argued that the Tasian culture was significantly related to the Sudanese-Saharan traditions from the Neolithic era which extended from regions north of Khartoum to locations near Dongola in Sudan.[76]
The Badarian culture, from about 4400 to 4000 BC,[77] is named for the Badari site near Der Tasa. It followed the Tasian culture, but was so similar that many consider them one continuous period. The Badarian Culture continued to produce the kind of pottery called blacktop-ware (albeit much improved in quality) and was assigned Sequence Dating numbers 21–29.[73] The primary difference that prevents scholars from merging the two periods is that Badarian sites use copper in addition to stone and are thus Chalcolithic settlements, while the Neolithic Tasian sites are still considered Stone Age.[73]
Badarian flint tools continued to develop into sharper and more shapely blades, and the first faience was developed.[78] Distinctly Badarian sites have been located from Nekhen to a little north of Abydos.[79] It appears that the Faiyum A culture and the Badarian and Tasian Periods overlapped significantly; however, the Faiyum A culture was considerably less agricultural and was still Neolithic in nature.[78][80] Many biological anthropological studies have shown strong biological affinities between the Badarians and other Northeast African populations.[81][82][83][84][85][86] However, according to Eugene Strouhal and other anthropologists, Predynastic Egyptians like the Badarians were similar to the Capsian culture of North Africa and to Berbers.[87]
In 2005, Keita examined Badarian crania from predynastic upper Egypt in comparison to various European and tropical African crania. He found that the predynastic Badarian series clustered much closer with the tropical African series. Although, no Asian or other North African samples were included in the study as the comparative series were selected based on "Brace et al.'s (1993) comments on the affinities of an upper Egyptian/Nubian epipaleolithic series". Keita further noted that additional analysis and material from Sudan, late dynastic northern Egypt (Gizeh), Somalia, Asia and the Pacific Islands "show the Badarian series to be most similar to a series from the northeast quadrant of Africa and then to other Africans".[88]
Dental trait analysis of Badarian fossils conducted in a thesis study found that they were closely related to both Afroasiatic-speaking populations inhabiting Northeast Africa, as well as the Maghreb. Among the ancient populations, the Badarians were nearest to other ancient Egyptians (Naqada, Hierakonpolis, Abydos and Kharga in Upper Egypt; Hawara in Lower Egypt), and C-Group and Pharaonic era skeletons excavated in Lower Nubia, followed by the A-Group culture bearers of Lower Nubia, the Kerma and Kush populations in Upper Nubia, the Meroitic, X-Group and Christian period inhabitants of Lower Nubia, and the Kellis population in the Dakhla Oasis.[89]: 219–20  Among the recent groups, the Badari markers were morphologically closest to the Shawia and Kabyle Berber populations of Algeria as well as Bedouin groups in Morocco, Libya and Tunisia, followed by other Afroasiatic-speaking populations in the Horn of Africa.[89]: 222–224  The Late Roman era Badarian skeletons from Kellis were also phenotypically distinct from those belonging to other populations in Sub-Saharan Africa.[89]: 231–32
The Naqada culture is an archaeological culture of Chalcolithic Predynastic Egypt (c. 4000–3000 BC), named for the town of Naqada, Qena Governorate. It is divided in three sub-periods: Naqada I, II and III.
Similar to the preceding Badarian culture, studies have found Naqada skeletal remains to have Northeast African affinities.[90][91][92][93][94][95] A study by Dr. Shormaka Keita found that Naqada remains were conforming almost equally to two local types, a southern Egyptian pattern (which shares closest resemblance with Kerma), and a northern Egyptian pattern (most similar to Coastal Maghreb).[96]
In 1996, Lovell and Prowse also reported the presence of individuals buried at Naqada in what they interpreted to be elite, high status tombs, showing them to be an endogamous ruling or elite segment of the local population at Naqada, which is more closely related to populations in northern Nubia (A-Group) than to neighbouring populations in southern Egypt. Specifically, they stated the Naqda samples were "more similar to the Lower Nubian protodynastic sample than they are to the geographically more proximate southern Egyptian samples" in Qena and Badari. However, they found the skeletal samples from the Naqada cemeteries to be significantly different to protodynastic populations in northern Nubia and predynastic Egyptian samples from Badari and Qena, which were also significantly different to northern Nubian populations.[97] Overall, both the elite and nonelite individuals in the Naqada cemeteries were more similar to each other than they were to the samples in northern Nubia or to samples from Badari and Qena in southern Egypt.[98]
In 2023, Christopher Ehret reported that the physical anthropological findings from the "major burial sites of those founding locales of ancient Egypt in the fourth millennium BCE, notably El-Badari as well as Naqada, show no demographic indebtedness to the Levant". Ehret specified that these studies revealed cranial and dental affinities with "closest parallels" to other longtime populations in the surrounding areas of northeastern Africa "such as Nubia and the northern Horn of Africa". He further commented that "members of this population did not come from somewhere else but were descendants of the long-term inhabitants of these portions of Africa going back many millennia". Ehret also cited existing, archaeological, linguistic and genetic data which he argued supported the demographic history.[99]
The Amratian culture lasted from about 4000 to 3500 BC.[77] It is named after the site of El-Amra, about 120 km south of Badari. El-Amra is the first site where this culture group was found unmingled with the later Gerzean culture group, but this period is better attested at the Naqada site, so it also is referred to as the Naqada I culture.[78] Black-topped ware continues to appear, but white cross-line ware, a type of pottery which has been decorated with close parallel white lines being crossed by another set of close parallel white lines, is also found at this time. The Amratian period falls between S.D. 30 and 39 in Petrie's Sequence Dating system.[100]
Newly excavated objects attest to increased trade between Upper and Lower Egypt at this time. A stone vase from the north was found at el-Amra, and copper, which is not mined in Egypt, was imported from the Sinai, or possibly Nubia. Obsidian[101] and a small amount of gold[100] were both definitely imported from Nubia. Trade with the oases also was likely.[101]
New innovations appeared in Amratian settlements as precursors to later cultural periods. For example, the mud-brick buildings for which the Gerzean period is known were first seen in Amratian times, but only in small numbers.[102] Additionally, oval and theriomorphic cosmetic palettes appear in this period, but the workmanship is very rudimentary and the relief artwork for which they were later known is not yet present.[103][104][full citation needed]
The Gerzean culture, from about 3500 to 3200 BC,[77] is named after the site of Gerzeh. It was the next stage in Egyptian cultural development, and it was during this time that the foundation of Dynastic Egypt was laid. Gerzean culture is largely an unbroken development out of Amratian Culture, starting in the delta and moving south through upper Egypt, but failing to dislodge Amratian culture in Nubia.[105] Gerzean pottery is assigned values from S.D. 40 through 62, and is distinctly different from Amratian white cross-lined wares or black-topped ware.[100] Gerzean pottery was painted mostly in dark red with pictures of animals, people, and ships, as well as geometric symbols that appear derived from animals.[105] Also, "wavy" handles, rare before this period (though occasionally found as early as S.D. 35) became more common and more elaborate until they were almost completely ornamental.[100]
Gerzean culture coincided with a significant decline in rainfall,[106] and farming along the Nile now produced the vast majority of food,[105] though contemporary paintings indicate that hunting was not entirely forgone. With increased food supplies, Egyptians adopted a much more sedentary lifestyle and cities grew as large as 5,000.[105]
It was in this time that Egyptian city dwellers stopped building with reeds and began mass-producing mud bricks, first found in the Amratian Period, to build their cities.[105]
Egyptian stone tools, while still in use, moved from bifacial construction to ripple-flaked construction. Copper was used for all kinds of tools,[105] and the first copper weaponry appears here.[79] Silver, gold, lapis, and faience were used ornamentally,[105] and the grinding palettes used for eye-paint since the Badarian period began to be adorned with relief carvings.[79]
The first tombs in classic Egyptian style were also built, modeled after ordinary houses and sometimes composed of multiple rooms.[101] Although further excavations in the Delta are needed, this style is generally believed to originate there and not in Upper Egypt.[101]
Although the Gerzean Culture is now clearly identified as being the continuation of the Amratian period, significant Mesopotamian influence worked its way into Egypt during the Gerzean, interpreted in previous years as evidence of a Mesopotamian ruling class, the so-called dynastic race, coming to power over Upper Egypt. This idea no longer attracts academic support.
Distinctly foreign objects and art forms entered Egypt during this period, indicating contacts with several parts of Asia. Objects such as the Gebel el-Arak knife handle, which has patently Mesopotamian relief carvings on it, have been found in Egypt,[109] and the silver which appears in this period can only have been obtained from Asia Minor.[105]
In addition, Egyptian objects are created which clearly mimic Mesopotamian forms, although not slavishly.[111] Cylinder seals appear in Egypt, as well as recessed paneling architecture, the Egyptian reliefs on cosmetic palettes are clearly made in the same style as the contemporary Mesopotamian Uruk culture, and the ceremonial mace heads which turn up from the late Gerzean and early Semainean are crafted in the Mesopotamian "pear-shaped" style, instead of the Egyptian native style.[106]
The route of this trade is difficult to determine, but contact with Canaan does not predate the early dynastic, so it is usually assumed to have been conducted over water.[112] During the time when the Dynastic Race Theory was still popular, it was theorized that Uruk sailors circumnavigated Arabia, but a Mediterranean route, probably by middlemen through Byblos, is more likely, as evidenced by the presence of Byblian objects in Egypt.[112]
The fact that so many Gerzean sites are at the mouths of wadis that lead to the Red Sea may indicate some amount of trade via the Red Sea (though Byblian trade potentially could have crossed the Sinai and then taken the Red Sea).[113] Also, it is considered unlikely that something so complicated as recessed panel architecture could have worked its way into Egypt by proxy, and at least a small contingent of migrants is often suspected.[112]
Despite this evidence of foreign influence, Egyptologists generally agree that the Gerzean Culture is still predominantly indigenous to Egypt.
The Naqada III period, from about 3200 to 3000 BC,[77] is generally taken to be identical with the Protodynastic period, during which Egypt was unified.
Naqada III is notable for being the first era with hieroglyphs (though this is disputed by some), the first regular use of serekhs, the first irrigation, and the first appearance of royal cemeteries.[114]
The relatively affluent Maadi suburb of Cairo is built over the original Naqada stronghold.[115]
Bioarchaeologist Nancy Lovell had stated that there is a sufficient body of morphological evidence to indicate that ancient southern Egyptians had physical characteristics "within the range of variation" of both ancient and modern indigenous peoples in the Sahara and tropical Africa. She summarised that "In general, the inhabitants of Upper Egypt and Nubia had the greatest biological affinity to people of the Sahara and more 
southerly areas",[116] but exhibited local variation in an African context.[117]
Lower Nubia is located within the borders of modern-day Egypt but is south of the border of Ancient Egypt, which was located at the first cataract of the Nile.
Nabta Playa was once a large internally drained basin in the Nubian Desert, located approximately 800 kilometers south of modern-day Cairo[118] or about 100 kilometers west of Abu Simbel in southern Egypt,[119] 22.51° north, 30.73° east.[120] Today the region is characterized by numerous archaeological sites.[119] The Nabta Playa archaeological site, one of the earliest of the Egyptian Neolithic Period, is dated to circa 7500 BC.[121][122] Also, excavations from Nabta Playa, located about 100 km west of Abu Simbel for example, suggest that the Neolithic inhabitants of the region included migrants from both Sub-Saharan Africa and the Mediterranean area.[123][124] According to Christopher Ehret, the material cultural indicators correspond with the conclusion that the inhabitants of the wider Nabta Playa region were a Nilo-Saharan-speaking population.[125]

Migration studies is the academic study of human migration. Migration studies is an interdisciplinary field which draws on anthropology, prehistory, history, economics, law, sociology and postcolonial studies.
Migration studies did not develop along a uni-linear path and it has developed with significantly different trajectories in different academic cultures and traditions. Migration studies does not exist as a self-contained discipline and instead finds its heritage in a variety of places. Developments in the sociology of migration, the study of the history of human migration, theories and policies concerning labour migration, and postcolonial studies all fed into the growth of migration studies. The development of migration studies is also bound up with the growth in interdisciplinary pursuits which has resulted from the popularisation of postmodern thought in the past thirty years. In recent years, scholarship which takes interest in humanitarian issues has become increasingly popular. In part, this reflects displacement and refugee movements which have resulted from conflicts throughout the end of the 20th century and at the turn of the 21st century.
Archaeological studies frequently focus on early human migration flows, the spread of civilisation and the development of trade routes and settlements by early humans. The debate over migrationism and diffusionism features prominently in archaeological approaches to migration studies. The study of empire, colonisation, and diaspora constitute significant themes in historical approaches to migration studies. This has, for instance, manifested in studies of the forced migrations during the 1947 Partition of India,[1]< the internal displacement of the 1861-1865 American Civil War, or the Great Migration of 6 million African Americans from the rural southern states to the urban Northeast, Midwest, and West.[2][3] Scholars can research migration histories through a variety of methods including quantitative approaches based on censuses and government documents, social histories, the examination of material culture, or through autobiography.
One branch of research in migration studies involves the consideration of how migration, settlement, and diaspora interact with literature and the arts. For example, in a 2017 paper Dr Michelle Keown discussed how US military imperialism and Marshallese migration affected the poetry of Kathy Jetnil-Kijiner.[4] Researchers have also examined migration in relation to the circulation of music, particularly of folk-songs.[5] Migration is a recurring theme in much popular media, such as in Chimamanda Ngozi Adichie's 2013 novel Americanah or in contemporary film such as Roma (2018), consequently, discussions on migration and the arts are some of the more publicly-visible avenues of scholarship in migration studies. More recently, attention has been paid to how theatre becomes a site for migrant performing their agency within public spaces.[6] In 2020 article, Kasia Lech studied responses to Brexit by multilingual UK-based migrant theatre practitioners, Situating their work in "the paradox of simultaneous hyper- and in-visibility of immigrants in the UK" and argued that "the migrant perspective is crucial for the debate on Brexit as part of the broader European Union’s crisis of commonality and solidarity.".[7]
Scholars of migration in the context of urbanism consider the dynamics of how cities and migrant populations interact. This can include issues of town planning, issues of ghettoisation and social exclusion, and processes of integration and community-building. Urbanists may also consider how one can regard refugee camps as global cities, and how to plan, develop, and operate these camp spaces.
The economic results of migration are a popular area of research and stimulate much consequent debate. Perhaps[original research?] the most publicly-discussed topic within the economics of migration is transnational labour migration and how migrants are either encouraged or discouraged to move as a result of economic considerations, this remains a controversial and multifaceted topic. Migration is researched in relation to its impacts on both sending and destination communities. The study of how migrant workers send remittances is another frequent topic for scholars studying the economics of migration.[8] Some research has focused on novel topics including the internal economies of refugee camps,[9] the economics of human trafficking, and how employment law affects undocumented workers.[10][11]
Studies of migration demography take a statistical approach to the size, structure, and distribution of migrant populations. One can research migrant communities either in isolation or as part of a broader population. Demographic studies of migration often consider issues such as migrant health, welfare, employment, and education in relation to the non-migrant population of a given society.
Migration scholars investigate migrant reception through surveying and studying how host populations understand and respond to immigration. This might include feelings concerning refugee reception and support, considerations of multiculturalism and integration, and attitudes towards government policies. Frequently, scholars investigate how non-migrant publics consider migrants, this has resulted in a relative lack of scholarship which considers the opposite dynamic.[citation needed]
Critical border studies (CBS) explores alternatives to how territorial borders are currently imagined and operated. Part of this approach means identifying and investigating how borders function, to whose benefit these borders function, and whom such borders affect negatively.[12] Scholars[which?] associated with critical border studies regard borders as part of a system of performances by which states maintain and exert power over territory; this intersects with understandings of sovereignty and securitization. Scholars associated with this branch of migration studies are frequently critical of how states may rely on a monopoly of force to assert governance over given territories, consequently strains of anarchist philosophy, postcolonial thought, and anti-statism are popular within critical border studies.[13]
Immigration law necessarily affects many subsections of migration studies and it is consequently a point of interest for a wide range of migration scholars. One can approach the subject of immigration law through sub-national, national, and international frameworks. The study of immigration law frequently intersects with discussions of human rights.
In common with approaches found in critical border studies, many scholars consider how systems which govern migration construct illegality and thereby criminalize migrant populations.[14][15] This approach displaces the discussion of illegality from the migrant subject and instead scrutinises the behaviour of nation-state governments.[16] Critical approaches to the construction of citizenship recur within this strand of scholarship.[17] This approach to understanding migration is particularly relevant for scholars working on issues involving undocumented populations. The term "crimmigration" has emerged as a way to conceptualize how migrants are frequently treated as criminals, deviants, and security risks.[18]
Forced migration is the coerced movement of humans from their origin to a (frequently undesired) new destination. Studies concerning forced migration explore the processes by which people are displaced, how destination countries receive and support displaced people, and the experiences of forced migrations. Studies concerning forced migration frequently overlap with issues concerning genocide studies, settler colonialism, humanitarianism, deportation and ethnic cleansing. The term "forced" is frequently debated for its suggestion that there is a clear distinction between voluntary and involuntary human movement.
Scholars focusing on refugee studies typically consider the experiences of people affected by transnational forced-migration processes. The definition of a refugee varies considerably within the refugee-studies community, with some insisting on the strict definitions of 1951 Convention Relating to the Status of Refugees and others relying on more fluid or amorphous definitions.
Refugee studies represent a distinct overarching group within migration studies as it differs significantly from the topics which are central to more voluntary phenomena such as labour-market migration. The concept of refugees as weapons is analyzed as a forced experience of a mass exodus of refugees from a state to a hostile state as a "weapon."
Internally displaced persons have been compelled to move from their origin but have not crossed national boundaries. This means that they do not meet the Convention Relating to the Status of Refugees definition of a refugee, even though they may have similar experiences, and though the conditions which led to their displacement may closely resemble those which provoke refugee movements. Studies on internal displacement frequently focus on the complications raised by the fact that such migrants are not supported by the same international frameworks which can provide for refugees and other transnational migrants.
Scholars increasingly study how climate change is interacting with human migration,[citation needed] something which is discussed to a much greater extent in relation to migrating animals. Key issues in studies of climate change and migration revolve around how climate change will affect coastal communities, small-island nations, and communities living in areas likely to become deserts.[19] Essentially, this topic considers how climate change may cause large-scale human movement. The idea of "climate refugees" is a key focus of debate within this topic, particularly as this brings a novel category to established understandings of refugee status.[20]
Beyond climate change, there is a long history of human migration because of other environmental factors. Studies concerned with early human migrations frequently consider how humanity responded to issues such as adapting to the harsh cold of the Late Pleistocene.
Scholars focusing on gender consider how gender structures migrant experience, the treatment and reception of migrants, and how migration interacts with the performance of gender. Contemporary treatments of migration and gender tend to take an intersectional approach where gender is part of a dynamic set of identities including class, race, age, and health.[21] Studies which consider gendered experiences of migration look at topics such as gender-based violence against migrant populations, gendered differences in asylum and detention processes, and how family dynamics are affected by migration processes.
Scholars influenced by approaches from the growing field of Queer studies explore how queer sexualities affect understandings of migration.
One approach involves critically engaging with "the intersectionality of nationality and sexuality, [to show] how national norms and values can be used instrumentally by social and political actors" to affect human movement.[22] For example, topics in Queer migration might include how LGBT+ asylum seekers are differently affected by asylum processes, how discrimination against LGBT+ people affects their migration experiences, or how migrant welfare is stratified according to sexuality.
A recent trend in migration studies scholarship has been to critically evaluate how humanitarian actors interact with immigrants, particularly in the context of conflict environments, disaster relief, and crises. The refugee camp has become a significant point of interest for scholars working on the intersection of migration and humanitarianism, especially in relation to biopower.[23] Recently, there have been increased efforts to critically engage with how humanitarian actors deliver aid and the ethics of humanitarianism in the context of migration.[24] This criticism of humanitarian actors has led to discussions of the links between carceral systems and humanitarianism.
In epidemiology, an "immigration study" is a method of understanding the relative importance of inherited genetics and environmental factor in medical conditions whose incidence varies around the world. It examines the incidence of conditions in populations who have moved (or whose recent ancestors have moved) between places at different rates. Often the immigrant population can be shown to have similar rates to the population of the new location, suggesting that environmental factors such as diet, obesity and exercise are the dominant determinants.[25]
Research on human migration may be used to enforce or inform the strategies of national governments and law enforcement bodies. For instance, studying how undocumented immigrants access healthcare may enable a government to clamp down on these practices, or critical studies of clandestine migration may inform border securitisation policies which might restrict those same migration flows. Furthermore, migrants frequently represent vulnerable or marginalised subjects and scholarly research into migrant groups might compound or worsen conditions for these groups. The ethics of how to navigate the tensions and questions raised by carrying out research on migration, and the consequences of investigating sensitive topics within migration studies, are becoming an increasingly discussed topic.[26] This is part of a broader development of greater reflexivity regarding research ethics in the social sciences.
Migration studies is a relatively new specialism, consequently many universities and colleges have yet to develop degree programmes which formally address the topic.[27] Whilst migration studies rarely exist as an available major for undergraduate study, Master's degrees which focus on migration and international movement are increasingly available. However, the availability of this focus varies greatly by region and academic culture. At present, most teaching of migration studies as a distinct topic is focused in European universities.[28]
In Europe, the University of Copenhagen, Aalborg University, Malmö University, Utrecht University, Radboud University Nijmegen, Université de Liège, University of Côte d'Azur Linköping University, University of Neuchâtel, Universität Osnabrück, University of A Coruña, The University of Sussex, University of Riga, University of Deusto, The University of Oxford, the University of Kent (Brussels School of International Studies) and Vrije Universiteit Brussel[29] (Brussels school of Governance) all offer graduate training in migration studies.[30][31][32][33][34][35][36][37][38][39] As of 2017, several of these universities work collaboratively to provide a single degree which can be acquired through work at multiple participating institutions.[40][41] This represents teaching in Denmark, Sweden, Norway, Germany, the United Kingdom, the Netherlands, France, Latvia, and Spain.
Few African universities offer an explicit focus on migration studies, however programmes are available in South Africa, Egypt and Ghana. The University of the Witwatersrand offers degree programmes in migration studies at both undergraduate and graduate levels.[42] The American University in Cairo currently offers a Master's degree in migration and refugee studies.[43] University of Ghana runs a Master's degree and PhD programme in migration studies through their Centre for Migration Studies.[44]
Universities and colleges in the United States have been slow to develop degree programmes which explicitly focus on migration studies and have lagged behind European institutions in this regard.[45] Whilst there has been a growth in migration studies within disciplines, there has not been the same attention to interdisciplinary approaches or the establishment of migration studies as a freestanding field.[46] PhD programmes which focus on migration studies are very rare, however, The University of San Francisco, DePaul University, and City University of New York offer terminal Master's degrees in migration studies. Migration studies are increasingly available as an undergraduate minor-subject, with the University of California San Diego among the first to offer such a minor.[47] Since 2015,[48] the University of California Los Angeles also offers a minor in International Migration Studies.[49] Much teaching on international migration in the United States is instead framed as ethnic studies, Latin American studies. or Border Studies. Migration is also frequently available as a specialisation within sociology, economics, and political science degree programmes. Diasporic identity and the history and consequences of human movement are also extensively explored in African American studies or Africana studies programmes.
In Canada, Ryerson University (now Toronto Metropolitan University) has developed a graduate programme in Immigration and Settlement studies whereas Carleton University offers an undergraduate degree in Migration and Diaspora Studies.[50][51] Similarly to the situation in the United States, Canadian universities frequently address the study of migration as a topic within other disciplines rather than as a field in its own right. Thus, whilst there may be few programmes which explicitly address migration in their titles, there is still a broad range on research and teaching on the topic through other avenues.
Few universities in South and Central America offer named programmes in migration studies. In Mexico, Universidad Iberoamericana co-delivers a Master's programme with the University of San Francisco through an optional exchange semester.[52]
Tel Aviv University offers a terminal master's degree in Global Migration & Policy.[53] Central University of Gujarat offers a Master's and PhD in Diaspora studies.[54] In the Philippines, Miriam College offers an MA in migration studies.[55] In Nepal, migration is included in the graduate programmes in population studies at Tribhuvan University. Similarly, in Sri Lanka at the University of Colombo the graduate programs in demography give significant attention to the study of migration.[56] National University of Singapore has a migration studies research cluster located in its Faculty of Arts and Social Sciences, however, does not offer a degree programme on this topic.[57] Similarly, the Chinese University of Hong Kong hosts a research centre on migration and mobility but does not operate a degree programme.[58] In Russia, no university offers a degree in migration studies, however, the Higher School of Economics organises a regular seminar on migration studies through its Institute for Social Policy.[59]
The University of Melbourne offers graduate training in migration studies through an interdisciplinary PhD programme.[60] The Australian Catholic University has developed a graduate diploma in Australian Migration Law and Practice.[61] The University of Sydney and University of Queensland do not offer degree programmes in migration studies, however both universities have dedicated research clusters and teach modules on topics within migration studies.[62][63]
Victoria University Wellington offers a Master's programme in Migration Studies, alongside a postgraduate diploma, or postgraduate certificate.[64] The Centre for Global Migrations at University of Otago addresses issues and themes in migration studies, and The University of Waikato used to host a Migration Research Group from 1993 to 2009, neither university offers a degree in migration studies.[65][66]
As a rapidly growing field of study, there are numerous journals dedicated to migration studies. The following titles explicitly focus on migration studies, most are peer-reviewed.[67]
The following think tanks address issues which overlap with migration studies.

Big History is an academic discipline that examines history from the Big Bang to the present. Big History resists specialization and searches for universal patterns or trends. It examines long time frames using a multidisciplinary approach based on combining numerous disciplines from science and the humanities.[1][2][3][4] It explores human existence in the context of this bigger picture.[5] It integrates studies of the cosmos, Earth, life, and humanity using empirical evidence to explore cause-and-effect relations.[6][7] It is taught at universities[8] as well as primary and secondary schools[9][10] often using web-based interactive presentations.[11][12][13]
Historian David Christian has been credited with coining the term "Big History" while teaching one of the first such courses at Macquarie University.[6][8][14] An all-encompassing study of humanity's relationship to cosmology[15] and natural history[16] has been pursued by scholars since the Renaissance, and the new field, Big History, continues such work.
Big History examines the past using numerous time scales, from the Big Bang to modernity,[3] unlike conventional history courses which typically begin with the introduction of farming and civilization,[17] or with the beginning of written records. It explores common themes and patterns.[11] Courses generally do not focus on humans until one-third to halfway through,[6] and, unlike conventional history courses, there is not much focus on kingdoms or civilizations or wars or national borders.[6] If conventional history focuses on human civilization with humankind at the center, Big History focuses on the universe and shows how humankind fits within this framework[18] and places human history in the wider context of the universe's history.[19][20]
Unlike conventional history, Big History tends to go rapidly through detailed historical eras such as the Renaissance or Ancient Egypt.[21] It draws on the latest findings from biology,[3] astronomy,[3] geoscience,[3] chemistry, physics, archaeology, anthropology, psychology, sociology, economics,[3] prehistory, ancient history, and natural history, as well as standard history.[22] One teacher explained:
We're taking the best evidence from physics and the best evidence from chemistry and biology, and we're weaving it together into a story ... They're not going to learn how to balance [chemical] equations, but they're going to learn how the chemical elements came out of the death of stars, and that's really interesting.[11]
Big History arose from a desire to go beyond the specialized and self-contained fields that emerged in the 20th century. It tries to grasp history as a whole, looking for common themes across multiple time scales in history.[23][24] Conventional history typically begins with the invention of writing, and is limited to past events relating directly to the human race. Big Historians point out that this limits study to the past 5,000 years and neglects the much longer time when humans existed on Earth. Henry Kannberg sees Big History as being a product of the Information Age, a stage in history itself following speech, writing, and printing.[25] Big History covers the formation of the universe, stars, and galaxies, and includes the beginning of life as well as the period of several hundred thousand years when humans were hunter-gatherers. It sees the transition to civilization as a gradual one, with many causes and effects, rather than an abrupt transformation from uncivilized static cavemen to dynamic civilized farmers.[26] An account in The Boston Globe describes what it polemically asserts to be the conventional "history" view:
Early humans were slump-shouldered, slope-browed, hairy brutes. They hunkered over campfires and ate scorched meat. Sometimes they carried spears. Once in a while they scratched pictures of antelopes on the walls of their caves. That's what I learned during elementary school, anyway. History didn't start with the first humans—they were cavemen! The Stone Age wasn't history; the Stone Age was a preamble to history, a dystopian era of stasis before the happy onset of civilization, and the arrival of nifty developments like chariot wheels, gunpowder, and Google. History started with agriculture, nation-states, and written documents. History began in Mesopotamia's Fertile Crescent, somewhere around 4000 BC. It began when we finally overcame our savage legacy, and culture surpassed biology.[26]
Big History, in contrast to conventional history, has more of an interdisciplinary basis.[11] Advocates sometimes view conventional history as "microhistory" or "shallow history", and note that three-quarters of historians specialize in understanding the last 250 years while ignoring the "long march of human existence."[2] However, one historian disputed that the discipline of history has overlooked the big view, and described the "grand narrative" of Big History as a "cliché that gets thrown around a lot."[2] One account suggested that conventional history had the "sense of grinding the nuts into an ever finer powder."[22] It emphasizes long-term trends and processes rather than history-making individuals or events.[2] Historian Dipesh Chakrabarty of the University of Chicago suggested that Big History was less politicized than contemporary history because it enables people to "take a step back."[2] It uses more kinds of evidence than the standard historical written records, such as fossils, tools, household items, pictures, structures, ecological changes and genetic variations.[2]
Critics of Big History, including sociologist Frank Furedi, have deemed the discipline an "anti-humanist turn of history."[27] The Big History narrative has also been challenged for failing to engage with the methodology of the conventional history discipline. According to historian and educator Sam Wineburg of Stanford University, Big History eschews the interpretation of texts in favor of a purely scientific approach, thus becoming "less history and more of a kind of evolutionary biology or quantum physics."[28]
Another criticism of Big History made by associate professor Ian Hesketh, is that it mixes up science disciplines using holistic views that are very close to mythic or religious approaches, without mentioning this in its narrative.
Currently, the Big History is a consolidated academic field that is giving rise to new views and epistemological approaches, especially in Latin America and the Caribbean, whose decolonial vision of history, economics and Science has opened new questions. In this sense, the transdisciplinary and biomimetics research of Javier Collado [29] represents an ecology of knowledge between scientific knowledge and the ancestral wisdom of native peoples, such as Indigenous peoples in Ecuador.In approaching the Big History from the complexity sciences, the transdisciplinary methodology seeks to understand the interconnections of the human race with the different levels of reality that co-exist in nature and in the cosmos,[30]
Big History makes comparisons based on different time scales and notes similarities and differences between the human, geological, and cosmological scales. David Christian believes such "radical shifts in perspective" will yield "new insights into familiar historical problems, from the nature/nurture debate to environmental history to the fundamental nature of change itself."[22] It shows how human existence has been changed by both human-made and natural factors: for example, according to natural processes which happened more than four billion years ago, iron emerged from the remains of an exploding star and, as a result, humans could use this hard metal to forge weapons for hunting and war.[6] The discipline addresses such questions as "How did we get here?," "How do we decide what to believe?," "How did Earth form?," and "What is life?"[3] According to Fred Spier it offers a "grand tour of all the major scientific paradigms" and helps students to become scientifically literate quickly.[19] One interesting perspective that arises from Big History is that despite the vast temporal and spatial scales of the history of the Universe, it is actually very small pockets of the cosmos where most of the "history" is happening, due to the nature of complexity.[31]
Cosmic evolution, the scientific study of universal change, is closely related to Big History (as are the allied subjects of the epic of evolution and astrobiology); some researchers regard cosmic evolution as broader than Big History, since the latter mainly  examines the specific historical trek from Big Bang → Milky Way → Sun → Earth → humanity. Cosmic evolution, while fully addressing all complex systems (and not merely those that led to humans) has been taught and researched for decades, mostly by astronomers and astrophysicists. This Big-Bang-to-humankind scenario well preceded the subject that some historians began calling Big History in the 1990s. Cosmic evolution is an intellectual framework that offers a grand synthesis of the many varied changes in the assembly and composition of radiation, matter, and life throughout the history of the universe. While engaging in issues of the origins of humanity, this interdisciplinary subject attempts to unify the sciences within the entirety of natural history—a single, inclusive scientific narrative of the origin and evolution of all material things over ~14 billion years, from the origin of the universe to the present day on Earth.
The roots of the idea of cosmic evolution extend back millennia. Ancient Greek philosophers in the fifth century BCE, most notably Heraclitus, are celebrated for their reasoned claims that all things change. Early modern speculation about cosmic evolution began more than a century ago, including the broad insights of Robert Chambers, Herbert Spencer, Charles Sanders Peirce, and Lawrence Henderson. Only in the mid-20th century was the cosmic-evolutionary scenario articulated as a research paradigm to include empirical studies of galaxies, stars, planets, and life—in short, an expansive agenda that combines physical, biological, and cultural evolution. Harlow Shapley widely articulated the idea of cosmic evolution (often calling it "cosmography") in public venues at mid-century,[32] and NASA embraced it in the late 20th century as part of its more limited astrobiology program. Carl Sagan,[33] Eric Chaisson,[34] Hubert Reeves,[35] Erich Jantsch,[36] and Preston Cloud,[37] among others, extensively championed cosmic evolution at roughly the same time around 1980. This extremely broad subject now continues to be formulated as both a technical research program and a scientific worldview for the 21st century.[38][39][40]
One popular collection of scholarly materials on cosmic evolution is based on teaching and research that has been underway at Harvard University since the mid-1970s.[41]
Cosmic evolution is a quantitative subject, whereas big history typically is not; this is because cosmic evolution is practiced mostly by natural scientists, while big history by social scholars. These two subjects, closely allied and overlapping, benefit from each other; cosmic evolutionists tend to treat universal history linearly, thus humankind enters their story only at the most very recent times, whereas big historians tend to stress humanity and its many cultural achievements, granting human beings a larger part of their story. People can compare and contrast these different emphases by watching two short movies portraying the Big-Bang-to-humankind narrative, one animating time linearly, and the other  capturing time (actually look-back time) logarithmically; in the former, humans enter this 14-minute movie in the last second, while in the latter we appear much earlier—yet both are correct.[42]
These different treatments of time over ~14 billion years, each with different emphases on historical content, are further clarified by noting that some cosmic evolutionists divide the whole narrative into three phases and seven epochs:
This contrasts with the approach used by some big historians who divide the narrative into many more thresholds, as noted in the discussion at the end of this section below. Yet another telling of the Big-Bang-to-humankind story is one that emphasizes the earlier universe, particularly the growth of particles, galaxies, and large-scale cosmic structure, such as in physical cosmology.
Notable among quantitative efforts to describe cosmic evolution are Eric Chaisson's research efforts to describe the concept of energy flow through open, thermodynamic systems, including galaxies, stars, planets, life, and society.[38][43][44] The observed increase of energy rate density (energy/time/mass) among a whole host of complex systems is one useful way to explain the rise of complexity in an expanding universe that still obeys the cherished second law of thermodynamics and thus continues to accumulate net entropy. As such, ordered material systems—from buzzing bees and redwood trees to shining stars and thinking beings—are viewed as temporary, local islands of order in a vast, global sea of disorder. A recent review article, which is especially directed toward big historians, summarizes much of this empirical effort over the past decade.[45]
One striking finding of such complexity studies is the apparently ranked order among all known material systems in the universe. Although the absolute energy in astronomical systems greatly exceeds that of humans, and although the mass densities of stars, planets, bodies, and brains are all comparable, the energy rate density for humans and modern human society are approximately a million times greater than for stars and galaxies. For example, while the Sun's luminosity is extremely high (4×1026 W), its mass is also extremely high (2×1030 kg), resulting in a low radiant energy density (2×10−4 W/kg). Compared to stars, more energy flows through each gram of a plant's leaf during photosynthesis, and much more (nearly a million times) rushes through each gram of a human brain while thinking (~20 W per 1350 g).[46]
Cosmic evolution is more than a subjective listing of subsequent events or phenomena. This inclusive scientific worldview constitutes an objective, quantitative approach toward deciphering much of what comprises organized, material nature. Its uniform, consistent philosophy of approach toward all complex systems demonstrates that the basic differences, both within and among many varied systems, are of degree, not of kind. And, in particular, it suggests that optimal ranges of energy rate density grant opportunities for the evolution of complexity; those systems able to adjust, adapt, or otherwise take advantage of such energy flows survive and prosper, while other systems adversely affected by too much or too little energy are non-randomly eliminated.[47]
Fred Spier is foremost among those big historians who have found the concept of energy flows useful, suggesting that Big History is the rise and demise of complexity on all scales, from sub-microscopic particles to vast galaxy clusters, and not least many biological and cultural systems in between.[48]
David Christian, in an 18-minute TED talk, described some of the basics of the Big History course.[49] Christian describes each stage in the progression towards greater complexity as a "threshold moment" when things become more complex, but they also become more fragile and mobile.[49] Some of Christian's threshold stages are:
Christian elaborated that more complex systems are more fragile, and that while collective learning is a powerful force to advance humanity in general, it is not clear that humans are in charge of it, and it is possible in his view for humans to destroy the biosphere with the powerful weapons that have been invented.[49]
In the 2008 lecture series through The Teaching Company's Great Courses entitled Big History: The Big Bang, Life on Earth, and the Rise of Humanity, Christian explains Big History in terms of eight thresholds of increasing complexity:[50]
A theme in Big History is what has been termed Goldilocks conditions or the Goldilocks principle, which describes how "circumstances must be right for any type of complexity to form or continue to exist," as emphasized by Spier in his recent book.[19] For humans, bodily temperatures can neither be too hot nor too cold; for life to form on a planet, it can neither have too much nor too little energy from sunlight. Stars require sufficient quantities of hydrogen, sufficiently packed together under tremendous gravity, to cause nuclear fusion.[19]
Christian suggests that complexity arises when these Goldilocks conditions are met, that is, when things are not too hot or cold, not too fast or slow. For example, life began not in solids (molecules are stuck together, preventing the right kinds of associations) or gases (molecules move too fast to enable favorable associations) but in liquids such as water that permitted the right kinds of interactions at the right speeds.[49]
Somewhat in contrast, Chaisson has maintained for well more than a decade that "organizational complexity is mostly governed by the optimum use of energy—not too little as to starve a system, yet not too much as to destroy it".  Neither maximum energy principles nor minimum entropy states are likely relevant to appreciate the emergence of complexity in Nature writ large.[53]
Advances in particular sciences such as archaeology, gene mapping, and evolutionary ecology have enabled historians to gain new insights into the early origins of humans, despite the lack of written sources.[2] One account suggested that proponents of Big History were trying to "upend" the conventional practice in historiography of relying on written records.[2]
Big History proponents suggest that humans have been affecting climate change throughout history, by such methods as slash-and-burn agriculture, although past modifications have been on a lesser scale than in recent years during the Industrial Revolution.[2]
A book by Daniel Lord Smail in 2008 suggested that history was a continuing process of humans learning to self-modify our mental states by using stimulants such as coffee and tobacco, as well as other means such as religious rites or romance novels.[17] His view is that culture and biology are highly intertwined, such that cultural practices may cause human brains to be wired differently from those in different societies.[17]
Another theme that has been actively discussed recently by the Big History community is the issue of the Big History Singularity.[54][55][56][57][58]
A 2021 book, Expanding Worldviews: Astrobiology, Big History and Cosmic Perspectives,[59] edited by Ian Crawford explores links between Big History and astrobiology, and argues that both subjects have the potential to yield positive intellectual and societal benefits owing to their inherent cosmic and evolutionary perspectives.
Big History is more likely than conventional history to be taught with interactive "video-heavy" websites without textbooks, according to one account.[11] The discipline has benefited from having new ways of presenting themes and concepts in new formats, often supplemented by Internet and computer technology.[1] For example, the ChronoZoom project is a way to explore the 14 billion year history of the universe in an interactive website format.[8][60] It was described in one account:
ChronoZoom splays out the entirety of cosmic history in a web browser, where users can click into different epochs to learn about the events that have culminated to bring us to where we are today—in my case, sitting in an office chair writing about space. Eager to learn about the Stelliferous epoch? Click away, my fellow explorer. Curious about the formation of the earth? Jump into the "Earth and Solar System" section to see historian David Christian talk about the birth of our homeworld.
In 2012, the History channel showed the film History of the World in Two Hours.[1][8] It showed how dinosaurs effectively dominated mammals for 160 million years until an asteroid impact wiped them out.[1] One report suggested the History channel had won a sponsorship from StanChart to develop a Big History program entitled Mankind.[61] In 2013 the History channel's new H2 network debuted the 10-part series Big History, narrated by Bryan Cranston and featuring David Christian and an assortment of historians, scientists and related experts.[62] Each episode centered on a major Big History topic such as salt, mountains, cold, flight, water, meteors and megastructures.
While the emerging field of Big History in its present state is generally seen as having emerged in the past three decades beginning around 1990, there have been numerous precedents going back to the 1500s with Giordano Bruno's works, especially Lo spaccio della besta trionfante (1584)(The Return of the Triumphant Beast). In this work, Bruno traces out the decline of the Christian era and posits that this decline will be based on a massive ecological and economic crisis, which he allegorizes as a 'cetus', a whale, whose thrashings create disruptive waves and cause people to question the religious and philosophical underpinnings of the west. According to Bruno, it is exactly the laser-like focus, for hundreds of years, on the economic growth and spread of people (through colonialism and capitalism, which were rationalized through Christianity) that will lead to an environmental tipping point, or crisis, when humans recognize that their own material well being is predicated and completely dependent on the presence of myriad other beings, animals, plankton, plants, bacteria, and so forth. In a sense, Bruno's work foresaw the Material Turn, which dates to the 1990s. It should be stressed that Bruno sees this flow of history as based on evolution and the learning curve. In the mid-19th century, Alexander von Humboldt's book Cosmos, and Robert Chambers' 1844 book Vestiges of the Natural History of Creation[19] were seen as early precursors to the field.[19] In a sense, Darwin's theory of evolution was, in itself, an attempt to explain a biological phenomenon by examining longer term cause-and-effect processes. In the first half of the 20th century, secular biologist Julian Huxley originated the term "evolutionary humanism",[1] while around the same time the French Jesuit paleontologist Pierre Teilhard de Chardin examined links between cosmic evolution and a tendency towards complexification (including human consciousness), while envisaging compatibility between cosmology, evolution, and theology. In the mid and later 20th century, The Ascent of Man by Jacob Bronowski examined history from a multidisciplinary perspective. Later, Eric Chaisson explored the subject of cosmic evolution quantitatively in terms of energy rate density, and the astronomer Carl Sagan wrote Cosmos.[1] Thomas Berry, a cultural historian, and the academic Brian Swimme explored meaning behind myths and encouraged academics to explore themes beyond organized religion.[1]
The field continued to evolve from interdisciplinary studies during the mid-20th century, stimulated in part by the Cold War and the Space Race. Some early efforts were courses in Cosmic Evolution at Harvard University in the United States, and Universal History in the Soviet Union. One account suggested that the notable Earthrise photo, taken on December 24, 1968, by William Anders during a lunar orbit aboard Apollo 8, which showed Earth as a small blue and white ball behind a stark and desolate lunar landscape, not only stimulated the environmental movement but also caused an upsurge of interdisciplinary interest.[19] The French historian Fernand Braudel examined daily life with investigations of "large-scale historical forces like geology and climate".[22] Physiologist Jared Diamond in his 1997 book Guns, Germs, and Steel examined the interplay between geography and human evolution;[22] for example, he argued that the horizontal shape of the Eurasian continent enabled human civilizations to advance more quickly than the vertical north–south shape of the American continent, because an east–west continental axis and correspondingly similar climates facilitated the transfer and exchange of animals (as protein, for pulling carts, and other uses), ideas and information, as well as structures of human competition that honed and fine-tuned cultural and technological achievements.
In the 1970s, scholars in the United States including geologist Preston Cloud of the University of Minnesota, astronomer G. Siegfried Kutter at Evergreen State College in Washington state, and Harvard University astrophysicists George B. Field and Eric Chaisson started synthesizing knowledge to form a "science-based history of everything", although each of these scholars emphasized somewhat their own particular specializations in their courses and books.[19] In 1980, the Austrian philosopher Erich Jantsch wrote The Self-Organizing Universe which viewed history in terms of what he called "process structures".[19] There was an experimental course taught by John Mears at Southern Methodist University in Dallas, Texas, and more formal courses at the university level began to appear.
In 1991 Clive Ponting wrote A Green History of the World: The Environment and the Collapse of Great Civilizations. His analysis did not begin with the Big Bang, but his chapter "Foundations of History" explored the influences of large-scale geological and astronomical forces over a broad time period.
One exponent is David Christian of Macquarie University in Sydney, Australia.[63][64] He read widely in diverse fields in science, and believed that much was missing from the general study of history. His first university-level course was offered in 1989.[19] He developed a college course beginning with the Big Bang to the present[11] in which he collaborated with numerous colleagues from diverse fields in science and the humanities and the social sciences. This course eventually became a Teaching Company course entitled Big History: The Big Bang, Life on Earth, and the Rise of Humanity, with 24 hours of lectures,[1] which appeared in 2008.[8]
Since the 1990s, other universities began to offer similar courses. In 1994 at the University of Amsterdam and the Eindhoven University of Technology, college courses were offered.[19] In 1996, Fred Spier wrote The Structure of Big History.[19] Spier looked at structured processes which he termed "regimes":
I defined a regime in its most general sense as 'a more or less regular but ultimately unstable pattern that has a certain temporal permanence', a definition which can be applied to human cultures, human and non-human physiology, non-human nature, as well as to organic and inorganic phenomena at all levels of complexity. By defining 'regime' in this way, human cultural regimes thus became a subcategory of regimes in general, and the approach allowed me to look systematically at interactions among different regimes which together produce big history.
Christian's course caught the attention of philanthropist Bill Gates, who discussed with him how to turn Big History into a high school-level course. Gates said about David Christian:
He really blew me away. Here's a guy who's read across the sciences, humanities, and social sciences and brought it together in a single framework. It made me wish that I could have taken big history when I was young, because it would have given me a way to think about all of the school work and reading that followed. In particular, it really put the sciences in an interesting historical context and explained how they apply to a lot of contemporary concerns.
By 2002, a dozen college courses on Big History had sprung up around the world.[22] Cynthia Stokes Brown initiated Big History at the Dominican University of California, and she wrote Big History: From the Big Bang to the Present.[65] In 2010, Dominican University of California launched the world's first Big History program to be required of all first-year students, as part of the school's general education track. This program, directed by Mojgan Behmand, includes a one-semester survey of Big History, and an interdisciplinary second-semester course exploring the Big History metanarrative through the lens of a particular discipline or subject.[8][66] A course description reads:
Welcome to First Year Experience Big History at Dominican University of California. Our program invites you on an immense journey through time, to witness the first moments of our universe, the birth of stars and planets, the formation of life on Earth, the dawn of human consciousness, and the ever-unfolding story of humans as Earth's dominant species. Explore the inevitable question of what it means to be human and our momentous role in shaping possible futures for our planet.
The Dominican faculty's approach is to synthesize the disparate threads of Big History thought, in order to teach the content, develop critical thinking and writing skills, and prepare students to wrestle with the philosophical implications of the Big History metanarrative. In 2015, University of California Press published Teaching Big History, a comprehensive pedagogical guide for teaching Big History, edited by Richard B. Simon, Mojgan Behmand, and Thomas Burke, and written by the Dominican faculty.[68]
Barry Rodrigue, at the University of Southern Maine, established the first general education course and the first online version, which has drawn students from around the world.[18] The University of Queensland in Australia previously required all history majors to take an undergraduate big history course entitled Global History, but in 2020 remade the course to remove its big history aspects. The University of Queensland has since taken an active stance against big history, with Associate Professor Ian Hesketh being a world-leading critic.[69] By 2011, 50 professors around the world have offered courses. In 2012, one report suggested that Big History was being practiced as a "coherent form of research and teaching" by hundreds of academics from different disciplines.[8]
In 2008, Christian and his colleagues began developing a course for secondary school students.[19] In 2011, a pilot high school course was taught to 3,000 kids in 50 high schools worldwide.[11] In 2012, there were 87 schools, with 50 in the United States, teaching Big History, with the pilot program set to double in 2013 for students in the ninth and tenth grades,[3] and even in one middle school.[70] The subject is a STEM course at one high school.[71]
There are initiatives to make Big History a required standard course for university students throughout the world. An education project founded by philanthropist Bill Gates from his personal funds was launched in Australia and the United States, to offer a free online version of the course to high school students.[6]
The International Big History Association (IBHA) was founded at the Coldigioco Geological Observatory in Coldigioco, Marche, Italy, on 20 August 2010.[72] Its headquarters is located at Grand Valley State University in Allendale, Michigan, United States. Its inaugural gathering in 2012 was described as "big news" in a report in The Huffington Post.[1] 
The Second IBHA Conference took place in Dominican University of California (San Rafael, CA) on August 6–10, 2014. [1] The Third IBHA Conference was held in University of Amsterdam on 14–17 July 2016. [2]
Yuval Noah Harari's popular books, notably Sapiens: A Brief History of Humankind published in 2011, are said to belong to the genre, with Ian Parker writing in 2020 in The New Yorker that "Harari did not invent Big History, but updated it with hints of self-help and futurology, as well as a high-altitude, almost nihilistic composure about human suffering."[73]
Some notable academics involved with the concept include:[8]

Prehistory of Ohio provides an overview of the activities that occurred prior to Ohio's recorded history. The ancient hunters, Paleo-Indians (13000 B.C. to 7000 B.C.), descended from humans that crossed the Bering Strait. There is evidence of Paleo-Indians in Ohio, who were hunter-gatherers that ranged widely over land to hunt large game. For instance, mastodon bones were found at the Burning Tree Mastodon site that showed that it had been butchered. Clovis points have been found that indicate interaction with other groups and hunted large game. The Paleo Crossing site and Nobles Pond site provide evidence that groups interacted with one another. The Paleo-Indian's diet included fish, small game, and nuts and berries that gathered. They lived in simple shelters made of wood and bark or hides. Canoes were created by digging out trees with granite axes.
The weather warmed and forest grew more dense with next period of the Archaic people (8000 B.C. to 500 B.C.). Although they were also hunter-gatherers, they did not travel as far for food and began to live longer periods of time in one place. They dug pits to store food and built studier lodging. 
Tools, like spear-throwers, were more sophisticated. Base camps were established for winter lodging. The Glacial Kame culture, a late Archaic group, traded for sea shell and copper with other groups and were used as a sign of prestige within the group, for respected healers and hunters. The objects were buried with their owners.
By the Woodland period (800 B.C. to A.D. 1200) of the Post-Archaic Period, people lived in villages, developed a rich ritual and artistic life, began building earthworks and mounds, some of which were used for burial. While they still hunted and gathered food, they cultivated crops. The Adena and Hopewell cultures flourished during the Early and Middle Woodland periods, respectively. The population of Woodland people expanded dramatically and groups lived in larger villages with defensive walls or ditches built for protection. Ritual and artistic endeavors waned during the Late Woodland period, as did trading with other groups. There were not new earthworks or mounds during this later period.
During the late prehistoric period (A.D. 900 to 1650), villages were larger, often built on high ground, near a river, and often surrounded by a wooden stockade. Earthworks returned during this period, but were not built in the frequency that they were during the Woodland period. The Serpent Mound, the country's largest effigy mound, was created by the Fort Ancient culture. Maize became a staple of their diet, while squash and beans were cultivated. The centers of villages were the locations where ceremonial rituals were conducted.
The Early Contact period (1600–1750) began when Ohio tribes met Europeans, but they had begun to acquire European trade items in as much as a hundred years before they met through trade with other Native American groups. They traded for glass beads, brass, and copper items. Direct contact began in the late 17th century when traders, white explorers, and settlers came to Ohio. With them came smallpox which devastated Native American tribes, particularly affecting children and the elderly.
Humans and animals migrated over the Bering Strait to populate the Americas. They then traveled over hundreds of years into what is now the United States. The animals that came from Asia include mastodons, mammoths, elk, bison, caribous, and muskox.[1] Some of the earliest humans may have migrated through Canada to the Great Lakes region and then south from there, according to some archaeologists. During the time of the migration and until about 7,500 years ago, the Great Lakes extended south as far as Allen County, Ohio.[1]
The glaciers left bogs and small lakes and the flat Erie Plain along the Lake Erie shore. The Glaciated Allegheny Plateau southeast of Cleveland included steep Appalachian Mountains foothills that thousands of years ago had a dense Beech-Maple forest. By the time that Europeans made contact with Native Americans, the Erie Plain had wide river estuaries, coastal marshes, small prairies, and a mixed oak forest. The Central Till Plain had areas of native beech-oak-hickory woodland and "rolling terrain of glacial soils".[2]: 1
The Paleolithic period (13000 B.C. to 7000 B.C.) occurred during the last centuries of the Ice age. The native people of Ohio descended from those who crossed the Bering Strait land bridge from Asia to North America. The Paleo Indians are the earliest hunter-gatherers that ranged across what is now the state of Ohio. Their diet was based upon the food that they hunted—evidenced by distinctive spear points—fished, and gathered. Mammoth and mastodon, now extinct, were the big game animals that they hunted. They also hunted small game and deer. Fish was in their diet, as was fruit and nuts that they gathered seasonally.[3] Bones from the Burning Tree Mastodon indicate that Paleo-Indians hunted and butchered the mastodon. Because evidence of Paleo-Indians is generally limited to the finding of stone tools, this has been considered a "one of the most spectacular Paleoindian finds in Ohio."[4]
The Clovis culture (9500 to 8000 B.C.) is the earliest known Paleo Indian culture in Ohio. They are named by the type of spear point that they used, the clovis point, which were discovered by archaeologists near Clovis, New Mexico. The points were attached to spears for hunting[3] and are believed to have been used to hunt mastodons and mammoths.[5] They ranged across the land for food and lived in shelters made of wooden pole covered with hides or tree bark. Their diet consisted of small and large game animals that they hunted, fish, berries and nuts. They created tools from flint, particularly Flint Ridge flint from Licking County and Upper Mercer flint from Coshocton County.[3] They used flint because it was hard, durable, easy to flake when heated up, and could be made to have very sharp edges.[6] Their tools included scrapers, knives, and spear points.[2]: 1
Culturally, there was a laissez-faire attitude. People could remain with a group of people, join another group of people, or split off from groups. If they were particularly skilled in a certain area, like healing, hunting, or another important skill, they earned the respect of the tribe. Elders were respected for their knowledge and experience. They usually, though, did not have chiefs.[3][verification needed] Most Native American people descended from the Clovis people.[7]
Clovis artifacts dated to 13,000 years ago were found at the Paleo Crossing site in Medina County provides evidence of Paleo-Indians in northern Ohio and may be the area's oldest residents and archaeologist Dr. David Brose believes that they may be "some of the oldest certain examples of human activity in the New World."[7] Small bands of hunters used the four acre site as a place to meet up with one another and exchange information, perform ceremonial rituals, and plan hunts for big game.[2]: 2  The site contains charcoal recovered from refuse pits. There were also two post holes and tools that were made from flint from the Ohio River Valley in Indiana, 500 miles from Paleo Crossing, which indicates that the hunter-gatherers had a widespread social network and traveled across distances relatively quickly.[7] The 22-acre Nobles Pond site in Stark County was a larger meeting place for bands of hunters, with a large collection of tools made from Ohio flint.[2]: 2 [4]
Other early sites are the Sandy Springs Site in Adams County, Sheriden Cave in Wyandot County, and the Welling site in Coshocton County.[8]
Archaic people (8000 B.C. to 500 B.C.) lived in a warmer climate after the end of the Ice Age and with thicker forests than their ancestors.[9] The climate was similar to present-day Ohio in that the climate evolved into having four seasons.[10] They used technology in a more sophisticated manner, and the same openness for members to join other groups of people or set out on their own. Over time, they began to live longer periods of time in one place, dug pits to store food and built studier lodging.[9] Traveling for food, they hunted turkeys, deer, waterfowl, and passenger pigeons. They also fished and gathered berries, acorns and hickory nuts.[10] As the climate changed, there were different foods available, resulting in dietary changes.[11]
This period is divided up by archaeologists into Early (10,000-8,000 before present) and Middle Archaic cultures (8,000-5,000 before present) because there are different technological and cultural characteristics between the two groups. Early Archaic groups were smaller, but as the population increased they lived in larger groups. In the Middle Archaic period there was an increased variety of food.[10] Later in the Archaic period, people developed trade routes which introduced new goods and ideas and bands became more culturally diverse.[11]
The Archaic people continued to make their tools from flint, but they made a wider range of tools.[9] The stone tools of the Paleo-Indians disappeared.[2]: 3  The Early Archaic group had more spear points.[10] New stone tools were made of the same flint to chert, but included spear points with large corner- and side-notches and large chert knives.[2]: 3  Their shapes varied depending upon how they were to be used and some may have been attached to short handles to use like knives or to long spears for hunting.[12] The Middle Archaic group created a wide range of tools, including knives, grinding tools and stones, scraping tools, plummets, and net sinkers.[10] Mortars and pestles were used to grind food, like acorns, hickory nuts, walnuts, seeds, tubers, and rootes.[2]: 3
They made spear-throwers, or atlatls, that could be thrown with greater force and at a farther distance and with more accuracy.[9][11] Bannerstones made of slate were attached to the shafts of the spear-thrower.[2]: 3  They made axes out of granite, which they used to cut down trees and hollow out canoes or build houses.[9][2]: 3  Slate was carved and polished and used as decorations or weights.[9] They made jewelry, weights, and sinkers by grinding stone.[11] They created base camps for the winter.[10] A day in the life of an Archaic family member could include building a fire, fishing, grinding nuts for storage, and carving out a dugout canoe.[10]
The last several thousands of years of the Archaic period saw a dramatic increase in the number of Archaic sites, indicating a rise in the population. There have also been a larger number of stone tools found during this period, including flake tools, drills, knives, scrapers, projectile points, and tools for grinding food. Fishing tackle, such as harpoon heads, net sinkers, and bone hooks, indicated that they were fishing in Lake Erie and in rivers. With climate change, beech-maple forests were well established.[2]: 3, 5
Archaic sites include along the Maumee River or at Dupont Site, Weilnau Site, Raisch-Smith Site,[10] Bowman Site in Montgomery County, and the Stephan Site in Darke County.[8]
The Glacial Kame culture, a late Archaic group, traded for sea shell and copper with other groups and were used as a sign of prestige within the group, such as respected healers and hunters. The objects were buried with their owners.[9]
People from the Woodland Period (800 B.C. to A.D. 1200) lived in small villages of several households, with more permanent houses. They domesticated plants and grew sunflower, maygrass, squash, lamb's quarter and erect knotweed. This meant a very different way of life than relying on hunting and gathering. Another characteristic is that they made pottery. Burial mounds were used to bury the dead. People of the Hopewell and Adena cultures created elaborate earthworks in geometric patterns, like the Newark Earthworks, earthworks found near Chillicothe at the Hopewell Culture National Historical Park, and earthworks at Portsmouth and Marietta.[13] They embraced artistic and ritual endeavors, creating works with materials obtained due to larger trade networks.[14]
The cultures of the woodland period are defined by how they lived, rather than a connection with a particular contemporary cultural or ethnic group.[15] Except for the Hopewell, people of the Early and Middle Woodland periods did not have a dramatic change in their lifestyles, but they all had a richer cultural, architectural, and artistic life.[15]
Small villages or substantial campsites were established in northeast Ohio the Middle Woodland period, about 2,000 years ago. Built by rivers, they were located on high ridge-tops with steep-sided cliffs, making them difficult to access. Some of them were also surrounded by ditches and earthen walls. Some did not seem to be lived in much, while others showed evidence of long-term use by the presence of middens, human burials, pottery, food storage pits, and cooking pits. Leimbach Fort in Lorain County and Seaman's Fort in Erie County are examples of settlements that had long-term use. The Adena and Hopewell cultures had sites that were used just for ceremonial purposes.[2]: 6
During the Woodland period, people began to create crude pottery of soapstone by carved the stone. Other pottery was created from crushed granite rocks and clay from local rivers. When the pottery was formed, fabric was pressed against the inside and outside of the pot before the pot dried and was fired in a shallow pit. The traces of fabric left in the pottery shows that people were creating textiles, and likely mats, cordage, and woven baskets. Pots were used to store food, protect it from burrowing animals, and for cooking food over a fire.[2]: 6
The population grew rapidly[16] so that people of the Late Woodland Period (A.D. 600 to A.D. 1200) lived in larger and more permanent villages than those of the Adena and Hopewell people and they built defensive walls or ditches around their villages.[17][16] This is believed to have been due to increased regional conflict among the villages.[16] It marked an end to a culture focused on artistic expression and earthworks construction.[16] They relied more on food that they grew—like sunflower, knotweek, goosefoot, and maygrass[16]—and began growing maize about A.D. 800.[17] Food was stored in large pottery jars.[16]
Projectile points became thinner and smaller–either with no notches or finely notched triangular points for bow and arrows,[2]: 6  which could be used for hunting or as a weapon. It marks the decline of the Hopewell culture and cultures began to diversify in different areas. They stopped trading over long distance, and stopped importing large quantities of mica and obsidian. The building of large earthworks generally stopped in the Woodland period, but some buried their dead in the Hopewell earthworks.[17]
The Adena culture—found in southern Ohio and neighboring states—flourished during the Early Woodland period. People of the Adena culture were artistic and had created pipes and earthworks as part of ceremonial rituals. They created elaborate siltstone pipes and other items for ceremonial and burial practices. They constructed earthworks, like conical burial mounds and circular enclosures—some of the earliest earthworks built in Ohio.[14]
There were also people in northern Ohio who lived similarly to the Adena culture, but their earthworks were oval enclosures called forts and walls along bluffs. They did not create large earthen mounds.[14]
The Hopewell culture—the center of which lies in Ohio—flourished during the Middle Woodland period. They are known for their architectural and artistic endeavors, as well as having a "complex ceremonial life unrivaled in North America at the time." They spent up to thousands of hours to create large-scale geometric earthworks, meant to fit specific pre-conceived site plans which might have been planned based upon other earth mounds and based on the landscape. Alignment with the moon and stars may also have been a consideration.[15]
Goods were made from copper that were imported from hundreds of miles away. They created a wide range of artistic artifacts, including figurines, decorated pottery, effigy pipes, copper plates, mica cut-outs, and bone adornments.[15]
During the late prehistoric period (A.D. 900 to 1650), villages were larger, often built on high ground, and often surrounded by a wooden stockade.[21][22] There were more permanent settlements established in river valleys. People lived in village from late spring to early fall, and some lived in villages permanently, particularly by 800 years ago.[2]: 8–9  Plazas were placed in the center of villages and was where they conducted rituals. In other places, large cities were established, such as in the Southeastern United States and the Mississippi Valley. Cahokia in Illinois was the largest city of the Late prehistoric period, also called the Mississippian Period.[21] The dead were buried in effigy mounds or in graves surrounding the village plaza. Alligator Effigy Mound and Serpent Mound may have been created to honor important spirits.[21]
Maize became a staple of their diet[22] and squash and beans were added to their diet. Hunting, fishing, and gathering wild plants also provided food for the village. The term Late prehistoric is also a grouping for the various Native American cultures before contact by Europeans.[17][21] The villages had one or two leaders. A war chief may be a village's leader.[21] This period saw a shift in ritual practices.[22]
Several cultures were prominent during this period, including the Monongahela, Whittlesey, Sandusky and Fort Ancient cultures.[21]
The Whittlesey culture has been studied most extensively due to number of sites, such as Fairport Harbor, Reeves, South Park, and Tuttle Hill. There is also evidence of house walls and cooking and storage pits. There is a wealth of artifacts and material, like pottery, arrow points, and debris. Charred remains of squash, beans, and maize are evidence that they cultivated these foods. Bones and clam shells were found that showed that they continued to hunt and fish for food. The Whittlesey sites were selected with defense in mind, set on hilltops and with stockades, ditches, or earthen walls to surround the village.[2]: 8–9
The Fort Ancient culture—of southern Ohio and northern Kentucky—flourished during the Late Prehistoric Period. Rectangular or circular houses surrounded a plaza area. Although they did not build earthworks in the same frequency as the Hopewell culture, they built several large earthworks, like the Serpent Mound, the country's largest effigy mound. They made petroglyphs, like the Leo Petroglyph that has images of footprints, people, and animals. Stone tools were made in shapes particular to their culture, like triangular arrow points and pentagonal flint knives.[22]
The Early Contact period (1600–1750) began when Ohio tribes met Europeans, but they had begun to acquire European trade items in as much as a hundred years before they met through trade with other Native American groups, perhaps from the Appalachian Mountains or the southern shore of the Great Lakes. They traded for glass beads, brass, and copper items from Europeans. They also were influenced by Spanish settlers and explorers from the south.
[23]
Direct contact began in the late 1600s when traders, white explorers, and settlers came to Ohio. The native peoples from Ohio had not been subjected to, and had not immunity for European diseases, like small pox, which was devastating to the population of Native Americans. Children and the elderly were particularly affected.[23]
Conflicts between the Iroquois and the French resulted in expansion of Iroquois territory into Ohio after they won battles with the French.[23] About 350 years ago, people of the Whittlesey culture abandoned Ohio. At that time, during the Beaver Wars, the Iroquois raided other native groups settlements, and the Whittlesey people may have been victims of the Iroquois. By 1650, there were no native inhabitants in northern Ohio.[2]: 8–9  Other local populations were also pushed out of the state. Many native people returned after the conflicts subsided. The Native American groups later in Ohio included the Huron, Wyandot, Miami, Delaware, Ottawa, Shawnee, Mingo, and Erie people.[23]
In the 1800s, Native Americans were pushed out of Ohio and westward across the Mississippi River.[2]: 9

Toomas Kivisild (born 11 August 1969, in Tapa, Estonia) is an Estonian population geneticist. He graduated as a biologist and received his PhD in Genetics, from University of Tartu, Estonia, in 2000. Since then he has worked as a postdoctoral research fellow in the School of Medicine, at Stanford University (2002-3), Estonian Biocentre (since 2003), as the Professor of Evolutionary Biology, University of Tartu (2005-6), and as a Lecturer and Reader in Human Evolutionary Genetics in the Department of Archaeology and Anthropology at the University of Cambridge (2006-2018). From 2018 he is a professor in the Department of Human Genetics at KU Leuven and a senior researcher at the Institute of Genomics, University of Tartu.[1][2]
Kivisild has focused in his research on questions relating global genetic population structure with evolutionary processes such as selection, drift, migrations and admixture.[3] He coauthored the second edition of the textbook Human Evolutionary Genetics (2013).[4][5]

A hunter-gatherer or forager is a human living in a community, or according to an ancestrally derived lifestyle, in which most or all food is obtained by foraging,[1][2] that is, by gathering food from local naturally occurring sources, especially wild edible plants but also insects, fungi, honey, bird eggs, or anything safe to eat, or by hunting game (pursuing or trapping and killing wild animals, including catching fish). This is a common practice among most vertebrates that are omnivores. Hunter-gatherer societies stand in contrast to the more sedentary agricultural societies, which rely mainly on cultivating crops and raising domesticated animals for food production, although the boundaries between the two ways of living are not completely distinct.
Hunting and gathering was humanity's original and most enduring successful competitive adaptation in the natural world, occupying at least 90 percent of human history.[3] Following the invention of agriculture, hunter-gatherers who did not change were displaced or conquered by farming or pastoralist groups in most parts of the world.[4] Across Western Eurasia, it was not until approximately 4,000 BC that farming and metallurgical societies completely replaced hunter-gatherers. These technologically advanced societies expanded faster in areas with less forest, pushing hunter-gatherers into denser woodlands. Only the middle-late Bronze Age and Iron Age societies were able to fully replace hunter-gatherers in their final stronghold located in the most densely forested areas. Unlike their Bronze and Iron Age counterparts, Neolithic societies could not establish themselves in dense forests, and Copper Age societies had only limited success.[5]
In addition to men, a single study found that women engage in hunting in 79% of modern hunter-gatherer societies.[6] However, an attempted verification of this study found "that multiple methodological failures all bias their results in the same direction...their analysis does not contradict the wide body of empirical evidence for gendered divisions of labor in foraging societies".[7] Only a few contemporary societies of uncontacted people are still classified as hunter-gatherers, and many supplement their foraging activity with horticulture or pastoralism.[8][9]
Hunting and gathering was presumably the subsistence strategy employed by human societies beginning some 1.8 million years ago, by Homo erectus, and from its appearance some 200,000 years ago by Homo sapiens. Prehistoric hunter-gatherers lived in groups that consisted of several families resulting in a size of a few dozen people.[10] It remained the only mode of subsistence until the end of the Mesolithic period some 10,000 years ago, and after this was replaced only gradually with the spread of the Neolithic Revolution.
The Late Pleistocene witnessed the spread of modern humans outside of Africa as well as the extinction of all other human species. Humans spread to the Australian continent and the Americas for the first time, coincident with the extinction of numerous predominantly megafaunal species.[12] Major extinctions were incurred in Australia beginning approximately 50,000 years ago and in the Americas about 15,000 years ago.[13] Ancient North Eurasians lived in extreme conditions of the mammoth steppes of Siberia and survived by hunting mammoths, bison and woolly rhinoceroses.[14] The settlement of the Americas began when Paleolithic hunter-gatherers entered North America from the North Asian mammoth steppe via the Beringia land bridge.[15]
During the 1970s, Lewis Binford suggested that early humans obtained food via scavenging, not hunting.[16] Early humans in the Lower Paleolithic lived in forests and woodlands, which allowed them to collect seafood, eggs, nuts, and fruits besides scavenging. Rather than killing large animals for meat, according to this view, they used carcasses of such animals that had either been killed by predators or that had died of natural causes.[17] Scientists have demonstrated that the evidence for early human behaviors for hunting versus carcass scavenging vary based on the ecology, including the types of predators that existed and the environment.[18]
According to the endurance running hypothesis, long-distance running as in persistence hunting, a method still practiced by some hunter-gatherer groups in modern times, was likely the driving evolutionary force leading to the evolution of certain human characteristics. This hypothesis does not necessarily contradict the scavenging hypothesis: both subsistence strategies may have been in use sequentially, alternately or even simultaneously.
Starting at the transition between the Middle to Upper Paleolithic period, some 80,000 to 70,000 years ago, some hunter-gatherer bands began to specialize, concentrating on hunting a smaller selection of (often larger) game and gathering a smaller selection of food. This specialization of work also involved creating specialized tools such as fishing nets, hooks, and bone harpoons.[19] The transition into the subsequent Neolithic period is chiefly defined by the unprecedented development of nascent agricultural practices. Agriculture originated as early as 12,000 years ago in the Middle East, and also independently originated in many other areas including Southeast Asia, parts of Africa, Mesoamerica, and the Andes.
Forest gardening was also being used as a food production system in various parts of the world over this period.[citation needed]
Many groups continued their hunter-gatherer ways of life, although their numbers have continually declined, partly as a result of pressure from growing agricultural and pastoral communities. Many of them reside in the developing world, either in arid regions or tropical forests. Areas that were formerly available to hunter-gatherers were—and continue to be—encroached upon by the settlements of agriculturalists. In the resulting competition for land use, hunter-gatherer societies either adopted these practices or moved to other areas. In addition, Jared Diamond has blamed a decline in the availability of wild foods, particularly animal resources. In North and South America, for example, most large mammal species had gone extinct by the end of the Pleistocene—according to Diamond, because of overexploitation by humans,[20] one of several explanations offered for the Quaternary extinction event there.
As the number and size of agricultural societies increased, they expanded into lands traditionally used by hunter-gatherers. This process of agriculture-driven expansion led to the development of the first forms of government in agricultural centers, such as the Fertile Crescent, Ancient India, Ancient China, Olmec, Sub-Saharan Africa and Norte Chico.
As a result of the now near-universal human reliance upon agriculture, the few contemporary hunter-gatherer cultures usually live in areas unsuitable for agricultural use.
Archaeologists can use evidence such as stone tool use to track hunter-gatherer activities, including mobility.[21][22]
Ethnobotany is the field of study whereby food plants of various peoples and tribes worldwide are documented.
Most hunter-gatherers are nomadic or semi-nomadic and live in temporary settlements. Mobile communities typically construct shelters using impermanent building materials, or they may use natural rock shelters, where they are available.
Some hunter-gatherer cultures, such as the indigenous peoples of the Pacific Northwest Coast and the Yokuts, lived in particularly rich environments that allowed them to be sedentary or semi-sedentary. Amongst the earliest example of permanent settlements is the Osipovka culture (14–10.3 thousand years ago),[23] which lived in a fish-rich environment that allowed them to be able to stay at the same place all year.[24] One group, the Chumash, had the highest recorded population density of any known hunter and gatherer society with an estimated 21.6 persons per square mile.[25]
Hunter-gatherers tend to have an egalitarian social ethos,[26][27] although settled hunter-gatherers (for example, those inhabiting the Northwest Coast of North America and the Calusa in Florida) are an exception to this rule.[28][29][30] For example, the San people or "Bushmen" of southern Africa have social customs that strongly discourage hoarding and displays of authority, and encourage economic equality via sharing of food and material goods.[31] Karl Marx defined this socio-economic system as primitive communism.[32]
The egalitarianism typical of human hunters and gatherers is never total but is striking when viewed in an evolutionary context. One of humanity's two closest primate relatives, chimpanzees, are anything but egalitarian, forming themselves into hierarchies that are often dominated by an alpha male. So great is the contrast with human hunter-gatherers that it is widely argued by paleoanthropologists that resistance to being dominated was a key factor driving the evolutionary emergence of human consciousness, language, kinship and social organization.[33][34][35][36]
Most anthropologists believe that hunter-gatherers do not have permanent leaders; instead, the person taking the initiative at any one time depends on the task being performed.[37][38][39]
Within a particular tribe or people, hunter-gatherers are connected by both kinship and band (residence/domestic group) membership.[40] Postmarital residence among hunter-gatherers tends to be matrilocal, at least initially.[41] Young mothers can enjoy childcare support from their own mothers, who continue living nearby in the same camp.[42] The systems of kinship and descent among human hunter-gatherers were relatively flexible, although there is evidence that early human kinship in general tended to be matrilineal.[43]
The conventional assumption has been that women did most of the gathering, while men concentrated on big game hunting. In recent years, however, this assumption has been challenged by new research findings. Women in many hunter-gatherer societies hunted small game and, in some cases, even participated in big-game hunting.[44][45] An illustrative account is Megan Biesele's study of the southern African Ju/'hoan, 'Women Like Meat'.[46] A 2006 study suggests that the sexual division of labor was the fundamental organizational innovation that gave Homo sapiens the edge over the Neanderthals, allowing our ancestors to migrate from Africa and spread across the globe.[47]
A 1986 study found most hunter-gatherers have a symbolically structured sexual division of labor.[48] However, it is true that in a small minority of cases, women hunted the same kind of quarry as men, sometimes doing so alongside men. Among the Ju'/hoansi people of Namibia, women help men track down quarry.[49] In the Australian Martu, both women and men participate in hunting but with a different style of gendered division; while men are willing to take more risks to hunt bigger animals such as kangaroo for political gain as a form of "competitive magnanimity", women target smaller game such as lizards to feed their children and promote working relationships with other women, preferring a more constant supply of sustenance.[50] In 2018, 9000-year-old remains of a female hunter along with a toolkit of projectile points and animal processing implements were discovered at the Andean site of Wilamaya Patjxa, Puno District in Peru.[44] A 2020 study inspired by this discovery found that of 27 identified burials with hunter gatherers of a known sex who were also buried with hunting tools, 11 were female hunter gatherers, while 16 were male hunter gatherers. Combined with uncertainties, these findings suggest that anywhere from 30 to 50 percent of big game hunters were female.[44] A 2023 study that looked at studies of contemporary hunter gatherer societies from the 1800s to the present day found that women hunted in 79 percent of hunter gatherer societies.[45] However, an attempted verification of this study found "that multiple methodological failures all bias their results in the same direction...their analysis does not contradict the wide body of empirical evidence for gendered divisions of labor in foraging societies".[7]
At the 1966 "Man the Hunter" conference, anthropologists Richard Borshay Lee and Irven DeVore suggested that egalitarianism was one of several central characteristics of nomadic hunting and gathering societies because mobility requires minimization of material possessions throughout a population. Therefore, no surplus of resources can be accumulated by any single member. Other characteristics Lee and DeVore proposed were flux in territorial boundaries as well as in demographic composition.
At the same conference, Marshall Sahlins presented a paper entitled, "Notes on the Original Affluent Society", in which he challenged the popular view of hunter-gatherers lives as "solitary, poor, nasty, brutish and short", as Thomas Hobbes had put it in 1651. According to Sahlins, ethnographic data indicated that hunter-gatherers worked far fewer hours and enjoyed more leisure than typical members of industrial society, and they still ate well. Their "affluence" came from the idea that they were satisfied with very little in the material sense.[51] Later, in 1996, Ross Sackett performed two distinct meta-analyses to empirically test Sahlin's view. The first of these studies looked at 102 time-allocation studies, and the second one analyzed 207 energy-expenditure studies. Sackett found that adults in foraging and horticultural societies work on average, about 6.5 hours a day, whereas people in agricultural and industrial societies work on average 8.8 hours a day.[52] Sahlins' theory has been criticized for only including time spent hunting and gathering while omitting time spent on collecting firewood, food preparation, etc. Other scholars also assert that hunter-gatherer societies were not "affluent" but suffered from extremely high infant mortality, frequent disease, and perennial warfare.[53][54]
Researchers Gurven and Kaplan have estimated that around 57% of hunter-gatherers reach the age of 15. Of those that reach 15 years of age, 64% continue to live to or past the age of 45. This places the life expectancy between 21 and 37 years.[55] They further estimate that 70% of deaths are due to diseases of some kind, 20% of deaths come from violence or accidents and 10% are due to degenerative diseases.
Mutual exchange and sharing of resources (i.e., meat gained from hunting) are important in the economic systems of hunter-gatherer societies.[40] Therefore, these societies can be described as based on a "gift economy".
A 2010 paper argued that while hunter-gatherers may have lower levels of inequality than modern, industrialised societies, that does not mean inequality does not exist. The researchers estimated that the average Gini coefficient amongst hunter-gatherers was 0.25, equivalent to the country of Denmark in 2007. In addition, wealth transmission across generations was also a feature of hunter-gatherers, meaning that "wealthy" hunter-gatherers, within the context of their communities, were more likely to have children as wealthy as them than poorer members of their community and indeed hunter-gatherer societies demonstrate an understanding of social stratification. Thus while the researchers agreed that hunter-gatherers were more egalitarian than modern societies, prior characterisations of them living in a state of egalitarian primitive communism were inaccurate and misleading.[56]
This study, however, exclusively examined modern hunter-gatherer communities, offering limited insight into the exact nature of social structures that existed prior to the Neolithic Revolution. Alain Testart and others have said that anthropologists should be careful when using research on current hunter-gatherer societies to determine the structure of societies in the paleolithic era, emphasising cross-cultural influences, progress and development that such societies have undergone in the past 10,000 years.[57]
As one moves away from the equator, the importance of plant food decreases and the importance of aquatic food increases. In cold and heavily forested environments, edible plant foods and large game are less abundant and hunter-gatherers may turn to aquatic resources to compensate. Hunter-gatherers in cold climates also rely more on stored food than those in warm climates. However, aquatic resources tend to be costly, requiring boats and fishing technology, and this may have impeded their intensive use in prehistory. Marine food probably did not start becoming prominent in the diet until relatively recently, during the Late Stone Age in southern Africa and the Upper Paleolithic in Europe.[58]
Fat is important in assessing the quality of game among hunter-gatherers, to the point that lean animals are often considered secondary resources or even starvation food. Consuming too much lean meat leads to adverse health effects like protein poisoning, and can in extreme cases lead to death. Additionally, a diet high in protein and low in other macronutrients results in the body using the protein as energy, possibly leading to protein deficiency. Lean meat especially becomes a problem when animals go through a lean season that requires them to metabolize fat deposits.[59]
In areas where plant and fish resources are scarce, hunter-gatherers may trade meat with horticulturalists for carbohydrates. For example, tropical hunter-gatherers may have an excess of protein but be deficient in carbohydrates, and conversely tropical horticulturalists may have a surplus of carbohydrates but inadequate protein. Trading may thus be the most cost-effective means of acquiring carbohydrate resources.[60]
Hunter-gatherer societies manifest significant variability, depending on climate zone/life zone, available technology, and societal structure. Archaeologists examine hunter-gatherer tool kits to measure variability across different groups. Collard et al. (2005) found temperature to be the only statistically significant factor to impact hunter-gatherer tool kits.[61] Using temperature as a proxy for risk, Collard et al.'s results suggest that environments with extreme temperatures pose a threat to hunter-gatherer systems significant enough to warrant increased variability of tools. These results support Torrence's (1989) theory that the risk of failure is indeed the most important factor in determining the structure of hunter-gatherer toolkits.[62]
One way to divide hunter-gatherer groups is by their return systems. James Woodburn uses the categories "immediate return" hunter-gatherers for egalitarianism and "delayed return" for nonegalitarian. Immediate return foragers consume their food within a day or two after they procure it. Delayed return foragers store the surplus food.[63][64]
Hunting-gathering was the common human mode of subsistence throughout the Paleolithic, but the observation of current-day hunters and gatherers does not necessarily reflect Paleolithic societies; the hunter-gatherer cultures examined today have had much contact with modern civilization and do not represent "pristine" conditions found in uncontacted peoples.[65]
The transition from hunting and gathering to agriculture is not necessarily a one-way process.
It has been argued that hunting and gathering represents an adaptive strategy, which may still be exploited, if necessary, when environmental change causes extreme food stress for agriculturalists.[66] In fact, it is sometimes difficult to draw a clear line between agricultural and hunter-gatherer societies, especially since the widespread adoption of agriculture and resulting cultural diffusion that has occurred in the last 10,000 years.[67]
Nowadays,[when?] some scholars speak about the existence within cultural evolution of the so-called mixed-economies or dual economies which imply a combination of food procurement (gathering and hunting) and food production or when foragers have trade relations with farmers.[68]
Some of the theorists who advocate this "revisionist" critique[clarification needed] imply that, because the "pure hunter-gatherer" disappeared not long after colonial (or even agricultural) contact began, nothing meaningful can be learned about prehistoric hunter-gatherers from studies of modern ones (Kelly,[69] 24–29; see Wilmsen[70])
Lee and Guenther have rejected most of the arguments put forward by Wilmsen.[71][72][73][clarification needed] Doron Shultziner and others have argued that we can learn a lot about the life-styles of prehistoric hunter-gatherers from studies of contemporary hunter-gatherers—especially their impressive levels of egalitarianism.[74]
There are nevertheless a number of contemporary hunter-gatherer peoples who, after contact with other societies, continue their ways of life with very little external influence or with modifications that perpetuate the viability of hunting and gathering in the 21st century.[8] One such group is the Pila Nguru (Spinifex people) of Western Australia, whose land in the Great Victoria Desert has proved unsuitable for European agriculture (and even pastoralism).[citation needed] Another are the Sentinelese of the Andaman Islands in the Indian Ocean, who live on North Sentinel Island and to date have maintained their independent existence, repelling attempts to engage with and contact them.[76][77] The Savanna Pumé of Venezuela also live in an area that is inhospitable to large scale economic exploitation and maintain their subsistence based on hunting and gathering, as well as incorporating a small amount of manioc horticulture that supplements, but is not replacing, reliance on foraged foods.[78]
Evidence suggests big-game hunter-gatherers crossed the Bering Strait from Asia (Eurasia) into North America over a land bridge (Beringia), that existed between 47,000 and 14,000 years ago.[79] Around 18,500–15,500 years ago, these hunter-gatherers are believed to have followed herds of now-extinct Pleistocene megafauna along ice-free corridors that stretched between the Laurentide and Cordilleran ice sheets.[80] Another route proposed is that, either on foot or using primitive boats, they migrated down the Pacific coast to South America.[81][82]
Hunter-gatherers would eventually flourish all over the Americas, primarily based in the Great Plains of the United States and Canada, with offshoots as far east as the Gaspé Peninsula on the Atlantic coast, and as far south as Chile, Monte Verde.[citation needed] American hunter-gatherers were spread over a wide geographical area, thus there were regional variations in lifestyles. However, all the individual groups shared a common style of stone tool production, making knapping styles and progress identifiable. This early Paleo-Indian period lithic reduction tool adaptations have been found across the Americas, utilized by highly mobile bands consisting of approximately 25 to 50 members of an extended family.[83]
The Archaic period in the Americas saw a changing environment featuring a warmer more arid climate and the disappearance of the last megafauna.[84] The majority of population groups at this time were still highly mobile hunter-gatherers. Individual groups started to focus on resources available to them locally, however, and thus archaeologists have identified a pattern of increasing regional generalization, as seen with the Southwest, Arctic, Poverty Point, Dalton and Plano traditions. These regional adaptations would become the norm, with reliance less on hunting and gathering, with a more mixed economy of small game, fish, seasonally wild vegetables and harvested plant foods.[85][86]
Scholars like Kat Anderson have suggested that the term Hunter-gatherer is reductive because it implies that Native Americans never stayed in one place long enough to affect the environment around them. However, many of the landscapes in the Americas today are due to the way the Natives of that area originally tended the land. Anderson specifically looks at California Natives and the practices they utilized to tame their land. Some of these practices included pruning, weeding, sowing, burning, and selective harvesting. These practices allowed them to take from the environment in a sustainable manner for centuries.[87]
California Indians view the idea of wilderness in a negative light. They believe that wilderness is the result of humans losing their knowledge of the natural world and how to care for it. When the earth turns back to wilderness after the connection with humans is lost then the plants and animals will retreat and hide from the humans.[87]

Stephen Oppenheimer (born 1947) is a British paediatrician, geneticist, and writer.  He is a graduate of Balliol College, Oxford and an honorary fellow of the Liverpool School of Tropical Medicine.  In addition to his work in medicine and tropical diseases, he has published popular works in the fields of genetics and human prehistory.  This latter work has been the subject of a number of television and film projects.
Oppenheimer trained in medicine at Oxford and London universities, qualifying in 1971.  From 1972 he worked as a clinical paediatrician, mainly in Malaysia, Nepal and Papua New Guinea.  He carried out and published clinical research in the areas of nutrition, infectious disease (including malaria), and genetics, focussing on the interactions between nutrition, genetics and infection, in particular iron nutrition, thalassaemia and malaria.  From 1979 he moved into medical research and teaching, with positions at the Liverpool School of Tropical Medicine, Oxford University, a research centre in Kilifi, Kenya, and the Universiti Sains Malaysia in Penang.[1]
He spent three years undertaking fieldwork in Papua New Guinea, studying the effects of iron supplementation on susceptibility to infection.  His fieldwork, published in the late 1980s, identified the role of genetic mutation in malarious areas as a result of natural selection due to its protective effect against malaria, and that different genotypes for alpha-thalassaemia traced different migrations out to the Pacific.  Following that work, he concentrated on researching the use of unique genetic mutations as markers of ancient migrations.[1]
From 1990 to 1994 Oppenheimer served as chairman and chief of clinical service in the Department of Paediatrics in the Chinese University of Hong Kong. He worked as senior specialist paediatrician in Brunei from 1994 to 1996.  He returned to England in 1997, writing the book Eden in the East: the drowned continent of Southeast Asia, published in 1998.  The book synthesised work across a range of disciplines, including oceanography, archaeology, linguistics, social anthropology and human genetics.[1]
He continued to write books and articles, and began a second career as a researcher and popular-science writer on human prehistory. He worked as consultant on two television documentary series,  The Real Eve (Discovery Channel) and Out of Eden (Channel 4), and published a second book, Out of Eden: the Peopling of the World (retitled The Real Eve in USA).  This was followed in 2006 by The Origins of the British: a genetic detective story, on the post-glacial peopling of Great Britain and Ireland.  In 2009 he was consultant on the BBC TV series The Incredible Human Journey.[1]
In his book Eden in the East: The Drowned Continent of Southeast Asia, published in 1998, Oppenheimer argues that the rise in ocean levels that accompanied the waning of the ice age—as much as 500 feet (150 m)—during the period 14,000–7,000 years ago, must be taken into account when trying to understand the flow of genes and culture in Eurasia. Citing evidence from geology, archaeology, genetics, linguistics, and folklore, he hypothesizes that the Southeast Asian subcontinent of Sundaland was home to a rich and original culture that was dispersed when Sundaland was mostly submerged and its population moved westward. According to Oppenheimer, Sundaland's culture may have reached India and Mesopotamia, becoming the root for the innovative cultures that developed in those areas. He also suggests that the Austronesian languages originate from Sundaland and that a Neolithic Revolution may have started there.[2]
In 2002, Oppenheimer worked as consultant on a television documentary series, The Real Eve, produced by the American cable TV network the Discovery Channel and directed by Andrew Piddington.  The series was known as Where We Came From in the United Kingdom.
The "Eve" in the title refers to Mitochondrial Eve, a name used for the most recent common ancestor of all humans in the matrilineal (mother to daughter) line of descent.
Following the series, Oppenheimer published a book on the same theme, originally titled Out of Eden in the UK and republished as The Real Eve in the US.  This work, published in 2004, focuses on Oppenheimer's hypothesis: that approximately 85 thousand years ago, a group of modern humans migrated from East Africa across the Red Sea to South Asia in a single major exodus numbering no more than a few hundred individuals. This lone group of wanderers, he suggests, were the ancestors of all the peoples of the earth except sub-Saharan Africans, their descendants having since migrated all over the Eurasian continent, North Africa, the Pacific islands, and the New World, and radiated into a plurality of physical characteristics, languages, ethnicities and cultures as seen today.[3]
In his 2006 book The Origins of the British (revised in 2007), Oppenheimer argued that neither Anglo-Saxons nor Celts had much impact on the genetics of the inhabitants of the British Isles, instead he argued for a substantial continuity pf British ancestry mainly traced back to Palaeolithic Iberian people, now represented best by Basques. He also argued that the Scandinavian input has been underestimated. He published an introduction to his book in Prospect magazine[4] and answered some of his critics in a further Prospect magazine article in June 2007.[5]
He reviewed early Y chromosome studies carried out by Michael Weale[6] and Cristian Capelli[7] and suggested that correlations of gene frequency mean nothing without a knowledge of the genetic prehistory of the regions in question. His criticism of these studies is that they generated models based on the historical evidence of Gildas and Procopius, and then selected methodologies to test against these populations. Weale's transect spotlights that Belgium is further west in the genetic map than North Walsham, Asbourne and Friesland. In Oppenheimer's view, this is evidence that the Belgae and other continental people – and hence continental genetic markers indistinguishable from those ascribed to Anglo-Saxons – arrived earlier and were already strong in the 5th century in particular regions or areas.[8] Oppenheimer, basing his research on the Weale and Capelli studies, maintains that none of the invasions following the Romans have had a significant impact on the gene pool of the British Isles, and that the inhabitants from prehistoric times belong to an Iberian genetic grouping. He says that most people in the British Isles are genetically similar to the Basque people of northern Spain and southwestern France, from 90% in Wales to 66% in East Anglia.[8] Oppenheimer suggests that the division between the West and the East of England is not due to the Anglo-Saxon invasion but originates with two main routes of genetic flow – one up the Atlantic coast, the other from neighbouring areas of Continental Europe – which occurred just after the Last Glacial Maximum.[8] Bryan Sykes, a former geneticist at Oxford University, came to fairly similar conclusions as Oppenheimer.[9]
Recent archaeogenetics studies have contradicted Oppenheimer's theory,
indicating a population replacement in Britain by a migration of Early European Farmers, ultimately from the Aegean, after c. 4,000 BCE,[10] and another population replacement around the middle of the third millennium BCE, when a migration of Bell Beaker groups carrying significant levels of Steppe Ancestry resulted in the replacement of around 90% of the gene pool in Britain.[11]

Shell jewelry is jewelry that is primarily made from seashells, the shells of marine mollusks. Shell jewelry is a type of shellcraft. One very common form of shell jewelry is necklaces that are composed of large numbers of beads, where each individual bead is the whole (but often drilled) shell of a small sea snail. Numerous other varieties of shell jewelry are made, including bracelets and earrings.
As well as sea snail shells, shell jewelry also sometimes uses the shells of clams (bivalves) and tusk shells (scaphopods). Occasionally shell jewelry is made from the shells of non-marine mollusks such as the shells of land snails,[1] or the shells of freshwater mollusks. Not all shell jewelry is made from whole shells; some kinds are made from parts of shells, including the shell layer known as mother of pearl or nacre, and the "trapdoor" or operculum which is part of some sea snails.
In recent times, inexpensive shell jewelry is often found at tropical beach destinations, where it is offered to tourists as informal wear, or as a souvenir. However, shell jewelry has a very ancient past, and is of great importance in archeology and anthropology. In fact, shell beads are the oldest form of jewelry known, dating back over 100,000 years.
The oldest known jewelry in the world consists of two perforated beads made from shells of the sea snail Nassarius gibbosulus. These beads were discovered at Skhul in Israel, and were recently dated to between 100,000 and 135,000 years ago.[2][3] Similar ornaments (some made from shells of Nassarius kraussianus and the bittersweet clam Glycymeris nummaria as well as from Nassarius gibbosulus) have been discovered at a number of Middle Paleolithic sites, and are considered a key piece of evidence for the theory that early anatomically modern humans in Africa and the Levant were more culturally sophisticated than had previously been thought.[4][5][6] In some cases shells had been transported a considerable distance from the species' natural habitat. One example is the site of Oued Djebbana in Algeria, for example, where an N. gibbosulus bead was found; at the time the shell was used there, this site was at least 190 km away from the sea.[3]
Shell ornaments were very common during the Upper Paleolithic, from 50–40,000 years ago onwards, when they spread with modern humans to Europe and Asia. They generally take the form of perforated shells (as well as other hard organic material such as tooth, bone, antler and mammoth ivory) which are thought to have been suspended and used as jewelry. The most commonly found species are Homalopoma sanguineum, Littorina obtusata, Cyclope species, Nassarius mutabilis and Nassarius gibbosulus. Fossil shells were used alongside those of contemporary species. Some shells were stained with ochre. In Europe, the shells of both Atlantic and Mediterranean species were used, again circulating over distances of hundreds of kilometers.[7] During the Neolithic period shell necklaces were made with the shells of 3 genera Spondylus, Glycymeris and Charonia.[8]

Human rights in the United Arab Emirates (UAE) are severely restricted. The UAE does not have democratically elected institutions and citizens do not have the right to change their government or form political parties. Activists and academics who criticize the government are detained and imprisoned, and their families are often harassed by the state security apparatus.[1] There are reports of forced disappearances of foreign nationals and Emirati citizens, who have been abducted, illegally detained and tortured in undisclosed locations, and denied the right to a speedy trial or access to counsel during investigations by the UAE government.[a] The non-governmental organisation (NGO), Human Rights Watch states that Emirati laws maintain capital punishment and discriminate against women, migrants and LGBTQ individuals.[1]
The government restricts freedom of speech and freedom of the press, and the local media are censored to prevent criticism of the government, government officials or royal families. As a result, the UAE routinely ranks near the bottom of many international measures for human rights and press freedom.
Despite being elected to the UN Council, the UAE is not a signatory of many international human rights and labour rights treaties, including the International Covenant on Civil and Political Rights, the International Covenant on Economic, Social and Cultural Rights, and the United Nations Convention on the Protection of the Rights of All Migrant Workers and Members of Their Families.
In November 2020, the UAE overhauled its legal system and enacted a number of reforms, including lowering restrictions on alcohol consumption, permitting cohabitation, imposing harsher sentences for honor killings, and removing corporal punishment as a legal form of punishment in its penal code.[10][11]
Although authorised, the death penalty is rarely applied in the UAE as the law requires that a panel of three judges agree on the decision of a sentence to death, which can be commuted if the family of the victim forgives the convicted or accept a financial compensation for the crime. When a family accepts financial compensation, the court can jail a convict to a minimum of three years and a maximum of seven years.[12]
Execution in the UAE is applied through a firing squad.[13] Before 2020, the law included stoning as a form of punishment due to Sharia law being incorporated in the penal code; there are no reports of the sentence ever being applied.[11][14][15][16]
Sharia is the principal source of law for Muslim family law. Sharia courts have exclusive jurisdiction to hear family disputes, including matters involving divorce, inheritances, child custody, child abuse and guardianship for Muslims in the UAE.[17] All other laws are based on civil law.[11]
The UAE penal code was updated in 2020 to remove all legal forms of punishment prescribed in Sharia law, except for blood money payments; these punishments included flogging and stoning.[18] The update amended Article 1 to state "The provisions of the Islamic Shari’a shall apply to the retribution and blood money crimes. Other crimes and their respective punishments shall be provided for in accordance with the provisions of this Law and other applicable penal codes".[11] All known sentences that included flogging were issued before 2016. These were for various charges, including[19]: 2092 [20] verbal abuse,[21] adultery,[22] physical abuse,[23] alcohol consumption by Muslims,[24] and extra-marital sex.[25][26] The sentences ranged from 40 to 200 lashes.[27] The last known stoning sentence is from 2014. All cases handed a stoning sentence were for adultery; none were ever carried out.[26][28][29]
Apostasy is technically a crime punishable by death in the UAE, but there are no known prosecutions for apostasy.[30][31]
The Sharia-based personal status law regulates matters such as marriage, divorce and child custody. Sharia courts have exclusive jurisdiction to hear family disputes, including matters involving divorce, inheritances, child custody, child abuse and guardianship of minors.[32] Polygamy is legal for men, and Muslim women must receive permission from male guardian to marry and remarry, and are not allowed to marry non-Muslims.[33] Before 2020, Sharia law was sometimes applied to non-Muslims personal status matters, but the law was changed federally to apply civil family law to non-Muslims, during this time co-habitation was legalised and a legal process was made for children born outside of wedlock.[34][35]
Blasphemy of all faiths is illegal since 2015, with punishments of a 5-year prison term, a fine of five-hundred thousand to 2 million dirhams, or both; expats involved in insulting Islam are liable for deportation.[36][37]
A federal law in the UAE prohibits swearing in WhatsApp and penalises swearing by a $68,061 fine and imprisonment;[38] expats are penalised by deportation.[b] In July 2015, an Australian expat was deported for swearing on Facebook.[42]
In 2020, the penalties for homosexuality and extra-marital sex were altered. Such activity continues to be illegal and are punishable by a minimum of 6-months in prison; they are not prosecuted, however, "except on the basis of a complaint from the husband or [male] legal guardian". Any penalty may be suspended if the complaint is waived.[43] Even before this change, there were no known arrests or prosecutions for same-sex sexual activity since at least 2015,[44] when, under the previous UAE penal code, the penalty for same-sex or extra-marital sexual activity was between one and fifteen years in prison, and prosecution could proceed at the discretion of police or other authorities.[45] While Sharia stipulates that death is a penalty for sodomy and adultery, so that theoretically they could be capital offences as zina crimes under UAE's adherence to Sharia. However, the penal code did not prescribe capital punishment for these offences and there has never been a documented case of the death penalty being applied for these offences in the UAE.[45]
The UAE runs secret prisons in Yemen where prisoners are forcibly disappeared and tortured.[46]
In numerous instances, the UAE government has tortured people in custody (especially expatriate residents and political dissidents).[c] UAE authorities are known to be using torture as a means to extract forced confessions of guilt.[49][50]
UAE escaped the Arab Spring; however, more than 100 Emirati activists were jailed and tortured because they sought reforms.[51][52] Since 2011, the UAE government has increasingly carried out forced disappearances.[d] Many foreign nationals and Emirati citizens have been arrested and abducted by the state, the UAE government denies these people are being held (to conceal their whereabouts), placing these people outside the protection of the law.[8][51][56]
Human Rights Watch considers the reports of forced disappearance and torture in the UAE a grave concern.[9] The Arab Organisation of Human Rights has obtained the testimonies of many defendants. In its report on "forced disappearance and torture in the UAE", victims give evidence that they had been kidnapped, tortured and abused in detention centres.[8][2][56] Noted in the report are sixteen different methods of torture applied in the UAE, including severe beatings, threats of electrocution, and denying access to medical care.[8][56] According to Amnesty International's 2016 annual report on Human Rights, enforced disappearance has been widely practiced in the UAE against citizens and foreign nationals. The international organisation said UAE government has forcibly disappeared dozens of people for months in secret and unacknowledged detention for interrogation, an action commonly undertaken against people who criticised the government or its allies.
The UAE's human rights issues were discussed by activists and experts present at a conference titled "Arbitrary Detention in the US: Addressing the Crisis of Civil Society Suppression", held during the 57th session of the UN Human Rights Council. Testimonies from individuals affected by the country's legal framework were also featured at the event, where the main focus was on the challenges faced by dissenters and civil society advocates.[57] Concerns around the UAE's human rights records, particularly regarding "UAE84" mass trial and the suppression of dissent, were previously raised by Volker Türk. He also highlighted the cases of Mohamed al-Mansoori and Ahmed Mansoor, the activists detained for cooperating with the UN. In May 2023, the UAE was called to provide freedom to human rights defenders, but it declined suggestions to safeguard activists and journalists from reprisals.[58]
The UAE's State Security Apparatus (SSA) was established on 10 June 1947. It is a top-level UAE's authority on state security matters, which has been mainly involved in the suppression of public dissent in the Emirates. SSA has been continuously violating human rights, including forced disappearances, torture and unlawful detention. On the 50th anniversary of the SSA; MENA Rights Group, the Emirates Detainees Advocacy Center, and Human Rights Watch condemned the SSA's violations, and called upon the Emirates to stop its brutal human rights abuses and crackdown on peaceful dissent through false terrorism charges. The human rights organizations appealed the Emirati authorities to align the SSA's legal framework and operations with international human rights standards. They also urged the UAE to hold perpetrators accountable and provide remedies for victims.[59]
In April 2009, a video tape of torture was smuggled out of the UAE showed Sheikh Issa bin Zayed Al Nahyan torturing a man with whips, electric cattle prods, and wooden planks with protruding nails, and running him over repeatedly with a car.[60] In December 2009, Issa appeared in court and proclaimed his innocence.[61] The trial ended on 10 January 2010, when Issa was cleared of the torture of Mohammed Shah Poor.[62] Human Rights Watch criticised the trial and called on the government to establish an independent body to investigate allegations of abuse by UAE security personnel and other persons of authority.[63] The US State Department expressed concern over the verdict and said all members of Emirati society "must stand equal before the law" and called for a careful review of the decision to ensure that the demands of justice are fully met in this case.[64]
Major General Ahmed Naser Al-Raisi, a senior UAE policeman, became a controversial candidate for the presidency of international policing body Interpol in 2021. He is alleged to have been ultimately responsible for and complicit in the tortured of detainees in the UAE. He is also accused of direct involvement in the torture two British men, Matthew Hedges and Ali Issa Ahmad.[65] Despite international opposition, Al-Raisi was elected president of Interpol on 25 November 2021 for a four-year term.[66]
On 1 October 2021, lawyers submitted a complaint to the French Prosecutor in Paris against Al-Raisi for the unlawful detention and torture of Hedges and Ahmad. The complaint against him was made under the principle of universal jurisdiction, allowing the French authorities to investigate and arrest foreign nationals for certain crimes even if they occurred outside France.[67]
On 4 November 2024, Ahmed Naser Al-Raisi visited Glasgow for annual general assembly of the Interpol. Ahead of his arrival, Mathew Hedges and Ali Issa Ahmad, who faced extensive torture in the UAE, called for the Scottish police to follow up the case and open a separate investigation. Al-Raisi was responsible of state security services, when the two Britishers were tortured by the Emirati police. Hedges and Ahmad submitted a criminal complaint supported with evidence to the Scottish police against Al-Raisi. The case was filed under the universal jurisdiction, enabling the country to directly take legal action against Al-Raisi.[68]
A British businessman, Ryan Cornelius was being held arbitrarily by the authorities in the UAE since 2008. He was arrested from Dubai airport and was detained after some complicated business dealings with influential Emiratis. In 2010, he was sentenced to 10 years in prison after charges of fraud. The UAE authorities sentenced him for additional 20 years, two months before his release date in 2018. Cornelius contracted tuberculosis in detention. He was subjected to human rights abuses, including aggressive interrogations in absence of legal representative and prolonged solitary confinement. In June 2022, the UN officials highlighted the case and called on the UAE to immediately release Cornelius.[69]
In 2012, Dubai police subjected three British citizens to beatings and electric shocks after arresting them on drugs charges.[70] The British Prime Minister, David Cameron, expressed "concern" over the case and raised it with the UAE President, Sheikh Khalifa bin Zayed Al Nahyan, during his 2013 state visit to Britain.[71] The three men were pardoned under a Ramadan amnesty three months into their jail terms and deported in July 2013, a year after first being detained.[72]
In 2013, 94 Emirati activists were held in secret detention centres and put on trial for allegedly attempting to overthrow the government.[73] Human rights organisations have spoken out against the secrecy of the trial. An Emirati, whose father is among the defendants, was arrested for tweeting about the trial. In April 2013, he was sentenced to 10 months in jail.[74]
Foreign nationals subjected to forced disappearance include two Libyans[75] and two Qataris.[56][76] Amnesty reported that the Qatari men had been abducted by the UAE government that withheld information about the men's fate from their families.[56][76] Among the foreign nationals detained, imprisoned and expelled is Iyad El-Baghdadi, a popular blogger and Twitter personality.[56] He was arrested by UAE authorities, detained, imprisoned and then expelled from the country. Despite his lifetime residence in the UAE, as a Palestinian citizen, El-Baghdadi had no recourse to contest this order.[56] He could not be deported back to the Palestinian territories, therefore he was deported to Malaysia.[56]
In February 2015, Human Rights Watch documented a case in which three Emirati sisters, Asma, Mariam, and Al Yazzyah al-Suweidi, were forcibly disappeared by Emirates authorities. They released them without charge after they had spent three months in detention, incommunicado. The three sisters were arrested after posting comments criticising the government for arresting their brother Dr. Issa al-Suweidi. Similarly, Abdulrahman Bin Sobeih was subjected to enforced disappearance for three months by UAE authorities.
In August 2015, Emirati academic and economist Nasser bin Ghaith was forcibly disappeared by the authorities, being held incommunicado for more than 10 months. He was subjected to torture and ill-treatment.[77] He was arrested after posting comments on social media in which he criticised the mass killing of Rab'a protesters in Cairo in 2013.[78] He remains in prison as of 2025[update], after receiving a life sentence for expressing the critical views.[79]
In November 2017, Abu Dhabi security forces arrested two journalists covering the opening of the Louvre Abu Dhabi museum for Swiss public broadcaster RTS. The journalists were held for more than 50 hours, with no ability to communicate with the outside world. According to RTS, the journalists were interrogated for up to nine hours at a time and were blindfolded as they were shuttled between different locations. Their camera, computers, hard drives and other material were confiscated.[80]
In March 2018, an Emirati princess Latifa bint Mohammed Al Maktoum II, daughter of Sheikh Mohammed bin Rashid al-Maktoum, was seized by commandos from a yacht away from the Indian coast, after she fled from UAE.[81] A BBC documentary reported how the princess planned her escape from her residential palace.[82] In a video recorded by Latifa prior to her escape, she claimed to have tried escaping from the UAE previously. However, she was captured at the border and jailed for three years; beaten and tortured. In December, a statement released by her family quoted that the princess was "safe" at her home. Since early March, the whereabouts of the princess were unknown.[83] On 5 March 2020, a UK family court's 34-page ruling confirmed that the Sheikhas Latifa and Shamsa bint Mohammed al-Maktoum had been abducted and forcibly detained by their father and the Dubai ruler, Mohammed bin Rashid al-Maktoum.[84]
In June 2020, it was reported that the UAE had been holding captive a Turkish aid worker Mehmet Ali Ozturk, since 2018. Reportedly, Ali Ozturk has been detained on frivolous grounds and was tortured inside UAE's prison. He was arrested in Dubai, where he, alongside his wife Emine Ozturk, was participating in Dubai's food festival. "He lost 25kg after the torture they subjected him to, from denailing to strappado. They would do these things when he refused to take part in a video accusing Erdogan of some crimes", his wife was quoted as saying.[85] In 2017, a Yemen human rights activist Huda Al-Sarari exposed a UAE secret detention facility in Yemen where thousands of Emiratis were held and tortured. Al-Sarari was forced into exile.[86]
On 9 July 2020, reports claimed that the UAE authorities declined requests from human rights organisations to provide information about an Omani man, Abdullah al-Shaamsi, who was sentenced to life imprisonment in May 2020 after a seriously unfair trial. Al-Shaamsi was arrested in 2018 at the age of 19, while attending high school in the UAE. The security forces subjected him to a sustained period of detention without communication, solitary confinement and torture, leaving him with kidney cancer and depression. Despite his health conditions, he was being held in an overcrowded prison known for unsanitary conditions and lack of access to adequate health care, during the COVID-19 crisis.[87]
In March 2021, the US State Department released a report on the human rights practices in the UAE. It highlighted that while the disappearance cases and unlawful killings were not reported to media throughout 2020, there were cases of torture, arbitrary detentions, abuses, threats of rape, and beatings. The department reported the conditions of Emirati prisons, which remained overcrowded, had poor sanitary conditions and provided no easy access to medical care, during the COVID-19 pandemic. The UAE prisons were described as extremely torturous, where prisoners were discriminated against and abused using various ways. The detainees were in most cases not provided with the details of their case for months, while many receive the charges written in Arabic with no translation and forced to sign such documents.[88]
In July 2021, a private letter written by prominent human rights defender Ahmed Mansoor, detailing his mistreatment in detention and grossly unfair trial, was published by a London-based Arabic news site. Despite having ratified the UN Convention against Torture and Other Cruel, Inhuman or Degrading Treatment or Punishment in 2012, the UAE grossly violated the act's obligations by holding Mansoor in isolation for at least four years, amounting to physical and mental torture.[89] On 7 January 2022, issued an update on the case, reporting that Emirati authorities further penalised Mansoor after the prison letter was published  The UAE authorities held Mansoor largely incommunicado and denied him access to critical medical care. The UAE violated Mansoor's rights for many years with arbitrary arrest and detention, death threats, physical assault, government surveillance, and inhumane treatment in custody.[90]
In September 2021, the UAE sentenced an activist from Syria, Abdul Rahman Al-Nahhas, to ten years in prison. Founder of the Insan Watch Organization, the human rights activist was charged by the Public Prosecutor of terrorism for his alleged membership in a terrorist organization as he was linked with the Switzerland-based Al-Karama Organisation for Human Rights. Al-Nahhas was also charged for insulting the prestige of the state by approaching the French embassy seeking political asylum. He was arrested at the end of 2019 and was forcibly disappeared by the UAE authorities until the commencement of his trial in January 2021. During his detention, Al-Nahhas was threatened, tortured, and was not allowed to contact his family.[91]
Between 2015 and 2017, the United States sent a number of detainees of various nationalities, some only suspects, from Guantanamo Bay detention camp to the UAE. According to US officials, the agreement reached with UAE to accept these prisoners did not include their continued imprisonment. By 2020, nineteen remained in detention in often undisclosed locations, in harsh conditions, and with little access to outside communication. In at least one case, a detainee was sent to a facility reported by the Associated Press to be "a notorious prison rife with torture". One Afghani detainee was returned home after more than three years in UAE prisons, dying four months after his release. He recounted harsh, inhumane treatment in UAE, describing it as "mental torture".[92]
Since October 2020, UAE authorities have, on the basis of religious background, detained, at times incommunicado, at least four Pakistani men and deported at least six others. Reports of UAE authorities arbitrarily targeting Shia residents, whether Lebanese, Iraqi, Afghan, Pakistani, or otherwise, often emerge at times of increased regional tensions.[93]
A British football coach, Billy Hood was detained by the Dubai authorities and sentenced to ten years in prison over CBD vape oil left in his car by a visiting friend. Hood suffered rough prison conditions, where he was isolated in a tiny cell. During February 2022 visit of Prince William to the UAE for Dubai Expo 2020, Hood was "violently attacked" by four Emirati prison guards after he punched the wall of his jail cell out of "frustration". The assault against Billy Hood was completely opposite to Prince William's efforts to promote ties between the two nations.[94]
On 28 January 2022, the Emirati authorities arrested Steve Long, a British man from Stockport, for telling airline staff he feared there was a bomb on the plane he was about to board to return home, during an apparent psychotic breakdown. Long was arrested and taken to a local hospital, where he was diagnosed with acute psychosis and delirium; the UAE's medical board determined he was not of sound mind at the time. A court in Abu Dhabi did not accept the medical evidence and Long was transferred to prison from hospital. He was ordered to pay a fine of £100,000 or he would have to serve 13 years in prison in lieu. The airline did not press any charges over the incident and, when later informed of the medical evidence, requested Long's release. His family appealed the court verdict, but it was rejected, despite two medical reports saying Long lacked capacity at the time and was not responsible for his actions. Family members believed that Long, an ambulance paramedic, and former soldier who had served tours in Iraq and Bosnia, working closely with bomb disposal units, was affected by a drone strike in Abu Dhabi in January 2022, triggering his mental collapse. Long was released more than two months later, only when the fine was paid by family who had organised a GoFundMe campaign to raise the funds.[95]
On 17 July 2022, the UAE authorities arrested US citizen Asim Ghafoor, the former lawyer of murdered journalist Jamal Khashoggi, and sentenced him to three years in prison. Ghafoor is also a co-founder and board member of human rights group Democracy for the Arab World Now (DAWN). The Abu Dhabi Money Laundering Court convicted Ghafoor of committing crimes of tax evasion and money laundering and also ordered him to pay a fine of more than $800,000 stemming from his conviction, in absentia. Critics and human rights defenders believe that Ghafoor's detention is politically motivated revenge for his association with Khashoggi and DAWN, which has highlighted UAE human rights abuses and war crimes. Ghafoor has stated that he had no knowledge of any legal matter against him and no reason to believe he was involved in any legal dispute in the UAE.[96]
In August 2022, a Nigerian woman from Jos, Dinchi Lar, was detained at the airport when attempting to exit Dubai following a holiday with friends. She was arrested and charged with "breaching the privacy of government employees", stemming from an incident during Lar's arrival at Dubai airport, where she and her travelling companions were kept by immigration officials in a room for six hours without explanation. Lar recorded video of staff yelling at the detained Nigerian passengers, uploading "a few seconds" of the video to her Twitter account. While Lar was holidaying in Dubai, the video was viewed and shared thousands of times by Nigerians. The action, which Dubai authorities charged as "sharing a video of government employees online without their consent", resulted in a sentence of one year in prison, later reduced to three months on appeal, following social media campaigns and representations from Lar's parliamentarian.[97]
An American social media influencer, Tierra Young Allen was trapped in Dubai for months, after she was arrested over possible charges of "shouting" at an employee of a car rental agency she hired a car from. Allen travelled to the UAE in April 2023. The incident occurred when she went back to get her personal items from the agency, after the car met with an accident. However, she was asked to pay "an undisclosed amount of money". The car agent allegedly got aggressive and started "screaming at her". Allen shouted back at the employee, and was also followed out of the building. Shouting is technically a crime under the Emirati laws, which strictly govern speech. Speaking loudly, raising middle finger in a dispute, or swearing in public are all considered offensive crimes. The UAE authorities held Allen's passport. Radha Stirling, who was helping Allen to depart Dubai, approached the Texas lawmakers like Senator Ted Cruz and Sheila Jackson Lee to work with the US consulate in Dubai to stop her imprisonment.[98][99]
In August 2024, two brothers from Ohio, Joseph and Josua Lorenzo, were sentenced to four months in Dubai prison over allegations of alcohol consumption, resisting arrest, assaulting an officer and damaging a patrol vehicle. Joseph Lopez is an Air Force veteran turned influencer. The brothers claimed they were drugged during a yacht party, for which they were invited by local residents. Radha Stirling said the brothers were clearly "targeted by scammers", citing they were made to pay large dinner bills. She criticized the UAE's behaviour towards tourist, saying it is not at all the "safe tourist destination" as it markets itself. The Lopez brothers sought help from U.S. lawmakers, including Republican Ohio Senator JD Vance.[100]
In October 2024, Charles Wimberly, a US Navy veteran from Georgia, faced 3+ years of imprisonment in Dubai prison for carrying prescription medication. He travelled to the UAE on 21 September 2024 and carried CBD oil and Ibuprofen with prescription due to back injury and PTSD. He was arrested from Dubai International Airport over allegation of "trafficking" his own prescription. A human rights advocate, Radha Sterling said it was "every tourist's nightmare". Wimberly was later released on bail but denied permission to leave the Emirates.[101]
In 2019 media attention focused on the ill-treatment of a 42-year-old Emirati woman during her imprisonment in the UAE. While raising funds for Syrian refugees, Alia Abdel Nour was arrested in 2015 by the UAE authorities on accusations of funding terrorism. Sentenced to 10 years, she had been subjected to harsh conditions of solitary confinement, with no access to ventilation, toilet facilities, mattress, blankets, proper food or medicine. Despite being diagnosed with cancer – shortly after her arrest – she did not receive any medical treatment. Emirati authorities claim that Nour herself declined the medical treatment, while her family claims she was forced to sign documents that forbid her access to the treatment.[102][103]
On 4 May 2019, Alia Abdel Nour died in the UAE prison following prolonged mistreatment and denial of medical care by the Emirati authorities. Since her arrest, her hands and feet were shackled to her hospital bed for long periods of time. The UAE authorities ignored requests by the international rights groups, European parliamentarians, and United Nations experts to release her on the grounds of her deteriorating health.[104]
In January 2019, the UAE police detained 26-year-old Ali Issa Ahmad for reportedly wearing a T-shirt with Qatar's flag on it after the Qatar vs Iraq AFC Asian Cup match in Abu Dhabi.[105] Ahmad complains that FIFA "failed to protect" his human rights. Pictures of scars on Ahmad's body from the torture sustained during detention were released by the BBC. The victim complained about racial discrimination and of being stabbed and deprived of food and water while inside the prison. Complaints have been registered against FIFA as well as directed to UAE authorities through the Foreign and Commonwealth Office (FCO) and the UN Human Rights Council.[106] According to UAE authorities, the police took Ahmed to a hospital to be examined for signs of abuse, which he complained of to the police — as is customary in cases of assault in the UAE. A medical report revealed that his injuries were inconsistent with the account of events he gave to police, and that his wounds were self-inflicted. UAE embassy in Britain denied the news allegations that he was arrested for wearing a Qatari shirt, stating "He was categorically not arrested for wearing a Qatar football shirt". Ahmed was charged for wasting police time and filing a false report, which is an illegal act. During the AFC Asian Cup, fans were seen wearing the Qatari football shirt and waving Qatari flags without any instances of arrest.[107]
On 10 June 2020, Human Rights Watch urged UAE authorities to take care of the mental and physical health of prisoners due to the ongoing COVID-19 outbreak in three detention facilities.[108]
Dinchi Lar, a Nigerian tourist who was imprisoned in 2022 at Al Awir for three months, described conditions as "very demoralising, ... humiliating, ... traumatising". There was extreme overcrowding, with ten prisoners vying for three bunk beds, leaving many with no place to sleep. She reported being subject to discriminatory practices from prison guards. A racial hierarchy existed within the prison, with Lar noting differential treatment according to prisoners' race: For example, Africans being given less access to services such as medical treatment, while Arab and, especially, the "British", were more likely to be provided with such benefits.[97]
In the UAE, there is no formal commitment to free speech.[109] It is not permitted to be in any way critical of the government, government officials, police, or the royal families. Any attempt to form a union in public and protest against any issue, will be met with severe action.[110] Free speech restrictions apply to critics, as well as to ordinary social media users.
On 16 November 2007 Tecom stopped broadcast of two major Pakistani satellite news channels, uplinked from Dubai Media City, which was initially marketed by Tecom under the tagline "Freedom to Create". The Dubai government had ordered Tecom to shut down the popular independent Pakistani news channels Geo News and ARY One World on the demand of Pakistan's military regime led by General Pervez Musharraf. This was implemented by Du Samacom disabling their SDI and ASI streams. Later, policy makers in Dubai permitted these channels to air their entertainment programs, but news, current affairs and political analysis were forbidden. Although subsequently the conditions were removed, marked differences have since been observed in their coverage. This incident has had a serious impact on all organisations in the media city with Geo TV and ARY OneWorld considering relocation.[111][112][113]
During the 2017 Qatar diplomatic crisis, Hamad Saif al-Shamsi, the Attorney-General of the United Arab Emirates announced on 7 June that publishing expressions of sympathy towards Qatar through social media, or in any type of written, visual or verbal form is considered illegal under UAE's Federal Penal Code and the Federal law on Combating Information Technology Crimes. Violators face between 3 and 15 years imprisonment, a fine of up to 500,000 Emirati dirhams ($136,000) or both.[114][115]
In December 2019, US intelligence identified that the UAE, which banned VoIP options on several applications, developed its own messaging and video calling app ToTok and has been using it as a spying tool. The country had forbidden the calling options on applications like WhatsApp, FaceTime and Skype, prompting suspicion over the self-developed app.[116]
In April 2020, authorities in the United Arab Emirates introduced criminal penalties for the spread of misinformation and rumours related to the COVID-19 pandemic in the United Arab Emirates.[117]
The UAE launched repressive campaigns in 2024 to suppress voices opposing its policies and legal decisions. Both Emirati nationals and expats, who opposed Israeli crimes or the Abraham Accords over their social media posts, were arrested and summoned. A Jordanian national of Palestinian descent and a contract employee in Dubai, "K. H." was summoned for interrogation on 10 April 2024 by Abu Dhabi State Security Service over a Facebook post condemning Israel's genocidal assault on Gaza. He was imprisoned for three days, denied legal representation, and forced to leave the UAE. On 25 March 2024, an Egyptian national was also interrogated by the UAE security officials over social media posts, in which he criticized Arab and Islamic negligence to the famine eruption in the Gaza during Ramadan, and called for an end to normalization of ties with Israel. After being questioned for several hours, he was fired from his job, and was given 48 hours to leave the Emirates.[118]
The Abu Dhabi Federal Court of Appeal sentenced 43 activists to life imprisonment, after convicting them of creating and managing a terrorist organisation. The Emirati authorities imprisoned ten other defendants for 10 to 15 years on the charges of money laundering and "co-operating with al-Islah". The court acquitted one defendant and dismissed 24 cases. Nasser bin Ghaith, Abdulsalam Darwish al-Marzouqi and Sultan Bin Kayed al-Qasimi were among those who were sentenced to life imprisonment, while Ahmed Mansoor was also among the defendants. Devin Kenney, Gulf Researcher for Amnesty International, called upon the UAE to "urgently revoke this unlawful verdict" and immediately release those sentenced. He called the trial a shameless parody of justice, which violated multiple fundamental principles of law. GCHR founder, Khalid Ibrahim also said that "it is a real tragedy" that the human rights defenders will be kept in prison for decades.[119]
In July 2024, the UAE sentenced 57 Bangladeshi individuals to 10-year prison sentences or life sentences for protesting against their home government in Bangladesh.[120]
The UAE's history of not allowing human rights campaigners, who acted against the Emirates, was questioned. In 2022, an investigative journalist, Maggie Michael, who reported on Emirati secret detention centres in Yemen, was blocked by the UAE to join the UN committee focused on Yemen as a regional expert. A Canadian researched Shawn Blore, who published research on the UAE's illicit gold trade, was also barred from serving as a UN expert on a committee working around the exploitation of minerals in DRC. In the same year, the UAE authorities banned a British national, Dinesh Mahtani, from continuing his trip to the Emirates over security concerns. Mahtani was on the UN panel of experts that were monitoring sanctions on Al-Shabaab, which was being funded through the coal shipments from Somalia to the Emirates.[121] The UAE was alleged of using similar tactics to deny its support to the RSF in Sudan war, where it was supplying weapons and drones in the name of humanitarian aid.[122][123][124]
Human Rights Watch reported that on 15 March 2017, Tayseer Najjar, a Jordanian journalist, was sentenced to a three-year prison term and a fine of 500,000 UAE Dirhams by Abu Dhabi Federal Appeals Court. He had been charged with insulting the state's symbols and criticising Egypt, Israel and Gulf countries through comments he made on Facebook during Israeli military operations in Gaza in 2014, before he moved to the UAE. Ten days after preventing his travel to Jordan for a visit to his wife and children on 3 December 2015, UAE authorities summoned al-Najjar to a police station in Abu Dhabi and detained him. They also blocked contact with a lawyer for more than a year before bringing him to trial in January 2017.[125] Najjar was set to be released on 13 December 2018, after completing a three-year prison sentence. However, his sentence was extended for another six months as he failed to pay the substantial fine. Human Rights Watch and Reporters Without Borders urged Anwar Gargash, the UAE minister of state for foreign affairs, for an immediate release of the journalist. Sarah Leah Whitson, Director of Human Rights Watch said, "If the UAE were truly committed to its rhetoric of tolerance, it would not have ripped Najjar away from his wife and children for years-old innocuous Facebook posts."[126]
For criticising Jordanian authorities and state corruption, the Jordanian activist Ahmed al-Atoum was arrested in Аbu Dhаbi in May 2020, detained incommunicado, and held in solitary confinement for four months before being sentenced to prison in October 2020. The court convicted him solely on the basis of his Facebook posts criticising the Jordanian royal family and government, handing al-Atoum a 10-year prison sentence. Calls from human rights groups for al-Atoum's immediate release, including from the UN Working Group on Arbitrary Detention, went unheeded and he remained in al-Wathba prison in the UАЕ, until his mother's death on 10 February 2024.[127]
The UAE requested extradition of an Egyptian activist Abdulrahman al-Qaradawi, who was arrested in Lebanon in December 2024 on his return from Syria after celebrating the fall of Bashar al-Assad's regime. Al-Qaradawi is a vocal critic of the UAE and son of late spiritual leader of the Muslim Brotherhood, Yusuf al-Qaradawi. His arrest was based on an Egyptian arrest warrant, following a ruling that sentenced him in absentia to five years in prison on charged of "opposing the state and inciting terrorism". The UAE requested his extradition following a video in which al-Qaradawi called for the ousting of "shameful Arab regimes" and "Zionist Arabs" like "the UAE, Saudi Arabia and Egypt". The Emirates had no extradition treaty with Lebanon. Al-Qaradawi's attorney, Mohammad Sablouh gave account of some "suspicious" activities around his client's case, and said his extradition to the UAE was unlawful. There were concerns that if extradited to the Emirates, al-Qaradawi could be subjected to torture.[e] Amnesty International also urged the Lebanese authorities to reject al-Qaradawi's extradition request by the Emirates.[132]
On 16 September 2021, in a strongly worded resolution, European Union legislators condemned alleged human rights violations in the UAE and urged the government to free several prominent human rights activists and other "peaceful dissidents" imprisoned in the country.[133]
On 11 May 2020, a US-based gulf rights group, Americans for Democracy and Human Rights in Bahrain in its report said that "impunity in the United Arab Emirates is endemic." Their study documented tactics used by Emirati authorities to stifle dissidents, besides revealing use of torture against those recognised as imminent threat to national security. "This 'threat' most commonly includes human rights defenders, political opposition, religious figures, and journalists," a statement from the report read.[134] In June 2020, International Campaign for Freedom in the UAE (ICFUAE) informed that the UAE continues to detain human rights activists who demanded democratic reform in the country. The campaign group stated Fahad al-Hijri, Abdallah Ali Alhajery, Oman Alharethy and Mahmoud Alhoseny, all have completed their sentences, but continued to remain imprisoned.[135]
On 3 July 2024, HRW called upon the UAE allies, including the US, UK, and EU, to voice concerns regarding the unfair mass trial of at least 84 political dissidents and human rights defenders. They also urged the observers to attend the verdict session on 10 July 2024. Joey Shea, UAE researcher at HRW, called it the second largest unfair mass trial in the UAE's history, for which the international communities failed to raise concerns. She appealed the Emirati allies to urge for the activists’ immediate and unconditional release, meeting with their families, to send trial monitors, and to publicly condemn the unfair trial. She said the Emirates had been circumventing backlash for its human rights records by using its economic and security relationships.[136]
Human rights advocates were pushing the UK government to attain guarantee from the UAE that Britishers attending the COP28 summit should not face arrests if they protest. They wrote a letter to James Cleverly calling for him to ask for an undertaking from the UAE assuring the security and rights of the UK citizens travelling to the UAE for COP28. The human right activist said dissidents are often arrested in the country, and protests against the Emirati government are crushed. In April 2023, COP28 organizers asked the speakers attending the summit to not protest against the UAE laws.[137]
In August 2023, Human Rights Watch urged Emirati authorities to release all those detained unlawfully before the COP28 event. The UAE continued to detain people who had already completed their prison sentences, including the 55 dissidents, lawyers and others convicted in mass trial of the "UAE94" case. The "UAE94" detainees covered a majority of those held in prison beyond their sentences. The UAE was called to release the imprisoned human rights activist, Ahmed Mansoor, who was being held in prolonged solitary confinement since his arrest in March 2017. NGOs such as Human Rights Watch demanded the immediate release of other rights defenders, Dr Naseer bin Ghait, Amina Al Abdouli and Maryam Al Balushi. HRW also called on the UAE to end the human rights violations, including monitoring through sophisticated surveillance, use of repressive laws to imprison human rights defenders, denying the right to freedom of expression, denying right to peaceful assembly, and denying the right to form union to the migrant workers.[138]
In March 2017, UAE's prominent economist, academic and human rights defender Dr Nasser bin Ghaith was arrested and imprisoned for 10 years, for his comments on Twitter related to the treatment he received during his previous arrest. About his previous detention, Amnesty International stated that the trial, where he and four other Emirates prosecuted on charges of publicly insulting the countries' leaders over comments posted online, was not a fair one. He was forcibly disappeared, held in secret detention for months, and subjected to beatings and deliberate sleep deprivation. Nasser bin Ghaith was sentenced to 10 years in jail by the Federal Appeal Court in Abu Dhabi. The later arrest was for again posting false information on Twitter about UAE leaders and their policies. Amnesty International condemned and criticised the arrest, asking for his immediate release.[139] Ghaith went on a hunger strike in October 2018; his health has been deteriorating since then[needs update]. He was denied access to a lawyer during his trial period.[140]
Also in 2017, prominent human rights defender Ahmed Mansoor was arrested at 3:15am by ten male and two female uniformed security officials according to an Amnesty International report. The security personnel raided the family's apartment, carried out a lengthy room-by-room search, including of the children's bedroom, and confiscated electronic devices. Mansoor was detained for the peaceful expression of conscientiously held belief.[141]
According to Amnesty International, Israeli company NSO Group's Pegasus spyware was used to target human rights activist Ahmed Mansoor.[142][143] citizen Lab's August 2016 report, "The Million Dollar Dissident", documents the attempts made to infect Mansoor's phone with Pegasus spyware.[144] On 20 March 2020, Amnesty International and the Gulf Centre for Human Rights wrote a joint-letter and called for the immediate and unconditional release of rights activist Ahmed Mansoor. The groups also called the UAE as an "incubator of tolerance".[145]
As the international committee was about to get together in Dubai for the COP28 climate summit, Amnesty International's Deputy Director for the Middle East and North Africa, Aya Majzoub, urged the world leaders to pressurize the UAE to release Mansoor, who remained unjustly persecuted and detained for 6 years. The rights group said the global community should publicly condemn the injustice and push the UAE to immediately release Mansoor, ahead of COP28.[146]
Hussain al-Najjar has served an 11-year prison sentence; he is one of a number of prisoners convicted in 2013 following the grossly unfair mass trial of 94 government critics and reform advocates. Accordingly, on 17 March 2014, the activist Osama al-Najjar who is a 28-year-old son of Hussain, was sentenced to three years in prison after sending tweets to the Minister of Interior expressing concern about his father who had been ill-treated in jail. During the detention, he was denied access to a lawyer for over six months and held in solitary confinement at a secret detention facility for four days after his arrest.[147]
In 2013, the UAE arrested five men, including an American citizen for making a satirical video. The American, who had moved to Dubai for work, was sentenced to a year in prison.[148][149]
Andrew Ross, a professor at New York University was not allowed to enter the UAE (where the university has a campus), after he had commented on the treatment of workers who built the campus there. Airline staff at the airport informed him that the UAE authorities told them that they will refuse him entry.[150]
Alaa al-Siddiq, a UAE dissident and critic of the Kingdom of Saudi Arabia, allegedly died in a car crash in Oxfordshire, South East England. However, campaigners and a close colleague of Siddiq's have demanded the British police thoroughly investigate the incident, claiming that the activist's "life was at risk all the time", Khalid Ibrahim, executive-director of the GCHR. Al-Siddiq was a human rights activist, who had been fighting for the release of her father Mohammad al-Siddiq, also an activist, detained since 2013. According to Mr Ibrahim, threat to the life of Al Siddiq heightened since her commencing work for the Saudi human rights firm, ALQST. Other passengers in the BMW car, two adults and a child, received injuries and were taken to the hospital for treatment.[151]
In August 2022, a Nigerian woman, Dinchi Lar, was detained at the airport when attempting to exit Dubai following a holiday there with friends. She was arrested and charged with "breaching the privacy of government employees", stemming from an incident during Lar's arrival at Dubai airport, where she and her travelling companions were kept by immigration officials in a room for six hours without explanation. Lar used her phone to record staff yelling at the detained Nigerian passengers, uploading "a few seconds" of the video to her Twitter account. Having eventually been permitted entry following the delay, Lar continued with her holiday plans in Dubai. At the same time, her uploaded video was viewed and shared thousands of times by Nigerians. The action, which Dubai authorities charged as "sharing a video of government employees online without their consent", resulted in a sentence of one year in prison, later reduced to three months on appeal.[97]
In 2012, a cybercrime decree was issued, imposing severe restrictions on freedom of speech in social networking, blogs, text messages and emails. The law outlawed criticism of senior officials and demands for political reform.[152] The law stipulates an imprisonment and a fine of up to 1,000,000 dirhams for publishing information which is deemed to be critical towards the state.[152]
In 2015, a man was detained for commenting on his employer's Facebook page after a disagreement with his employer, even though the posts were made while the man was in the United States. Police in Abu Dhabi contacted him after he came back to the UAE and asked him to meet officers at a police station, where he was later detained.[153]
Secret Dubai was an independent blog in Dubai, from 2002 until 2010. It generated a significant following in the Middle East Blogosphere until the UAE's Telecoms Regulatory Authority (TRA) in the UAE blocked the website.
In July 2016, Americans for Democracy and Human Rights in Bahrain released a report accusing UAE government of enacting further laws to restrict the freedom of political and social expression. According to the organisation, Federal Law No. 12 of 2016 inhibits social and political resistance, by constraining an individual's right to privacy and the right to freedom of expression. ADHR also said counter terrorism Laws in UAE are used to legalise arbitrary arrests, detainment, prosecution and imprisonment of peaceful protestors and government critics.[154]
In 2018, Internet service providers in UAE blocked all VoIP apps, but permitting "government-approved VoIP apps (C'ME and BOTIM)".[155][156] In opposition, a petition on Change.org garnered over 5000 signatures, in response to which the website was blocked in UAE.[157]
In July 2023, a social media influencer, Hamdan Al Rind, was detained by the UAE over a TikTok video. Rind, also referred as "Car Expert", was seen tossing stacks of cash at the employees inside a luxury car showroom in the TikTok video, and offered to buy the most expensive car. The video made fun of Dubai's luxurious lifestyle, but the video was alleged of promoting "a wrong and offensive mental image of Emirati citizens and ridicules them". Rind was alleged of "abusing the internet" through a "propaganda that stirs up the public opinion and harms the public interest". The UAE's cybercrime laws extensively restrict expression and assembly, and criminalize any form of opposition of the country and its leaders.[158]
In the 2010s, a large number of Shia Muslim expatriates were deported from the UAE.[159][160][161] Lebanese Shia families have been deported for their alleged sympathy for Hezbollah.[f] According to some organisations, more than 4,000 Shia expats have been deported from the UAE[168][169]
Repression and lack of religious freedom in China has led to Uyghur Muslims fleeing the country to take refuge in other parts of the world. However, the diplomatic relations of Beijing have resulted in the abuse and detention of Uyghur Muslims even abroad. The government of UAE was reportedly one of the three Arab nations to have detained and deported Uyghur Muslims living in asylum in Dubai, back to China. The decision received a lot of criticism due to China's poor human rights records and no extradition agreement shared between the two countries.[170]
The United Arab Emirates ratified the Convention on the Elimination of All Forms of Discrimination Against Women (CEDAW) in 2004. This Convention regards violence against women as a form of discrimination and calls on participating governments to put measures in place to combat violence in all forms, be it domestic or public. The UAE regularly participates in and hosts international and GCC conferences on women's issues. The UAE has signed several other international treaties on protecting the rights of women. Among these are the Convention on the Rights of a Child, the Hours of Work (Industry) Convention, the Equal Remuneration Convention, the Conventions Concerning Employment of Women During the Night and the Minimum Age Convention.
The 2015 United Nations Development Programme (UNDP) status report on Millennium Development Goals noted that the state legislations in the UAE do not discriminate on the basis of gender with respect to education, employment or the quality of services provided.[171]
Through several initiatives women in the UAE are playing an increasingly important role in the economy, politics and technology and are viewed by some as leaders of gender equality in the Gulf region.[172]
There is an alternative for women to dissolve their marriage found under article 110 of the Personal Status Code, or khul', however this means a woman relinquishes her right to the mahr – or the dowry she received as part of the marriage contract.[20]: 2101–2102
As to custody of children, women are considered physical guardians, they have the right to custody up to the age of 13 for girls and 10 for boys. But if a woman chooses to remarry she automatically forfeits her right to custody of her children. Current laws gives custody of children to the parent who is capable and appropriate for their upbringing.[citation needed] Under article 71, women who leave their husbands can be ordered to return to their marital home.[citation needed]
In November 2021, a report by The Independent highlighted the lack of fundamental protection of women's rights in the UAE. British politicians — Sir Peter Bottomley, Debbie Abrahams and Helena Kennedy — generated a report based on the testimony of British women who experienced the UAE legal system. On the other side of the many reforms and PR experts laundering efforts, the Emirati laws still leave women vulnerable to serious abuses of their rights, with little legal recourse.[173][174]
In one case the Federal Court sanctioned a husband's beating of his wife so long as he did not leave physical marks and does so lightly, and in another case a man was ordered to pay a compensation for taking it too far by leaving physical injuries on his beaten wife.[175]
Furthermore, there is growing concern at the UAE's lack of action against domestic violence. Human Rights Watch has documented three cases where it was alleged that police discouraged UK nationals from reporting cases of domestic violence.[176]
On 24 March 2022, a senior British judge concluded that Sheikh Mohammed bin Rashid al-Maktoum inflicted 'exorbitant' domestic abuse on his ex-wife Princess Haya bint Hussein. Princess Haya was awarded the sole responsibility for their children by the High Court in London, in regards to their medical care and schooling. Sheikh Maktoum was barred from taking any decisions about the children's lives and from having any direct contact with them. The court said that Sheikh Maktoum's "coercive and controlling" behaviour could only have been "most harmful to the emotional and psychological welfare" of their children. As per the previous hearings, he spied on his second wife and her legal team by ordering their phones to be hacked using the Pegasus spyware.[177]
In 2024, an Irish woman was detained and had her passport destroyed following a suicide attempt after allegedly experiencing domestic violence.[178] She was charged with attempted suicide and alcohol consumption following the suicide attempt, and temporarily had a travel ban imposed on her, which was lifted following the Irish government's intervention in the case.[179]
Women subjected to sexual assault crimes face several obstacles in seeking justice. They often face zina charges if they report a crime committed against them.[180] Alicia Gali was imprisoned for eight months for sex outside of marriage after reporting an assault by her co-workers.[181] A Norwegian woman was jailed for 16 months for reporting a rape before being pardoned and returned home. However, police said she was imprisoned for filling a false case as she had withdrawn her complaint.[182]
The credibility of the victim's allegations are called into question by the police and Courts will enquire as to whether alcohol was involved, whether the alleged perpetrator was known, and whether the victim resisted the attack.[183]
According to the International Labour Organization there are 146,000 female migrant domestic workers employed in the UAE. In 2014 a Human Rights Watch report spoke to domestic workers who complained about abuse and not being paid due earnings, getting rest periods or days off and excessive workloads as well as documented some cases of psychological, physical and sexual abuse.[184]
The kafala system ties a migrant worker to their employers, who act as their sponsors and makes it difficult for them to change employers. If a domestic worker attempts to leave her sponsor before the end of her contract without her sponsor's approval she will be deemed to have "absconded" which usually results in fines and deportation.[185] however government has changed the law since then.
Federal law No.8 excludes domestic workers from labour laws and the environment in which they work is not regulated by the Ministry of Labour. This means domestic migrant workers have fewer rights than other migrant workers. In 2012 the government stated that the cabinet had approved a bill on domestic workers, however, Human Rights Watch has received no response to requests to obtain a draft.[186]
In January 2016, Amnesty international said UAE government continues to violate rights of migrant workers in the country. The international organisation said workers have been tied with Kafalah system and denied collective bargaining rights. Amnesty also said that women workers from Asia and Africa are explicitly excluded from labour law protections and particularly vulnerable to serious abuses, including forced labour and human trafficking.[77]
In March 2019, the Human Rights Watch reported that eight Lebanese nationals have been detained by the Emirati authorities on the accusations of terrorism charges, without any evidence. The defendants have been held in prolonged solitary confinement in an unknown location for more than a year, without any access to lawyers and family members. The detainees have also been forced to sign on blank papers while some of them were blindfolded.[187][188]
In January 2020, Emirati employers were reported to have been hiring the Indian migrant workers on tourist visas, exploiting them and leaving them helpless with illegal status. Recruiters in the UAE chose visit visas because they are cheaper and easier to obtain than work permits.[189]
Women's employment in the labour market has risen significantly and in the public sector women make up 66% of employees, with 30% of them in high level positions of responsibility.[190][191]
The UAE cabinet is made up of 27.5% women, all of whom play key roles in supporting innovation in the country with results indicating that the UAE is a new hub for women in technology.[192][193] Women represent 50 percent of scientists in STEM programmes at UAE universities and female nationals in the nuclear sector have tripled between 2014 and 2015.[194]
In 2004 the first woman was appointed as minister, Lubna Al Qasimi.[195] In 2006, in the first parliamentary elections, the first woman was elected to the National Federal Council and in 2016, Noura Al Kaabi was named Minister of state for the NFC. Reem Al Hashimi and Shamma Al Mazrui are two other female ministers.
In addition to this the UAE is one of only two countries in the Gulf that permits women to hold the position of a judge or prosecutor, with Bahrain being the first country in the region to elect a female judge in 2006.[196][197]
Abortion in the United Arab Emirates is only legal in cases of rape, incest, if the continuity of the pregnancy would endanger the woman's life, the foetus' deformation is proven, or after approval of a regulatory committee.[198] A 2011 report by Gulf News found that the illegality and inaccessibility of abortion resulted in women purchasing cheap ulcer medication to end unwanted pregnancies, which failed in 15-20% of cases and often resulted in extensive bleeding and, in rare cases, death.[199] There have been several instances of arrests of women procuring abortions, including rape victims.[200][201][202]
Education has been a prime area of growth in the whole Gulf region. Primary school completion rates have grown by 15% for girls and the UAE, as well as Qatar, have the highest female-to-male ratio of university enrolments worldwide. 77% of Emirati women enrol in higher education after secondary school and make up 70% of all university graduates in the UAE.[203]
Traditionally women were encouraged to pursue female disciplines such as education and health care but this has changed recently with surges in areas such as technology and engineering. The UAE currently has four women fighter pilots and thirty trained females in the nation's special security forces.[204] In September 2014, the UAE opened the region's first military college for women, Khawla bint Al Azwar Military School.
Migrants, particularly migrant workers, make up a majority (approximately 80%) of the resident population of the UAE, and account for 90% of its workforce.[205] They generally lack rights associated with citizenship and face a variety of restrictions on their rights as workers.[206] There are reports of undocumented Emiratis who, because of their inability to be recognised as full citizens, receive no government benefits and have no labour rights. These stateless Emiratis – also known as bidun – either migrated to the UAE before independence or were natives who failed to register as citizens.[207] In addition, there are various incidents where local individuals have ill-treated people from overseas, just on the basis of nationality or race.[208]
A number of UAEs royals have been charged for abusing and ill-treating servants in overseas countries.[209]
Emiratis receive preferential treatment in employment via the Emiratisation programme forcing companies by law to limit the number of migrant workers in a company. This is done for the purposes of stabilising the labour market and protecting the rights of this group as a minority in their own country. At the same time, however, due to the welfare benefits of the UAE government, many Emiratis are reluctant to take up low paying jobs especially those in the private sector; private sector employers are also generally more inclined to hire overseas temporary workers as they are cheaper and can be retrenched for various reasons, for example, if they go on strike[g] Most UAE locals also prefer government jobs and consider private sector jobs to be below them.[214]
Migrants, mostly of South Asian origin, constitute 42.5% of the UAE's workforce[215] and have reportedly been subject to a range of human rights abuses. Workers have sometimes arrived in debt to recruitment agents from home countries and upon arrival were made to sign a new contract in English or Arabic that pays them less than had originally been agreed, although this is illegal under UAE law.[216] Further to this, some categories of workers have had their passports withheld by their employer. This practice, although illegal, is to ensure that workers do not abscond or leave the country on un-permitted trips.[217] In 2012, a workers' camp in Sonapur, Dubai, had their water cut for 20 days and electricity for 10 days, as well as no pay for three months. They were told that they had been forewarned that the lease was about to expire, and their option was to go to the Sharjah camp, which the workers did not want to do because it was "very dirty and [had] a foul smell.[218]
On 21 March 2006, tensions boiled over at the construction site of the Burj Khalifa, as workers upset over low wages and poor working conditions rioted, damaging cars, offices, computers, and construction tools. A Dubai Interior Ministry official said the rioters caused approximately US$1 million in damage. On 22 March most workers returned to the construction site but refused to work. Workers building a new terminal at Dubai International Airport went on strike in sympathy.[231]
A strike by foreign workers took place in October 2007, resulting in 159 deportations.[232]
In the past, the UAE government has denied any kind of labour injustices and has stated that the accusations by Human Rights Watch were misguided. Towards the end of March 2006, the government announced steps to allow construction unions. UAE labour minister Ali al-Kaabi said, "Laborers will be allowed to form unions."[233]
The strikes and negative media attention provided exposure of this regional problem and in 2008 the UAE government decreed and implemented a "midday break" during summer for construction companies, ensuring laborers were provided several hours to escape the summer heat. Illegal visa overstayers were assured amnesty and even repatriated to their home countries at the expense of friends, embassies or charities.[234]
In July 2013, a video was uploaded onto YouTube, which depicted a local driver hitting an expatriate worker, following a road related incident. Using part of his headgear, the local driver whips the expatriate and also pushes him around, before other passers-by intervene. A few days later, Dubai Police announced that both the local driver and the person who filmed the video, had been arrested. It was also revealed that the local driver was a senior UAE government official, although the exact government department is not known.[235] The video once again brings into question the way that lower classes of foreign workers are treated. Police in November 2013, also arrested a US citizen and some UAE citizens, in connection with a YouTube parody video which allegedly portrayed Dubai in bad light.[236] The parody video was shot in areas of Satwa and depicted gangs learning how to fight using simple weapons, including shoes, the aghal, etc. During the UN Universal Periodic Review (UPR) Pre-session of 2017 addressing the human rights violation affairs, a UAE delegate, Ahmed Awad, departed from the session after pronouncing it as a "waste of time".[237]
A report released by the Human Rights Watch in November 2020 cited that hundreds of Sudanese migrant workers were tricked into fighting alongside the UAE-backed forces loyal to General Khalifa Haftar in the Libya civil war. The Sudanese men were hired as security guards by an Emirati firm, Black Shield Security Services, for working at malls and hotels in the UAE.[238]
The UAE has four main types of labour laws:
Labour laws generally favour the employer and are less focused on the rights of employees. The Ministry of Labour is criticised for loosely enforcing these laws, most notably late or no wage or overtime payment for both blue collar and white collar employees.[242][243]
A gratuity (Arabic: مكافأة) is a lump-sum payment given to an employee by the employer or hiring company in the UAE and Dubai at the end of employment tenure. Per Emirates' Labor Law, employers are liable to disburse gratuity benefits to their workers upon exceeding one year of service. Companies incorporated within UAE, be it mainland or free zones are bound to comply with the provisions of Labour Law (Federal Law number 8 of 1980) which is further regulated through Ministry of the Human Resources and Emiratisation (MoHRE).[244]
End-of-service benefits is an extensive topic; there are a variety of benefits available, of which the gratuity obligation is the biggest proportion. Considering the multiple terms and conditions attached to calculating gratuity, the benefit is technical and complex to understand and administer.[245][246]
According to the Ansar Burney Trust (ABT), an illegal sex industry thrives in the emirates, where a large number of the workers are victims of human trafficking and sexual exploitation, especially in Dubai. This complements the tourism and hospitality industry, a major part of Dubai's economy.[247]
Prostitution, though illegal by law, is conspicuously present in the emirate because of an economy that is largely based on tourism and trade. There is a high demand for women from Europe and Asia. According to the World Sex Guide, a website catering to sex tourists, Eastern European and Ethiopian women are the most common prostitutes, while Eastern European prostitutes are part of a well-organised trans-Oceanic prostitution network.[248] The government has been trying to curb prostitution. In March 2007, it was reported that the UAE has deported over 4,300 sex workers mainly from Dubai.[249][250]
The UAE government enshrines conservative values in its constitution and therefore has adopted significant measures to combat this regional problem. The government of the UAE has worked with law enforcement officials to build capacity and awareness through holding training workshops and implementing monitoring systems to report human rights violations. Despite this, the system led to registration of only ten human-trafficking related cases in 2007 and half as many penalised convictions.[251]
Businesses participating in exploiting women and conducting illegal activities have licenses revoked and operations are forced to close. In 2007, after just one year, the efforts led to prosecution of prostitution cases rose by 30 percent. A year later, an annual report on the UAE's progress on human trafficking measures was issued and campaigns to raise public awareness of the issue are also planned.[252] Internationally, the UAE has led various efforts in combating human trafficking, particularly with the main countries of origin. The state has signed numerous bilateral agreements meant to regulate the labour being sent abroad by ensuring transactions are conducted by labour ministries and not profiting recruitment agencies.[citation needed]
In 2020, International United Nations Watch released a report on trafficking to the Middle East from Europe. The documentation highlighted multiple patterns of trafficking and made a mention of how the number of Moldovan women and girls are being trafficked to the UAE. It also said that although the UAE has anti-trafficking measures in place, Dubai continues to be the "amusement park of the Arabian Peninsula".[253]
A 2004 HBO documentary accuses UAE citizens of illegally using child jockeys in camel racing, where they are subjected also to physical and sexual abuse. Anti-Slavery International has documented similar allegations.[254]
The practice is officially banned in the UAE since 2002. The UAE was the first to ban the use of children under 15 as jockeys in the popular local sport of camel-racing when Sheikh Hamdan bin Zayed Al Nahyan, UAE's Deputy Prime Minister and Minister of State for Foreign Affairs announced the ban on 29 July 2002.[255]
Announcing the ban, Sheikh Hamdan made it very clear that "no-one would be permitted to ride camels in camel-races unless they had a minimum weight of 45 kg (99 lb), and are not less than 15 years old, as stated in their passports". He said a medical committee would examine each candidate to be a jockey to check that the age stated in their passport was correct and that the candidate was medically fit. Sheikh Hamdan said all owners of camel racing stables would be responsible for returning children under 15 to their home countries. He also announced the introduction of a series of penalties for those breaking the new rules. For a first offence, a fine of 20,000 AED was to be imposed. For a second offence, the offender would be banned from participating in camel races for a period of a year, while for third and subsequent offence, terms of imprisonment would be imposed.[256]
The Ansar Burney Trust,[257] which was featured heavily in the HBO documentary, announced in 2005 that the government of the UAE began actively enforcing a ban on child camel jockeys, and that the issue "may finally be resolved".[258]
Special funds to provide support for victims have been created such as Dubai's Foundation for the Protection of Women and Children, Abu Dhabi's Social Support Centre, the Abu Dhabi Shelter for Victims of Human Trafficking and the UAE Red Crescent Authority. Services offered include counseling, schooling, recreational facilities, psychological support and shelter. Mainly women and children receive assistance and in certain cases are even repatriated to their home countries.[251]

The peopling of the Americas began when Paleolithic hunter-gatherers (Paleo-Indians) entered North America from the North Asian Mammoth steppe via the Beringia land bridge, which had formed between northeastern Siberia and western Alaska due to the lowering of sea level during the Last Glacial Maximum (26,000 to 19,000 years ago).[2] These populations expanded south of the Laurentide Ice Sheet and spread rapidly southward, occupying both North and South America by 12,000 to 14,000 years ago.[3][4][5][6][7] The earliest populations in the Americas, before roughly 10,000 years ago, are known as Paleo-Indians.  Indigenous peoples of the Americas have been linked to Siberian populations by proposed linguistic factors, the distribution of blood types, and in genetic composition as reflected by molecular data, such as DNA.[8][9]
While there is general agreement that the Americas were first settled from Asia, the pattern of migration and the place(s) of origin in Eurasia of the peoples who migrated to the Americas remain unclear.[4] The traditional theory is that Ancient Beringians moved when sea levels were significantly lowered due to the Quaternary glaciation,[10][11] following herds of now-extinct Pleistocene megafauna along ice-free corridors that stretched between the Laurentide and Cordilleran ice sheets.[12] Another route proposed is that, either on foot or using boats, they migrated down the Pacific coast to South America as far as Chile.[13] Any archaeological evidence of coastal occupation during the last Ice Age would now have been covered by the sea level rise, up to a hundred metres since then.[14]
The precise date for the peopling of the Americas is a long-standing open question. While advances in archaeology, Pleistocene geology, physical anthropology, and DNA analysis have progressively shed more light on the subject, significant questions remain unresolved.[15][16] The "Clovis first theory" refers to the hypothesis that the Clovis culture represents the earliest human presence in the Americas about 13,000 years ago.[17] Evidence of pre-Clovis cultures has accumulated and pushed back the possible date of the first peopling of the Americas.[18][19][20][21] Academics generally  believe that humans reached North America south of the Laurentide Ice Sheet at some point between 15,000 and 20,000 years ago.[15][18][22][23][24][25] Some new controversial archaeological evidence suggests the possibility that human arrival in the Americas may have occurred prior to the Last Glacial Maximum more than 20,000 years ago.[18][26][27][28][29][30]
Historically, researchers believed a single theory explained the peopling of the Americas, focusing on findings from Blackwater Draw New Mexico, where human artifacts dated from the last ice age were found alongside the remains of extinct animals in 1930s[31] This led to the widespread belief in the "Clovis-first model," proposing that the first Americans migrated over the Beringia land bridge from Asia during a time when glacial passages opened. This model linked the first inhabitants to distinctive spear points, known as Clovis points, ranging in age from 13,250 to 12,800 years old.[32]
Numerous claims of earlier human presence began to challenge the Clovis first model beginning in the 1990s,[33] culminating in significant discoveries at Monte Verde, Chile, dating back 14,500 years.[34] At Oregon’s Paisley Caves, fossilized human feces date back 14,300 years.[35]  In Texas,  at Buttermilk Creek complex, stone tool fragments date back 15,500 years.[36] At Arroyo Seco 2 in Argentina, archaeologists discovered 14,000-year-old butchered animal bones.[37] Meadowcroft Rockshelter in Pennsylvania may have a history of at least 16,000 years.[38] As research progressed in the 2000s, the narrative shifted from a single migration event to multiple small, diverse groups entering the continent at various points in time.[39] This indicates that people might have populated North and South America as early as 15,000 to 20,000 years ago,[32][15][18][22][23] which some believe support a coastal migration route.[40][41]
The genetic history of the Indigenous peoples of the Americas has highlighted populations that adapted over tens of thousands of years. Geneticists discovered that a Beringian population split from Siberian groups about 36,000 years ago.[42] Around 25,000 years ago, they became isolated, forming a new genetic group linked to today’s Indigenous populations,[43] which divided into two main lineages between 14,500 and 17,000 years ago reflecting the dispersal associated with the early peopling of the Americas.[44][45]
The Chiquihuite Cave in Mexico suggest that ancient stone tools were being utilized as long ago as 30,000 years ago.[46] However, proving these tools were made by humans has faced criticism.[47] In New Mexico, researchers uncovered fossilized footprints, dating back perhaps 23,000 years,[48] showing humans and megafauna like mammoths co-existing, which is also under great debate.[49] In addition, some argue that evidence points towards human presence extending back 130,000 years,[50] though this is outright rejected by the vast majority of scholars across multiple academic fields.[51][52][53] Scholars such as David J. Meltzer have emphasized the fact that there are no verified archaeological sites in the Americas older than 16,000 years accepted by the academic community at large,[54] which questions claims of sites being 20,000, 25,000, or even 130,000 years old.[55][56][46]
During the Wisconsin glaciation, the Earth's ocean water was, to varying degrees over time, stored in glacier ice. As water accumulated in glaciers, the volume of water in the oceans correspondingly decreased, resulting in lowering of global sea level. The variation of sea level over time has been reconstructed using oxygen isotope analysis of deep sea cores, the dating of marine terraces, and high-resolution oxygen isotope sampling from ocean basins and modern ice caps. A drop of eustatic sea level by about 60 to 120 metres (200 to 390 ft) from present-day levels, commencing around 30,000 years Before Present (BP), created Beringia, a durable and extensive geographic feature connecting Siberia with Alaska.[57] With the rise of sea level after the Last Glacial Maximum (LGM), the Beringian land bridge was again submerged. Estimates of the final re-submergence of the Beringian land bridge based purely on present bathymetry of the Bering Strait and eustatic sea level curve place the event around 11,000 years BP (Figure 1). Ongoing research reconstructing Beringian paleogeography during deglaciation could change that estimate and possible earlier submergence could further constrain models of human migration into North America.[57]
The onset of the Last Glacial Maximum after 30,000 years BP saw the expansion of alpine glaciers and continental ice sheets that blocked migration routes out of Beringia. By 21,000 years BP, and possibly thousands of years earlier, the Cordilleran and Laurentide ice sheets coalesced east of the Rocky Mountains, closing off a potential migration route into the center of North America.[58][59][60] Alpine glaciers in the coastal ranges and the Alaskan Peninsula isolated the interior of Beringia from the Pacific coast. Coastal alpine glaciers and lobes of Cordilleran ice coalesced into piedmont glaciers that covered large stretches of the coastline as far south as Vancouver Island and formed an ice lobe across the Straits of Juan de Fuca by 18,000 BP.[61][62] Coastal alpine glaciers started to retreat around 19,000 BP[63] while Cordilleran ice continued advancing in the Puget lowlands up to 16,800 BP.[62] Even during the maximum extent of coastal ice, unglaciated refugia persisted on present-day islands, that supported terrestrial and marine mammals.[60] As deglaciation occurred, refugia expanded until the coast became ice-free by 15,000 BP.[60] The retreat of glaciers on the Alaskan Peninsula provided access from Beringia to the Pacific coast by around 17,000 BP.[64] The ice barrier between interior Alaska and the Pacific coast broke up starting around 16,200 BP.[61] The ice-free corridor to the interior of North America opened between 13,000 and 12,000 BP.[58][59][60] Glaciation in eastern Siberia during the LGM was limited to alpine and valley glaciers in mountain ranges and did not block access between Siberia and Beringia.[57]
The paleoclimates and vegetation of eastern Siberia and Alaska during the Wisconsin glaciation have been deduced from high resolution oxygen isotope data and pollen stratigraphy.[57][65][66] Prior to the Last Glacial Maximum, climates in eastern Siberia fluctuated between conditions approximating present day conditions and colder periods. The pre-LGM warm cycles in Arctic Siberia saw flourishes of megafaunas.[57] The oxygen isotope record from the Greenland Ice Cap suggests that these cycles after about 45,000 BP lasted anywhere from hundreds to between one and two thousand years, with greater duration of cold periods starting around 32,000 BP.[57] The pollen record from Elikchan Lake, north of the Sea of Okhotsk, shows a marked shift from tree and shrub pollen to herb pollen prior to 30,000 BP, as herb tundra replaced boreal forest and shrub steppe going into the LGM.[57] A similar record of tree/shrub pollen being replaced with herb pollen as the LGM approached was recovered near the Kolyma River in Arctic Siberia.[66] The abandonment of the northern regions of Siberia due to rapid cooling or the retreat of game species with the onset of the LGM has been proposed to explain the lack of archaeological sites in that region dating to the LGM.[66][67] The pollen record from the Alaskan side shows shifts between herb/shrub and shrub tundra prior to the LGM, suggesting less dramatic warming episodes than those that allowed forest colonization on the Siberian side. Diverse, though not necessarily plentiful, megafauna were present in those environments. Herb tundra dominated during the LGM, due to cold and dry conditions.[65]
Coastal environments during the Last Glacial Maximum were complex. The lowered sea level, and an isostatic bulge equilibrated with the depression beneath the Cordilleran Ice Sheet, exposed the continental shelf to form a coastal plain.[68] While much of the coastal plain was covered with piedmont glaciers, unglaciated refugia supporting terrestrial mammals have been identified on Haida Gwaii, Prince of Wales Island, and outer islands of the Alexander Archipelago.[65] The now-submerged coastal plain has potential for more refugia.[65] Pollen data indicate mostly herb/shrub tundra vegetation in unglaciated areas, with some boreal forest towards the southern end of the range of Cordilleran ice.[65] The coastal marine environment remained productive, as indicated by fossils of pinnipeds.[68] The highly productive kelp forests over rocky marine shallows may have been a lure for coastal migration.[69][70] Reconstruction of the southern Beringian coastline also suggests potential for a highly productive coastal marine environment.[70]
Pollen data indicate a warm period culminating between 17,000 and 13,000 BP followed by cooling between 13,000 and 11,500 BP.[68] Coastal areas deglaciated rapidly as coastal alpine glaciers, then lobes of Cordilleran ice, retreated. The retreat was accelerated as sea levels rose and floated glacial termini.  It has been estimated that the coast range was fully ice-free between 16,000 and 15,000 BP.[68][60] Littoral marine organisms colonized shorelines as ocean water replaced glacial meltwater. Replacement of herb/shrub tundra by coniferous forests was underway by 15,000 BP north of Haida Gwaii. Eustatic sea level rise caused flooding, which accelerated as the rate grew more rapid.[68]
The inland Cordilleran and Laurentide ice sheets retreated more slowly than did the coastal glaciers. Opening of an ice-free corridor did not occur until after 13,000 to 12,000 BP.[58][59][60] The early environment of the ice-free corridor was dominated by glacial outwash and meltwater, with ice-dammed lakes and periodic flooding from the release of ice-dammed meltwater.[58] Biological productivity of the deglaciated landscape increased slowly.[60] The earliest possible viability of the ice-free corridor as a human migration route has been estimated at 11,500 BP.[60]
Birch forests were advancing across former herb tundra in Beringia by 17,000 BP in response to climatic amelioration, indicating increased productivity of the landscape.[66]
Analyses of biomarkers and microfossils preserved in sediments from Lake E5 and Burial Lake in northern Alaska suggest early humans burned Beringian landscapes as early as 34,000 years ago.[71][72] The authors of these studies suggest that fire was used as means of hunting megafauna.
The Indigenous peoples of the Americas have an ascertained archaeological presence in the Americas dating back to about 15,000 years ago.[73][74] More recent research, however, suggests a human presence dating to between 18,000 and 26,000 years ago, during the Last Glacial Maximum.[75][76][7]
There remain uncertainties regarding the precise dating of individual sites and regarding conclusions drawn from population genetics studies of contemporary Native Americans.
In the early 21st century, the models of the chronology of migration are divided into two general approaches.[78][79]
The first is the short chronology theory, that the first migration occurred after the LGM, which went into decline after about 19,000 years ago,[63] and was then followed by successive waves of immigrants.[80]
The second theory is the long chronology theory, which proposes that the first group of people entered Beringia, including ice-free parts of Alaska, at a much earlier date, possibly 40,000 years ago,[81][82][83] followed by a much later second wave of immigrants.[79][84]
The Clovis First theory, which dominated thinking on New World anthropology for much of the 20th century, was challenged in the 2000s by the secure dating of archaeological sites in the Americas to before 13,000 years ago.[58][59][60][85][74]
The archaeological sites in the Americas with the oldest dates that have gained broad acceptance are all compatible with an age of about 15,000 years. This includes the Buttermilk Creek Complex in Texas,[73] the Meadowcroft Rockshelter site in Pennsylvania and the Monte Verde site in southern Chile.[74] Archaeological evidence of pre-Clovis people points to the South Carolina Topper Site being 16,000 years old, at a time when the glacial maximum would have theoretically allowed for lower coastlines.
It has often been suggested that an ice-free corridor, in what is now Western Canada, would have allowed migration before the beginning of the Holocene. However, a 2016 study has argued against this, suggesting that the peopling of North America via such a corridor is unlikely to significantly pre-date the earliest Clovis sites. The study concludes that the ice-free corridor in what is now Alberta and British Columbia "was gradually taken over by a boreal forest dominated by spruce and pine trees" and that the "Clovis people likely came from the south, not the north, perhaps following wild animals such as bison".[86][87]
An alternative hypothesis for the peopling of America is coastal migration, which may have been feasible along the deglaciated (but now submerged) coastline of the Pacific Northwest from about 16,000 years ago.
Pre-LGM migration across Beringia has been proposed to explain purported pre-LGM ages of archaeological sites in the Americas such as Bluefish Caves[82] and Old Crow Flats[83] in the Yukon Territory, and Meadowcroft Rock Shelter in Pennsylvania.[79][84] The oldest archaeological sites on the Alaskan side of Beringia date to around 14,000 BP.[66][88] It is possible that a small founder population had entered Beringia before that time. However, archaeological sites that date closer to the LGM on either the Siberian or the Alaskan side of Beringia are lacking. Biomarker and microfossil analyses of sediments from Lake E5 and Burial Lake in northern Alaska suggest human presence in eastern Beringia as early as 34,000 years ago.[71] These sedimentary analyses have been suggested to be the only possibly recoverable remnants of humans living in Alaska during the last Glacial period.[72]
At Old Crow Flats, mammoth bones have been found that are broken in distinctive ways indicating human butchery. The radiocarbon dates on these vary between 25,000 and 40,000 BP. Also, stone microflakes have been found in the area indicating tool production.[89] However, the interpretations of butcher marks and the geologic association of bones at the Bluefish Cave and Old Crow Flats sites, and the related Bonnet Plume site, have been called into question.[29] No evidence of human remains have been discovered at these sites. In addition to disputed archaeological sites, support for pre-LGM human presence has been found in lake sediment records of northern Alaska. Biomarker and microfossil analyses of sediments from Lake E5 and Burial Lake in suggest human presence in eastern Beringia as early as 34,000 years ago.[71][72] These analyses are indeed compelling in that they corroborate the inferences made from the Bluefish Cave and Old Crow Flats sites.
In 2020, evidence emerged for a new pre-LGM site in North-Central Mexico. Chiquihuite cave, an archaeological site in Zacatecas State, has been dated to 26,000 years BP based on numerous lithic artefacts discovered there.[90] However, there is scholarly debate over whether the artifacts should be considered evidence of human activity or if they were formed naturally.[91][28] No evidence of human DNA or hearth have been unearthed.[92]
Pre-LGM human presence in South America rests partly on the chronology of the controversial Pedra Furada rock shelter in Piauí, Brazil. More recently, studies at the archaeological sites Santa Elina (27000–10000 years BP)[93] in the midwest, and Rincão I (20000–12000 years BP)[94]  in southeastern Brazil also show associations of evidence of human presence with sediments dating from before the LGM. A 2003 study dated evidence for the controlled use of fire to before 40,000 years ago.[95] Additional evidence has been adduced from the morphology of Luzia Woman fossil, which was described as Australo-Melanesian. This interpretation was challenged in a 2003 review which concluded the features in question could also have arisen by genetic drift.[96] In November 2018, scientists of the University of São Paulo and Harvard University released a study that contradicts the alleged Australo-Melanesian origin of Luzia. Using DNA sequencing, the results showed that Luzia's ancestry was entirely Native American.[97][98]
Stones described as probable tools, hammerstones and anvils, have been found in southern California, at the Cerutti Mastodon site, that are associated with a mastodon skeleton which appeared to have been processed by humans. The mastodon skeleton was dated by thorium-230/uranium radiometric analysis, using diffusion–adsorption–decay dating models, to around 130 thousand years ago.[99] No human bones were found and expert reaction was mixed;  claims of tools and bone processing were called "not plausible" by Prof. Tom Dillehay.[100]
The Yana River Rhino Horn site (RHS) has dated human occupation of eastern Arctic Siberia to 31,300 BP.[101] That date has been interpreted by some as evidence that migration into Beringia was imminent, lending credence to occupation of Beringia during the LGM.[102][103] However, the Yana RHS date is from the beginning of the cooling period that led into the LGM.[57] A compilation of archaeological site dates throughout eastern Siberia suggest that the cooling period caused a retreat of humans southwards.[66][67] Pre-LGM lithic evidence in Siberia indicate a settled lifestyle that was based on local resources, while post-LGM lithic evidence indicate a more migratory lifestyle.[67]
A 2021 discovery of human footprints in relict lake sediments near White Sands National Park in New Mexico demonstrated there was a verifiable human presence in the region dating back to the LGM between 18,000 and 26,000 years ago.[75][76] Later studies, reported in October 2023, confirmed that the age of the human footprints to be "up to 23,000 years old".[104][105]
The Clovis-first advocates have not accepted the veracity of these findings. In 2022, they said, "The oldest evidence for archaeological sites in the New World with large numbers of artifacts occurring in discrete and minimally disturbed stratigraphic contexts occur in eastern Beringia between 13,000 and 14,200 BP. South of the ice sheets, the oldest such sites occur in association with the Clovis complex. If humans managed to breach the continental ice sheets significantly before 13,000 BP, there should be clear evidence for it in the form of at least some stratigraphically discrete archaeological components with a relatively high artifact count. So far, no such evidence exists."[106]
Genetic studies have used high resolution analytical techniques applied to DNA samples from modern Native Americans and Asian populations regarded as their source populations to reconstruct the development of human Y-chromosome DNA haplogroups (yDNA haplogroups) and human mitochondrial DNA haplogroups (mtDNA haplogroups) characteristic of Native American populations.[81][102][103] Models of molecular evolution rates were used to estimate the ages at which Native American DNA lineages branched off from their parent lineages in Asia and to deduce the ages of demographic events. One model (Tammetal 2007) based on Native American mtDNA Haplotypes (Figure 2) proposes that migration into Beringia occurred between 30,000 and 25,000 BP, with migration into the Americas occurring around 10,000 to 15,000 years after isolation of the small founding population.[102] Another model (Kitchen et al. 2008) proposes that migration into Beringia occurred approximately 36,000 BP, followed by 20,000 years of isolation in Beringia.[103] A third model (Nomatto et al. 2009) proposes that migration into Beringia occurred between 40,000 and 30,000 BP, with a pre-LGM migration into the Americas followed by isolation of the northern population following closure of the ice-free corridor.[81] Evidence of  Australo-Melanesians admixture in Amazonian populations was found  by Skoglund and Reich (2016).[107]
A study of the diversification of mtDNA Haplogroups C and D from southern Siberia and eastern Asia, respectively, suggests that the parent lineage (Subhaplogroup D4h) of Subhaplogroup D4h3, a lineage found among Native Americans and Han Chinese,[108][109] emerged around 20,000 BP, constraining the emergence of D4h3 to post-LGM.[110] Age estimates based on Y-chromosome micro-satellite diversity place origin of the American Haplogroup Q1a3a (Y-DNA) at around 15,000 to 10,000 BP.[111] Greater consistency of DNA molecular evolution rate models with each other and with archaeological data may be gained by the use of dated fossil DNA to calibrate molecular evolution rates.[108]
Although there is no archaeological evidence that can be used to direct support a coastal migration route during the Last Glacial Maximum, genetic analysis has been used to support this thesis. In addition to human genetic lineage, megafaunal DNA lineage can be used to trace movements of megafauna – large mammalian – as well as the early human groups who hunted them.
Bison, a type of megafauna, have been identified as an ideal candidate for the tracing of human migrations out of Europe because of both their abundance in North America as well as being one of the first megafauna for which ancient DNA was used to trace patterns of population movement. Unlike other types of fauna that moved between the Americas and Eurasia (mammoths, horses, and lions), Bison survived the North American extinction event that occurred at the end of the Pleistocene. Their genome, however, contains evidence of a bottleneck – something that can be used to test hypothesis on migrations between the two continents.[116] Early human groups were largely nomadic, relying on following food sources for survival. Mobility was part of what made humans successful. As nomadic groups, early humans likely followed the food from Eurasia to the Americas – part of the reason why tracing megafaunal DNA is so helpful for garnering insight to these migratory patterns.[117]
The grey wolf originated in the Americas and migrated into Eurasia prior to the Last Glacial Maximum – during which it was believed that remaining populations of the grey wolf residing in North America faced extinction and were isolated from the rest of the population. This, however, may not be the case. Radiocarbon dating of ancient grey wolf remains found in permafrost deposits in Alaska show a continuous exchange of population from 12,500 radiocarbon years BP to beyond radiocarbon dating capabilities. This indicates that there was viable passage for grey wolf populations to exchange between the two continents.[118]
These faunas' ability to exchange populations during the period of the Last Glacial Maximum along with genetic evidence found from early human remains in the Americas provides evidence to support pre-Clovis migrations into the Americas.
The Native American source population was formed in Siberia by the mixing of two distinct populations: Ancient North Eurasians and an ancient East Asian (ESEA) population.[119][120] According to Jennifer Raff, the Ancient North Eurasian population mixed with a daughter population of ancient East Asians, who they encountered around 25,000 years ago, which led to the emergence of Native American ancestral populations. However, the exact location where the admixture took place is unknown, and the migratory movements that united the two populations are a matter of debate.[121]
One theory supposes that Ancient North Eurasians migrated south to East Asia, or Southern Siberia, where they would have encountered and mixed with ancient East Asians. Genetic evidence from Lake Baikal in Russia supports this area as the location where the admixture took place.[122]
However, a third theory, the "Beringian standstill hypothesis", suggests that East Asians instead migrated north to Northeastern Siberia, where they mixed with ANE, and later diverged in Beringia, where distinct Native American lineages formed. This theory is supported by maternal and nuclear DNA evidence.[123] According to Grebenyuk, after 20,000 BP, a branch of Ancient East Asians migrated to Northeastern Siberia, and mixed with descendants of the ANE, leading to the emergence of Ancient Paleo-Siberian and Native American populations in Extreme Northeastern Asia.[124]
However, the Beringian standstill hypothesis is not supported by paternal DNA evidence, which may reflect different population histories for paternal and maternal lineages in Native Americans, which is not uncommon and has been observed in other populations.[125]
A 2019 study suggested that Native Americans are the closest living relatives to 10,000-year-old fossils found near the Kolyma River in northeastern Siberia.[126]
A study  published in July 2022 suggested that people in southern China may have contributed to the Native American gene pool, based on the discovery and DNA analysis of 14,000-year-old human fossils.[127][128]
The contrast between the genetic profiles of the Hokkaido Jōmon skeletons and the modern Ainu illustrates another uncertainty in source models derived from modern DNA samples.[129]
The development of high-resolution genomic analysis has provided opportunities to further define Native American subclades and narrow the range of Asian subclades that may be parent or sister subclades.
The common occurrence of the mtDNA Haplogroups A, B, C, and D among eastern Asian and Native American populations has long been recognized, along with the presence of haplogroup X.[130] As a whole, the greatest frequency of the four Native American associated haplogroups occurs in the Altai-Baikal region of southern Siberia.[131] Some subclades of C and D closer to the Native American subclades occur among Mongolian, Amur, Japanese, Korean, and Ainu populations.[130][132]
With further definition of subclades related to Native American populations, the requirements for sampling Asian populations to find the most closely related subclades grow more specific. Subhaplogroups D1 and D4h3 have been regarded as Native American specific based on their absence among a large sampling of populations regarded as potential descendants of source populations, over a wide area of Asia.[102] Among the 3,764 samples, the Sakhalin–lower Amur region was represented by 61 Oroks.[102] In another study, Subhaplogroup D1a has been identified among the Ulchis of the lower Amur River region (4 among 87 sampled, or 4.6%), along with Subhaplogroup C1a (1 among 87, or 1.1%).[132] Subhaplogroup C1a is regarded as a close sister clade of the Native American Subhaplogroup C1b.[132]
Subhaplogroup D1a has also been found among ancient Jōmon skeletons from Hokkaido[129] The modern Ainu are regarded as descendants of the Jōmon.[129] The occurrence of the Subhaplogroups D1a and C1a in the lower Amur region suggests a source population from that region distinct from the Altai-Baikal source populations, where sampling did not reveal those two particular subclades.[132] The conclusions regarding Subhaplogroup D1 indicating potential source populations in the lower Amur[132] and Hokkaido[129] areas stand in contrast to the single-source migration model.[81][102][103]
Subhaplogroup D4h3 has been identified among Han Chinese.[108][109] Subhaplogroup D4h3 from China does not have the same geographic implication as Subhaplotype D1a from Amur-Hokkaido, so its implications for source models are more speculative. Its parent lineage, Subhaplotype D4h, is believed to have emerged in East Asia, rather than Siberia, around 20,000 BP.[110] Subhaplogroup D4h2, a sister clade of D4h3, has also been found among Jōmon skeletons from Hokkaido.[133] D4h3 has a coastal trace in the Americas.[109]
X is one of the five mtDNA haplogroups found in Indigenous Americans. Native Americans mostly belong to the X2a clade, which has never been found in the Old World.[134] According to Jennifer Raff, X2a probably originated in the same Siberian population as the other four founding maternal lineages, and that there is no compelling reason to believe it is related to X lineages found in Europe or West Eurasia. The Kennewick man fossil was found to carry the deepest branch of the X2a haplogroup, and he did not have any European ancestry that would be expected for a European origin of the lineage.[135]
The Human T cell Lymphotrophic Virus 1 (HTLV-1) is a virus transmitted through exchange of bodily fluids and from mother to child through breast milk. The mother-to-child transmission mimics a hereditary trait, although such transmission from maternal carriers is less than 100%.[136] The HTLV virus genome has been mapped, allowing identification of four major strains and analysis of their antiquity through mutations. The highest geographic concentrations of the strain HLTV-1 are in sub-Saharan Africa and Japan.[137] In Japan, it occurs in its highest concentration on Kyushu.[137] It is also present among African descendants and native populations in the Caribbean region and South America.[137] It is rare in Central America and North America.[137] Its distribution in the Americas has been regarded as due to importation with the slave trade.[138]
The Ainu have developed antibodies to HTLV-1, indicating its endemicity to the Ainu and its antiquity in Japan.[139] A subtype "A" has been defined and identified among the Japanese (including Ainu), and among Caribbean and South American isolates.[140] A subtype "B" has been identified in Japan and India.[140] In 1995, Native Americans in coastal British Columbia were found to have both subtypes A and B.[141] Bone marrow specimens from an Andean mummy about 1500 years old were reported to have shown the presence of the A subtype.[142] The finding ignited controversy, with contention that the sample DNA was insufficiently complete for the conclusion and that the result reflected modern contamination.[143] However, a re-analysis indicated that the DNA sequences were consistent with, but not definitely from, the "cosmopolitan clade" (subtype A).[143] The presence of subtypes A and B in the Americas is suggestive of a Native American source population related to the Ainu ancestors, the Jōmon.
Paleo-Indian skeletons in the Americas such as Kennewick Man (Washington State), Hoya Negro skeleton (Yucatán), Luzia Woman and other skulls from the Lagoa Santa site (Brazil), Buhl Woman (Idaho), Peñon Woman III,[144] two skulls from the Tlapacoya site (Mexico City),[144] and 33 skulls from Baja California[145] have exhibited certain craniofacial traits distinct from most modern Native Americans, leading physical anthropologists to posit an earlier "Paleoamerican" population wave.[146] The most basic measured distinguishing trait is the dolichocephaly of the skull. Some modern isolated populations such as the Pericúes of Baja California and the Fuegians of Tierra del Fuego exhibit that same morphological trait.[145]
Other anthropologists advocate an alternative hypothesis that evolution of an original Beringian phenotype gave rise to a distinct morphology that was similar in all known Paleoamerican skulls, followed by later convergence towards the modern Native American phenotype.[147][148]
Archaeogenetic studies do not support a two-wave model or the Paleoamerican hypothesis of an Australo-Melanesian origin, and firmly assign all Paleo-Indians and modern Native Americans to one ancient population that entered the Americas in a single migration from Beringia. Only in one ancient specimen (Lagoa Santa) and a few modern populations in the Amazon region, a small Australasian ancestry component of c. 3% was detected, which remains unexplained by the current state of research (as of 2021[update]), but may be explained by the presence of the more basal Tianyuan-related ancestry, a deep East Asian lineage which did not directly contribute to modern East Asians but may have contributed to the ancestors of Native Americans in Siberia, as such ancestry is also found among previous Paleolithic Siberians (Ancient North Eurasians).[107][149][119]
A report published in the American Journal of Physical Anthropology in January 2015 reviewed craniofacial variation focusing on differences between early and late Native Americans and explanations for these based on either skull morphology or molecular genetics. Arguments based on molecular genetics have in the main, according to the authors, accepted a single migration from Asia with a probable pause in Beringia, plus later bi-directional gene flow. Some studies focusing on craniofacial morphology have previously argued that Paleoamerican remains have been described as closer to Australo-Melanesians and Polynesians than to the modern series of Native Americans, suggesting two entries into the Americas, an early one occurring before a distinctive East Asian morphology developed (referred to in the paper as the "Two Components Model"). Another "third model", the "Recurrent Gene Flow" (RGF) model, attempts to reconcile the two, arguing that circumarctic gene flow after the initial migration could account for morphological changes. It specifically re-evaluates the original report on the Hoya Negro skeleton which supported the RGF model, the authors disagreed with the original conclusion which suggested that the skull shape did not match those of modern Native Americans, arguing that the "skull falls into a subregion of the morphospace occupied by both Paleoamericans and some modern Native Americans."[150]
Stemmed points are a lithic technology distinct from Beringian and Clovis types. They have a distribution ranging from coastal East Asia to the Pacific coast of South America.[69] The emergence of stemmed points has been traced to Korea during the upper Paleolithic.[151] The origin and distribution of stemmed points have been interpreted as a cultural marker related to a source population from coastal East Asia.[69]
Historically, theories about migration into the Americas have revolved around migration from Beringia through the interior of North America. The discovery of artifacts in association with Pleistocene faunal remains near Clovis, New Mexico in the early 1930s required extension of the timeframe for the settlement of North America to the period during which glaciers were still extensive. That led to the hypothesis of a migration route between the Laurentide and Cordilleran ice sheets to explain the early settlement. The Clovis site was host to a lithic technology characterized by spear points with an indentation, or flute, where the point was attached to the shaft. A lithic complex characterized by the Clovis Point technology was subsequently identified over much of North America and in South America. The association of Clovis complex technology with late Pleistocene faunal remains led to the theory that it marked the arrival of big game hunters that migrated out of Beringia and then dispersed throughout the Americas, otherwise known as the Clovis First theory.
Recent radiocarbon dating of Clovis sites has yielded ages of between 13,000 and 12,600 BP, somewhat later than dates derived from older techniques.[152] The re-evaluation of earlier radiocarbon dates led to the conclusion that no fewer than 11 of the 22 Clovis sites with radiocarbon dates are "problematic" and should be disregarded, including the type site in Clovis, New Mexico. Numerical dating of Clovis sites has allowed comparison of Clovis dates with dates of other archaeological sites throughout the Americas, and of the opening of the ice-free corridor. Both lead to significant challenges to the Clovis First theory. The Monte Verde site of Southern Chile has been dated at 14,800 BP.[74] The Paisley Cave site in eastern Oregon yielded a 14,500 BP, on a coprolite with human DNA and radiocarbon dates of 13,200 and 12,900 BP on horizons containing western stemmed points.[153] Artifact horizons with non-Clovis lithic assemblages and pre-Clovis ages occur in eastern North America, although the maximum ages tend to be poorly constrained.[85][154]
Recent studies have suggested that the ice-free corridor opened later (around 13,800 ± 500 years ago) than the earliest widely accepted archaeological sites in the Americas, suggesting that it could have not have been used as the migration route for the earliest peoples to migrate south.[155]
An alternative to the Beringia route is proposed by the "stepping stones" hypothesis. The frequent submergence of islands along the Bering Transitory Archipelago would have forced the inhabitants of these island to continue traveling across the archipelago until they reached the mainland.[156]
Geological findings on the timing of the ice-free corridor also challenge the notion that Clovis and pre-Clovis human occupation of the Americas was a result of migration through that route following the Last Glacial Maximum. Pre-LGM closing of the corridor may approach 30,000 BP and estimates of ice retreat from the corridor are in the range of 13,000 to 12,000 years ago.[58][59][60] Viability of the corridor as a human migration route has been estimated at 11,500 BP, later than the ages of the Clovis and pre-Clovis sites.[60] Dated Clovis archaeological sites suggest a south-to-north spread of the Clovis culture.[58]
Pre-LGM migration into the interior has been proposed to explain pre-Clovis ages for archaeological sites in the Americas,[79][84] although pre-Clovis sites such as Meadowcroft Rock Shelter,[85][154] Monte Verde,[74] and Paisley Cave have not yielded confirmed pre-LGM ages.
There are many pre-Clovis sites in the American Southwest, particularly in the Mojave Desert. Lake Mojave quarries dating back to the Pleistocene hold lithic remains of Silver Lake projectile points and Lake Mojave projectile points. This indicates an interior movement into the region as early as 13,800 BP, if not earlier.[157]
A relationship between the Na-Dené languages of North America (such as Navajo and Apache), and the Yeniseian languages of Siberia was first proposed as early as 1923, and developed further by others. A detailed study was done by Edward Vajda and published in 2010.[158] This theory received support from many linguists, with archaeological and genetic studies providing it with further support.[citation needed]
The Arctic Small Tool tradition of Alaska and the Canadian Arctic may have originated in East Siberia about 5,000 years ago. This is connected with the ancient Paleo-Eskimo peoples of the Arctic.
The Arctic Small Tool tradition source may have been the Syalakh-Bel'kachi-Ymyakhtakh culture sequence of East Siberia, dated to 6,500–2,800 BP.[159]
The interior route is consistent with the spread of the Na-Dene language group[158] and subhaplogroup X2a into the Americas after the earliest paleoamerican migration.[109]
Nevertheless, some scholars suggest that the ancestors of western North Americans speaking Na-Dene languages made a coastal migration by boat.[160]
It is possible that humans arrived in the Americas by way of interior routes that existed prior to the Last Glacial Maximum. Cosmogenic exposure dating, a technique that analyzes when in Earth's history a landscape was exposed to cosmic rays (and therefore unglaciated), performed by Mark Swisher suggests that an older ice-free corridor existed in North America 25,000 years ago. Swisher attributes sites such as Monte Verde, Meadowcroft Rockshelter, Manis Mastodon site and Paisley Caves to this corridor.[161]
The Pacific coastal migration theory proposes that people first reached the Americas via water travel, following coastlines from northeast Asia into the Americas, originally proposed in 1979 by Knute Fladmark as an alternative to the hypothetical migration through an ice-free inland corridor.[162] This model would help to explain the rapid spread to coastal sites extremely distant from the Bering Strait region, including sites such as Monte Verde in southern Chile and Taima-Taima in western Venezuela.
The very similar marine migration hypothesis is a variant of coastal migration; essentially its only difference is that it postulates that boats were the principal means of travel. The proposed use of boats adds a measure of flexibility to the chronology of coastal migration, because a continuous ice-free coast (16–15,000 calibrated years BP) would then not be required: Migrants in boats 
could have easily bypassed ice barriers and settled in scattered coastal refugia, before the deglaciation of the coastal land route was complete. A maritime-competent source population in coastal East Asia is an essential part of the marine migration hypothesis.[69][70]
A 2007 article in the Journal of Island and Coastal Archaeology proposed a "kelp highway hypothesis", a variant of coastal migration based on the exploitation of kelp forests along  much of the Pacific Rim from Japan to Beringia, the Pacific Northwest, and California, and as far as the Andean Coast of South America. Once the coastlines of Alaska and British Columbia had deglaciated about 16,000 years ago, these kelp forest (along with estuarine, mangrove, and coral reef) habitats would have provided an ecologically homogenous migration corridor, entirely at sea level, and essentially unobstructed.
A 2016 DNA analysis of plants and animals suggest a coastal route was feasible.[163][164]
Mitochondrial subhaplogroup D4h3a, a rare subclade of D4h3 occurring along the west coast of the Americas, has been identified as a clade associated with coastal migration.[109]
This haplogroup was found in a skeleton referred to as Anzick-1, found in Montana in close association with several Clovis artifacts, dated 12,500 years ago.[165]
The coastal migration models provide a different perspective on migration to the New World, but they are not without their own problems: One such problem is that global sea levels have risen over 120 metres (390 ft)[166] since the end of the last glacial period, and this has submerged the ancient coastlines that maritime people would have followed into the Americas. Finding sites associated with early coastal migrations is extremely difficult—and systematic excavation of any sites found in deeper waters is challenging and expensive. Strategies for finding earliest migration sites include identifying potential sites on submerged paleoshorelines, seeking sites in areas uplifted either by tectonics or isostatic rebound, and looking for riverine sites in areas that may have attracted coastal migrants.[69][167] On the other hand, there is evidence of marine technologies found in the hills of the Channel Islands of California, circa 12,000 BP.[168] If there was an early pre-Clovis coastal migration, there is always the possibility of a "failed colonization".

Muscular evolution in humans is an overview of the muscular adaptations made by humans from their early ancestors to the modern man. Humans are believed to be predisposed to develop muscle density as early humans depended on muscle structures to hunt and survive. Modern man's need for muscle is not as dire, but muscle development is still just as rapid if not faster due to new muscle building techniques and knowledge of the human body.[1]
DNA and anthropologic data consider modern humans (Homo sapiens) a primate and the descendants of ape-like species. Species of the genus ‘Homo’ are all extinct except humans, which are thought to have evolved from australopithecine ancestors originating in East Africa.[2] The development of the modern human has taken place over some 300,000 years and unique adaptations have resulted from ecological pressures that Homo Sapiens has faced. Due prominently to ecological and behavioral factors, the modern human muscular system differs greatly from that of our early primate ancestors.[3] These adaptations and changes have allowed Homo sapiens to function as they do today.
As is the standard for all evolutionary adaptations, the human muscle system evolved in its efforts to increase survivability. Since muscles and the accompanying ligaments and tendons are present all throughout the body aiding in many functions, it is apparent that our behavior and decisions are based upon what we are and how we can operate. It is believed that our ancestor's original habitat was not on the ground but in the trees and we developed new habits that eventually allowed us to thrive on the ground, such as changes in diet, gathering of food, energy expenditure, social interactions, and predators. Life in the canopy meant a food supply similar to that of herbivores: leaves, fruits, berries; mostly low-protein foods that did not require a large amount of energy to find. However, if any could be found, meat was also consumed. At this time our ancestors had not yet switched to full-time bipedalism and so searching for food on the ground did not make sense because there was too much energy and risk involved. This habitat also lacked the predators found on the ground that our chimp-like ancestors would have been poor defenders against. As they became bipedal, they began to live in groups that used weapons to fend off predators and hunt down prey. Running became a key aspect to the survival of the species.[4] Even with all this, it is the development of the brain that has guided the development of the muscle functions and structures in humans.
It is suspected that H. sapiens ancestors’ did not initially forage on the forest floor; instead they migrated from the trees for various reasons. In that environment, they survived on a diet high in plant matter with some insects and little amounts of meat. They were not very formidable opponents to more dominant mammals such as large ancient cats (lions, leopards) but their ability to be better hunters and gatherers along with their corresponding brain development, gave them the advantage to add high-calorie nutrient supplies such as meat to their diet. Analysis of the jaws and skull of the supposed human ancestors show that they had larger, stronger jaw muscles attached to the skull which would be expected with a diet rich in fruit and plants. The back set of molars were much larger for this reason also. The dependence on these higher-calorie foods came from the inefficiency of bipedalism and the growing energy costs of climbing tall trees.[5] Human ancestors are thought to have had more muscles connecting the skull, neck, and shoulders/back area (similar to apes) which caused their neck and skull regions to appear to sag, such as non-human primate species do. These diminished muscles allow the human head to be held in its current ‘upright’ position and lets the occipitofrontalis muscle, or the forehead, to function as an aid to expressions.[6]
Humans became taller as the years passed after becoming bipedal which lengthened back muscles at the base of the tail bone and hips which in effect made them weigh more, further hampering their abilities in the trees. Early human ancestors had a tail where modern humans’ tail bone is located. This aided in balance when in the trees but lost its prominence when bipedalism was adapted. The arms also became shorter (opposite in comparison to legs) for carrying objects and using them as multi-tasking agents instead of climbing and swinging in trees. It is well known that the Homo sapiens line of primates developed the opposable thumb which opened the door to many muscle functions not yet possible in the hand and other upper body regions.[7] The stretching muscles of the forearms whose tendons allowed the human to concentrate its force and abilities within his/her hands and fingers contributed to great new abilities.[8] Overall, upper body muscles developed to deal with more activities that involved the concentration of strength in those muscles such as: holding, throwing, lifting, running with something to assist in escaping danger, hunting, and the construction of habitats and shelters.
The conversion to full-time bipedalism in our distant ancestors is the main argument for the adaptations our muscle structure and function have made. By having to center the force of gravity on two feet, the human thigh bone developed an inward slope down to the knee which may have allowed their gluteal abductors to adapt to the stress and build the necessary muscle. This allows the human to manage their balance on a single foot and when “in-stride” during walking. Muscles near the ankle helped provide the push during walking and running.  There are many advantages and disadvantages to this altered posture and gait. The ability to grab something with four appendages was lost but what was gained was the ability to hold a club or throw a spear and use the other free hand for another task.[9] This adaptation also helped humans stand up straight with locked knees for longer periods of time.[10]  The plantaris muscle in the foot which helped our ancestors grab and manipulate objects like chimps do, has adapted to its new evolutionary role appropriately, becoming so underdeveloped that it cannot grip or grab anything, the foot has grown more elongated as a result and now 9% of humans are born without it. Homo sapiens benefitted by becoming a better defender and hunter. An increase in running as a hunting and survival activity was perhaps fundamental to this development.[11]
Compared to our closest living relatives, chimpanzees and bonobos, Homo sapiens' skeletal muscle is on average about 1.35 to 1.5 times weaker when normalized for size. As little biomechanical difference was found between individual muscle fibers from the different species, this strength difference is likely the result of different muscle fiber type composition. Humans' limb muscles tend to be more biased toward fatigue-resistant, slow twitch Type I muscle fibers.[12] While there is no proof that modern humans have become physically weaker than past generations of humans, inferences from such things as bone robusticity and long bone cortical thickness can be made as a representation of physical strength. Taking such factors into account, there has been a rapid decrease in overall robusticity in those populations that take to sedentism.[13] For instance, bone shaft thickness since the 17th and 18th centuries have decreased in the United States, indicating a less physically stressful life.[14] This is not, however, the case for current hunter gatherer and foraging populations, such as the Andaman Islanders, who retain overall robusticity.[15] In general, though, hunter gatherers tend to be robust in the legs and farmers tend to be robust in the arms, representing different physical load (i.e., walking many miles a day versus grinding wheat).

Elizabeth Jane Wayland "E.J.W." Barber (née Wayland; born 1940) is an American scholar and expert on archaeology, linguistics, textiles, and folk dance as well as professor emerita of archaeology and linguistics at Occidental College.[1]
Elizabeth Jane Wayland was born in 1940 in Pasadena.[2] She became interested in archaeology at a young age because of her love of interdisciplinary sciences.[2] Her family moved to France during her childhood, where she learned French, beginning her interest in linguistics.[2] She first developed expert sewing and weaving skills under her mother's tutelage.[3]
She earned a bachelor's degree from Bryn Mawr College in Archaeology and Greek in 1962.[4]  Her chief mentor was Mabel Lang from whom she learned Linear B and who advised her honors thesis on Linear A. In addition to Lang, Wayland wrote her thesis under Emmett L. Bennett Jr. Her thesis used computer indices of the Hagia Triada Linear A texts in an attempt to decipher its signs and symbols.[5] The computer indices were made via punched cards, a method which was preceded by the work of Alice E. Kober on Linear B. She earned her PhD from Yale University in linguistics in 1968.[6] Her doctoral study at Yale University was supervised by Sydney Lamb, under whom she wrote her dissertation, "The Computer Aided Analysis of Undeciphered Ancient Texts."[7]
Her books include Prehistoric Textiles: The Development of Cloth in the Neolithic and Bronze Ages with Special Reference to the Aegean (1992), Women's Work: The First 20,000 Years; Women, Cloth, and Society in Early Times (1995), The Mummies of Ürümchi (1999), When They Severed Earth from Sky: How the Human Mind Shapes Myth (2004; coauthored with husband Paul T. Barber), The Dancing Goddesses: Folklore, Archaeology, and the Origins of European Dance (2013), Resplendent Dress from Southeastern Europe: A History in Layers (2013), and Two Thoughts with but a Single Mind: Crime and Punishment and the Writing of Fiction (2013; co-authored with husband  P.T. Barber and Mary F. Zirin).[1][8][9][10]
Among other things, she has proposed that if 19th-century scientists had thought to name prehistorical periods with an eye on women's work and the things they invented, instead of focusing their naming only on men's more durable inventions (Iron Age, Bronze Age, etc.), that they might have acknowledged women's invention of string as what she has named “The String Revolution.”[11]
In addition to her academic work, as of 2009 she has directed and choreographed for her own folk and historical dance troupe for 38 years.[12]
In 2016 and 2017, Barber's dance troupe performed at UCLA (See Video), Occidental College, and 2017 Sunshine Statewide Folk Dance Festival.

Transhuman, or trans-human, is the concept of an intermediary form between human and posthuman.[1] In other words, a transhuman is a being that resembles a human in most respects but who has powers and abilities beyond those of standard humans. These abilities might include improved intelligence, awareness, strength, and/or durability. Transhumans appear in science-fiction, sometimes as cyborgs or genetically-enhanced humans.
In his Divine Comedy, Dante Alighieri coined the word "trasumanar" meaning "to transcend human nature, to pass beyond human nature" in the first canto of Paradiso.[2][3]
The use of the term "transhuman" goes back to French philosopher Pierre Teilhard de Chardin, who wrote in his 1949 book The Future of Mankind.
Liberty: that is to say, the chance offered to every man (by removing obstacles and placing the appropriate means at his disposal) of 'trans-humanizing' himself by developing his potentialities to the fullest extent.[4]
And in a 1951 unpublished revision of the same book:
In consequence one is the less disposed to reject as unscientific the idea that the critical point of planetary Reflection, the fruit of socialization, far from being a mere spark in the darkness, represents our passage, by Translation or dematerialization, to another sphere of the Universe: not an ending of the ultra-human but its accession to some sort of trans-humanity at the ultimate heart of things.[5]
In 1957 book New Bottles for New Wine, English evolutionary biologist Julian Huxley wrote:
The human species can, if it wishes, transcend itself—not just sporadically, an individual here in one way, an individual there in another way, but in its entirety, as humanity. We need a name for this new belief. Perhaps transhumanism will serve: man remaining man, but transcending himself, by realizing new possibilities of and for his human nature.  "I believe in transhumanism": once there are enough people who can truly say that, the human species will be on the threshold of a new kind of existence, as different from ours as ours is from that of Peking man. It will at last be consciously fulfilling its real destiny.[6]
One of the first professors of futurology, FM-2030, who taught "new concepts of the Human" at The New School of New York City in the 1960s, used "transhuman" as shorthand for "transitional human". Calling transhumans the "earliest manifestation of new evolutionary beings", FM argued that signs of transhumans included physical and mental augmentations including prostheses, reconstructive surgery, intensive use of telecommunications, a cosmopolitan outlook and a globetrotting lifestyle, androgyny, mediated reproduction (such as in vitro fertilisation), absence of religious beliefs, and a rejection of traditional family values.[7]
FM-2030 used the concept of transhuman as an evolutionary transition, outside the confines of academia, in his contributing final chapter to the 1972 anthology Woman, Year 2000.[8] In the same year, American cryonics pioneer Robert Ettinger contributed to conceptualization of "transhumanity" in his book Man into Superman.[9] In 1982, American Natasha Vita-More authored a statement titled Transhumanist Arts Statement and outlined what she perceived as an emerging transhuman culture.[10]
Jacques Attali, writing in 2006, envisaged transhumans as an altruistic vanguard of the later 21st century:
Vanguard players (I shall call them transhumans) will run (they are already running) relational enterprises in which profit will be no more than a hindrance, not a final goal. Each of these transhumans will be altruistic, a citizen of the planet, at once nomadic and sedentary, his neighbor's equal in rights and obligations, hospitable and respectful of the world. Together, transhumans will give birth to planetary institutions and change the course of industrial enterprises.[11]
In March 2007, American physicist Gregory Cochran and paleoanthropologist John Hawks published a study, alongside other recent research on which it builds, which amounts to a radical reappraisal of traditional views, which tended to assume that humans have reached an evolutionary endpoint. Physical anthropologist Jeffrey McKee argued the new findings of accelerated evolution bear out predictions he made in a 2000 book The Riddled Chain. Based on computer models, he argued that evolution should speed up as a population grows because population growth creates more opportunities for new mutations; and the expanded population occupies new environmental niches, which would drive evolution in new directions. Whatever the implications of the recent findings, McKee concludes that they highlight a ubiquitous point about evolution: "every species is a transitional species".[12]
Examples of transhuman entities in fiction exist within many popular video games. For example, the Bioshock media franchise depicts individuals receiving doses of a substance called ADAM, harvested from a fictional type of sea slugs, able to give the user fantastical powers through genetic engineering. Thus, previously standard humans can gain the ability to summon ice, wield lightning, turn invisible, and commit other seeming miracles due to their enhancement.[13]
A 2014 article from Ars Technica speculated that mutating clumps of mobile genetic elements known as "transposons" could possibly be used as a semi-parasitic tool to raise people to a higher status in terms of their abilities, making at least part of the game's scenario theoretically plausible.[13] Similar commentary later occurred from gamers with the advent of CRISPR gene editing.
Transhumans also have played a major role in the Star Trek media franchise. For example, in "Space Seed", the twenty-second episode of the first season of Star Trek: The Original Series that initially aired on February 16, 1967, a charismatic and physically intimidating genius called Khan Noonien Singh attempts to take control of the Enterprise operated by the show's protagonists. The selectively bred individual had advanced beyond simple human status and nearly succeeds. The starship's crew opt to exile the leader and his league of similar beings to a habitable but isolated alien planet instead of assigning a true punishment per se, a ruling which he accepts without protest. Played by Ricardo Montalbán, Khan returns in the 1982 film Star Trek II: The Wrath of Khan, which broadly serves as a sequel to the episode. References to "Space Seed" appear in episodes of Star Trek: Deep Space Nine, Star Trek: Enterprise, and the 2013 film Star Trek Into Darkness as well.

Recorded history or written history describes the historical events that have been recorded in a written form or other documented communication which are subsequently evaluated by historians using the historical method. For broader world history, recorded history begins with the accounts of the ancient world around the 4th millennium BCE, and it coincides with the invention of writing.
For some geographic regions or cultures, written history is limited to a relatively recent period in human history because of the limited use of written records. Moreover, human cultures do not always record all of the information which is considered relevant by later historians, such as the full impact of natural disasters or the names of individuals. Recorded history for particular types of information is therefore limited based on the types of records kept. Because of this, recorded history in different contexts may refer to different periods of time depending on the topic.
The interpretation of recorded history often relies on historical method, or the set of techniques and guidelines by which historians use primary sources and other evidence to research and then to write accounts of the past. The question of what constitutes history, and whether there is an effective method for interpreting recorded history, is raised in the philosophy of history as a question of epistemology. The study of different historical methods is known as historiography, which focuses on examining how different interpreters of recorded history create different interpretations of historical evidence.
Prehistory traditionally refers to the span of time before recorded history, ending with the invention of writing systems.[1] Prehistory refers to the past in an area where no written records exist, or where the writing of a culture is not understood.
Protohistory refers to the transition period between prehistory and history, after the advent of literacy in a society but before the writings of the first historians. Protohistory may also refer to the period during which a culture or civilization has not yet developed writing, but other cultures have noted its existence in their own writings.
More complete writing systems were preceded by proto-writing. Early examples are the Jiahu symbols (c. 6600 BCE), Vinča signs (c. 5300 BCE), early Indus script (c. 3500 BCE) and Nsibidi script (c. before 500 CE). There is disagreement concerning exactly when prehistory becomes history, and when proto-writing became "true writing".[2] However, invention of the first writing systems is roughly contemporary with the beginning of the Bronze Age in the late Neolithic of the late 4th millennium BCE. The Sumerian archaic cuneiform script and the Egyptian hieroglyphs are generally considered the earliest writing systems, both emerging out of their ancestral proto-literate symbol systems from 3400 to 3200 BCE, with earliest coherent texts from about 2600 BCE.
The earliest chronologies date back to the earliest civilizations of Early Dynastic Period of Egypt, Mesopotamia and the Sumerians,[3] which emerged independently of each other from roughly 3500 BCE.[4] Earliest recorded history, which varies greatly in quality and reliability, deals with Pharaohs and their reigns, as preserved by ancient Egyptians.[5] Much of the earliest recorded history was re-discovered relatively recently due to archaeological dig sites findings.[6] A number of different traditions have developed in different parts of the world as to how to interpret these ancient accounts.
Dionysius of Halicarnassus knew of seven predecessors of Herodotus, including Hellanicus of Lesbos, Xanthus of Lydia and Hecataeus of Miletus. He described their works as simple, unadorned accounts of their own and other cities and people, Greek or foreign, including popular legends.
Herodotus (484 BCE – c. 425 BCE)[7] has generally been acclaimed as the "father of history" composing his The Histories from the 450s to the 420s BCE. However, his contemporary Thucydides (c. 460 BCE – c. 400 BCE) is credited[by whom?] with having first approached history with a well-developed historical method in his work the History of the Peloponnesian War. Thucydides, unlike Herodotus, regarded history as being the product of the choices and actions of human beings, and looked at cause and effect, rather than as the result of divine intervention.[7] History developed as a popular form of literature in later Greek and Roman societies in the works of Polybius, Tacitus and others.
Saint Augustine was influential in Christian and Western thought at the beginning of the medieval period. Through the Medieval and Renaissance periods, history was often studied through a sacred or religious perspective. Around 1800, German philosopher and historian Georg Wilhelm Friedrich Hegel brought philosophy and a more secular approach into historical study.[8]
According to John Tosh, "From the High Middle Ages (c.1000–1300) onwards, the written word survives in greater abundance than any other source for Western history."[9] Western historians developed methods comparable to modern historiographic research in the 17th and 18th centuries, especially in France and Germany, where they began investigating these source materials to write histories of their past. Many of these histories had strong ideological and political ties to their historical narratives. In the 20th century, academic historians began focusing less on epic nationalistic narratives, which often tended to glorify the nation or great men, to attempt more objective and complex analyses of social and intellectual forces. A major trend of historical methodology in the 20th century was a tendency to treat history more as a social science rather than as an art, which traditionally had been the case. French historians associated with the Annales School introduced quantitative history, using raw data to track the lives of typical individuals, and were prominent in the establishment of cultural history.
The Zuo Zhuan, attributed to Zuo Qiuming in the 5th century BCE covers the period from 722 to 468 BCE in a narrative form. The Book of Documents is one of the Five Classics of Chinese classic texts and one of the earliest narratives of China. The Spring and Autumn Annals, the official chronicle of the State of Lu covering the period from 722 to 481 BCE, is arranged on annalistic principles. It is traditionally attributed to Confucius (551–479 BCE). Zhan Guo Ce was a renowned ancient Chinese historical compilation of sporadic materials on the Warring States period compiled between the 3rd and 1st centuries BCE.
Sima Qian (around 100 BCE) was the first in China to lay the groundwork for professional historical writing. His written work was the Records of the Grand Historian, a monumental lifelong achievement in literature. Its scope extends as far back as the 16th century BCE, and it includes many treatises on specific subjects and individual biographies of prominent people, and also explores the lives and deeds of commoners, both contemporary and those of previous eras. His work influenced every subsequent author of history in China, including the prestigious Ban family of the Eastern Han dynasty era.
In Sri Lanka, the oldest historical text is the Mahavamsa (c. 5th century CE). Buddhist monks of the Anuradhapura Maha Viharaya maintained chronicles of Sri Lankan history starting from the 3rd century BCE. These annals were combined and compiled into a single document in the 5th century by the Mahanama of Anuradhapura while Dhatusena of Anuradhapura was ruling the Anuradhapura Kingdom. It was written based on prior ancient compilations known as the Atthakatha, which were commentaries written in Sinhala.[10][page needed] An earlier document known as the Dipavamsa (4th century CE) "Island Chronicles" is much simpler and contains less information than the Mahavamsa and was probably compiled using the Atthakatha on the Mahavamsa as well.
A companion volume, the Culavamsa "Lesser Chronicle", compiled by Sinhala monks, covers the period from the 4th century to the British takeover of Sri Lanka in 1815. The Culavamsa was compiled by a number of authors of different time periods.
The combined work, sometimes referred to collectively as the Mahavamsa, provides a continuous historical record of over two millennia, and is considered one of the world's longest unbroken historical accounts.[11] It is one of the few documents containing material relating to the Nāga and Yakkha peoples, indigenous inhabitants of Lanka prior to the legendary arrival of Prince Vijaya from Singha Pura of Kalinga.
The Sangam literature offers a window into some aspects of the ancient South Indian culture, secular and religious beliefs, and the people. For example, in the Sangam era Ainkurunuru poem 202 is one of the earliest mentions of "pigtail of Brahmin boys".[12] These poems also allude to historical incidents, ancient Tamil kings, the effect of war on loved ones and households.[13] The Pattinappalai poem in the Ten Idylls group, for example, paints a description of the Chola capital, the king Karikala, the life in a harbor city with ships and merchandise for seafaring trade, the dance troupes, the bards and artists, the worship of the Hindu god Murugan and the monasteries of Buddhism and Jainism.
Indica is an account of Mauryan India by the Greek writer Megasthenes. The original book is now lost, but its fragments have survived in later Greek and Latin works. The earliest of these works are those by Diodorus Siculus, Strabo (Geographica), Pliny, and Arrian (Indica).[14][15]
In the preface to his book, the Muqaddimah (1377), the Arab historian and early sociologist, Ibn Khaldun, warned of seven mistakes that he thought that historians regularly committed. In this criticism, he approached the past as strange and in need of interpretation. Ibn Khaldun often criticised "idle superstition and uncritical acceptance of historical data." As a result, he introduced a scientific method to the study of history, and he often referred to it as his "new science".[16] His historical method also laid the groundwork for the observation of the role of state, communication, propaganda and systematic bias in history,[17] and he is thus considered to be the "father of historiography"[18][19] or the "father of the philosophy of history".[20]
While recorded history begins with the invention of writing, over time new ways of recording history have come along with the advancement of technology. History can now be recorded through photography, audio recordings, and video recordings. More recently, Internet archives have been saving copies of webpages, documenting the history of the Internet. Other methods of collecting historical information have also accompanied the change in technologies; for example, since at least the 20th century, attempts have been made to preserve oral history by recording it. Until the 2000s this was done using analogue recording methods such as cassettes and reel-to-reel tapes. With the onset of new technologies, there are now digital recordings, which may be recorded to compact disks.[21] Nevertheless, historical record and interpretation often relies heavily on written records, partially because it dominates the extant historical materials, and partially because historians are used to communicating and researching in that medium.[22]
The historical method comprises the techniques and guidelines by which historians use primary sources and other evidence to research and then to write history. Primary sources are first-hand evidence of history (usually written, but sometimes captured in other mediums) made at the time of an event by a present person. Historians think of those sources as the closest to the origin of the information or idea under study.[23][24] These types of sources can provide researchers with, as Dalton and Charnigo put it, "direct, unmediated information about the object of study."[25]
Historians use other types of sources to understand history as well. Secondary sources are written accounts of history based upon the evidence from primary sources. These are sources which, usually, are accounts, works, or research that analyse, assimilate, evaluate, interpret, and/or synthesize primary sources. Tertiary sources are compilations based upon primary and secondary sources and often tell a more generalized account built on the more specific research found in the first two types of sources.[23][26][27]

Giant skeletons reported in the United States until the early 20th century were a combination of hoaxes, scams, fabrications, and the misidentifications of extinct megafauna. Many were reported to have been found in Native American burial mounds. Examples from 7 ft (2.1 m) to 20 ft (6.1 m) tall were reported in many parts of the United States.
The claims of "giant skeletons" were debunked in 1934 by Aleš Hrdlička, curator of anthropology at the Smithsonian Institution. The Smithsonian Institution opposed the popular myth that an "ancient white race" were the Mound Builders. The role of the Smithsonian Institution in debunking such claims led to a conspiracy theory that Smithsonian archeologists were destroying giants' bones in order to cover up the existence of giants.[1]
Hrdlička blamed the reports of giant skeletons on the "will to believe" coupled with "amateur anthropologists" who were unfamiliar with human anatomy. In 2014 an internet story began circulating which claimed that the Smithsonian Institution had custody of giant skeletons but they destroyed "thousands of giant skeletons" in the early 20th century. The internet story about the Smithsonian was debunked by Reuters and the Associated Press.
It calls up the indefinite past. When Columbus first sought this continent—when Christ suffered on the cross—when Moses led Israel through the Red-Sea—nay, even, when Adam first came from the hand of his Maker—then as now, Niagara was roaring here. The eyes of that species of extinct giants, whose bones fill the mounds of America, have gazed on Niagara, as ours do now. Contemporary with the whole race of men, and older than the first man, Niagara is strong, and fresh to-day as ten thousand years ago.
During the nineteenth century, there was widespread belief in North America of a prehistoric lost race. European settlers embraced myths of pre-Columbian settlements from the Old World, which reframed colonization as the continuation of a primordial past in which the roles of native peoples were diminished or dismissed. Through a process that historian Douglas Hunter described as "White Tribism", the settlers interpreted signs of "intellectual and cultural capabilities" in North American ruins, as signs of whiteness in their creators.[3] Based on the legendary voyages of the Welsh prince Madoc, the earliest English settlers sought and failed to uncover evidence of a civilizing Welsh influence in native peoples like the Mandan.[4] By the late eighteenth century, this paternalistic narrative had become strained, due in part to violence against the native peoples on the western frontier. White Americans developed the myth of the mound builder race, which provided a rationale for the colonization of the American Midwest. The various versions of the myth held that the massive earthworks of the Mississippi Valley, like Grave Creek Mound and the Great Serpent Mound, were not built by the ancestors of Native Americans, as is now widely believed. According to the myth, the Indians had exterminated a prehistoric, white race of mound builders. This cast genocidal violence towards the Native Americans as defensive or retributive.[5] Josiah Priest's American Antiquities, released in 1833, crystallized the idea of a lost race—mentioned in the Book of Genesis—that created the monuments of North America before being exterminated by savages.[6][3][7]: ch. 6
Between 1812 and the American Civil War (1861–1865), nearly all Americans writing about the continent's history used the myth of the white mound-building race.[5] In literature, Henry Wadsworth Longfellow imagined The Skeleton in Armor (now accepted to be a native leader), as a lovesick Norse Viking eloping with his "fair" and "blue-eyed" lover.[8] Sarah Josepha Hale accompanied her The Genius of Oblivion with end notes that claim "the ancient inhabitants [buried in the mounds] were of a different race from the Indians."[9] Preachers taught a biblical basis for the primordial race, including connections to the lost tribes or the Nephilim, giants from the Book of Genesis.[7]: 214-217, ch. 4
In academia, the creators of North America's earthworks were conflated with the creators of Mesoamerican megalithic structures, then understood to be the mythical Toltecs.[10]: 131  According to historian Christen Mucher, the Toltec connection facilitated the adoption of Spanish stories of a race of giants, like the 1519 account of the Tlaxcala leaders presenting Hernán Cortés with a massive bone, allegedly from a giant defeated by their ancestors.[10]: 153–154  In New Views of the Origin of the Tribes and Nations of America (1798), naturalist Benjamin Smith Barton who was familiar with the legends of giants across North America and discoveries of mastodon bones, warned against interpreting the "discovery of bones, sculls, and entire skeletons of prodigious size" as evidence of prehistoric giants.[10]: 128, 151, 154  Despite his warning, New Views would provide the basic framework for the Mound Builder myth and the later waves of giant skeleton reports.[10]: 164  As more information was discovered about Native American cultural complexity, the lost race became increasingly described as physically superior giants.[7]: 242 [10]: 154  Con artist George Hull created the Cardiff Giant hoax after an argument with Henry Turk, a preacher who taught that the biblical giants had literally walked the Earth.[7]: 245–249  PT Barnum commissioned a copy of the hoaxed giant, after a plan to have an 18-foot tall "skeleton prepared from various bones" failed.[7]: 243  The belief became so widespread, that Abraham Lincoln referenced the lost race of giants along with extinct Mastodons when describing the age of Niagara Falls.[2]
Hundreds of newspaper articles credulously described the purported discovery of giant skeletons, sometimes with anatomical irregularities attributed to the Nephilim.[1] For example, a massive skeleton unearthed in Tennessee toured the state as a specimen of this lost race. The reconstructed skeleton was mounted to a timber frame in a standing position with missing bones recreated from wood and rawhide.[11] Preachers, doctors, and journalists confirmed it to "belong to the genus homo" despite a standing height estimated up to twenty feet tall.[11][7]: 215  When the giant was taken to New Orleans, medical doctor William Carpenter found it to be a young mastodon's remains. Carpenter reported that there was not fraud—the man exhibiting the bones boxed them up after discovering they were not human—but rather a widespread desire to believe.[11] The mastodon ceased to be exhibited as a person, but soon other purported giants were unearthed, exhibited, reported, or sold for profit.[7]: 215 [12]
Throughout the 19th century, some scholars expressed doubt about the excavations of purported giants but had little impact on public perception.[7]: 248  Many readers embraced the skeletons as evidence of biblical history, against unpopular experts whose discoveries undermined a literal interpretation of the bible.[1]: 251  With a rise in white literacy rates and the emergence of the cheaper penny press newspapers, there was a strong market for these tales that gave them greater impact than university scholarship.[7]: 243  Stories frequently ran presenting as straight fact: hoaxes, scams, and misinterpretations of extinct megafauna. Some newspapers outright fabricated stories. The St. Louis Evening Chronicle published the account of researchers who explored a subterranean city beneath Moberly, Missouri. The account described massive underground streets where a skeleton—three times larger than a typical human—was found slumped over a public fountain.[7]: 243  Other newspapers reported the Moberly claims as factual[13] and republished the entire story including implausible details like the researcher who, upon drinking from the skeleton's fountain, described the water as "very sweet and nice".[14]
Ethnologist Cyrus Thomas spent years compiling his Report on the Mound Explorations for the Smithsonian Institution. The 1894 in-depth study on North America's earthworks provided over seven hundred pages of conclusive evidence that they were built by native peoples.[7]: 302  Thomas' report shifted academic attitudes but news reports of giant skeletons continued to come out for decades afterward.[7]: 312–319  It was common for the stories to claim that the bones were sent to the Smithsonian Institution. The Smithsonian's Bureau of Ethnology did encourage those excavating mounds, to send Native American bones to their Mound Exploration Division.[1][7]: 312  Prior to the 1990 Native American Graves Protection and Repatriation Act,[1] the Smithsonian collected over 18,000 of these skeletons. However, the sensationalist newspaper articles were often invoking the Smithsonian's name in order to lend credibility.[15][7]: 312  
As late as 1950, Paxson Hayes was featured for his claim that "blonde giants" once lived in the Americas.[16]   Paxson's claims were repeated in Frank Scully's 1950 book Behind the Flying Saucers and the writings of Meade Layne.[17][18]
In 1934, Aleš Hrdlička, curator of anthropology at the Smithsonian Institution rejected the existence of a race of giants between seven and 8 ft (2.4 m) tall. Hrdlička blamed the "will to believe" for the many reports of giant "discoveries".[19] Hrdlička blamed amateur anthropologists for being fooled by the bones. He stated that people were most often fooled by the length of the femur bone because they are often not familiar with human anatomy. Hrdlička also stated that reports of giant skeletons occurred two or three times per month.[20]
In 2020 The Columbus Dispatch reported that archeologist, Donald Ball collected articles about giant skeletons which were purportedly found in burial mounds dating as far back as 1845. He determined that when the claims about giant skeletons were scrutinized they did not reveal giant skeletons. One story in the Indianapolis Journal reported on August 29, 1883, that a 9 ft (2.7 m) skeleton had been found. Dr. M. M. Adams investigated and concluded that the bones were "not of a giant" and the individual was not "above five feet eight inches in height". He determined that it was a "giant fraud" upon the people.[21]
In 2014, an internet story reported that the Smithsonian Institution had custody of many giant skeletons and destroyed "thousands of giant skeletons" in the early 1900s. Reuters determined that the origin of the story was a satirical website called World News Daily Report. A spokesperson for the Smithsonian confirmed that the story was not true.[22] The satirical story claimed that the American Institution of Alternative Archeology accused the Smithsonian of a coverup.[23] The Associated Press also investigated and determined that the story was false.[24]
"Giant of Castelnau" refers to three bone fragments (a humerus, tibia, and femoral mid-shaft) discovered by Georges Vacher de Lapouge in 1890 in the sediment used to cover a Bronze Age burial tumulus, and dating possibly back to the Neolithic. Lapouge determined that the fossil bones may belong to one of the largest humans known to have existed. However, in 2022, Katherine Hacanyan asserted that this discovery by Lapouge was most likely a cave bear and not a human.[25] Giant skeletons of animals were often mistaken for giant human bones during previous centuries and during the early 20th century. This was due to a lack of expertise in human bone structure by those who discovered the bones. Also, some discoveries were intentionally misrepresented for various reasons.[26]
In 1984, the anthropologist Sheilagh Brooks examined the Reid Collection, an assemblage of Native American skeletons unearthed in Nevada by John Reid in the early 1900s which was said to contain an individual that measured 9 ft 6 in (2.90 m). However, Brooks found that no skeleton measured more than 5 ft 11 in (1.80 m). Brooks concluded that since Reid estimated the heights of these skeletons by measuring their femurs against his thigh, his overestimate likely occurred because he was unaware that the head of the femur is inserted in the pelvic socket and does not extend outward.[27]
In 2019, the Travel Channel series Code of the Wild aired an episode in which a pre-Columbian skeleton was presented that was allegedly 7 feet tall and Salasaca storytellers were interviewed that related oral traditions of giants.
In 2024, Nicholas Landol used mathematical formula to determine that the individual was actually only between 153.34 cm and 162.37 cm and that due to the disarticulation that a skeleton experiences after death, a skeleton can appear larger than it is. As this was only one sample they also wrote that "Future analysis remains essential, however, to the evaluation of the Indigenous oral traditions of Ecuador".[28]
The fact-checking website Snopes records similar hoaxes in Saudi Arabia and India.[29]

The origin of speech differs from the origin of language because language is not necessarily spoken; it could equally be written or signed. Speech is a fundamental aspect of human communication and plays a vital role in the everyday lives of humans. It allows them to convey thoughts, emotions, and ideas, and providing the ability to connect with others and shape collective reality.[1][2]
Many attempts have been made to explain scientifically how speech emerged in humans, although to date no theory has generated agreement.
Non-human primates, like many other animals, have evolved specialized mechanisms for producing sounds for purposes of social communication.[3] On the other hand, no monkey or ape uses its tongue for such purposes.[4][5] The human species' unprecedented use of the tongue, lips and other moveable parts seems to place speech in a quite separate category, making its evolutionary emergence an intriguing theoretical challenge in the eyes of many scholars.[6]
The term modality means the chosen representational format for encoding and transmitting information. A striking feature of language is that it is modality-independent. Should an impaired child be prevented from hearing or producing sound, its innate capacity to master a language may equally find expression in signing. Sign languages of the deaf are independently invented and have all the major properties of spoken language except for the modality of transmission.[7][8][9][10] From this it appears that the language centres of the human brain must have evolved to function optimally, irrespective of the selected modality.
"The detachment from modality-specific inputs may represent a substantial change in neural organization, one that affects not only imitation but also communication; only humans can lose one modality (e.g. hearing) and make up for this deficit by communicating with complete competence in a different modality (i.e. signing)."
Animal communication systems routinely combine visible with audible properties and effects, but none is modality-independent. For example, no vocally-impaired whale, dolphin, or songbird could express its song repertoire equally in visual display. Indeed, in the case of animal communication, message and modality are not capable of being disentangled. Whatever message is being conveyed stems from the intrinsic properties of the signal.
Modality independence should not be confused with the ordinary phenomenon of multimodality. Monkeys and apes rely on a repertoire of species-specific "gesture-calls" – emotionally-expressive vocalisations inseparable from the visual displays which accompany them.[12][13] Humans also have species-specific gesture-calls – laughs, cries, sobs, etc. – together with involuntary gestures accompanying speech.[14][15][16] Many animal displays are polymodal in that each appears designed to exploit multiple channels simultaneously.
The human linguistic property of modality independence is conceptually distinct from polymodality. It allows the speaker to encode the informational content of a message in a single channel whilst switching between channels as necessary. Modern city-dwellers switch effortlessly between the spoken word and writing in its various forms – handwriting, typing, email, etc. Whichever modality is chosen, it can reliably transmit the full message content without external assistance of any kind. When talking on the telephone, for example, any accompanying facial or manual gestures, however natural to the speaker, are not strictly necessary. When typing or manually signing, conversely, there is no need to add sounds. In many Australian Aboriginal cultures, a section of the population – perhaps women observing a ritual taboo – traditionally restrict themselves for extended periods to a silent (manually-signed) version of their language.[17] Then, when released from the taboo, these same individuals resume narrating stories by the fireside or in the dark, switching to pure sound without sacrifice of informational content.
Speaking is the default modality for language in all cultures. Humans' first recourse is to encode their thoughts in sound – a method which depends on sophisticated capacities for controlling the lips, tongue and other components of the vocal apparatus.
The speech organs evolved in the first instance not for speech but for more basic bodily functions such as feeding and breathing. Nonhuman primates have broadly similar organs, but with different neural controls.[6] Non-human apes use their highly-flexible, maneuverable tongues for eating but not for vocalizing. When an ape is not eating, fine motor control over its tongue is deactivated.[4][5] Either it is performing gymnastics with its tongue or it is vocalising; it cannot perform both activities simultaneously. Since this applies to mammals in general, Homo sapiens are exceptional in harnessing mechanisms designed for respiration and ingestion for the radically different requirements of articulate speech.[18]
The word "language" derives from the Latin lingua, "tongue". Phoneticians agree that the tongue is the most important speech articulator, followed by the lips. A natural language can be viewed as a particular way of using the tongue to express thought.
The human tongue has an unusual shape. In most mammals, it is a long, flat structure contained largely within the mouth. It is attached at the rear to the hyoid bone, situated below the oral level in the pharynx. In humans, the tongue has an almost circular sagittal (midline) contour, much of it lying vertically down an extended pharynx, where it is attached to a hyoid bone in a lowered position. Partly as a result of this, the horizontal (inside-the-mouth) and vertical (down-the-throat) tubes forming the supralaryngeal vocal tract (SVT) are almost equal in length (whereas in other species, the vertical section is shorter). As humans move their jaws up and down, the tongue can vary the cross-sectional area of each tube independently by about 10:1, altering formant frequencies accordingly. That the tubes are joined at a right angle permits pronunciation of the vowels [i], [u] and [a], which nonhuman primates cannot do.[19] Even when not performed particularly accurately, in humans the articulatory gymnastics needed to distinguish these vowels yield consistent, distinctive acoustic results, illustrating the quantal[clarification needed] nature of human speech sounds.[20] It may not be coincidental that [i], [u] and [a] are the most common vowels in the world's languages.[21] Human tongues are a lot shorter and thinner than other mammals and are composed of a large number of muscles, which helps shape a variety of sounds within the oral cavity. The diversity of sound production is also increased with the human’s ability to open and close the airway, allowing varying amounts of air to exit through the nose. The fine motor movements associated with the tongue and the airway, make humans more capable of producing a wide range of intricate shapes in order to produce sounds at different rates and intensities.[22]
In humans, the lips are important for the production of stops and fricatives, in addition to vowels. Nothing, however, suggests that the lips evolved for those reasons. During primate evolution, a shift from nocturnal to diurnal activity in tarsiers, monkeys and apes (the haplorhines) brought with it an increased reliance on vision at the expense of olfaction. As a result, the snout became reduced and the rhinarium or "wet nose" was lost. The muscles of the face and lips consequently became less constrained, enabling their co-option to serve purposes of facial expression. The lips also became thicker, and the oral cavity hidden behind became smaller.[22] Hence, according to Ann MacLarnon, "the evolution of mobile, muscular lips, so important to human speech, was the exaptive result of the evolution of diurnality and visual communication in the common ancestor of haplorhines".[23] It is unclear whether human lips have undergone a more recent adaptation to the specific requirements of speech.
Compared with nonhuman primates, humans have significantly enhanced control of breathing, enabling exhalations to be extended and inhalations shortened as we speak. Whilst we are speaking, intercostal and interior abdominal muscles are recruited to expand the thorax and draw air into the lungs, and subsequently to control the release of air as the lungs deflate. The muscles concerned are markedly more innervated in humans than in nonhuman primates.[24] Evidence from fossil hominins suggests that the necessary enlargement of the vertebral canal, and therefore spinal cord dimensions, may not have occurred in Australopithecus or Homo erectus but was present in the Neanderthals and early modern humans.[25][26]
The larynx or voice box is an organ in the neck housing the vocal folds, which are responsible for phonation. In humans, the larynx is descended, it is positioned lower than in other primates. This is because the evolution of humans to an upright position shifted the head directly above the spinal cord, forcing everything else downward. The repositioning of the larynx resulted in a longer cavity called the pharynx, which is responsible for increasing the range and clarity of the sound being produced. Other primates have almost no pharynx; therefore, their vocal power is significantly lower.[22] Humans are not unique in this respect: goats, dogs, pigs and tamarins lower the larynx temporarily, to emit loud calls.[27] Several deer species have a permanently lowered larynx, which may be lowered still further by males during their roaring displays.[28] Lions, jaguars, cheetahs and domestic cats also do this.[29] However, laryngeal descent in nonhumans (according to Philip Lieberman) is not accompanied by descent of the hyoid; hence the tongue remains horizontal in the oral cavity, preventing it from acting as a pharyngeal articulator.[30]
Despite all this, scholars remain divided as to how "special" the human vocal tract really is. It has been shown that the larynx does descend to some extent during development in chimpanzees, followed by hyoidal descent.[31] As against this, Philip Lieberman points out that only humans have evolved permanent and substantial laryngeal descent in association with hyoidal descent, resulting in a curved tongue and two-tube vocal tract with 1:1 proportions.[citation needed] Uniquely in the human case, simple contact between the epiglottis and velum is no longer possible, disrupting the normal mammalian separation of the respiratory and digestive tracts during swallowing. Since this entails substantial costs – increasing the risk of choking whilst swallowing food – we are forced to ask what benefits might have outweighed those costs. Some claim the clear benefit must have been speech, but other contest this. One objection is that humans are in fact not seriously at risk of choking on food: medical statistics indicate that accidents of this kind are extremely rare.[32] Another objection is that in the view of most scholars, speech as we know it emerged relatively late in human evolution, roughly contemporaneously with the emergence of Homo sapiens.[33] A development as complex as the reconfiguration of the human vocal tract would have required much more time, implying an early date of origin. This discrepancy in timescales undermines the idea that human vocal flexibility was initially driven by selection pressures for speech.
At least one orangutan has demonstrated the ability to control the voice box.[34]
To lower the larynx is to increase the length of the vocal tract, in turn lowering formant frequencies so that the voice sounds "deeper" – giving an impression of greater size. John Ohala argued that the function of the lowered larynx in humans, especially males, is probably to enhance threat displays rather than speech itself.[35] Ohala pointed out that if the lowered larynx were an adaptation for speech, we would expect adult human males to be better adapted in this respect than adult females, whose larynx is considerably less low. In fact, females invariably outperform males in verbal tests, falsifying this whole line of reasoning.[citation needed] William Tecumseh Fitch likewise argues that this was the original selective advantage of laryngeal lowering in humans. Although, according to Fitch, the initial lowering of the larynx in humans had nothing to do with speech, the increased range of possible formant patterns was subsequently co-opted for speech. Size exaggeration remains the sole function of the extreme laryngeal descent observed in male deer. Consistent with the size exaggeration hypothesis, a second descent of the larynx occurs at puberty in humans, although only in males. In response to the objection that the larynx is descended in human females, Fitch suggests that mothers vocalising to protect their infants would also have benefited from this ability.[36]
Most specialists credit the Neanderthals with speech abilities not radically different from those of modern Homo sapiens. An indirect line of argument is that their toolmaking and hunting tactics would have been difficult to learn or execute without some kind of speech.[37] A recent extraction of DNA from Neanderthal bones indicates that Neanderthals had the same version of the FOXP2 gene as modern humans. This gene, mistakenly described as the "grammar gene", plays a role in controlling the orofacial movements which (in modern humans) are involved in speech.[38]
During the 1970s, it was widely believed that the Neanderthals lacked modern speech capacities.[39] It was claimed that they possessed a hyoid bone so high up in the vocal tract as to preclude the possibility of producing certain vowel sounds.
The hyoid bone is present in many mammals. It allows a wide range of tongue, pharyngeal and laryngeal movements by bracing these structures alongside each other in order to produce variation.[40] It is now realised that its lowered position is not unique to Homo sapiens, whilst its relevance to vocal flexibility may have been overstated: although men have a lower larynx, they do not produce a wider range of sounds than women or two-year-old babies. There is no evidence that the larynx position of the Neanderthals impeded the range of vowel sounds they could produce.[41] The discovery of a modern-looking hyoid bone of a Neanderthal man in the Kebara Cave in Israel led its discoverers to argue that the Neanderthals had a descended larynx, and thus human-like speech capabilities.[42][43] However, other researchers have claimed that the morphology of the hyoid is not indicative of the larynx's position.[6] It is necessary to take into consideration the skull base, the mandible, the cervical vertebrae and a cranial reference plane.[44][45]
The morphology of the outer and middle ear of Middle Pleistocene hominins from Atapuerca, Spain, believed to be proto-Neanderthal, suggests they had an auditory sensitivity similar to modern humans and very different from chimpanzees. They were probably able to differentiate between many different speech sounds.[46]
The hypoglossal nerve plays an important role in controlling movements of the tongue. In 1998, a research team used the size of the hypoglossal canal in the base of fossil skulls in an attempt to estimate the relative number of nerve fibres, claiming on this basis that Middle Pleistocene hominins and Neanderthals had more fine-tuned tongue control than either Australopithecines or apes.[47] Subsequently, however, it was demonstrated that hypoglossal canal size and nerve sizes are not correlated,[48] and it is now accepted that such evidence is uninformative about the timing of human speech evolution.[49]
Legend: unrounded • rounded
According to one influential school,[50][51] the human vocal apparatus is intrinsically digital on the model of a keyboard or digital computer[clarification needed] (see below). Nothing about a chimpanzee's vocal apparatus suggests a digital keyboard[clarification needed], notwithstanding the anatomical and physiological similarities. This poses the question as to when and how, during the course of human evolution, the transition from analog to digital structure and function occurred.
The human supralaryngeal tract is said to be digital in the sense that it is an arrangement of moveable toggles or switches, each of which, at any one time, must be in one state or another. The vocal cords, for example, are either vibrating (producing a sound) or not vibrating (in silent mode). By virtue of simple physics, the corresponding distinctive feature – in this case, "voicing" – cannot be somewhere in between. The options are limited to "off" and "on". Equally digital is the feature known as "nasalisation". At any given moment the soft palate or velum either allows or does not allow sound to resonate in the nasal chamber. In the case of lip and tongue positions, more than two digital states may be allowed.
The theory that speech sounds are composite entities constituted by complexes of binary phonetic features was first advanced in 1938 by the Russian linguist Roman Jakobson.[52] A prominent early supporter of this approach was Noam Chomsky, who went on to extend it from phonology to language more generally, in particular to the study of syntax and semantics.[53][54][55] In his 1965 book, Aspects of the Theory of Syntax,[56] Chomsky treated semantic concepts as combinations of binary-digital atomic elements explicitly on the model of distinctive features theory. The lexical item "bachelor", on this basis, would be expressed as [+ Human], [+ Male], [- Married].
Supporters of this approach view the vowels and consonants recognised by speakers of a particular language or dialect at a particular time as cultural entities of little scientific interest. From a natural science standpoint, the units which matter are those common to Homo sapiens by virtue of biological nature. By combining the atomic elements or "features" with which all humans are innately equipped, anyone may in principle generate the entire range of vowels and consonants to be found in any of the world's languages, whether past, present or future. The distinctive features are in this sense atomic components of a universal language.
In recent years, the notion of an innate "universal grammar" underlying phonological variation has been called into question. The most comprehensive monograph ever written about speech sounds, The Sounds of the World's Languages, by Peter Ladefoged and Ian Maddieson,[21] found virtually no basis for the postulation of some small number of fixed, discrete, universal phonetic features. Examining 305 languages, for example, they encountered vowels that were positioned basically everywhere along the articulatory and acoustic continuum. Ladefoged concluded that phonological features are not determined by human nature: "Phonological features are best regarded as artifacts that linguists have devised in order to describe linguistic systems".[57]
Self-organisation characterises systems where macroscopic structures are spontaneously formed out of local interactions between the many components of the system.[58] In self-organised systems, global organisational properties are not to be found at the local level. In colloquial terms, self-organisation is roughly captured by the idea of "bottom-up" (as opposed to "top-down") organisation. Examples of self-organised systems range from ice crystals to galaxy spirals in the inorganic world.
According to many phoneticians, the sounds of language arrange and re-arrange themselves through self-organisation.[58][59][60] Speech sounds have both perceptual (how one hears them) and articulatory (how one produces them) properties, all with continuous values. Speakers tend to minimise effort, favouring ease of articulation over clarity. Listeners do the opposite, favouring sounds that are easy to distinguish even if difficult to pronounce. Since speakers and listeners are constantly switching roles, the syllable systems actually found in the world's languages turn out to be a compromise between acoustic distinctiveness on the one hand, and articulatory ease on the other.
Agent-based computer models take the perspective of self-organisation at the level of the speech community or population. The two main paradigms are (1) the iterated learning model and (2) the language game model. Iterated learning focuses on transmission from generation to generation, typically with just one agent in each generation.[61] In the language game model, a whole population of agents simultaneously produce, perceive and learn language, inventing novel forms when the need arises.[62][63]
Several models have shown how relatively simple peer-to-peer vocal interactions, such as imitation, can spontaneously self-organise a system of sounds shared by the whole population, and different in different populations. For example, models elaborated by Berrah et al. (1996)[64] and de Boer (2000),[65] and recently reformulated using Bayesian theory,[66] showed how a group of individuals playing imitation games can self-organise repertoires of vowel sounds which share substantial properties with human vowel systems. For example, in de Boer's model, initially vowels are generated randomly, but agents learn from each other as they interact repeatedly over time. Agent A chooses a vowel from her repertoire and produces it, inevitably with some noise. Agent B hears this vowel and chooses the closest equivalent from her own repertoire. To check whether this truly matches the original, B produces the vowel she thinks she has heard, whereupon A refers once again to her own repertoire to find the closest equivalent. If this matches the one she initially selected, the game is successful, otherwise, it has failed. "Through repeated interactions", according to de Boer, "vowel systems emerge that are very much like the ones found in human languages".[67]
In a different model, the phonetician Björn Lindblom[68] was able to predict, on self-organisational grounds, the favoured choices of vowel systems ranging from three to nine vowels on the basis of a principle of optimal perceptual differentiation.
Further models studied the role of self-organisation in the origins of phonemic coding and combinatoriality, which is the existence of phonemes and their systematic reuse to build structured syllables. Pierre-Yves Oudeyer developed models which showed that basic neural equipment for adaptive holistic vocal imitation, coupling directly motor and perceptual representations in the brain, can generate spontaneously shared combinatorial systems of vocalisations, including phonotactic patterns, in a society of babbling individuals.[58][69] These models also characterised how morphological and physiological innate constraints can interact with these self-organised mechanisms to account for both the formation of statistical regularities and diversity in vocalisation systems.
The gestural theory states that speech was a relatively late development, evolving by degrees from a system that was originally gestural. Human ancestors were unable to control their vocalisation at the time when gestures were used to communicate; however, as they slowly began to control their vocalisations, spoken language began to evolve.
Three types of evidence support this theory:
Research has found strong support for the idea that spoken language and signing depend on similar neural structures. Patients who used sign language, and who suffered from a left-hemisphere lesion, showed the same disorders with their sign language as vocal patients did with their oral language.[71] Other researchers found that the same left-hemisphere brain regions were active during sign language as during the use of vocal or written language.[72]
Humans spontaneously use hand and facial gestures when formulating ideas to be conveyed in speech.[73][74] There are also, of course, many sign languages in existence, commonly associated with deaf communities; as noted above, these are equal in complexity, sophistication, and expressive power, to any oral language. The main difference is that the "phonemes" are produced on the outside of the body, articulated with hands, body, and facial expression, rather than inside the body articulated with tongue, teeth, lips, and breathing.
Many psychologists and scientists have looked into the mirror system in the brain to answer this theory as well as other behavioural theories. Evidence to support mirror neurons as a factor in the evolution of speech includes mirror neurons in primates, the success of teaching apes to communicate gesturally, and pointing/gesturing to teach young children language. Fogassi and Ferrari (2014)[citation needed] monitored motor cortex activity in monkeys, specifically area F5 in the Broca’s area, where mirror neurons are located. They observed changes in electrical activity in this area when the monkey executed or observed different hand actions performed by someone else. Broca’s area is a region in the frontal lobe responsible for language production and processing. The discovery of mirror neurons in this region, which fire when an action is done or observed specifically with the hand, strongly supports the belief that communication was once accomplished with gestures. The same is true when teaching young children language. When one points at a specific object or location, mirror neurons in the child fire as though they were doing the action, which results in long-term learning[75]
Critics note that for mammals in general, sound turns out to be the best medium in which to encode information for transmission over distances at speed. Given the probability that this applied also to early humans, it is hard to see why they should have abandoned this efficient method in favour of more costly and cumbersome systems of visual gesturing – only to return to sound at a later stage.[76]
By way of explanation, it has been proposed that at a relatively late stage in human evolution, hands became so much in demand for making and using tools that the competing demands of manual gesturing became a hindrance. The transition to spoken language is said to have occurred only at that point.[77] Since humans throughout evolution have been making and using tools, however, most scholars remain unconvinced by this argument. (For a different approach to this issue – one setting out from considerations of signal reliability and trust – see "from pantomime to speech" below).
Recent insights in human evolution – more specifically, human Pleistocene littoral evolution[78] – may help understand how human speech evolved. One controversial suggestion is that certain pre-adaptations for spoken language evolved during a time when ancestral hominins lived close to river banks and lake shores rich in fatty acids and other brain-specific nutrients. Occasional wading or swimming may also have led to enhanced breath-control (breath-hold diving).
Independent lines of evidence suggest that "archaic" Homo spread intercontinentally along the Indian Ocean shores (they even reached overseas islands such as Flores) where they regularly dived for littoral foods such as shell- and crayfish,[79] which are extremely rich in brain-specific nutrients, explaining Homo's brain enlargement.[80] Shallow diving for seafoods requires voluntary airway control, a prerequisite for spoken language. Seafood such as shellfish generally does not require biting and chewing, but stone tool use and suction feeding. This finer control of the oral apparatus was arguably another biological pre-adaptation to human speech, especially for the production of consonants.[81]
Little is known about the timing of language's emergence in the human species. Unlike writing, speech leaves no material trace, making it archaeologically invisible. Lacking direct linguistic evidence, specialists in human origins have resorted to the study of anatomical features and genes arguably associated with speech production. Whilst such studies may provide information as to whether pre-modern Homo species had speech capacities, it is still unknown whether they actually spoke. Whilst they may have communicated vocally, the anatomical and genetic data lack the resolution necessary to differentiate proto-language from speech.
Using statistical methods to estimate the time required to achieve the current spread and diversity in modern languages today, Johanna Nichols – a linguist at the University of California, Berkeley – argued in 1998 that vocal languages must have begun diversifying at least 100,000 years ago.[82]
In 2012, anthropologists Charles Perreault and Sarah Mathew used phonemic diversity to suggest a date consistent with this.[83] "Phonemic diversity" denotes the number of perceptually distinct units of sound – consonants, vowels and tones – in a language. The current worldwide pattern of phonemic diversity potentially contains the statistical signal of the expansion of modern Homo sapiens out of Africa, beginning around 60-70 thousand years ago. Some scholars argue that phonemic diversity evolves slowly and can be used as a clock to calculate how long the oldest African languages would have to have been around in order to accumulate the number of phonemes they possess today. As human populations left Africa and expanded into the rest of the world, they underwent a series of bottlenecks – points at which only a very small population survived to colonise a new continent or region. Allegedly such a population crash led to a corresponding reduction in genetic, phenotypic and phonemic diversity. African languages today have some of the largest phonemic inventories in the world, whilst the smallest inventories are found in South America and Oceania, some of the last regions of the globe to be colonised. For example, Rotokas, a language of New Guinea, and Pirahã, spoken in South America, both have just 11 phonemes,[84][85] whilst !Xun, a language spoken in Southern Africa has 141 phonemes.
The authors use a natural experiment – the colonization of mainland Southeast Asia on the one hand, the long-isolated Andaman Islands on the other – to estimate the rate at which phonemic diversity increases through time. Using this rate, they estimate that the world's languages date back to the Middle Stone Age in Africa, sometime between 350 thousand and 150 thousand years ago. This corresponds to the speciation event which gave rise to Homo sapiens.
These and similar studies have however been criticised by linguists who argue that they are based on a flawed analogy between genes and phonemes, since phonemes are frequently transferred laterally between languages unlike genes, and on a flawed sampling of the world's languages, since both Oceania and the Americas also contain languages with very high numbers of phonemes, and Africa contains languages with very few. They argue that the actual distribution of phonemic diversity in the world reflects recent language contact and not deep language history - since it is well demonstrated that languages can lose or gain many phonemes over very short periods. In other words, there is no valid linguistic reason to expect genetic founder effects to influence phonemic diversity.[86][87]

Southeast Asia was first reached by anatomically modern humans possibly before 70,000 years ago.[1] Anatomically modern humans are suggested to have reached Southeast Asia twice in the course of the Southern Dispersal migrations during and after the formation of a distinct East Asian clade from 70,000 to 50,000 years ago.[2][3]
In Asia, the most recent late archaic human fossils were found in Thailand (125-100 ka), the Philippines (58-24 ka), Malaysia (c. 40 ka), and Sri Lanka (c.36 ka).[4] The artifacts from these sites include partial skeleton, crania, deep skull, and other related skeletons indicate that modern human migrated to Asia earlier than the western theory might have discussed.[5]
In 2007, an analysis of cut marks on two bovid bones found in Sangiran, showed them to have been made 1.5 to 1.6 million years ago by clamshell tools. This may be the oldest evidence for the presence of early humans in today Indonesia and are to date the oldest evidence of shell tool use in the world.[6]
In 2009, archaeologists discovered the partial cranium and some teeth of a modern human at Tam Pa Ling in mainland Laos, which shed light on the understanding of anatomically modern human migration and evolution in the region during the Late Pleistocene Period.[5] The site is located in Houaphanh Province, around 170 miles north of Vientiane, the capital city of modern Laos. Within this site, only human remains were found, and there is no evidence of human occupation or other artifacts. The radiocarbon dating of the charcoal and the sediment dating analyses identify the remains to date at least c. 56.5 ka, while the dental artifacts from the remains that analyzed by the isotope-ratio measurement indicate c. 63.6 ka.[5] The analysis of the cranium and dentition of the remains suggest that these are the remains of early modern human populations in Southeast Asia. This date is older than the fossils that were found in Niah cave in Malaysia, which offers another explanation for human evolution in Southeast Asia.
In addition to the discovery in Laos, there are also a number of human remains and related artifacts found across mainland Southeast Asia in which it suggests the new ideas of the regional Late Pleistocene development as well. More teeth and molars that were found in Thailand and Vietnam sites (Tham Wihan Naki, Thailand; Tham Kuyean, Vietnam, etc.) indicate transitions between H. erectus and H. sapiens.[7] In fact, these remains might indicate the possible interbreeding between H. erectus and H. sapiens, such as the tooth at Wihan Nakin at Chaiyaphum province in Thailand.[7]
The earliest modern human inhabitants of Southeast Asia were hunter-gatherers that arrived in the area at least c. 40,000 BP.[8] Contemporary remnant groups of these earliest inhabitants (e.g. the Semang of Malaysia or the Aetas of the Philippines) are usually included under the cover term "Negrito".[9] The earliest settlers had sufficient maritime technology to cross the Wallace Line, probably at a similar date to the first settlement of Sahul (c. 45,000 BP/49,000 – 43,000 BP).[10]: 50  During the last glacial maximum, the sea level decreased and promoted human migrations that increased genetic admixture among Southeast Asian populations.
The Neolithic was characterized by several early migrations from southern China into Mainland and Island Southeast Asia by Austroasiatic and Austronesian-speakers.[9][11][8]
These routes also allowed the early arrival to Philippines and have been formally evaluated through extensive genetic analyses.
Therefore, during the Neolithic, Austroasiatic peoples populated Indochina via a variety of land routes. The earliest agricultural societies that cultivated millet and wet-rice emerged around 1700 BCE in the lowlands and river floodplains of Indochina.[13] Based on archaeological and genetic evidence, it is assumed that Austroasiatic speakers also expanded into Insular Southeast Asia in the Neolithic, but were later supplanted or assimilated by Austronesian speakers.[12]
The most widespread migration event was the Austronesian expansion, which began at around 5,500 BP (3500 BC) from coastal southern China via Taiwan. Due to their use of ocean-going outrigger boats and voyaging catamarans,[a] Austronesians rapidly colonized Island Southeast Asia, before spreading further into Micronesia, Melanesia, Polynesia, Madagascar[b] and the Comoros. They dominated the lowlands and coasts of Island Southeast Asia, giving rise to modern Islander Southeast Asians, Micronesians, Polynesians, and Malagasy.[15][16][17][18] The first Austronesians reached the Philippines at around 2200 BC, settling the Batanes Islands and northern Luzon from Taiwan. From there, they rapidly spread downwards to the rest of the islands of the Philippines and Southeast Asia.[19][20]
The widespread presence of Kra-Dai, Tibeto-Burman, and Hmong-Mien speakers in Mainland Southeast Asia is the result of later migrations. Originating from southern China, where many languages of these families are still spoken, they expanded southwards into Southeast Asia in historical times around the second half of the first millennium CE.[9]
Territorial principalities in both Insular and Mainland Southeast Asia, characterised as Agrarian kingdoms[21] had by around 500 BCE developed an economy based on surplus crop cultivation and moderate coastal trade of domestic natural products. Several states of the Malayan-Indonesian "thalassian" zone[22] shared these characteristics with Indochinese polities like the Pyu city-states in the Irrawaddy river valley, Van Lang in the Red River delta and Funan around the lower Mekong.[23] Văn Lang, founded in the 7th century BCE endured until 258 BCE under the rule of the Hồng Bàng dynasty, as part of the Đông Sơn culture eventually sustained a dense and organised population, that produced an elaborate Bronze Age industry.[24][25]
Intensive wet-rice cultivation in an ideal climate enabled the farming communities to produce a regular crop surplus, that was used by the ruling elite to raise, command and pay work forces for public construction and maintenance projects such as canals and fortifications.[24][22] Though millet and rice cultivation was introduced around 2000 BCE, hunting and gathering remained an important aspect of food provision, in particular in forested and mountainous inland areas.[26]
Historians have emphasized the maritime connectivity of the Southeast Asian region whereby it can be analyzed as a single cultural and economic unit, as has been done with the Mediterranean basin.[27] This region stretches from the Yangtze delta in China down to the Malay Peninsula, including the South China Sea, Gulf of Thailand and Java Sea. The region was dominated by the thalassocratic cultures of the Austronesian peoples.[28][29][30]
One study (Chaubey 2015) found evidence for ancient gene flow from East Asian-related groups into the Andamanese people, suggesting that Andamanese (Onge) had about 30% East Asian-related ancestry next to their original Negrito ancestry, though the authors also suggest that this latter finding may in fact reflect the genetic affinity of the Andamanese to Melanesian, Southeast Asian, and Asian Negrito populations rather than true East Asian admixture (stating that "The Han ancestry measured in Andaman Negrito is probably partially capturing both Melanesian and Malaysian Negrito ancestry"),[32] as a previous study by the authors (Chaubey et al.) indicated "a deep common ancestry" between Andamanese, Melanesians and other Negrito groups (as well as South Asians), and an affinity between Southeast Asian Negritos and Melanesians with East Asians.[33]
According to McColl et. al (2018), present-day Southeast Asians derive their ancestry from at least four ancient populations. The oldest source derives from mainland Hòabìnhians, who share ancestry with present-day Andamanese Önge, Malaysian Jehai, and the ancient Japanese Ikawazu Jōmon. During the Neolithic period, East Asian farmers intermixed with the native inhabitants and contrary to popular opinion, did not replace them. These farmers also shared ancestry with present-day Austroasiatic-speaking hill tribes themselves. About 2000 years ago, new East Asian ancestral components were introduced and coincided with the introduction of ancestral Kradai languages and Austronesian culture in mainland  and maritime Southeast Asia respectively.[34]
A 2020 genetic study on Southeast Asian populations focusing on ethnic groups in Vietnam by Liu et al. 2020 found that most sampled groups are closely related to East Asians and carry mostly "East Asian-related" ancestry. Modern Austronesian and Austroasiatic speaking populations of Southeast Asia were found to have mostly East Asian-related ancestry (89% to 96%, with 94% on average). Taiwanese indigenous peoples had on average 99% East Asian-related ancestry. Kra–Dai-speaking populations had, similar to the Taiwanese indigenous peoples, nearly exclusively East Asian-related ancestry.[35]
A recent study from 2021 found that an ancient Holocene hunter-gatherer from South Sulawesi had ancestry from both a distinct lineage related to modern Papuans and Aboriginal Australians and from the East-Eurasian lineage (represented by modern East Asians). The hunter-gatherer individual had approximately ~50% "Basal-East Asian" ancestry, and was positioned in between modern East Asians and Papuans of Oceania. The authors concluded that East Asian-related ancestry expanded much earlier into Maritime Southeast Asia than previously suggested, long before the expansion of Austroasiatic and Austronesian groups.[36]
Another study about the ancestral composition of modern ethnic groups in the Philippines from 2021 suggests that distinctive Basal-East Asian (East-Eurasian) ancestry originated in Mainland Southeast Asia at ~50,000BC, and expanded through multiple migration waves southwards and northwards respectively.[3]
A 2022 study stated that there was substantial South Asian admixture in various Southeast Asian populations in Thailand, Cambodia, Vietnam, Myanmar and Singapore. Exceptions were isolated hill tribes and present hunter-gatherer groups in Thailand. Admixture rates for Southeast Asians range from 2% to 16% and originated from Indian cultural influence in the region.[37]

Anthropology is the scientific study of humanity, concerned with human behavior, human biology, cultures, societies, and linguistics, in both the present and past, including archaic humans.[1] Social anthropology studies patterns of behavior, while cultural anthropology studies cultural meaning, including norms and values.[1] The term sociocultural anthropology is commonly used today. Linguistic anthropology studies how language influences social life. Biological or physical anthropology studies the biological development of humans.[1]
Archaeology, often termed as "anthropology of the past," studies human activity through investigation of physical evidence. It is considered a branch of anthropology in North America and Asia, while in Europe, archaeology is viewed as a discipline in its own right or grouped under other related disciplines, such as history and palaeontology.
The abstract noun anthropology is first attested in reference to history.[2][n 1] Its present use first appeared in Renaissance Germany in the works of Magnus Hundt and Otto Casmann.[3] Their Neo-Latin anthropologia derived from the combining forms of the Greek words ánthrōpos (ἄνθρωπος, "human") and lógos (λόγος, "study").[2] Its adjectival form appeared in the works of Aristotle.[2] It began to be used in English, possibly via French Anthropologie, by the early 18th century.[2][n 2]
In 1647, the Bartholins, early scholars of the University of Copenhagen, defined l'anthropologie as follows:[5]
Anthropology, that is to say the science that treats of man, is divided ordinarily and with reason into Anatomy, which considers the body and the parts, and Psychology, which speaks of the soul.[n 3]
Sporadic use of the term for some of the subject matter occurred subsequently, such as the use by Étienne Serres in 1839 to describe the natural history, or paleontology, of man, based on comparative anatomy, and the creation of a chair in anthropology and ethnography in 1850 at the French National Museum of Natural History by Jean Louis Armand de Quatrefages de Bréau. Various short-lived organizations of anthropologists had already been formed. The Société Ethnologique de Paris, the first to use the term ethnology, was formed in 1839 and focused on methodically studying human races. After the death of its founder, William Frédéric Edwards, in 1842, it gradually declined in activity until it eventually dissolved in 1862.[6]
Meanwhile, the Ethnological Society of New York, currently the American Ethnological Society, was founded on its model in 1842, as well as the Ethnological Society of London in 1843, a break-away group of the Aborigines' Protection Society.[7] These anthropologists of the times were liberal, anti-slavery, and pro-human-rights activists. They maintained international connections.[citation needed]
Anthropology and many other current fields are the intellectual results of the comparative methods developed in the earlier 19th century. Theorists in diverse fields such as anatomy, linguistics, and ethnology, started making feature-by-feature comparisons of their subject matters, and were beginning to suspect that similarities between animals, languages, and folkways were the result of processes or laws unknown to them then.[8] For them, the publication of Charles Darwin's On the Origin of Species was the epiphany of everything they had begun to suspect. Darwin himself arrived at his conclusions through comparison of species he had seen in agronomy and in the wild.
Darwin and Wallace unveiled evolution in the late 1850s. There was an immediate rush to bring it into the social sciences. Paul Broca in Paris was in the process of breaking away from the Société de biologie to form the first of the explicitly anthropological societies, the Société d'Anthropologie de Paris, meeting for the first time in Paris in 1859.[9][n 4] When he read Darwin, he became an immediate convert to Transformisme, as the French called evolutionism.[10] His definition now became "the study of the human group, considered as a whole, in its details, and in relation to the rest of nature".[11]
Broca, being what today would be called a neurosurgeon, had taken an interest in the pathology of speech. He wanted to localize the difference between man and the other animals, which appeared to reside in speech. He discovered the speech center of the human brain, today called Broca's area after him. His interest was mainly in Biological anthropology, but a German philosopher specializing in psychology, Theodor Waitz, took up the theme of general and social anthropology in his six-volume work, entitled Die Anthropologie der Naturvölker, 1859–1864. The title was soon translated as "The Anthropology of Primitive Peoples". The last two volumes were published posthumously.
Waitz defined anthropology as "the science of the nature of man". Following Broca's lead, Waitz points out that anthropology is a new field, which would gather material from other fields, but would differ from them in the use of comparative anatomy, physiology, and psychology to differentiate man from "the animals nearest to him". He stresses that the data of comparison must be empirical, gathered by experimentation.[12] The history of civilization, as well as ethnology, are to be brought into the comparison. It is to be presumed fundamentally that the species, man, is a unity, and that "the same laws of thought are applicable to all men".[13]
Waitz was influential among British ethnologists. In 1863, the explorer Richard Francis Burton and the speech therapist James Hunt broke away from the Ethnological Society of London to form the Anthropological Society of London, which henceforward would follow the path of the new anthropology rather than just ethnology. It was the 2nd society dedicated to general anthropology in existence. Representatives from the French Société were present, though not Broca. In his keynote address, printed in the first volume of its new publication, The Anthropological Review, Hunt stressed the work of Waitz, adopting his definitions as a standard.[14][n 5] Among the first associates were the young Edward Burnett Tylor, inventor of cultural anthropology, and his brother Alfred Tylor, a geologist. Previously Edward had referred to himself as an ethnologist; subsequently, an anthropologist.
Similar organizations in other countries followed: The Anthropological Society of Madrid (1865), the American Anthropological Association in 1902, the Anthropological Society of Vienna (1870), the Italian Society of Anthropology and Ethnology (1871), and many others subsequently. The majority of these were evolutionists. One notable exception was the Berlin Society for Anthropology, Ethnology, and Prehistory (1869) founded by Rudolph Virchow, known for his vituperative attacks on the evolutionists. Not religious himself, he insisted that Darwin's conclusions lacked empirical foundation.
During the last three decades of the 19th century, a proliferation of anthropological societies and associations occurred, most independent, most publishing their own journals, and all international in membership and association. The major theorists belonged to these organizations. They supported the gradual osmosis of anthropology curricula into the major institutions of higher learning. By 1898, 48 educational institutions in 13 countries had some curriculum in anthropology. None of the 75 faculty members were under a department named anthropology.[15]
Anthropology as a specialized field of academic study developed much through the end of the 19th century. Then it rapidly expanded beginning in the early 20th century to the point where many of the world's higher educational institutions typically included anthropology departments. Thousands of anthropology departments have come into existence, and anthropology has also diversified from a few major subdivisions to dozens more. Practical anthropology, the use of anthropological knowledge and technique to solve specific problems, has arrived; for example, the presence of buried victims might stimulate the use of a forensic archaeologist to recreate the final scene. The organization has also reached a global level. For example, the World Council of Anthropological Associations (WCAA), "a network of national, regional and international associations that aims to promote worldwide communication and cooperation in anthropology", currently contains members from about three dozen nations.[16]
Since the work of Franz Boas and Bronisław Malinowski in the late 19th and early 20th centuries, social anthropology in Great Britain and cultural anthropology in the US have been distinguished from other social sciences by their emphasis on cross-cultural comparisons, long-term in-depth examination of context, and the importance they place on participant-observation or experiential immersion in the area of research. Cultural anthropology, in particular, has emphasized cultural relativism, holism, and the use of findings to frame cultural critiques.[17] This has been particularly prominent in the United States, from Boas' arguments against 19th-century racial ideology, through Margaret Mead's advocacy for gender equality and sexual liberation, to current criticisms of post-colonial oppression and promotion of multiculturalism. Ethnography is one of its primary research designs as well as the text that is generated from anthropological fieldwork.[18][19][20]
In Great Britain and the Commonwealth countries, the British tradition of social anthropology tends to dominate. In the United States, anthropology has traditionally been divided into the four field approach developed by Franz Boas in the early 20th century: biological or physical anthropology; social, cultural, or sociocultural anthropology; archaeological anthropology; and linguistic anthropology. These fields frequently overlap but tend to use different methodologies and techniques.[21]
European countries with overseas colonies tended to practice more ethnology (a term coined and defined by Adam F. Kollár in 1783). It is sometimes referred to as sociocultural anthropology in the parts of the world that were influenced by the European tradition.[22]
Anthropology is a global discipline involving humanities, social sciences and natural sciences. Anthropology builds upon knowledge from natural sciences, including the discoveries about the origin and evolution of Homo sapiens, human physical traits, human behavior, the variations among different groups of humans, how the evolutionary past of Homo sapiens has influenced its social organization and culture, and from social sciences, including the organization of human social and cultural relations, institutions, social conflicts, etc.[23][24] Early anthropology originated in Classical Greece and Persia and studied and tried to understand observable cultural diversity.[25][26] As such, anthropology has been central in the development of several new (late 20th century) interdisciplinary fields such as cognitive science,[27] global studies, and various ethnic studies.
According to Clifford Geertz,
...anthropology is perhaps the last of the great nineteenth-century conglomerate disciplines still for the most part organizationally intact. Long after natural history, moral philosophy, philology, and political economy have dissolved into their specialized successors, it has remained a diffuse assemblage of ethnology, human biology, comparative linguistics, and prehistory, held together mainly by the vested interests, sunk costs, and administrative habits of academia, and by a romantic image of comprehensive scholarship.[28]
Sociocultural anthropology has been heavily influenced by structuralist and postmodern theories, as well as a shift toward the analysis of modern societies. During the 1970s and 1990s, there was an epistemological shift away from the positivist traditions that had largely informed the discipline.{{[29][30]}} During this shift, enduring questions about the nature and production of knowledge came to occupy a central place in cultural and social anthropology. In contrast, archaeology and biological anthropology remained largely positivist. Due to this difference in epistemology, the four sub-fields of anthropology have lacked cohesion over the last several decades.[citation needed]
Sociocultural anthropology draws together the principal axes of cultural anthropology and social anthropology. Cultural anthropology is the comparative study of the manifold ways in which people make sense of the world around them, while social anthropology is the study of the relationships among individuals and groups.[31] Cultural anthropology is more related to philosophy, literature and the arts (how one's culture affects the experience for self and group, contributing to a more complete understanding of the people's knowledge, customs, and institutions), while social anthropology is more related to sociology and history.[31] In that, it helps develop an understanding of social structures, typically of others and other populations (such as minorities, subgroups, dissidents, etc.). There is no hard-and-fast distinction between them, and these categories overlap to a considerable degree.
Inquiry in sociocultural anthropology is guided in part by cultural relativism, the attempt to understand other societies in terms of their own cultural symbols and values.[18] Accepting other cultures in their own terms moderates reductionism in cross-cultural comparison.[32] This project is often accommodated in the field of ethnography. Ethnography can refer to both a methodology and the product of ethnographic research, i.e. an ethnographic monograph. As a methodology, ethnography is based upon long-term fieldwork within a community or other research site. Participant observation is one of the foundational methods of social and cultural anthropology.[33] Ethnology involves the systematic comparison of different cultures. The process of participant-observation can be especially helpful to understanding a culture from an emic (conceptual, vs. etic, or technical) point of view.
The study of kinship and social organization is a central focus of sociocultural anthropology, as kinship is a human universal. Sociocultural anthropology also covers economic and political organization, law and conflict resolution, patterns of consumption and exchange, material culture, technology, infrastructure, gender relations, ethnicity, childrearing and socialization, religion, myth, symbols, values, etiquette, worldview, sports, music, nutrition, recreation, games, food, festivals, and language (which is also the object of study in linguistic anthropology).[citation needed]
Comparison across cultures is a key element of method in sociocultural anthropology, including the industrialized (and de-industrialized) West. The Standard Cross-Cultural Sample (SCCS) includes 186 such cultures.[34]
Biological anthropology and physical anthropology are synonymous terms to describe anthropological research focused on the study of humans and non-human primates in their biological, evolutionary, and demographic dimensions. It examines the biological and social factors that have affected the evolution of humans and other primates, and that generate, maintain or change contemporary genetic and physiological variation.[35]
Archaeology is the study of the human past through its material remains. Artifacts, faunal remains, and human altered landscapes are evidence of the cultural and material lives of past societies. Archaeologists examine material remains in order to deduce patterns of past human behavior and cultural practices. Ethnoarchaeology is a type of archaeology that studies the practices and material remains of living human groups in order to gain a better understanding of the evidence left behind by past human groups, who are presumed to have lived in similar ways.[36]
Linguistic anthropology (not to be confused with anthropological linguistics) seeks to understand the processes of human communications, verbal and non-verbal, variation in language across time and space, the social uses of language, and the relationship between language and culture.[37] It is the branch of anthropology that brings linguistic methods to bear on anthropological problems, linking the analysis of linguistic forms and processes to the interpretation of sociocultural processes. Linguistic anthropologists often draw on related fields including sociolinguistics, pragmatics, cognitive linguistics, semiotics, discourse analysis, and narrative analysis.[38]
Ethnography is a method of analysing social or cultural interaction. It often involves participant observation though an ethnographer may also draw from texts written by participants of in social interactions. Ethnography views first-hand experience and social context as important.[39]
Tim Ingold distinguishes ethnography from anthropology arguing that anthropology tries to construct general theories of human experience, applicable in general and novel settings, while ethnography concerns itself with fidelity. He argues that the anthropologist must make his writing consistent with their understanding of literature and other theory but notes that ethnography may be of use to the anthropologists and the fields inform one another.[40]
One of the central problems in the anthropology of art concerns the universality of 'art' as a cultural phenomenon. Several anthropologists have noted that the Western categories of 'painting', 'sculpture', or 'literature', conceived as independent artistic activities, do not exist, or exist in a significantly different form, in most non-Western contexts.[41] To surmount this difficulty, anthropologists of art have focused on formal features in objects which, without exclusively being 'artistic', have certain evident 'aesthetic' qualities. Boas' Primitive Art, Claude Lévi-Strauss' The Way of the Masks (1982) or Geertz's 'Art as Cultural System' (1983) are some examples in this trend to transform the anthropology of 'art' into an anthropology of culturally specific 'aesthetics'.[citation needed]
Media anthropology (also known as the anthropology of media or mass media) emphasizes ethnographic studies as a means of understanding producers, audiences, and other cultural and social aspects of mass media. The types of ethnographic contexts explored range from contexts of media production (e.g., ethnographies of newsrooms in newspapers, journalists in the field, film production) to contexts of media reception, following audiences in their everyday responses to media. Other types include cyber anthropology, a relatively new area of internet research, as well as ethnographies of other areas of research which happen to involve media, such as development work, social movements, or health education. This is in addition to many classic ethnographic contexts, where media such as radio, the press, new media, and television have started to make their presences felt since the early 1990s.[42]
Ethnomusicology is an academic field encompassing various approaches to the study of music (broadly defined), that emphasize its cultural, social, material, cognitive, biological, and other dimensions or contexts instead of or in addition to its isolated sound component or any particular repertoire.
Ethnomusicology can be used in a wide variety of fields, such as teaching, politics, cultural anthropology etc. While the origins of ethnomusicology date back to the 18th and 19th centuries, it was formally termed "ethnomusicology" by Dutch scholar Jaap Kunst c. 1950. Later, the influence of study in this area spawned the creation of the periodical Ethnomusicology and the Society of Ethnomusicology.[43]
Visual anthropology is concerned, in part, with the study and production of ethnographic photography, film and, since the mid-1990s, new media. While the term is sometimes used interchangeably with ethnographic film, visual anthropology also encompasses the anthropological study of visual representation, including areas such as performance, museums, art, and the production and reception of mass media. Visual representations from all cultures, such as sandpaintings, tattoos, sculptures and reliefs, cave paintings, scrimshaw, jewelry, hieroglyphs, paintings, and photographs are included in the focus of visual anthropology.[44]
Economic anthropology attempts to explain human economic behavior in its widest historic, geographic and cultural scope. It has a complex relationship with the discipline of economics, of which it is highly critical. Its origins as a sub-field of anthropology begin with the Polish-British founder of anthropology, Bronisław Malinowski, and his French compatriot, Marcel Mauss, on the nature of gift-giving exchange (or reciprocity) as an alternative to market exchange. Economic Anthropology remains, for the most part, focused upon exchange. The school of thought derived from Marx and known as Political Economy focuses on production, in contrast.[45] Economic anthropologists have abandoned the primitivist niche they were relegated to by economists, and have now turned to examine corporations, banks, and the global financial system from an anthropological perspective.[46]
Political economy in anthropology is the application of the theories and methods of historical materialism to the traditional concerns of anthropology, including, but not limited to, non-capitalist societies. Political economy introduced questions of history and colonialism to ahistorical anthropological theories of social structure and culture. Three main areas of interest rapidly developed. The first of these areas was concerned with the "pre-capitalist" societies that were subject to evolutionary "tribal" stereotypes. Sahlin's work on hunter-gatherers as the "original affluent society" did much to dissipate that image. The second area was concerned with the vast majority of the world's population at the time, the peasantry, many of whom were involved in complex revolutionary wars such as in Vietnam. The third area was on colonialism, imperialism, and the creation of the capitalist world-system.[47] More recently, these political economists have more directly addressed issues of industrial (and post-industrial) capitalism around the world.
Applied anthropology refers to the application of the method and theory of anthropology to the analysis and solution of practical problems. It is a "complex of related, research-based, instrumental methods which produce change or stability in specific cultural systems through the provision of data, initiation of direct action, and/or the formulation of policy".[48] Applied anthropology is the practical side of anthropological research; it includes researcher involvement and activism within the participating community. It is closely related to development anthropology (distinct from the more critical anthropology of development).[citation needed]
Anthropology of development tends to view development from a critical perspective. The kind of issues addressed and implications for the approach involve pondering why, if a key development goal is to alleviate poverty, is poverty increasing? Why is there such a gap between plans and outcomes? Why are those working in development so willing to disregard history and the lessons it might offer? Why is development so externally driven rather than having an internal basis? In short, why does so much planned development fail?
Kinship can refer both to the study of the patterns of social relationships in one or more human cultures, or it can refer to the patterns of social relationships themselves. Over its history, anthropology has developed a number of related concepts and terms, such as "descent", "descent groups", "lineages", "affines", "cognates", and even "fictive kinship". Broadly, kinship patterns may be considered to include people related both by descent (one's social relations during development), and also relatives by marriage. Within kinship you have two different families. People have their biological families and it is the people they share DNA with. This is called consanguinity or "blood ties".[49][better source needed] People can also have a chosen family in which they chose who they want to be a part of their family. In some cases, people are closer with their chosen family more than with their biological families.[50]
Feminist anthropology is a four field approach to anthropology (archeological, biological, cultural, linguistic) that seeks to reduce male bias in research findings, anthropological hiring practices, and the scholarly production of knowledge. Anthropology engages often with feminists from non-Western traditions, whose perspectives and experiences can differ from those of white feminists of Europe, America, and elsewhere. From the perspective of the Western world, historically such 'peripheral' perspectives have been ignored, observed only from an outsider perspective, and regarded as less-valid or less-important than knowledge from the Western world.[51] Exploring and addressing that double bias against women from marginalized racial or ethnic groups is of particular interest in intersectional feminist anthropology.
Feminist anthropologists have stated that their publications have contributed to anthropology, along the way correcting against the systemic biases beginning with the "patriarchal origins of anthropology (and (academia)" and note that from 1891 to 1930 doctorates in anthropology went to males more than 85%, more than 81% were under 35, and only 7.2% to anyone over 40 years old, thus reflecting an age gap in the pursuit of anthropology by first-wave feminists until later in life.[52] This correction of systemic bias may include mainstream feminist theory, history, linguistics, archaeology, and anthropology. Feminist anthropologists are often concerned with the construction of gender across societies. Gender constructs are of particular interest when studying sexism.[citation needed]
According to St. Clair Drake, Vera Mae Green was, until "[w]ell into the 1960s", the only African American female anthropologist who was also a Caribbeanist. She studied ethnic and family relations in the Caribbean as well as the United States, and thereby tried to improve the way black life, experiences, and culture were studied.[53] However, Zora Neale Hurston, although often primarily considered to be a literary author, was trained in anthropology by Franz Boas, and published Tell my Horse about her "anthropological observations" of voodoo in the Caribbean (1938).[54]
Feminist anthropology is inclusive of the anthropology of birth[55] as a specialization, which is the anthropological study of pregnancy and childbirth within cultures and societies.
Medical anthropology is an interdisciplinary field which studies "human health and disease, health care systems, and biocultural adaptation".[56] It is believed that William Caudell was the first to discover the field of medical anthropology. Currently, research in medical anthropology is one of the main growth areas in the field of anthropology as a whole. It focuses on the following six basic fields:[57]
Other subjects that have become central to medical anthropology worldwide are violence and social suffering (Farmer, 1999, 2003; Beneduce, 2010) as well as other issues that involve physical and psychological harm and suffering that are not a result of illness. On the other hand, there are fields that intersect with medical anthropology in terms of research methodology and theoretical production, such as cultural psychiatry and transcultural psychiatry or ethnopsychiatry.
Nutritional anthropology is a synthetic concept that deals with the interplay between economic systems, nutritional status and food security, and how changes in the former affect the latter. If economic and environmental changes in a community affect access to food, food security, and dietary health, then this interplay between culture and biology is in turn connected to broader historical and economic trends associated with globalization. Nutritional status affects overall health status, work performance potential, and the overall potential for economic development (either in terms of human development or traditional western models) for any given group of people.
Psychological anthropology is an interdisciplinary subfield of anthropology that studies the interaction of cultural and mental processes. This subfield tends to focus on ways in which humans' development and enculturation within a particular cultural group – with its own history, language, practices, and conceptual categories – shape processes of human cognition, emotion, perception, motivation, and mental health.[58] It also examines how the understanding of cognition, emotion, motivation, and similar psychological processes inform or constrain our models of cultural and social processes.[59][60]
Cognitive anthropology seeks to explain patterns of shared knowledge, cultural innovation, and transmission over time and space using the methods and theories of the cognitive sciences (especially experimental psychology and evolutionary biology) often through close collaboration with historians, ethnographers, archaeologists, linguists, musicologists and other specialists engaged in the description and interpretation of cultural forms. Cognitive anthropology is concerned with what people from different groups know and how that implicit knowledge changes the way people perceive and relate to the world around them.[59]
Transpersonal anthropology studies the relationship between altered states of consciousness and culture. As with transpersonal psychology, the field is much concerned with altered states of consciousness (ASC) and transpersonal experience. However, the field differs from mainstream transpersonal psychology in taking more cognizance of cross-cultural issues – for instance, the roles of myth, ritual, diet, and text in evoking and interpreting extraordinary experiences.[61][62]
Political anthropology concerns the structure of political systems, looked at from the basis of the structure of societies. Political anthropology developed as a discipline concerned primarily with politics in stateless societies, a new development started from the 1960s, and is still unfolding: anthropologists started increasingly to study more "complex" social settings in which the presence of states, bureaucracies and markets entered both ethnographic accounts and analysis of local phenomena. The turn towards complex societies meant that political themes were taken up at two main levels. Firstly, anthropologists continued to study political organization and political phenomena that lay outside the state-regulated sphere (as in patron-client relations or tribal political organization). Secondly, anthropologists slowly started to develop a disciplinary concern with states and their institutions (and on the relationship between formal and informal political institutions). An anthropology of the state developed, and it is a most thriving field today. Geertz's comparative work on "Negara", the Balinese state, is an early, famous example.
Legal anthropology or anthropology of law specializes in "the cross-cultural study of social ordering".[63] Earlier legal anthropological research often focused more narrowly on conflict management, crime, sanctions, or formal regulation. More recent applications include issues such as human rights, legal pluralism,[64] and political uprisings.
Public anthropology was created by Robert Borofsky, a professor at Hawaii Pacific University, to "demonstrate the ability of anthropology and anthropologists to effectively address problems beyond the discipline – illuminating larger social issues of our times as well as encouraging broad, public conversations about them with the explicit goal of fostering social change".[65]
Cyborg anthropology originated as a sub-focus group within the American Anthropological Association's annual meeting in 1993. The sub-group was very closely related to STS and the Society for the Social Studies of Science.[66] Donna Haraway's 1985 Cyborg Manifesto could be considered the founding document of cyborg anthropology by first exploring the philosophical and sociological ramifications of the term. Cyborg anthropology studies humankind and its relations with the technological systems it has built, specifically modern technological systems that have reflexively shaped notions of what it means to be human beings.[citation needed]
Digital anthropology is the study of the relationship between humans and digital-era technology and extends to various areas where anthropology and technology intersect. It is sometimes grouped with sociocultural anthropology, and sometimes considered part of material culture. The field is new, and thus has a variety of names with a variety of emphases. These include techno-anthropology,[67] digital ethnography, cyberanthropology,[68] and virtual anthropology.[69]
Ecological anthropology is defined as the "study of cultural adaptations to environments".[70] The sub-field is also defined as, "the study of relationships between a population of humans and their biophysical environment".[71] The focus of its research concerns "how cultural beliefs and practices helped human populations adapt to their environments, and how their environments change across space and time.[72] The contemporary perspective of environmental anthropology, and arguably at least the backdrop, if not the focus of most of the ethnographies and cultural fieldworks of today, is political ecology. Many characterize this new perspective as more informed with culture, politics and power, globalization, localized issues, century anthropology and more.[73] The focus and data interpretation is often used for arguments for/against or creation of policy, and to prevent corporate exploitation and damage of land. Often, the observer has become an active part of the struggle either directly (organizing, participation) or indirectly (articles, documentaries, books, ethnographies). Such is the case with environmental justice advocate Melissa Checker and her relationship with the people of Hyde Park.[74]
Social sciences, like anthropology, can provide interdisciplinary approaches to the environment. Professor Kay Milton, Director of the Anthropology research network in the School of History and Anthropology,[75] describes anthropology as distinctive, with its most distinguishing feature being its interest in non-industrial indigenous and traditional societies. Anthropological theory is distinct because of the consistent presence of the concept of culture; not an exclusive topic but a central position in the study and a deep concern with the human condition. Milton describes three trends that are causing a fundamental shift in what characterizes anthropology: dissatisfaction with the cultural relativist perspective, reaction against cartesian dualisms which obstructs progress in theory (nature culture divide), and finally an increased attention to globalization (transcending the barriers or time/space).
Environmental discourse appears to be characterized by a high degree of globalization. (The troubling problem is borrowing non-indigenous practices and creating standards, concepts, philosophies and practices in western countries.) Anthropology and environmental discourse now have become a distinct position in anthropology as a discipline. Knowledge about diversities in human culture can be important in addressing environmental problems - anthropology is now a study of human ecology. Human activity is the most important agent in creating environmental change, a study commonly found in human ecology which can claim a central place in how environmental problems are examined and addressed. Other ways anthropology contributes to environmental discourse is by being theorists and analysts, or by refinement of definitions to become more neutral/universal, etc. In exploring environmentalism - the term typically refers to a concern that the environment should be protected, particularly from the harmful effects of human activities. Environmentalism itself can be expressed in many ways. Anthropologists can open the doors of environmentalism by looking beyond industrial society, understanding the opposition between industrial and non-industrial relationships, knowing what ecosystem people and biosphere people are and are affected by, dependent and independent variables, "primitive" ecological wisdom, diverse environments, resource management, diverse cultural traditions, and knowing that environmentalism is a part of culture.[76]
Ethnohistory is the study of ethnographic cultures and indigenous customs by examining historical records. It is also the study of the history of various ethnic groups that may or may not exist today. Ethnohistory uses both historical and ethnographic data as its foundation. Its historical methods and materials go beyond the standard use of documents and manuscripts. Practitioners recognize the utility of such source material as maps, music, paintings, photography, folklore, oral tradition, site exploration, archaeological materials, museum collections, enduring customs, language, and place names.[77]
The anthropology of religion involves the study of religious institutions in relation to other social institutions, and the comparison of religious beliefs and practices across cultures. Modern anthropology assumes that there is complete continuity between magical thinking and religion,[78][n 6] and that every religion is a cultural product, created by the human community that worships it.[79]
Urban anthropology is concerned with issues of urbanization, poverty, and neoliberalism. Ulf Hannerz quotes a 1960s remark that traditional anthropologists were "a notoriously agoraphobic lot, anti-urban by definition". Various social processes in the Western World as well as in the "Third World" (the latter being the habitual focus of attention of anthropologists) brought the attention of "specialists in 'other cultures'" closer to their homes.[80] There are two main approaches to urban anthropology: examining the types of cities or examining the social issues within the cities. These two methods are overlapping and dependent of each other. By defining different types of cities, one would use social factors as well as economic and political factors to categorize the cities. By directly looking at the different social issues, one would also be studying how they affect the dynamic of the city.[81]
Anthrozoology (also known as "human–animal studies") is the study of interaction between living things. It is an interdisciplinary field that overlaps with a number of other disciplines, including anthropology, ethology, medicine, psychology, veterinary medicine and zoology. A major focus of anthrozoologic research is the quantifying of the positive effects of human-animal relationships on either party and the study of their interactions.[82] It includes scholars from a diverse range of fields, including anthropology, sociology, biology, and philosophy.[83][84][n 7]
Biocultural anthropology is the scientific exploration of the relationships between human biology and culture. Physical anthropologists throughout the first half of the 20th century viewed this relationship from a racial perspective; that is, from the assumption that typological human biological differences lead to cultural differences.[85] After World War II the emphasis began to shift toward an effort to explore the role culture plays in shaping human biology.
Evolutionary anthropology is the interdisciplinary study of the evolution of human physiology and human behaviour and the relation between hominins and non-hominin primates. Evolutionary anthropology is based in natural science and social science, combining the human development with socioeconomic factors. Evolutionary anthropology is concerned with both biological and cultural evolution of humans, past and present. It is based on a scientific approach, and brings together fields such as archaeology, behavioral ecology, psychology, primatology, and genetics. It is a dynamic and interdisciplinary field, drawing on many lines of evidence to understand the human experience, past and present.
Forensic anthropology is the application of the science of physical anthropology and human osteology in a legal setting, most often in criminal cases where the victim's remains are in the advanced stages of decomposition. A forensic anthropologist can assist in the identification of deceased individuals whose remains are decomposed, burned, mutilated or otherwise unrecognizable. The adjective "forensic" refers to the application of this subfield of science to a court of law.
Paleoanthropology combines the disciplines of paleontology and physical anthropology. It is the study of ancient humans, as found in fossil hominid evidence such as petrifacted bones and footprints. Genetics and morphology of specimens are crucially important to this field.[86] Markers on specimens, such as enamel fractures and dental decay on teeth, can also give insight into the behaviour and diet of past populations.[87]
Contemporary anthropology is an established science with academic departments at most universities and colleges. The single largest organization of anthropologists is the American Anthropological Association (AAA), which was founded in 1903.[88] Its members are anthropologists from around the globe.[89]
In 1989, a group of European and American scholars in the field of anthropology established the European Association of Social Anthropologists (EASA) which serves as a major professional organization for anthropologists working in Europe. The EASA seeks to advance the status of anthropology in Europe and to increase visibility of marginalized anthropological traditions and thereby contribute to the project of a global anthropology or world anthropology.
Hundreds of other organizations exist in the various sub-fields of anthropology, sometimes divided up by nation or region, and many anthropologists work with collaborators in other disciplines, such as geology, physics, zoology, paleontology, anatomy, music theory, art history, sociology and so on, belonging to professional societies in those disciplines as well.[90][91]
As the field has matured it has debated and arrived at ethical principles aimed at protecting both the subjects of anthropological research as well as the researchers themselves, and professional societies have generated codes of ethics.[92]
Anthropologists, like other researchers (especially historians and scientists engaged in field research), have over time assisted state policies and projects, especially colonialism.[93][94]
Some commentators have contended:
As part of their quest for scientific objectivity, present-day anthropologists typically urge cultural relativism, which has an influence on all the sub-fields of anthropology.[18] This is the notion that cultures should not be judged by another's values or viewpoints, but be examined dispassionately on their own terms. There should be no notions, in good anthropology, of one culture being better or worse than another culture.[97][98][page needed]
Ethical commitments in anthropology include noticing and documenting genocide, infanticide, racism, sexism, mutilation (including circumcision and subincision), and torture. Topics like racism, slavery, and human sacrifice attract anthropological attention and theories ranging from nutritional deficiencies,[99] to genes,[100] to acculturation, to colonialism, have been proposed to explain their origins and continued recurrences.
To illustrate the depth of an anthropological approach, one can take just one of these topics, such as racism, and find thousands of anthropological references, stretching across all the major and minor sub-fields.[101][102][103][104]
Anthropologists' involvement with the U.S. government, in particular, has caused bitter controversy within the discipline. Franz Boas publicly objected to US participation in World War I, and after the war, he published a brief exposé and condemnation of the participation of several American archaeologists in espionage in Mexico under their cover as scientists.[105]
But by the 1940s, many of Boas' anthropologist contemporaries were active in the allied war effort against the Axis Powers (Nazi Germany, Fascist Italy, and Imperial Japan). Many served in the armed forces, while others worked in intelligence (for example, the Office of Strategic Services and the Office of War Information). At the same time, David H. Price's work on American anthropology during the Cold War provides detailed accounts of the pursuit and dismissal of several anthropologists from their jobs for communist sympathies.[106]
Attempts to accuse anthropologists of complicity with the CIA and government intelligence activities during the Vietnam War years have turned up little. Many anthropologists (students and teachers) were active in the antiwar movement. Numerous resolutions condemning the war in all its aspects were passed overwhelmingly at the annual meetings of the American Anthropological Association (AAA).[107]
Professional anthropological bodies often object to the use of anthropology for the benefit of the state. Their codes of ethics or statements may proscribe anthropologists from giving secret briefings. The Association of Social Anthropologists of the UK and Commonwealth (ASA) has called certain scholarship ethically dangerous. The "Principles of Professional Responsibility" issued by the American Anthropological Association and amended through November 1986 stated that "in relation with their own government and with host governments ... no secret research, no secret reports or debriefings of any kind should be agreed to or given."[108] The current "Principles of Professional Responsibility" does not make explicit mention of ethics surrounding state interactions.[109]
Anthropologists, along with other social scientists, were working with the US military as part of the US Army's strategy in Afghanistan.[110] The Christian Science Monitor reports that "Counterinsurgency efforts focus on better grasping and meeting local needs" in Afghanistan, under the Human Terrain System (HTS) program; in addition, HTS teams are working with the US military in Iraq.[111] In 2009, the American Anthropological Association's Commission on the Engagement of Anthropology with the US Security and Intelligence Communities (CEAUSSIC) released its final report concluding, in part, that:
When ethnographic investigation is determined by military missions, not subject to external review, where data collection occurs in the context of war, integrated into the goals of counterinsurgency, and in a potentially coercive environment – all characteristic factors of the HTS concept and its application – it can no longer be considered a legitimate professional exercise of anthropology. In summary, while we stress that constructive engagement between anthropology and the military is possible, CEAUSSIC suggests that the AAA emphasize the incompatibility of HTS with disciplinary ethics and practice for job seekers and that it further recognize the problem of allowing HTS to define the meaning of 'anthropology' within DoD.[112]
Before WWII British 'social anthropology' and American 'cultural anthropology' were still distinct traditions. After the war, enough British and American anthropologists borrowed ideas and methodological approaches from one another that some began to speak of them collectively as 'sociocultural' anthropology.
There are several characteristics that tend to unite anthropological work. One of the central characteristics is that anthropology tends to provide a comparatively more holistic account of phenomena and tends to be highly empirical.[17] The quest for holism leads most anthropologists to study a particular place, problem or phenomenon in detail, using a variety of methods, over a more extensive period than normal in many parts of academia.
In the 1990s and 2000s, calls for clarification of what constitutes a culture, of how an observer knows where his or her own culture ends and another begins, and other crucial topics in writing anthropology were heard. These dynamic relationships, between what can be observed on the ground, as opposed to what can be observed by compiling many local observations remain fundamental in any kind of anthropology, whether cultural, biological, linguistic or archaeological.[113][114]
Biological anthropologists are interested in both human variation[115][116] and in the possibility of human universals (behaviors, ideas or concepts shared by virtually all human cultures).[117][118] They use many different methods of study, but modern population genetics, participant observation and other techniques often take anthropologists "into the field," which means traveling to a community in its own setting, to do something called "fieldwork." On the biological or physical side, human measurements, genetic samples, nutritional data may be gathered and published as articles or monographs.
Along with dividing up their project by theoretical emphasis, anthropologists typically divide the world up into relevant time periods and geographic regions. Human time on Earth is divided up into relevant cultural traditions based on material, such as the Paleolithic and the Neolithic, of particular use in archaeology.[citation needed] Further cultural subdivisions according to tool types, such as Olduwan or Mousterian or Levalloisian help archaeologists and other anthropologists in understanding major trends in the human past.[citation needed] Anthropologists and geographers share approaches to culture regions as well, since mapping cultures is central to both sciences. By making comparisons across cultural traditions (time-based) and cultural regions (space-based), anthropologists have developed various kinds of comparative method, a central part of their science.
Because anthropology developed from so many different enterprises (see History of anthropology), including but not limited to fossil-hunting, exploring, documentary film-making, paleontology, primatology, antiquity dealings and curatorship, philology, etymology, genetics, regional analysis, ethnology, history, philosophy, and religious studies,[119][120] it is difficult to characterize the entire field in a brief article, although attempts to write histories of the entire field have been made.[121]
Some authors argue that anthropology originated and developed as the study of "other cultures", both in terms of time (past societies) and space (non-European/non-Western societies).[122] For example, the classic of urban anthropology, Ulf Hannerz in the introduction to his seminal Exploring the City: Inquiries Toward an Urban Anthropology mentions that the "Third World" had habitually received most of attention; anthropologists who traditionally specialized in "other cultures" looked for them far away and started to look "across the tracks" only in late 1960s.[80]
Now there exist many works focusing on peoples and topics very close to the author's "home".[123] It is also argued that other fields of study, like History and Sociology, on the contrary focus disproportionately on the West.[124]
In France, the study of Western societies has been traditionally left to sociologists, but this is increasingly changing,[125] starting in the 1970s from scholars like Isac Chiva and journals like Terrain ("fieldwork") and developing with the center founded by Marc Augé (Le Centre d'anthropologie des mondes contemporains, the Anthropological Research Center of Contemporary Societies).
Since the 1980s it has become common for social and cultural anthropologists to set ethnographic research in the North Atlantic region, frequently examining the connections between locations rather than limiting research to a single locale. There has also been a related shift toward broadening the focus beyond the daily life of ordinary people; increasingly, research is set in settings such as scientific laboratories, social movements, governmental and nongovernmental organizations and businesses.[126]
(AIO)

Cold and heat adaptations in humans are a part of the broad adaptability of Homo sapiens. Adaptations in humans can be physiological, genetic, or cultural, which allow people to live in a wide variety of climates. There has been a great deal of research done on developmental adjustment, acclimatization, and cultural practices, but less research on genetic adaptations to colder and hotter temperatures.
The human body always works to remain in homeostasis. One form of homeostasis is thermoregulation. Body temperature varies in every individual, but the average internal temperature is 37.0 °C (98.6 °F).[1] Sufficient stress from extreme external temperature may cause injury or death if it exceeds the ability of the body to thermoregulate. Hypothermia can set in when the core temperature drops to 35 °C (95 °F).[2] Hyperthermia can set in when the core body temperature rises above 37.5–38.3 °C (99.5–100.9 °F).[3][4] Humans have adapted to living in climates where hypothermia and hyperthermia were common primarily through culture and technology, such as the use of clothing and shelter.[5]
Modern humans emerged from Africa approximately 70,000 years ago during a period of unstable climate, leading to a variety of new traits among the population.[6][5] When modern humans spread into Europe, they outcompeted Neanderthals. Researchers hypothesize that this suggests early modern humans were more evolutionarily fit to live in various climates.[7][8] This is supported in the variability selection hypothesis proposed by Richard Potts, which says that human adaptability came from environmental change over the long term.[9]
Bergmann's rule states that endothermic animal subspecies living in colder climates have larger bodies than those of the subspecies living in warmer climates.[11] Individuals with larger bodies are better suited for colder climates because larger bodies produce more heat due to having more cells, and have a smaller surface area to volume ratio compared to smaller individuals, which reduces the proportional heat loss. A study by Frederick Foster and Mark Collard found that Bergmann's rule can be applied to humans when the latitude and temperature between groups differ widely.[12]
Allen's rule is a biological rule that says the limbs of endotherms are shorter in cold climates and longer in hot climates. Limb length affects the body's surface area, which helps with thermoregulation. Shorter limbs help to conserve heat, while longer limbs help to dissipate heat.[13] Marshall T. Newman argues that this can be observed in Eskimo, who have shorter limbs than other people and are laterally built.[14]
Paleoanthropologist John F. Hoffecker found that both Bermann's and Allen's biogeographical rules were confirmed, with it being seen that in modern populations, there is a clear trend of shorter distal limb segments in colder environments.[15]
Origins of heat and cold adaptations can be explained by climatic adaptation.[16][17] Ambient air temperature affects how much energy investment the human body must make. The temperature that requires the least amount of energy investment is 21 °C (70 °F).[5] [disputed – discuss]  The body controls its temperature through the hypothalamus. Thermoreceptors in the skin send signals to the hypothalamus, which indicate when vasodilation and vasoconstriction should occur.
The human body has two methods of thermogenesis, which produces heat to raise the core body temperature. The first is shivering, which occurs in an unclothed person when the ambient air temperature is under 25 °C (77 °F)[dubious – discuss].[18] It is limited by the amount of glycogen available in the body.[5] The second is non-shivering, which occurs in brown adipose tissue.[19]
Population studies have shown that the San tribe of Southern Africa and the Sandawe of Eastern Africa have reduced shivering thermogenesis in the cold, and poor cold-induced vasodilation in fingers and toes compared to that of Caucasians.[5]
The only mechanism the human body has to cool itself is by sweat evaporation.[5] Sweating occurs when the ambient air temperature is above 35 °C (95 °F)[dubious – discuss] and the body fails to return to the normal internal temperature.[18] The evaporation of the sweat helps cool the blood beneath the skin. It is limited by the amount of water available in the body, which can cause dehydration.[5]
Humans adapted to heat early on. In Africa, the climate selected for traits that helped them stay cool. Also, humans had physiological mechanisms that reduced the rate of metabolism and that modified the sensitivity of sweat glands to provide an adequate amount for cooldown without the individual becoming dehydrated.[17][20]
There are two types of heat the body is adapted to, humid heat and dry heat, but the body adapts to both in similar ways. Humid heat is characterized by warmer temperatures with a high amount of water vapor in the air, while dry heat is characterized by warmer temperatures with little to no vapor, such as desert conditions. With humid heat, the moisture in the air can prevent the evaporation of sweat.[21] Regardless of acclimatization, humid heat poses a far greater threat than dry heat; humans cannot carry out physical outdoor activities at any temperature above 32 °C (90 °F) when the ambient humidity is greater than 95%.[citation needed] When combined with this high humidity, the theoretical limit to human survival in the shade, even with unlimited water, is 35 °C (95 °F) – theoretically equivalent to a heat index of 70 °C (158 °F).[22][23] Dry heat, on the other hand, can cause dehydration, as sweat will tend to evaporate extremely quickly. Individuals with less fat and slightly lower body temperatures can more easily handle both humid and dry heat.[16]
When humans are exposed to certain climates for extended periods of time, physiological changes occur to help the individual adapt to hot or cold climates. This helps the body conserve energy.[19]
The Inuit have more blood flowing into their extremities, and at a hotter temperature, than people living in warmer climates. A 1960 study on the Alacaluf Indians shows that they have a resting metabolic rate 150 to 200 percent higher than the white controls used. The Sami do not have an increase in metabolic rate when sleeping, unlike non-acclimated people.[14] Aboriginal Australians undergo a similar process, where the body cools but the metabolic rate does not increase.[18]
Humans and their evolutionary predecessors in Central Africa have been living in similar tropical climates for millions of years, which means that they have similar thermoregulatory systems.[5]
A study done on the Bantus of South Africa showed that Bantus have a lower sweat rate than that of acclimated and nonacclimated white people. A similar study done on Aboriginal Australians produced similar results, with Indigenous Australians having a much lower sweat rate than Caucasians.[18]
Social adaptations enabled early modern humans to occupy environments with temperatures that were drastically different from that of Africa. (Potts 1998). Culture enabled humans to expand their range to areas that would otherwise be uninhabitable.[18]
Humans have been able to occupy areas of extreme cold through clothing, buildings, and manipulation of fire. Furnaces have further enabled the occupation of cold environments.[18][19]
Historically many Indigenous Australians wore only genital coverings. Studies have shown that the warmth from the fires they build is enough to keep the body from fighting heat loss through shivering.[18] Inuit use well-insulated houses that are designed to transfer heat from an energy source to the living area, which means that the average indoor temperature for coastal Inuit is 10 to 20 °C (50 to 68 °F).[18]
Humans inhabit hot climates, both dry and humid, and have done so for millions of years. Selective use of clothing and technological inventions such as air conditioning allows humans to live in hot climates.
One example is the Chaamba, who live in the Sahara Desert. They wear clothing that traps air in between skin and the clothes, preventing the high ambient air temperature from reaching the skin.[18]

Mother Tongue is an annual academic journal published by the Association for the Study of Language in Prehistory (ASLIP) that has been published since 1995.[1] Its goal is to encourage international and interdisciplinary information sharing, discussion, and debate among geneticists, paleoanthropologists, archaeologists, and historical linguists on questions relating to the origin of language and ancestral human spoken languages. This includes, but is not limited to, discussion of linguistic macrofamily hypotheses.
This article about a linguistics journal is a stub. You can help Wikipedia by expanding it.
See tips for writing articles about academic journals. Further suggestions might be found on the article's talk page.
This article about historical linguistics is a stub. You can help Wikipedia by expanding it.

New Zealand's archaeology started in the early 1800s and was largely conducted by amateurs with little regard for meticulous study.[2] However, starting slowly in the 1870s detailed research answered questions about human culture, that have international relevance and wide public interest.[3]
Archaeology has, along with oral traditions, defined New Zealand's prehistory (c. 1300 – c. 1642) and protohistory (c. 1642 – c. 1800) and has been a valuable aid in solving some later historical problems. Academically New Zealand's human prehistory is broadly divided into the periods of Archaic (~paleolithic then ~mesolithic after c. 1300 AD) and Classic (~neolithic) after c. 1500 AD, based on Māori culture. Eurasian labels do not perfectly fit as some level of horticulture was always present in northern New Zealand, even existing at the same time as megafauna. More simply it can also be divided into time periods of pre and post European contact. Large poorly documented sections of New Zealand's more recent history have also been supplemented by archaeological research, such as at old battle sites or early urban centres.[4][5]
Many questions about pre-contact New Zealand have been answered by archaeology and for most it is unlikely that new information will radically change our understanding. However some questions are still debated in the recent academic press in the hope that a new argument or data may bring resolution.
First attempts to date the arrival of Māori in New Zealand, by 19th-century scholars such as S. Percy Smith, were based on genealogies and oral histories, many of which – when assigned an average generation length of 25 years – converged on a settlement date around 1350 AD, while others appeared to go back much further. This resulted in the classic theory, which all schoolchildren were once taught, that New Zealand had been discovered around 750 AD, then settled by later migrations, culminating in the "Great Fleet" of seven canoes around 1350 AD.[6][7][8]
When radiocarbon dating started to be used in the 1950s, it appeared to support the idea of early settlement, though the "Great Fleet" itself fell out of favour when scholars showed that there were inconsistencies in the genealogies on which Smith had based his theory.[9] This was replaced by the idea of gradual settlement over many centuries, but this in turn has proved to be mistaken.[10] In 1989, for example, changes in the New Zealand biota, dated to about 1000 AD, were assumed to be linked to human settlement.[11] However, by the mid-1990s, as radiocarbon dating methods were improved and sources of error better understood, it was realised that the early dates were not reliable and that the most reliable radiocarbon dates all pointed to a more recent first settlement, closer to 1300 AD or even later,[12] In 1999, a sample from the Wairau Bar site gave a "late" age of 1230–1282 AD.[13] which roughly coincided with charcoal and pollen evidence of forest fires that may or may not have been human-lit.[14] The Wairau Bar settlement is known to be a first settler site because both its human remains and its artefacts came from tropical Polynesia.[15]
Against this emerging evidence for late settlement was some seemingly contradictory evidence from the first radiocarbon dating of ancient rat bones in 1996 which gave unusually early dates – as early as 10 AD – and led its author to suggest that rats had been brought here by early human voyagers who did not stay.[16] Some scholars saw the early rat bone dates as confirmation of their theory that humans had settled in New Zealand even earlier than the classic theory had suggested, living in small numbers for a thousand years or so without leaving artefacts or skeletal remains.[17]  However, further investigation found that those early rat bone results had been flawed, all coming from one laboratory during a limited time period, while all subsequent dating has found recent arrival times for both rats and humans.[18] By 2008, there was little doubt that rats came to New Zealand with Māori no earlier than 1280 AD.[19] This was confirmed in 2011 by a meta-analysis of dates from throughout the Pacific, which showed a sudden pulse of migration leading to all of New Zealand (including the Chatham Islands) being settled no earlier than c. 1290 AD.[20]
While most researchers now use this late-13th-century date,[21] others are revising it upward even further to around 1320 AD or later, based on new evidence from moa egg shells and from the Kaharoa eruption of Mount Tarawera (1314 ± 6 AD), whose tephra forms a geological layer below all well-dated human and rat sites.[22][23] Some researchers now conclude that the weight of all the radiocarbon and DNA evidence points to New Zealand having been settled rapidly in a mass migration sometime after the Tarawera eruption, somewhere in the decades between 1320 and 1350 CE[24] – which suggests that the "Great Fleet" theory, and the genealogical calculations on which it was based, were not totally inaccurate after all.
The debate over Māori population size has two main areas of interest: how many settlers came to New Zealand and what was the population when European contact occurred. The second number is partly a historical question, and estimated populations have not strayed far from Captain Cook's first estimate of 100,000,[25] with some researches going up to 150,000. This number, coupled with an inferred low growth rate, has led researchers to require either a large founding population (more than 300 people) or an early settlement date (600–850 AD).[26][27] Therefore, a date of c. 1300 AD requires a mass migration from tropical Polynesia,[28] even though mitochondrial DNA implies a medium[clarification needed] number of approximately 70 women settlers.[29]
This story is further complicated by the South Island's slow growth rates throughout prehistory.[22] This is because kumara was extremely difficult to grow in the South Island even during warm climatic periods.[30][31] There is evidence that the "little ice age" affected New Zealand and caused a shrinking of the population.[21] The extent of this cold period in New Zealand is unknown, but it may have peaked in the early 18th century.[32] By 1886 diseases like measles, war and disruption led to a Māori population of about 40,000 and 2,000 in the North and South Islands respectively.[33]
Māori culture has been in constant adaptation to New Zealand's changing environment. From the late 1950s onward the terms "Archaic" and "Classic" culture have been used to describe the early and late phases of pre-contact Māori,[3] with "Archaic" replacing the older term "moa hunter" as the hunter-gatherer society lasted beyond the megafauna (as with Eurasia's Mesolithic).
The Archaic and Classic labels were intentionally chronological and not descriptive. They did not offer a definitive definition of either cultural period that could be used across time and space; particularly in locations like the southern South Island, where Classic tribes may migrate to regions where only an Archaic lifestyle was possible.[2] Various transitional cultural artifacts and models have been proposed; however, there is still a dearth of evidence for a clear middle phase.[34] Currently the Archaic culture is seen as semi nomadic hunter-gatherers with small gardens and populations, while the later Classic culture had large gardens and fortified permanent villages. Kumara cultivation was limited to the north until the Classic period, when building of storage pits and gardening methods allowed its storage over winter further south.[34] In many sites in  New Zealand the absence of a middle phase or the constraint of only two options has led to other interpretations, including a sevenfold evolution of boom and bust cycles.[35] Growing kumara would have been just possible in the north of the South Island during some climatic conditions.[30]
As the early settlers to New Zealand came in great numbers with supplies for planting numerous crop types it is believed that it was a planned migration to a known location. However while there is some speculation from non archaeological sources that migration to New Zealand continued throughout the Archaic period,[36] evidence is absent in the archaeological record, and there is also no evidence for domestic pigs and chickens from the Pacific making it to New Zealand - something would have been expected if trade networks had been built.[25]
The early Māori did, however, maintain the technology for long sea voyages – reaching the Chatham Islands about 1500 CE, where they developed into the separate Moriori people.[37]
The earliest archaeological sites in New Zealand have implements from tropical Polynesia.[38] There is also evidence that obsidian was traded throughout New Zealand from soon after arrival. However it was only in the sixteenth century that pounamu (jade) was traded around New Zealand, with a different supply network to the obsidian.[39] Earthquakes caused changing living patterns and the movement of people.[40]
The Māori language has changed little in the 700 years since it separated from Cook Islands Māori.
The ability of pre-contact Māori to manage resources and foresee ecological collapses has been the source of much debate.[2][41] Natural fires were rare in New Zealand, yet much of the country was covered in dry forest, early Māori didn't protect fire-prone areas and there is no evidence of systematic burning of less fire-prone ones.[42] Many New Zealand species may have been heading for slow extinction after Polynesian settlement.[2] The extinction of the mega fauna (moa) seems to have occurred quickly, within 100 years.[43] The first settlers came to New Zealand from tropical Polynesia and adapted to a temperate environment while preserving many of their old practices. Some conservative use of tropical Polynesian  methods lasted well into the Archaic period.[44]
Historical archaeology in New Zealand started late and grew slowly; it was only by the 1960s that European structures were being systematically excavated.[45][46] One example is the evidence left by Taranaki Māori political prisoners who worked pounamu in the Dunedin jail in the late 1800s.[47] there is also interest in the study of post-contact Māori sites.[48]
Early archaeology in New Zealand  was performed by anthropologists and private collectors of Māori artifacts. Many sites were destroyed by careless scavenging or poorly documented research.[2][49] Systematical research was first conducted by the museums from the main cities, followed by anthropology departments in the universities of Auckland and Otago. In 1955 the  New Zealand Archaeological Association was founded.[50]
During this time in New Zealand the study of Māori oral tradition was more influential than archaeological techniques. The coming of the Māori "Great Fleet" to New Zealand was inferred to be in 1350 AD solely from traditional evidence (similar to modern estimates from carbon dating).[49]
In the 21st century high resolution Landsat data was being used to interpret archaeological sites,[51] although there was some doubt about the effectiveness of some modern tools.[52] Archaeology departments conduct research from the university of Otago, Auckland and Canterbury. New Zealand archaeology is published in the Journal of Pacific Archaeology, the Journal of the Polynesian Society and in other international journals.
Exceptional archaeological sites are included in the national register (administered by Heritage New Zealand) in five groups: historic places (Category 1 and 2), historic areas,  Wāhi Tūpuna (practical sites), Wāhi Tapu (spiritual sites) and Wahi Tapu areas.[53] New Zealand has thousands of pre-contact sites, many of which are documented by the Historic Places Trust. Only a small fraction of these have detailed published archaeological reports. For example, in the South Island there are 550 rock art sites and 107 in the North Island and 6956 Pā in all New Zealand.[54][4] The types of features present in New Zealand pre European archaeology are pā, storage pits, gardens (stone rows and banks), house floors, terraces, trenches, umu (earth ovens), middens, quarries, rock art and changes to the local flora.[4]

The Mumun pottery period is an archaeological era in Korean prehistory that dates to approximately 1500-300 BC.[1][2][3] This period is named after the Korean name for undecorated or plain cooking and storage vessels that form a large part of the pottery assemblage over the entire length of the period, but especially 850-550 BC.
The Mumun period is known for the origins of intensive agriculture and complex societies on both the Korean Peninsula and the Japanese Archipelago.[2][3][4] This period or parts of it have sometimes been labelled as the "Korean Bronze Age", after Thomsen's 19th century three-age system classification of human prehistory. However, the application of such terminology in the Korean case may be misleading since local bronze production is not proven to have occurred until approximately the 13th century BCE, early bronze artifacts are rare, and the distribution of bronze is highly regionalized until after 300 BC.[5][6] A boom in the archaeological excavations of Mumun Period sites since the mid-1990s has recently increased collective knowledge about this formative period in the prehistory of East Asia.
The Mumun period is preceded by the Jeulmun Pottery Period (c. 8000-1500 BC). The Jeulmun was a period of hunting, gathering, and small-scale cultivation of plants.[6] The origins of the Mumun Period are not well known, but the megalithic burials, Mumun pottery, and large settlements found in the Liao River Basin and North Korea c. 1800-1500 probably indicate the origins of the Mumun Period of Southern Korea. Slash-and-burn cultivators who used Mumun pottery displaced people using Jeulmun Period subsistence patterns.[7]
The Early (or Formative) Mumun (c. 1500-850 BC) is characterized by shifting cultivation, fishing, hunting, and discrete settlements with rectangular semi-subterranean pit-houses. The social scale of Early Mumun societies was egalitarian in nature, but the latter part of this period is characterized by increasing intra-settlement competition and perhaps the presence of part-time "big-man" leadership.[8] Early Mumun settlements are relatively concentrated in the river valleys formed by tributaries of the Geum River in West-central Korea. However, one of the largest Early Mumun settlements, Eoeun (Hangeul: 어은), is located in the Middle Nam River valley in South-central Korea. In the latter Early Mumun, large settlements composed of many long-houses such as Baekseok-dong (Hangeul: 백석동) appeared in the area of modern Cheonan City, Chungcheong Nam-do.
Important long-term traditions related to Mumun ceremonial and mortuary systems originated in this sub-period. These traditions include the construction of megalithic burials, the production of red-burnished pottery, and production of polished groundstone daggers.
The Middle (or Classic) Mumun (c. 850-550 BC) is characterized by intensive agriculture, as evidenced by the large and expansive dry-field remains (c. 32,500 square metres) recovered at Daepyeong, a sprawling settlement with several multiple ditch enclosures, hundreds of pit-houses, specialized production, and evidence of the presence of incipient elites and social competition.[2][3][9] A number of wet-field features have been excavated in southern Korea, indicating that paddy field rice-farming was also practiced.
Burials dating to the latter part of the Middle Mumun (c. 700-550 BC) contain a few high status mortuary offerings such as bronze artifacts. Bronze production probably began around this time in Southern Korea. Other high status burials contain greenstone (or jade) ornaments.[4][9] A number of megalithic burials with deep shaft interments, substantial 'pavements' of rounded cobblestone, and prestige artifacts such as bronze daggers, jade, and red-burnished vessels were built in the vicinity of the southern coast in the Late Middle Mumun. High status megalithic burials and large raised-floor buildings at the Deokcheon-ni (Hangeul: 덕천리) and Igeum-dong sites in Gyeongsang Nam-do provide further evidence of the growth of social inequality and the existence of polities that were organized in ways that appear to be similar to simple "chiefdoms".[4]
Korean archaeologists sometimes refer to Middle Mumun culture as Songguk-ri Culture (Hanja: 松菊里 文化; Hangeul: 송국리 문화).[1] Co-occurring artifacts and features that are grouped together as Songguk-ri Culture are found in settlement sites in the Hoseo and Honam regions of southeast Korea, but Songguk-ri Culture settlements are also found in western Yeongnam. Excavations have also revealed Songguk-ri settlements in the Ulsan and Gimhae areas. In 2005 archaeologists uncovered Songguk-ri Culture pit-houses at a site deep in the interior of Gangwon Province. The ultimate geographic reach of Songguk-ri Culture appears to have been Jeju Island and western Japan.
Mumun culture is the beginning of a long-term tradition of rice-farming in Korea that links Mumun Culture with the present day, but evidence from the Early and Middle Mumun suggests that, although rice was grown, it was not the dominant crop.[3] During the Mumun people grew millets, barley, wheat, legumes, and continued to hunt and fish.
The Late (or Post-classic) Mumun (550-300 BC) is characterized by increasing conflict, fortified hilltop settlements, and a concentration of population in the southern coastal area. A Late Mumun occupation was found at the Namsan settlement, located on the top of a hill 100 m above sea level in modern Changwon City, Gyeongsang Nam-do. A shellmidden (shellmound) was found in the vicinity of Namsan, indicating that, in addition to agriculture, shellfish exploitation was part of the Late Mumun subsistence system in some areas. Pit-houses at Namsan were located inside a ring-ditch that is some 4.2 m deep and 10 m in width. Why would such a formidable ring-ditch, so massive in size, have been necessary? One possible answer is intergroup conflict. Archaeologists propose that the Late Mumun was a period of conflict between groups of people.
The number of settlements in the Late Mumun is much lower than in the previous sub-period. This indicates that populations were reorganized and settlement was probably more concentrated in a smaller number of larger settlements. There are a number of reasons why this could have occurred. There are some indications that conflict increased or climatic change led to crop failures.
Notably, according to the traditional Yayoi chronological sequence, Mumun-esque settlements appeared in Northern Kyūshū (Japan) during the Late Mumun. The Mumun period ends when iron appeared in the archaeological record along with pit-houses that had interior composite hearth-ovens reminiscent of the historic period (agungi).
Some scholars suggest that the Mumun pottery period should be extended to c. 0 BC because of the presence of an undecorated ware that was popular between 400 BC and 0 BC called jeomtodae (Korean: 점토대). However, bronze became very important in ceremonial and elite life from 300 BC. Additionally, iron tools are increasingly found in Southern Korea after 300 BC. These factors clearly differentiate the time period 300 BC - 0 from the cultural, technological, and social scale that was present in the Mumun pottery period. The unequal presence of bronze and iron in increased amounts from a few high status graves after 300 BC as sets this time apart from the Mumun pottery period. Thus, the Mumun is described as ending, as a cultural-technical period, by approximately 300 BC.
From about 300 BC, bronze objects became the most valued prestige mortuary goods, but iron objects were traded and then produced in the Korean peninsula at that time. The Late Mumun-Early Iron Age Neuk-do Island Shellmidden Site yielded a small number of iron objects, Lelang and Yayoi pottery, and other evidence showing that beginning in the Late Mumun, local societies were drawn into closer economic and political contact with the societies of the Late Zhou dynasty, Final Jōmon, and Early Yayoi.
As an archaeological culture, the Mumun is composed of the following elements:
According to Juha Janhunen and Alexander Vovin, Japonic languages were spoken in parts of the Korean Peninsula before they were replaced by Koreanic speakers.[10][11] According to Whitman and several other researchers, Japonic/proto-Japonic arrived in the Korean peninsula around 1500 BC[12][13] and was brought to the Japanese archipelago by Yayoi wet-rice farmers at some time between 700-300 BC.[14][15] Whitman and Miyamoto associate Japonic as the language family associated with both Mumun and Yayoi cultures.[16][13] Several linguists believe that speakers of Koreanic/proto-Koreanic arrived in the Korean Peninsula at some time after the Japonic/proto-Japonic speakers and coexisted with these peoples (i.e. the descendants of both the Mumun and Yayoi cultures) and possibly assimilated them. Both Koreanic and Japonic had prolonged influence on each other and a later founder effect diminished the internal variety of both language families.
However, this viewpoint is not widely accepted in recent archaeological circles. Whitman's hypothesis presents clay pottery and saddle-shaped bronze daggers as indicators of the Korean language family, but Kim Jang-seok and Park Jin-ho, researchers of the Korean Bronze Age, believe that the saddle-shaped bronze daggers and clay pottery groups introduced around this time were relatively small in number, making it difficult to dismantle or replace the indigenous society, and therefore there is no basis to believe that the saddle-shaped bronze dagger culture group could have caused linguistic changes in the local indigenous society.[17]

The origins of society — the evolutionary emergence of distinctively human social organization — is an important topic within evolutionary biology, anthropology, prehistory and palaeolithic archaeology.[1][2] While little is known for certain, debates since Hobbes[3] and Rousseau[4] have returned again and again to the philosophical, moral and evolutionary questions posed.
Arguably the most influential theory of human social origins is that of Thomas Hobbes, who in his Leviathan[5] argued that without strong government, society would collapse into Bellum omnium contra omnes — "the war of all against all":
In such condition, there is no place for industry; because the fruit thereof is uncertain: and consequently no culture of the earth; no navigation, nor use of the commodities that may be imported by sea; no commodious building; no instruments of moving, and removing, such things as require much force; no knowledge of the face of the earth; no account of time; no arts; no letters; no society; and which is worst of all, continual fear, and danger of violent death; and the life of man, solitary, poor, nasty, brutish, and short.
Hobbes' innovation was to attribute the establishment of society to a founding 'social contract', in which the Crown's subjects surrender some part of their freedom in return for security.
If Hobbes' idea is accepted, it follows that society could not have emerged prior to the state. This school of thought has remained influential to this day.[6] Prominent in this respect is British archaeologist Colin Renfrew (Baron Renfrew of Kaimsthorn), who points out that the state did not emerge until long after the evolution of Homo sapiens. The earliest representatives of our species, according to Renfrew, may well have been anatomically modern, but they were not yet cognitively or behaviourally modern. For example, they lacked political leadership, large-scale cooperation, food production, organised religion, law or symbolic artefacts. Humans were simply hunter-gatherers, who — much like extant apes — ate whatever food they could find in the vicinity. Renfrew controversially suggests that hunter-gatherers to this day think and socialise along lines not radically different from those of their nonhuman primate counterparts. In particular, he says that they do not "ascribe symbolic meaning to material objects" and for that reason "lack fully developed 'mind.'"[citation needed]
However, hunter-gatherer ethnographers emphasise that extant foraging peoples certainly do have social institutions — notably institutionalised rights and duties codified in formal systems of kinship.[7] Elaborate rituals such as initiation ceremonies serve to cement contracts and commitments, quite independently of the state.[8] Other scholars would add that insofar as we can speak of "human revolutions" — "major transitions" in human evolution[9] — the first was not the Neolithic Revolution but the rise of symbolic culture that occurred toward the end of the Middle Stone Age.[10][11]
Arguing the exact opposite of Hobbes's position, anarchist anthropologist Pierre Clastres views the state and society as mutually incompatible: genuine society is always struggling to survive against the state.[12]
Like Hobbes, Jean-Jacques Rousseau argued that society was born in a social contract. In Rousseau's case, however, sovereignty is vested in the entire populace, who enter into the contract directly with one another. "The problem", he explained, "is to find a form of association which will defend and protect with the whole common force the person and goods of each associate, and in which each, while uniting himself with all, may still obey himself alone, and remain as free as before." This is the fundamental problem of which the Social Contract provides the solution. The contract's clauses, Rousseau continued, may be reduced to one — "the total alienation of each associate, together with all his rights, to the whole community. Each man, in giving himself to all, gives himself to nobody; and as there is no associate over whom he does not acquire the same right as he yields others over himself, he gains an equivalent for everything he loses, and an increase of force for the preservation of what he has". In other words: "Each of us puts his person and all his power in common under the supreme direction of the general will, and, in our corporate capacity, we receive each member as an indivisible part of the whole." At once, in place of the individual personality of each contracting party, this act of association creates a moral and collective body, composed of as many members as the assembly contains votes, and receiving from this act its unity, its common identity, its life and its will.[13] By this means, each member of the community acquires not only the capacities of the whole but also, for the first time, rational mentality:
The passage from the state of nature to the civil state produces a very remarkable change in man, by substituting justice for instinct in his conduct, and giving his actions the morality they had formerly lacked. Then only, when the voice of duty takes the place of physical impulses and right of appetite, does man, who so far had considered only himself, find that he is forced to act on different principles, and to consult his reason before listening to his inclinations.
In his influential book, Ancient Law (1861), Maine argued that in early times, the basic unit of human social organisation was the patriarchal family:
The effect of the evidence derived from comparative jurisprudence is to establish the view of the primeval condition of the human race which is known as the Patriarchal Theory.
Hostile to French revolutionary and other radical social ideas, Maine's motives were partly political. He sought to undermine the legacy of Rousseau and other advocates of man's natural rights by asserting that originally, no one had any rights at all – ‘every man, living during the greater part of his life under the patriarchal despotism, was practically controlled in all his actions by a regimen not of law but of caprice’.[14] Not only were the patriarch's children subject to what Maine calls his ‘despotism’: his wife and his slaves were equally affected. The very notion of kinship, according to Maine, was simply a way of categorizing those who were forcibly subjected to the despot's arbitrary rule. Maine later added a Darwinian strand to this argument. In his The Descent of Man, Darwin had cited reports that a wild-living male gorilla would monopolise for itself as large a harem of females as it could violently defend. Maine endorsed Darwin's speculation that ‘primeval man’ probably 'lived in small communities, each with as many wives as he could support and obtain, whom he would have jealously guarded against all other men’.[15] Under pressure to spell out exactly what he meant by the term 'patriarchy', Maine clarified that ‘sexual jealousy, indulged through power, might serve as a definition of the Patriarchal Family’.[16]
In his influential book, Ancient Society (1877), its title echoing Maine's Ancient Law, Lewis Henry Morgan proposed a very different theory. Morgan insisted that throughout the earlier periods of human history, neither the state nor the family existed.
It may be here premised that all forms of government are reducible to two general plans, using the word plan in its scientific sense. In their bases the two are fundamentally distinct. The first, in the order of time, is founded upon persons, and upon relations purely personal, and may be distinguished as a society (societas). The gens is the unit of this organization; giving as the successive stages of integration, in the archaic period, the gens, the phratry, the tribe, and the confederacy of tribes, which constituted a people or nation (populus). At a later period a coalescence of tribes in the same area into a nation took the place of a confederacy of tribes occupying independent areas. Such, through prolonged ages, after the gens appeared, was the substantially universal organization of ancient society; and it remained among the Greeks and Romans after civilization supervened. The second is founded upon territory and upon property, and may be distinguished as a state (civitas).
In place of both family and state, according to Morgan, was the gens — nowadays termed the 'clan' — based initially on matrilocal residence and matrilineal descent. This aspect of Morgan's theory, later endorsed by Karl Marx and Frederick Engels, is nowadays widely considered discredited (but for a critical survey of the current consensus, see Knight 2008, 'Early Human Kinship Was Matrilineal'[17]).
Friedrich Engels built on Morgan's ideas in his 1884 essay, The Origin of the Family, Private Property and the State in the light of the researches of Lewis Henry Morgan. His primary interest was the position of women in early society, and — in particular — Morgan's insistence that the matrilineal clan preceded the family as society's fundamental unit. 'The mother-right gens', wrote Engels in his survey of contemporary historical materialist scholarship, 'has become the pivot around which the entire science turns...' Engels argued that the matrilineal clan represented a principle of self-organization so vibrant and effective that it allowed no room for patriarchal dominance or the territorial state.
The first class antagonism which appears in human history coincides with the development of the antagonism between man and woman in monogamian marriage, and the first class oppression with that of the female sex by the male.
Emile Durkheim considered that in order to exist, any human social system must counteract the natural tendency for the sexes to promiscuously conjoin. He argued that social order presupposes sexual morality, which is expressed in prohibitions against sex with certain people or during certain periods — in traditional societies particularly during menstruation.
One first fact is certain: that is, that the entire system of prohibitions must strictly conform to the ideas that primitive man had about menstruation and about menstrual blood. For all these taboos start only with the onset of puberty: and it is only when the first signs of blood appear that they reach their maximum rigour.
The incest taboo, wrote Durkheim in 1898, is no more than a particular example of something more basic and universal - the ritualistic setting apart of 'the sacred' from 'the profane'. This begins as the segregation of the sexes, each of which - at least on important occasions - is 'sacred' or 'set apart' from the other. 'The two sexes', as Durkheim explains, 'must avoid each other with the same care as the profane flees from the sacred and the sacred from the profane.' Women as sisters act out the role of 'sacred' beings invested 'with an isolating power of some sort, a power which holds the masculine population at a distance.' Their menstrual blood in particular sets them in a category apart, exercising a 'type of repulsing action which keeps the other sex far from them'. In this way, the earliest ritual structure emerges — establishing morally regulated 'society' for the first time.[18]
Charles Darwin pictured early human society as resembling that of apes, with one or more dominant males jealously guarding a harem of females.[19] In his myth of the 'Primal Horde', Sigmund Freud later took all this as his starting point but then postulated an insurrection mounted by the tyrant's own sons:
All that we find there is a violent and jealous father who keeps all the females for himself and drives away his sons as they grow up…. One day the brothers who had been driven out came together, killed and devoured their father and so made an end of the patriarchal horde.
Following this, the band of brothers were about to take sexual possession of their mothers and sisters when suddenly they were overcome with remorse. In their contradictory emotional state, their dead father now became stronger than the living one had been. In memory of him, the brothers revoked their deed by forbidding the killing and eating of the 'totem' (as their father had now become) and renouncing their claim to the women who had just been set free. In this way, the two fundamental taboos of primitive society – not to eat the totem and not to marry one's sisters – were established for the first time.
A related but less dramatic version of Freud's 'sexual revolution' idea was proposed in 1960 by American social anthropologist Marshall Sahlins.[20] Somehow, he writes, the world of primate brute competition and sexual dominance was turned upside-down:
The decisive battle between early culture and human nature must have been waged on the field of primate sexuality….  Among subhuman primates sex had organized society; the customs of hunters and gatherers testify eloquently that now society was to organize sex…. In selective adaptation to the perils of the Stone Age, human society overcame or subordinated such primate propensities as selfishness, indiscriminate sexuality, dominance and brute competition. It substituted kinship and co-operation for conflict, placed solidarity over sex, morality over might. In its earliest days it accomplished the greatest reform in history, the overthrow of human primate nature, and thereby secured the evolutionary future of the species.
Once a prehistoric hunting band institutionalized a successful and decisive rebellion, and did away with the alpha-male role permanently... it is easy to see how this institution would have spread.
If we accept Rousseau's line of reasoning, no single dominant individual is needed to embody society, to guarantee security, or to enforce social contracts. The people themselves can do these things, combining to enforce the general will. A modern origins theory along these lines is that of evolutionary anthropologist Christopher Boehm. Boehm argues that ape social organisation tends to be despotic, typically with one or more dominant males monopolising access to the locally available females. But wherever there is dominance, we can also expect resistance. In the human case, resistance to being personally dominated intensified as humans used their social intelligence to form coalitions. Eventually, a point was reached when the costs of attempting to impose dominance became so high that the strategy was no longer evolutionarily stable, whereupon social life tipped over into 'reverse dominance' — defined as a situation in which only the entire community, on guard against primate-style individual dominance, is permitted to use force to suppress deviant behaviour.[21]
Human beings, writes social anthropologist Ernest Gellner, are not genetically programmed to be members of this or that social order. You can take a human infant and place it into any kind of social order and it will function acceptably. What makes human society so distinctive is the fabulous range of quite different forms it takes across the world. Yet in any given society, the range of permitted behaviours is quite narrowly constrained. This is not owing to the existence of any externally imposed system of rewards and punishments. The constraints come from within — from certain compulsive moral concepts which members of the social order have internalised. The society installs these concepts in each individual's psyche in the manner first identified by Emile Durkheim, namely, by means of collective rituals such as initiation rites. Therefore, the problem of the origins of society boils down to the problem of the origins of collective ritual.
How is a  society established, and a series of societies diversified, whilst each of them is restrained from chaotically exploiting that wide diversity of possible human behaviour? A theory is available concerning how this may be done and it is one of the basic theories of social anthropology. The way in which you restrain people from doing a wide variety of things, not compatible with the social order of which they are members, is that you subject them to ritual. The process is simple: you make them dance around a totem pole until they are wild with excitement, and become jellies in the hysteria of collective frenzy; you enhance their emotional state by any device, by all the locally available audio-visual aids, drugs, music and so on; and once they are really high, you stamp upon their minds the type of concept or notion to which they subsequently become enslaved.
Feminist scholars — among them palaeoanthropologists Leslie Aiello and Camilla Power — take similar arguments a step further, arguing that any reform or revolution which overthrew male dominance must have been led by women. Evolving human females, Power and Aiello suggest, actively separated themselves from males on a periodic basis, using their own blood (and/or pigments such as red ochre) to mark themselves as fertile and defiant:
The sexual division of labor entails differentiation of roles in food procurement, with logistic hunting of large game by males, co-operation and exchange of products. Our hypothesis is that symbolism arose in this context. To minimize energetic costs of travel, coalitions of women began to invest in home bases. To secure this strategy, women would have to use their attractive, collective signal of impending fertility in a wholly new way: by signalling refusal of sexual access except to males who returned "home" with provisions. Menstruation — real or artificial — while biologically the wrong time for fertile sex, is psychologically the right moment for focusing men's minds on imminent hunting, since it offers the prospect of fertile sex in the near future.
In similar vein, anthropologist Chris Knight argues that Boehm's idea of a 'coalition of everyone' is hard to envisage, unless — along the lines of a modern industrial picket line — it was formed to co-ordinate 'sex-strike' action against badly behaving males:
....male dominance had to be overthrown because the unending prioritising of male short-term sexual interests could lead only to the permanence and institutionalisation of behavioural conflict between the sexes, between the generations and also between rival males. If the symbolic, cultural domain was to emerge, what was needed was a political collectivity — an alliance — capable of transcending such conflicts. ... Only the consistent defence and self-defence of mothers with their offspring could produce a collectivity embodying interests of a sufficiently broad, universalistic kind.
In virtually all hunter-gatherer ethnographies, according to Knight, a persistent theme is that 'women like meat',[22] and that they determinedly use their collective bargaining power to motivate men to hunt for them and bring home their kills — on pain of exclusion from sex.[23][24] Arguments about women's crucial role in domesticating males — motivating them to cooperate — have also been advanced by anthropologists Kristen Hawkes,[25] Sarah Hrdy[26] and Bruce Knauft[27] among others. Meanwhile, other evolutionary scientists continue to envisage uninterrupted male dominance, continuity with primate social systems and the emergence of society on a gradualist basis without revolutionary leaps.[28]
I consider Trivers one of the great thinkers in the history of Western thought. It would not be too much of an exaggeration to say that he has provided a scientific explanation for the human condition: the intricately complicated and endlessly fascinating relationships that bind us to one another.
In his 1985 book, Social Evolution,[29] Robert Trivers outlines the theoretical framework used today by most evolutionary biologists to understand how and why societies are established. Trivers sets out from the fundamental fact that genes survive beyond the death of the bodies they inhabit, because copies of the same gene may be replicated in multiple different bodies. From this, it follows that a creature should behave altruistically to the extent that those benefiting carry the same genes — 'inclusive fitness', as this source of cooperation in nature is termed.[30] Where animals are unrelated, cooperation should be limited to 'reciprocal altruism' or 'tit-for-tat'.[31]
Where previously, biologists took parent-offspring cooperation for granted, Trivers predicted on theoretical grounds both cooperation and conflict — as when a mother needs to wean an existing baby (even against its will) in order to make way for another.[32] Previously, biologists had interpreted male infanticidal behaviour as aberrant and inexplicable or, alternatively, as a necessary strategy for culling excess population.[33] Trivers was able to show that such behaviour was a logical strategy by males to enhance their own reproductive success at the expense of conspecifics including rival males. Ape or monkey females whose babies are threatened have directly opposed interests, often forming coalitions to defend themselves and their offspring against infanticidal males.[34]
Human society, according to Trivers, is unusual in that it involves the male of the species investing parental care in his own offspring — a rare pattern for a primate. Where such cooperation occurs, it's not enough to take it for granted: in Trivers' view we need to explain it using an overarching theoretical framework applicable to humans and nonhumans alike.[35]
Everybody has a social life. All living creatures reproduce and reproduction is a social event, since at its bare minimum it involves the genetic and material construction of one individual by another. In turn, differences between individuals in the number of their surviving offspring (natural selection) is the driving force behind organic evolution. Life is intrinsically social and it evolves through a process of natural selection which is itself social. For these reasons social evolution refers not only to the evolution of social relationships between individuals but also to deeper themes of biological organization stretching from gene to community.
Robin Dunbar originally studied gelada baboons in the wild in Ethiopia, and has done much to synthesise modern primatological knowledge with Darwinian theory into a comprehensive overall picture. The components of primate social systems 'are essentially alliances of a political nature aimed at enabling the animals concerned to achieve more effective solutions to particular problems of survival and reproduction'.[36] Primate societies are in essence 'multi-layered sets of coalitions'.[37] Although physical fights are ultimately decisive, the social mobilisation of allies usually decides matters and requires skills that go beyond mere fighting ability. The manipulation and use of coalitions demands sophisticated social — more precisely political — intelligence.
Usually but not always, males exercise dominance over females. Even where male despotism prevails, females typically gang up with one another to pursue agendas of their own. When a male gelada baboon attacks a previously dominant rival so as to take over his harem, the females concerned may insist on their own say in the outcome. At various stages during the fighting, the females may 'vote' among themselves on whether to accept the provisional outcome. Rejection is signalled by refusing to groom the challenger; acceptance is signalled by going up to him and grooming him. According to Dunbar, the ultimate outcome of an inter-male 'sexual fight' always depends on the female 'vote'.[38]
Dunbar points out that in a primate social system, lower-ranking females will typically suffer the most intense harassment. Consequently, they will be the first to form coalitions in self-defence. But maintaining commitment from coalition allies involves much time-consuming manual grooming, putting pressure on time-budgets. In the case of evolving humans, who were living in increasingly large groups, the costs would soon have outweighed the benefits — unless some more efficient way of maintaining relationships could be found. Dunbar argues that 'vocal grooming' — using the voice to signal commitment — was the time-saving solution adopted, and that this led eventually to speech. Dunbar goes on to suggest (citing evolutionary anthropologist Chris Knight[39][40]) that distinctively human society may have been evolved under pressure from female ritual and 'gossiping' coalitions established to dissuade males from fighting one another and instead cooperate in hunting for the benefit of the whole camp:
If females formed the core of these early groups, and language evolved to bond these groups, it naturally follows that the early human females were the first to speak. This reinforces the suggestion that language was first used to create a sense of emotional solidarity between allies. Chris Knight has argued a passionate case for the idea that language first evolved to allow the females in these early groups to band together to force males to invest in them and their offspring, principally by hunting for meat. This would be consistent with the fact that, among modern humans, women are generally better at verbal skills than men, as well as being more skilful in the social domain.
Dunbar stresses that this is currently a minority theory among specialists in human origins — most still support the 'bison-down-at-the-lake' theory attributing early language and cooperation to the imperatives of men's activities such as hunting. Despite this, he argues that 'female bonding may have been a more powerful force in human evolution than is sometimes supposed'.[41] Although still controversial, the idea that female coalitions may have played a decisive role has subsequently received strong support from a number of anthropologists including Sarah Hrdy,[42] Camilla Power,[43] Ian Watts.[44] and Jerome Lewis.[45] It is also consistent with recent studies by population geneticists (see Verdu et al. 2013 [46] for Central African Pygmies; Schlebusch 2010[47] for Khoisan) showing a deep-time tendency to matrilocality among African hunter-gatherers.

The Austronesian peoples, sometimes referred to as Austronesian-speaking peoples,[44] are a large group of peoples who have settled in Taiwan, maritime Southeast Asia, parts of mainland Southeast Asia, Micronesia, coastal New Guinea, Island Melanesia, Polynesia, and Madagascar that speak Austronesian languages.[45][46] They also include indigenous ethnic minorities in Vietnam, Cambodia, Myanmar, Thailand, Hainan, the Comoros, and the Torres Strait Islands.[45][47][48] The nations and territories predominantly populated by Austronesian-speaking peoples are sometimes known collectively as Austronesia.[49]
The group originated from a prehistoric seaborne migration, known as the Austronesian expansion, from Taiwan, circa 3000 to 1500 BCE. Austronesians reached the Batanes Islands in the northernmost Philippines by around 2200 BCE. They used sails some time before 2000 BCE.[50]: 144  In conjunction with their use of other maritime technologies (notably catamarans, outrigger boats, lashed-lug boats, and the crab claw sail), this enabled phases of rapid dispersal into the islands of the Indo-Pacific, culminating in the settlement of New Zealand c. 1250 CE.[51] During the initial part of the migrations, they encountered and assimilated (or were assimilated by) the Paleolithic populations that had migrated earlier into Maritime Southeast Asia and New Guinea. They reached as far as Easter Island to the east, Madagascar to the west,[52] and New Zealand to the south. At the furthest extent, they might have also reached the Americas.[53][54][55]
Aside from language, Austronesian peoples widely share cultural characteristics, including such traditions and traditional technologies as tattooing, stilt houses, jade carving, wetland agriculture, and various rock art motifs. They also share domesticated plants and animals that were carried along with the migrations, including rice, bananas, coconuts, breadfruit, Dioscorea yams, taro, paper mulberry, chickens, pigs, and dogs.
The linguistic connections between Madagascar, Polynesia, and Southeast Asia, particularly the similarities between Malagasy, Malay, and Polynesian numerals, were recognized early in the colonial era by European authors.[56] The first formal publication on these relationships was in 1708 by Dutch Orientalist Adriaan Reland, who recognized a "common language" from Madagascar to western Polynesia, although Dutch explorer Cornelis de Houtman observed linguistic links between Madagascar and the Malay Archipelago a century earlier, in 1603.[47] German naturalist Johann Reinhold Forster, who traveled with James Cook on his second voyage, also recognized the similarities of Polynesian languages to those of Island Southeast Asia. In his book Observations Made during a Voyage round the World (1778), he posited that the ultimate origins of the Polynesians might have been the lowland regions of the Philippines and proposed that they arrived to the islands via long-distance voyaging.[57]
The Spanish philologist Lorenzo Hervás later devoted a large part of his Idea dell'universo (1778–1787) to the establishment of a language family linking the Malay Peninsula, the Maldives, Madagascar, Indonesia (Sunda Islands and Moluccas), the Philippines, and the Pacific Islands eastward to Easter Island. Multiple other authors corroborated this classification (except for the erroneous inclusion of Maldivian), and the language family came to be known as "Malayo-Polynesian", first coined by the German linguist Franz Bopp in 1841 (German: malayisch-polynesisch).[56][58] The connections between Southeast Asia, Madagascar, and the Pacific Islands were also noted by other European explorers, including the Orientalist William Marsden and the naturalist Johann Reinhold Forster.[59]
Johann Friedrich Blumenbach added Austronesians as the fifth category to his "varieties" of humans in the second edition of De Generis Humani Varietate Nativa (1781). He initially grouped them by geography and thus called Austronesians the "people from the southern world". In the third edition, published in 1795, he named Austronesians the "Malay race", or the "brown race", after correspondence with Joseph Banks, who was part of the first voyage of James Cook.[59][60] Blumenbach used the term "Malay" due to his belief that most Austronesians spoke the "Malay idiom" (i.e., the Austronesian languages), though he inadvertently caused the later confusion of his racial category with the Malay ethnic group.[61] The other varieties Blumenbach identified were the "Caucasians" (white), "Mongolians" (yellow), "Ethiopians" (black), and "Americans" (red). Blumenbach's definition of the "Malay" race is largely identical to the modern distribution of the Austronesian peoples, including not only Islander Southeast Asians but also the people of Madagascar and the Pacific Islands. Although Blumenbach's work was later used in scientific racism, Blumenbach was a monogenist and did not believe the human "varieties" were inherently inferior to each other. Rather, he believed that the Malay race was a combination of the "Ethiopian" and "Caucasian" varieties.[59][60]
Malay variety. Tawny-coloured; hair black, soft, curly, thick and plentiful; head moderately narrowed; forehead slightly swelling; nose full, rather wide, as it were diffuse, end thick; mouth large, upper jaw somewhat prominent with parts of the face when seen in profile, sufficiently prominent and distinct from each other. This last variety includes the islanders of the Pacific Ocean, together with the inhabitants of the Marianas, the Philippine, the Molucca and the Sunda Islands, and of the Malayan peninsula.
I wish to call it the Malay, because the majority of the men of this variety, especially those who inhabit the Indian islands close to the Malacca peninsula, as well as the Sandwich, the Society, and the Friendly Islanders, and also the Malambi of Madagascar down to the inhabitants of Easter Island, use the Malay idiom.
By the 19th century, however, a classification of Austronesians as being a subset of the "Mongolian" race was favored, as was polygenism. The Australo-Melanesian populations of Southeast Asia and Melanesia (whom Blumenbach initially classified as a "subrace" of the "Malay" race) were also now being treated as a separate "Ethiopian" race by authors like Georges Cuvier, Conrad Malte-Brun (who first coined the term "Oceania" as Océanique), Julien-Joseph Virey, and René Lesson.[59][63]
The British naturalist James Cowles Prichard originally followed Blumenbach by treating Papuans and Indigenous Australians as being descendants of the same stock as Austronesians. But by his third edition of Researches into the Physical History of Man (1836–1847), his work had become more racialized due to the influence of polygenism. He classified the peoples of Austronesia into two groups: the "Malayo-Polynesians" (roughly equivalent to the Austronesian peoples) and the "Kelænonesians" (roughly equivalent to the Australo-Melanesians). He further subdivided the latter into the "Alfourous" (also "Haraforas" or "Alfoërs", the Native Australians), and the "Pelagian or Oceanic Negroes" (the Melanesians and western Polynesians). Despite this, he acknowledges that "Malayo-Polynesians" and "Pelagian Negroes" had "remarkable characters in common", particularly in terms of language and craniometry.[59][56][58]
In linguistics, the Malayo-Polynesian language family also initially excluded Melanesia and Micronesia, due to the perceived physical differences between the inhabitants of these regions from Malayo-Polynesian speakers. However, there was growing evidence of their linguistic relationship to Malayo-Polynesian languages, notably from studies on the Melanesian languages by Georg von der Gabelentz, Robert Henry Codrington, and Sidney Herbert Ray. Codrington coined and used the term "Ocean" language family rather than "Malayo-Polynesian" in 1891, in opposition to the exclusion of Melanesian and Micronesian languages. This was adopted by Ray, who defined the "Oceanic" language family as encompassing the languages of Southeast Asia and Madagascar, Micronesia, Melanesia, and Polynesia.[47][64][65][66]
In 1899, the Austrian linguist and ethnologist Wilhelm Schmidt coined the term "Austronesian" (German: austronesisch, from Latin auster, "south wind"; and Greek νῆσος, "island") to refer to the language family.[67] Schmidt had the same motivations as Codrington: he proposed the term as a replacement to "Malayo-Polynesian", because he also opposed the implied exclusion of the languages of Melanesia and Micronesia in the latter name.[56][58] It became the accepted name for the language family, with Oceanic and Malayo-Polynesian languages being retained as names for subgroups.[47]
The term "Austronesian", or more accurately "Austronesian-speaking peoples", came to refer to people who speak the languages of the Austronesian language family. Some authors, however, object to the use of the term to refer to people, as they question whether there really is any biological or cultural shared ancestry between all Austronesian-speaking groups.[44][69] This is especially true for authors who reject the prevailing "Out of Taiwan" hypothesis and instead offer scenarios where the Austronesian languages spread among preexisting static populations through borrowing or convergence, with little or no population movements.[45][70]
Despite these objections, the general consensus is that the archeological, cultural, genetic, and especially linguistic evidence all separately indicate varying degrees of shared ancestry among Austronesian-speaking peoples that justifies their treatment as a "phylogenetic unit". This has led to the use of the term "Austronesian" in academic literature to refer not only to the Austronesian languages but also the Austronesian-speaking peoples, their societies, and the geographic area of Austronesia.[69][45][70][74][75]
Some Austronesian-speaking groups are not direct descendants of Austronesians and acquired their languages through language shift, but this is believed to have happened only in a few instances, since the Austronesian expansion was too rapid for language shifts to have occurred fast enough.[76] In parts of Island Melanesia, migrations and paternal admixture from Papuan groups after the Austronesian expansion (estimated to have started at around 500 BCE) also resulted in gradual population turnover. These secondary migrations were incremental and happened gradually enough that the culture and language of these groups remained Austronesian, even though in modern times, they are genetically more Papuan.[77] In the vast majority of cases, the language and material culture of Austronesian-speaking groups descend directly through generational continuity, especially in islands that were previously uninhabited.[76]
Serious research into the Austronesian languages and its speakers has been ongoing since the 19th century. Modern scholarship on Austronesian dispersion models is generally credited to two influential papers in the late 20th century: The Colonization of the Pacific: A Genetic Trail (Hill & Serjeantson, eds., 1989) and The Austronesian Dispersal and the Origin of Languages (Bellwood, 1991).[78][79] The topic is particularly interesting to scientists for the remarkably unique characteristics of the Austronesian speakers: their extent, diversity, and rapid dispersal.[80][81]
Regardless, certain disagreements still exist among researchers with regards to chronology, origin, dispersal, adaptations to the island environments, interactions with preexisting populations in areas they settled, and cultural developments over time. The mainstream accepted hypothesis is the "Out of Taiwan" model first proposed by Peter Bellwood. But there are multiple rival models that create a sort of "pseudo-competition" among their supporters due to narrow focus on data from limited geographic areas or disciplines.[80][81][82] The most notable of which is the "Out of Sundaland" (or "Out of Island Southeast Asia") model.
Austronesians were the first humans with seafaring vessels that could cross large distances on the open ocean; this technology allowed them to colonize a large part of the Indo-Pacific region.[83][failed verification][84] Prior to the 16th-century colonial era, the Austronesian language family was the most widespread in the world, spanning half the planet from Easter Island in the eastern Pacific Ocean to Madagascar in the western Indian Ocean.[45]
Languages of the Austronesian family are today spoken by about 386 million people (4.9% of the global population), making it the fifth-largest language family by number of speakers. Major Austronesian languages include Malay (around 250–270 million in Indonesia alone in its own literary standard, named Indonesian), Javanese, and Filipino (Tagalog). The family contains 1,257 languages, the second-largest number of any language family.[88]
The geographic region that encompasses native Austronesian-speaking populations is sometimes referred to as "Austronesia".[74] Other geographic names for various subregions include Malay Peninsula, Greater Sunda Islands, Lesser Sunda Islands, Island Melanesia, Island Southeast Asia, Malay Archipelago, Maritime Southeast Asia, Melanesia, Micronesia, Near Oceania, Oceania, Pacific Islands, Remote Oceania, Polynesia, and Wallacea. In Indonesia, the nationalistic term Nusantara, from Old Javanese, is also popularly used for the Indonesian islands.[74][89]
Austronesian regions are almost exclusively islands in the Pacific and Indian oceans, with predominantly tropical or subtropical climates with considerable seasonal rainfall.[47][91]
Inhabitants of these regions include Taiwanese indigenous peoples, most ethnic groups in Brunei, East Timor, Indonesia, Madagascar, Malaysia, Micronesia, the Philippines, and Polynesia. Also included are the Malays of Singapore; the Polynesians of New Zealand, Hawaii, and Chile; the Torres Strait Islanders of Australia; the non-Papuan peoples of Melanesia and coastal New Guinea; the Shibushi speakers of the Comoros, and the Malagasy and Shibushi speakers of Réunion. Austronesians are also found in the regions of Southern Thailand; the Cham areas in Vietnam, Cambodia, and Hainan; and the Mergui Archipelago of Myanmar.[45][47][48]
Additionally, modern-era migration has brought Austronesian-speaking people to the United States, Canada, Australia, the UK, mainland Europe, Cocos (Keeling) Islands, South Africa, Sri Lanka, Suriname, Hong Kong, Macau, and West Asian countries.[92]
Some authors also propose further settlements and contacts in the past in areas that are not inhabited by Austronesian speakers today. These range from likely hypotheses to very controversial claims with minimal evidence. In 2009, Roger Blench compiled an expanded map of Austronesia that encompassed these claims based on a variety of evidence, such as historical accounts, loanwords, introduced plants and animals, genetics, archeological sites, and material culture. They include areas like the Pacific coast of the Americas, Japan, the Yaeyama Islands, the Australian coast, Sri Lanka and coastal South Asia, the Persian Gulf, some Indian Ocean islands, East Africa, South Africa, and West Africa.[90]
Austronesian peoples include the following groupings by name and geographic location (incomplete):
The broad consensus on Austronesian origins is the "two-layer model", where an original Paleolithic indigenous population in Island Southeast Asia were assimilated to varying degrees by incoming migrations of Neolithic Austronesian-speaking peoples from Taiwan and Fujian, in southern China, from around 4,000 BP.[81][93] Austronesians also mixed with other preexisting populations as well as later migrant populations among the islands they settled, resulting in further genetic input. The most notable are the Austroasiatic-speaking peoples in western Island Southeast Asia (peninsular Malaysia, Sumatra, Borneo, and Java);[94] the Bantu peoples in Madagascar[52] and the Comoros; as well as Japanese,[95][96][97] Persian, Indian, Arab, and Han Chinese traders and migrants in more recent centuries.[98]
Island Southeast Asia was settled by modern humans in the Paleolithic following coastal migration routes, presumably starting before 70,000 BP from Africa, long before the development of Austronesian cultures.[99][100][101] These populations are typified by having dark skin, curly hair, and short statures, leading Europeans to believe, in the 19th century, that they were related to African Pygmies. However, despite these physical similarities, genetic studies have shown that they are more closely related to other Eurasian populations than to Africans.[102][101]
The lowered sea levels of the Pleistocene made some of the modern-day islands of Sundaland accessible via land bridges. However, the spread of humans across the Wallace line and into Sahul necessitated crossing bodies of water. Remains of stone tools and marine shells in Liang Sarru, Salibabu Island, North Sulawesi, dated to 32,000–35,000 years ago, is possible evidence for the longest sea voyage by Paleolithic humans ever recorded. The island was previously uninhabited by humans or hominins and can only be reached from either Mindanao or the Sangihe Islands by crossing an expanse of water at least 100 km (62 mi) wide, even during the low sea levels of the Pleistocene. Other evidence of early maritime transport are the appearance of obsidian tools with the same source on neighboring islands. These include the Philippine obsidian network (Mindoro and Palawan, ca.33,000-28,000 BP), and the Wallacea obsidian network (Timor, Atauro, Kisar, Alor, ca.22,000 BP). However, the method of crossing remains unknown and could have ranged from simple rafts to dugout canoes by the terminal Pleistocene.[103][104][105]
These early settlers are generally historically referred to as "Australo-Melanesians", though the terminology is problematic, as they are genetically diverse, and most groups within Austronesia have significant Austronesian admixture and culture. The unmixed descendants of these groups today include the interior Papuans and Indigenous Australians.[98][101]
In modern literature, descendants of these groups, located in Island Southeast Asia west of Halmahera, are usually collectively referred to as "Negritos", while descendants of these groups east of Halmahera (excluding Indigenous Australians) are referred to as "Papuans".[102] They can also be divided into two broad groups based on Denisovan admixture. Philippine Negritos, Papuans, Melanesians, and Indigenous Australians display Denisovan admixture, while Malaysian and western Indonesian Negritos (Orang Asli) and Andamanese islanders do not.[101][106][107][note 1]
Mahdi (2017) also uses the term "Qata" (from Proto-Malayo-Polynesian *qata) to distinguish the indigenous populations of Southeast Asia, versus "Tau" (from Proto-Austronesian *Cau) for the later settlers from Taiwan and mainland China. Both are based on proto-forms for the word "person" in Malayo-Polynesian languages that referred to darker-skinned and lighter-skinned groups, respectively.[102] Jinam et al. (2017) also proposed the term "First Sundaland People" in place of "Negrito", as a more accurate name for the original population of Southeast Asia.[101]
These populations are genetically distinct from later Austronesians, but through fairly extensive population admixture, most modern Austronesians have varying levels of ancestry from these groups. The same is true for some populations historically considered "non-Austronesians", due to physical differences—like Philippine Negritos, Orang Asli, and Austronesian-speaking Melanesians, all of whom have Austronesian admixture.[45][98] In Polynesians in Remote Oceania, for example, the admixture is around 20 to 30% Papuan and 70 to 80% Austronesian. The Melanesians in Near Oceania are roughly around 20% Austronesian and 80% Papuan, while in the natives of the Lesser Sunda Islands, the admixture is around 50% Austronesian and 50% Papuan. Similarly, in the Philippines, the groups traditionally considered to be "Negrito" vary between 30 and 50% Austronesian.[45][98][101]
The high degree of assimilation among Austronesian, Negrito, and Papuan groups indicates that the Austronesian expansion was largely peaceful. Rather than violent displacement, the settlers and the indigenous groups absorbed each other.[108] It is believed that in some cases, like in the Toalean culture of Sulawesi (c. 8,000–1,500 BP), it is even more accurate to say that the densely populated indigenous hunter-gatherer groups absorbed the incoming Austronesian farmers, rather than the other way around.[109] Mahdi (2016) further asserts that Proto-Malayo-Polynesian *tau-mata ("person")[note 2] is derived from a composite protoform *Cau ma-qata, combining "Tau" and "Qata" and indicative of the mixing of the two ancestral population types in these regions.[110]
The broad consensus on the Urheimat (homeland) of Austronesian languages as well as the Neolithic early Austronesian peoples is accepted to be Taiwan, as well as the Penghu Islands.[113][114][115] They are believed to have descended from ancestral populations in coastal mainland southern China, which are generally referred to as the "pre‑Austronesians".[note 3] Through these pre-Austronesians, Austronesians may also share a common ancestry with neighboring groups in Neolithic southern China.[116]
These Neolithic pre-Austronesians from the coast of southeastern China are believed to have migrated to Taiwan between approximately 10,000 and 6000 BCE.[117][68] Other research has suggested that, according to radiocarbon dates, Austronesians may have migrated from mainland China to Taiwan as late as 4000 BCE (Dapenkeng culture).[118] They continued to maintain regular contact with the mainland until 1500 BCE.[119][120]
The identity of the Neolithic pre-Austronesian cultures in China is contentious. Tracing Austronesian prehistory in Fujian and Taiwan has been difficult due to the southward expansion of the Han dynasty (2nd century BCE) and the recent Qing dynasty annexation of Taiwan (1683 CE).[111][121][122][123] Today, the only Austronesian language in southern China is Tsat, spoken in Hainan. The politicization of archaeology is also problematic, particularly erroneous reconstructions among some Chinese archaeologists of non-Sinitic sites as Han.[124] Some authors, favoring the "Out of Sundaland" model, like William Meacham, reject the southern Chinese mainland origin of pre-Austronesians entirely.[125]
Nevertheless, based on linguistic, archaeological, and genetic evidence, Austronesians are most strongly associated with the early farming cultures of the Yangtze River basin that domesticated rice from around 13,500 to 8,200 BP. They display typical Austronesian technological hallmarks, including tooth removal, teeth blackening, jade carving, tattooing, stilt houses, advanced boatbuilding, aquaculture, wetland agriculture, and the domestication of dogs, pigs, and chickens. These include the Kuahuqiao, Hemudu, Majiabang, Songze, Liangzhu, and Dapenkeng cultures that occupied the coastal regions between the Yangtze River delta and the Min River delta.[126][127][128][129]
Based on linguistic evidence, there have been proposals linking Austronesians with other linguistic families into linguistic macrofamilies that are relevant to the identity of the pre-Austronesian populations. The most notable are the connections of Austronesians to the neighboring Austroasiatic, Kra-Dai, and Sinitic peoples (as Austric, Austro-Tai, and Sino-Austronesian, respectively). These are still not widely accepted, as evidence of these relationships are still tenuous, and the methods used are highly contentious.[130]
In support of both the Austric and Austro-Tai hypothesis, Robert Blust connects the lower Yangtze Neolithic Austro-Tai entity with the rice-cultivating Austroasiatic cultures, assuming the center of East Asian rice domestication, and putative Austric homeland, to be located in the Yunnan/Burma border area,[131]: 188  instead of the Yangtze River basin, as is currently accepted.[132][133][134][135] Under that view, there was an east–west genetic alignment, resulting from a rice-based population expansion, in the southern part of East Asia: Austroasiatic-Kra-Dai-Austronesian, with unrelated Sino-Tibetan occupying a more northerly tier.[131]: 188  Depending on the author, other hypotheses have also included other language families like Hmong-Mien and even Japanese-Ryukyuan into the larger Austric hypothesis.[136]
While the Austric hypothesis remains contentious, there is genetic evidence that at least in western Island Southeast Asia, there had been earlier Neolithic overland migrations (pre-4,000 BP) by Austroasiatic-speaking peoples into what is now the Greater Sunda Islands when the sea levels were lower, in the early Holocene. These peoples were assimilated linguistically and culturally by incoming Austronesian peoples in what is now modern-day Indonesia and Malaysia.[94]
Several authors have also proposed that Kra-Dai speakers may actually be an ancient daughter subgroup of Austronesians that migrated back to the Pearl River Delta from Taiwan and/or Luzon, shortly after the Austronesian expansion, later migrating further westwards to Hainan, Mainland Southeast Asia, and Northeast India. They propose that the distinctiveness of Kra-Dai (it is tonal and monosyllabic) was the result of linguistic restructuring due to contact with Hmong-Mien and Sinitic cultures. Aside from linguistic evidence, Roger Blench has also noted cultural similarities between the two groups, like facial tattooing, tooth removal or ablation, teeth blackening, snake (or dragon) cults, and the multiple-tongued jaw harps shared by the indigenous Taiwanese and Kra-Dai-speakers. However, archaeological evidence for this is still sparse.[130][127][137][138] This is believed to be similar to what happened to the Cham people, who were originally Austronesian settlers (likely from Borneo) to southern Vietnam around 2100–1900 BP and had languages similar to Malay. Their languages underwent several restructuring events to syntax and phonology due to contact with the nearby tonal languages of Mainland Southeast Asia and Hainan.[138][139] Although the populations of the Malay peninsula, Sumatra, Java, and neighboring islands are Austronesian-speaking, they have significantly high admixture from Mainland Southeast Asian populations. These areas were already populated (most probably by speakers of Austroasiatic languages) before they were reached by the Austronesian expansion, roughly 3,000 years ago. Currently, only the indigenous Aslians still speak Austroasiatic languages. However, some of the languages in the region show signs of underlying Austroasiatic substrates.[citation needed]
According to Juha Janhunen and Ann Kumar, Austronesians may have also settled parts of southern Japan, especially on the islands of Kyushu and Shikoku, and influenced or created the Japanese hierarchical society. It is suggested that Japanese tribes like the Hayato people, the Kumaso, and the Azumi were of Austronesian origin. Until today, local traditions and festivals show similarities to Malayo-Polynesian culture.[140][141][142][143][144]
The Sino-Austronesian hypothesis, on the other hand, is a relatively new hypothesis by Laurent Sagart, first proposed in 1990. It argues for a north–south linguistic genetic relationship between Chinese and Austronesian. This is based on sound correspondences in basic vocabulary and morphological parallels.[131]: 188  Sagart places special significance in shared vocabulary on cereal crops, citing them as evidence of shared linguistic origin. However, this has largely been rejected by other linguists. The sound correspondences between Old Chinese and Proto-Austronesian can also be explained as a result of the Longshan interaction sphere, when pre-Austronesians from the Yangtze region came into regular contact with Proto-Sinitic speakers in the Shandong Peninsula, around the 4th to 3rd millennia BCE. This corresponded with the widespread introduction of rice cultivation to Proto-Sinitic speakers and conversely, millet cultivation to Pre-Austronesians.[145] An Austronesian substratum in formerly Austronesian territories that have been Sinicized after the Iron Age Han expansion is also another explanation for the correspondences that do not require a genetic relationship.[146][147]
In relation to Sino-Austronesian models and the Longshan interaction sphere, Roger Blench (2014) suggests that the single migration model for the spread of the Neolithic into Taiwan is problematic, pointing out the genetic and linguistic inconsistencies between different Taiwanese Austronesian groups.[148]: 1–17  The surviving Austronesian populations in Taiwan should rather be considered as the result of various Neolithic migration waves from the mainland and back-migration from the Philippines.[148]: 1–17  These incoming migrants almost certainly spoke languages related to Austronesian or pre-Austronesian, although their phonology and grammar would have been quite diverse.[148]
Blench considers the Austronesians in Taiwan to have been a melting pot of immigrants from various parts of the coast of East China that had been migrating to Taiwan by 4000 BP. These immigrants included people from the foxtail millet-cultivating Longshan culture of Shandong (with Longshan-type cultures found in southern Taiwan), the fishing-based Dapenkeng culture of coastal Fujian, and the Yuanshan culture of northernmost Taiwan, which Blench suggests may have originated from the coast of Guangdong. Based on geography and cultural vocabulary, Blench believes that the Yuanshan people may have spoken Northeast Formosan languages. Thus, Blench believes that there is in fact no "apical" ancestor of Austronesian in the sense that there was no true single Proto-Austronesian language that gave rise to present-day Austronesian languages. Instead, multiple migrations of various pre-Austronesian peoples and languages from the Chinese mainland that were related but distinct came together to form what we now know as Austronesian in Taiwan. Hence, Blench considers the single-migration model into Taiwan by pre-Austronesians to be inconsistent with both the archaeological and linguistic (lexical) evidence.[148]
The Austronesian expansion (also called the "Out of Taiwan" model) is a large-scale migration of Austronesians from Taiwan, occurring around 3000 to 1500 BCE. Population growth primarily fueled this migration. These first settlers settled in northern Luzon, in the archipelago of the Philippines, intermingling with the earlier Australo-Melanesian population who had inhabited the islands since about 23,000 years earlier. Over the next thousand years, Austronesian peoples migrated southeast to the rest of the Philippines, and into the islands of the Celebes Sea and Borneo.[117][149] From southwestern Borneo, Austronesians spread further west in a single migration event to both Sumatra and the coastal regions of southern Vietnam, becoming the ancestors of the speakers of the Malayic and Chamic branches of the Austronesian language family.[51]
Soon after reaching the Philippines, Austronesians colonized the Northern Mariana Islands by 1500 BCE or even earlier, becoming the first humans to reach Remote Oceania. The Chamorro migration was also unique in that it was the only Austronesian migration to the Pacific Islands to successfully retain rice cultivation. Palau and Yap were settled by separate voyages by 1000 BCE.[51][117][149]
Another important migration branch was by the Lapita culture, which rapidly spread into the islands off the coast of northern New Guinea and into the Solomon Islands and other parts of coastal New Guinea and Island Melanesia by 1200 BCE. They reached the islands of Fiji, Samoa, and Tonga by around 900 to 800 BCE. This remained the furthest extent of the Austronesian expansion into Polynesia until around 700 CE, when there was another surge of island colonization. It reached the Cook Islands, Tahiti, and the Marquesas by 700 CE; Hawaii by 900 CE; Rapa Nui by 1000 CE; and New Zealand by 1200 CE.[78][150][151] For a few centuries, the Polynesian islands were connected by bidirectional long-distance sailing, with the exception of Rapa Nui, which had limited further contact due to its isolated geographical location.[51] Island groups like the Pitcairns, the Kermadec Islands, and the Norfolk Islands were also formerly settled by Austronesians but later abandoned.[151] There is also putative evidence, based in the spread of the sweet potato, that Austronesians may have reached South America from Polynesia, where they might have traded with the Indigenous peoples of the Americas.[53][54]
In the Indian Ocean, Austronesians in Maritime Southeast Asia established trade links with South Asia.[152] They also established early long-distance contacts with Africa, possibly as early as before 500 BCE, based on archaeological evidence like banana phytoliths in Cameroon and Uganda and remains of Neolithic chicken bones in Zanzibar.[153][154] By the end of the first millennium BCE, Austronesians were already sailing maritime trade routes linking the Han dynasty of China with the western Indian Ocean trade in India, the Roman Empire, and Africa.[155]: 610–611  An Austronesian group, originally from the Makassar Strait region around Kalimantan and Sulawesi,[156][157] eventually settled Madagascar, either directly from Southeast Asia or from preexisting mixed Austronesian-Bantu populations from East Africa. Estimates for when this occurred vary, from the 5th to 7th centuries CE.[155][158][153][154] It is likely that the Austronesians that settled Madagascar followed a coastal route through South Asia and East Africa, rather than directly across the Indian Ocean.[51] Genetic evidence suggests that some individuals of Austronesian descent reached Africa and the Arabian Peninsula.[159]
A competing hypothesis to the "Out of Taiwan" model is the "Out of Sundaland" hypothesis, favored by a minority of authors. Notable proponents include William Meacham, Stephen Oppenheimer, and Wilhelm Solheim. For various reasons, they have proposed that the homelands of Austronesians were within Island Southeast Asia (ISEA), particularly in the Sundaland landmass drowned during the end of the Last Glacial Period by rising sea levels. Proponents of these hypotheses point to the ancient origins of mtDNA in Southeast Asian populations, pre-dating the Austronesian expansion, as proof that Austronesians originated from within Island Southeast Asia.[160][161][162]
However, these have been repudiated by studies using whole genome sequencing, which have found that all ISEA populations had genes originating from aboriginal Taiwanese.[163] Contrary to the claim of a south-to-north migration in the "Out of Sundaland" hypothesis, the new whole genome analysis strongly confirms the north-to-south dispersal of the Austronesian peoples in the prevailing "Out of Taiwan" hypothesis. Researchers have further pointed out that while humans have been living in Sundaland for at least 40,000 years, the Austronesian people were recent arrivals. The results of the previous studies failed to take into account admixture with the more ancient but unrelated Negrito and Papuan populations.[164][163]
By the beginning of the first millennium CE, most of the Austronesian inhabitants in Maritime Southeast Asia began trading with India and China. The adoption of the Hindu statecraft model allowed the creation of Indianized kingdoms, such as Tarumanagara, Champa, Butuan, Langkasuka, Melayu, Srivijaya, Mataram, Majapahit, and Bali. Between the 5th and the 15th century, Hinduism and Buddhism were established as the main religion in the region. Muslim traders from the Arabian Peninsula were thought to have brought Islam by the 10th century. This was established as the dominant religion in the Malay archipelago by the 16th century. The Austronesian inhabitants of Near Oceania and Remote Oceania were unaffected by this cultural trade and retained their indigenous culture in the Pacific region.[165]
The Kingdom of Larantuka in Flores, East Nusa Tenggara, was the only Christian (Roman Catholic) indigenous kingdom in Indonesia and in Southeast Asia, with its first king named Lorenzo.[166]
Western Europeans in search of spices and gold later colonized most of the Austronesian-speaking countries of the Asia-Pacific region, beginning in the 16th century, with the Portuguese and Spanish colonization of the Philippines, Palau, Guam, the Mariana Islands, and some parts of Indonesia (present-day East Timor); the Dutch colonization of the Indonesian archipelago; the British colonization of Malaysia and Oceania; the French colonization of French Polynesia; and later, the American governance of the Pacific.
Meanwhile, the British, Germans, French, Americans, and Japanese began establishing spheres of influence within the Pacific islands during the 19th and early 20th centuries. The Japanese later invaded most of Southeast Asia and some parts of the Pacific during World War II. The latter half of the 20th century initiated independence of modern-day Indonesia, Malaysia, East Timor, and many of the Pacific island nations, as well as the re-independence of the Philippines.
The native culture of Austronesia varies from region to region. The early Austronesian peoples considered the sea as the basic feature of their life.[citation needed] Following their diaspora to Southeast Asia and Oceania, they migrated by boat to other islands. Boats of different sizes and shapes have been found in every Austronesian culture, from Madagascar, Maritime Southeast Asia, to Polynesia, and have different names. In Southeast Asia, head-hunting was restricted to the highlands as a result of warfare. Mummification is only found among the highland Austronesian Filipinos and in some Indonesian groups in Celebes and Borneo.[citation needed]
Seagoing catamaran and outrigger ship technologies were the most important innovations of the Austronesian peoples.[103][83] They were the first humans with vessels capable of crossing vast distances of water. The crossing from the Philippines to the Mariana Islands at around 1500 BCE, a distance of more than 2,500 km (1,600 mi), is likely the world's first and longest ocean crossing of that time.[83][84] These maritime technologies enabled them to colonize the Indo-Pacific in prehistoric times. Austronesian groups continue to be the primary users of outrigger canoes today.[83][84]
Early researchers like Heine-Geldern (1932) and Hornell (1943) once believed that catamarans evolved from outrigger canoes, but modern authors specializing in Austronesian cultures, like Doran (1981) and Mahdi (1988), now believe it to be the opposite.[167][71][168]
Two canoes bound together developed directly from minimal raft technologies of two logs tied together. Over time, the double-hulled canoe form developed into the asymmetric double canoe, where one hull is smaller than the other. Eventually the smaller hull became the prototype outrigger, giving way to the single outrigger canoe, then to the reversible single outrigger canoe. Finally, the single outrigger types developed into the double outrigger canoe (or trimarans).[167][71][168]
This would also explain why older Austronesian populations in Island Southeast Asia tend to favor double outrigger canoes, as it keeps the boats stable when tacking. However, there are small regions where catamarans and single-outrigger canoes are still used. In contrast, more distant outlying descendant populations in Micronesia, Polynesia, Madagascar, and the Comoros retained the double-hull and the single-outrigger canoe types, but the technology for double outriggers never reached them (although it exists in western Melanesia). To deal with the problem of the boat's instability when the outrigger faces leeward when tacking, they instead developed the shunting technique in sailing, in conjunction with reversible[note 4] single-outriggers.[167][71][168][169][170]
The simplest form of all ancestral Austronesian boats had five parts. The bottom consisted of a single piece of hollowed-out log. At the sides were two planks, and two horseshoe-shaped wood pieces formed the prow and stern. These were fitted tightly together edge-to-edge, with dowels inserted into holes in between, and then lashed to each other with ropes (made from rattan or fiber) wrapped around protruding lugs on the planks. This characteristic and ancient Austronesian boatbuilding practice is known as the "lashed-lug" technique. They were commonly caulked with pastes made from various plants as well as tapa bark and fibers that would expand when wet, further tightening joints and making the hull watertight. They formed the shell of the boat, which was then reinforced by horizontal ribs. Shipwrecks of Austronesian ships can be identified from this construction as well as the absence of metal nails. Austronesian ships traditionally had no central rudders but were instead steered using an oar on one side.[171][172][173]
The ancestral rig was the mastless triangular crab claw sail, which had two booms that could be tilted to the wind. These were built in the double-canoe configuration or had a single outrigger on the windward side. In Island Southeast Asia, these developed into double outriggers on each side, which provided greater stability. The triangular crab claw sails also later developed into square or rectangular tanja sails, which, like crab claw sails, had distinctive booms spanning the upper and lower edges. Fixed masts also developed later in both Southeast Asia (usually as bipod or tripod masts) and Oceania.[171][172] Austronesians traditionally made their sails from woven mats of the resilient and salt-resistant pandanus leaves. These sails allowed them to embark on long-distance voyaging. In some cases, however, they were one-way voyages. The failure to establish populations in Rapa Nui and New Zealand is believed to have isolated their settlements from the rest of Polynesia.[175][176]
The ancient Champa of Vietnam also uniquely developed basket-hulled boats whose hulls were composed of woven and resin-caulked bamboo, either entirely or in conjunction with plank strakes. They ranged from small coracles (o thúng) to large oceangoing trading ships like the ghe mành.[178][179]
The acquisition of catamaran and outrigger technology by non-Austronesian peoples in Sri Lanka and southern India is due to the result of very early Austronesian contact with the region, including the Maldives and the Laccadive Islands, estimated to have occurred around 1000 to 600 BCE and onwards. This may have possibly included limited colonization by people who have since been assimilated. This is still evident in Sri Lankan and South Indian languages. For example, Tamil paṭavu, Telugu paḍava, and Kannada paḍahu, all meaning "ship", are all derived from Proto-Hesperonesian *padaw, "sailboat", with Austronesian cognates like Sundanese parahu, Javanese perahu, Kadazan padau, Maranao padaw, Cebuano paráw, Samoan folau, Hawaiian halau, and Māori wharau.[167]
Austronesian architecture is diverse but often shares certain characteristics that indicate a common origin. The reconstructed Proto-Austronesian and Proto-Malayo-Polynesian forms of various terms for "house", "building", or "granary" among the different linguistic subgroups of Austronesians include *rumaq ("house");[note 5] *balay ("public building", "community house", or "guest house");[note 6] *lepaw ("hut", "field hut", or "granary");[note 7] *kamaliR ("bachelor's house", or "men's house");[note 8] and *banua ("inhabited land", or "community territory").[note 9][180][181]
Austronesian structures commonly have raised floors. The structures are raised on piles, usually with space underneath also utilized for storage or domestic animals. The raised design has multiple advantages, including mitigating damage during flooding and (in very tall examples) acting as defensive structures during conflicts. The house posts are also distinctively capped with larger-diameter discs at the top, to prevent vermin and pests from entering the structures by climbing them. Austronesian houses and other structures are usually built in wetlands and alongside bodies of water but can also be constructed in the highlands or even directly on shallow water.[182][183][184]
Building structures on pilings is believed to be derived from the design of raised granaries and storehouses, which are highly important status symbols among the ancestrally rice-cultivating Austronesians.[182][184] The rice granary shrine was also the archetypal religious building among Austronesian cultures and was used to store carvings of ancestor spirits and local deities.[184]
Another common feature are pitched roofs with ornamented gables. The most notable of these are saddlebacked roofs, a design common for longhouses used for village meetings or ceremonies. The overall effect of this is reminiscent of a boat, underlining the strong maritime connections of Austronesian cultures. The boat motif is common throughout, particularly in Eastern Indonesia. In some ethnic groups, the houses are built on platforms that resemble catamarans. Among the Nage people, a woven representation of a boat is added to the ridge of the roof; among the Manggarai people, the roofs of houses are shaped like an upside-down boat; while among the people of Tanimbar and eastern Flores, the ridge itself is carved into a representation of a boat. Furthermore, elements of Austronesian structures (as well as society in general) are often referred to in terminologies used for boats and sailing. These include calling elements of structures "masts", "sails", or "rudders", or calling the village leaders "captains" or "steersmen". In the case of the Philippines, the villages themselves are referred to as barangay, from an alternate form of balangay, a type of sailboat used for trading and colonization.[108][183][185][184]
Austronesian buildings have spiritual significance, often containing what has been coined by anthropologist James J. Fox as a "ritual attractor". These are specific posts, beams, platforms, altars, and so on that embody the house as a whole, usually consecrated at the time of building.[180]
The Austronesian house itself also often symbolizes various aspects of indigenous Austronesian cosmology and animism. In the majority of cases, the loft of the house (usually placed above the hearth), is considered to be the domain of deities and spirits. It is essentially a raised granary built into the structure of the house itself and functioning as a second floor. It is commonly used to store sacred objects (like effigies of granary idols or deceased ancestors), heirlooms, and other important objects. These areas are usually not part of the regular living space and may only be accessible to certain members of the family or after performing a specific ritual. Other parts of the house may also be associated with certain deities, and thus certain activities like receiving guests or conducting marriage ceremonies can only be performed in specific areas.[182]
While rice cultivation wasn't among the technologies carried into Remote Oceania, raised storehouses still survived. The pātaka of the Māori people is an example. The largest pātaka are elaborately adorned with carvings and are often the tallest buildings in the Māori pā. These were used to store implements, weapons, ships, and other valuables; while smaller pātaka were used to store provisions. A special type of pātaka, supported by a single tall post, also had ritual importance and was used to isolate high-born children during their training for leadership.[182]
The majority of Austronesian structures are not permanent. They are made from perishable materials like wood, bamboo, plant fiber, and leaves. Similar to traditional Austronesian boats, they do not use nails but are traditionally constructed solely by joints, weaving, ties, and dowels. Elements of the structures are repaired and replaced regularly or as they get damaged. Because of this, archaeological records of prehistoric Austronesian structures are usually limited to traces of house posts, with no way of determining the original building plans.[186]
Indirect evidence of traditional Austronesian architecture, however, can be gleaned from their contemporary representations in art, such as friezes on the walls of later Hindu-Buddhist stone temples (like in reliefs at Borobudur and Prambanan). But these are limited to the recent centuries. They can also be reconstructed linguistically from shared terms for architectural elements, like ridge poles, thatch, rafters, house posts, hearths, notched log ladders, storage racks, public buildings, and so on. Linguistic evidence also makes it clear that stilt houses were already present among Austronesian groups since at least the Late Neolithic.[183][184]
In modern Indonesia, varying styles are collectively known as rumah adat.
Arbi et al. (2013) have also noted the striking similarities between Austronesian architecture and Japanese traditional raised architecture (shinmei-zukuri). Particularly the buildings of the Ise Grand Shrine, which contrast with the pit-houses typical of the Neolithic Yayoi period. They propose significant Neolithic contact between the people of southern Japan and Austronesians or pre-Austronesians that occurred prior to the spread of Han Chinese cultural influence to the islands.[183] Rice cultivation is also believed to have been introduced to Japan from a para-Austronesian group from coastal eastern China.[187] Waterson (2009) has also argued that the architectural tradition of stilt houses is originally Austronesian and that similar building traditions in Japan and mainland Asia (notably among Kra-Dai and Austroasiatic-speaking groups) correspond to contacts with a prehistoric Austronesian network.[184][108]
Outside of Taiwan, assemblages of red-slipped pottery, plainware, and incised and stamped pottery associated with Austronesian migrations are first documented from around 2000 to 1800 BCE in the northern Philippines, from sites in the Batanes Islands and the Cagayan Valley of Northern Luzon. From there, pottery technology rapidly spread to the east, south, and southwest.[191][192][84]
This type of pottery dispersed south and southwest to the rest of Island Southeast Asia. The eastward and southward branches of the migrations converged in Island Melanesia, resulting in what is now known as the Lapita culture, centered around the Bismarck Archipelago.[191][192][84]
The oldest known pottery assemblages in Oceania are circle- and punctate/dentate-stamped pottery in the Marianas Islands, securely dated to 1500 BCE–1300 BCE from multiple archaeological sites. It predates the earliest Lapita culture pottery assemblages (c. 1350 –1300 BCE) and bears closest resemblance to a subset of the more diverse Nagsabaran pottery of the northern Philippines. It is currently disputed whether this is indicative of a direct ancient voyage from the northern Philippines to the Marianas. Hung et al. (2011) proposed a direct deliberate voyage from eastern Luzon, which would make it the longest sea crossing undertaken by that time in human history.[84] This has also been proposed by earlier authors like Blust (2000) and Reid (2002), based on linguistics.[84][193][194]
Winter et al. (2012), on the other hand, dismissed the similarities as being generic rather than specific to the region. This is from both analysis of the microscopic structure of the shards (indicating manufacturing techniques) and the impossibility of drift voyaging from Luzon, due to the prevailing wind and currents. Instead of a voyage directly from Luzon, they instead proposed an origin either from a direct single voyage from Mindanao (southern Philippines) or Morotai (Maluku Islands) to Guam; or two voyages, with way stations in Palau or Yap.[195]
Hung et al. (2012) pointed out in response that no pottery assemblages older than 2,000 years old have been found in Morotai, which also has a Papuan-speaking population. They also mentioned that present-day data on wind and currents is not a reliable way of ascertaining migration routes, and that the voyages settling Remote Oceania would have been deliberate, not uncontrolled drifting. Similar presumptions by Thor Heyerdahl led to his erroneous conclusion that Polynesia was settled from the Americas. Pottery manufacturing techniques are also diverse, even within a single community. Thus, analysis of manufacturing methods is less significant than comparison of decorative systems. Nevertheless, Hung et al. (2012) emphasized that they also did not discount other sources (yet undiscovered) from the southern Philippines. They also proposed the Eastern Visayas as a likely point of origin. Sources south of the Philippines remain unlikely without further archaeological findings due to their related pottery assemblages being younger than 1500 BCE.[196]
The dentate-stamped pottery of the Lapita culture (c. 1350–1300 BCE) retained elements also found in the Nagsabaran pottery in the Philippines, including stamped circles as well as the cross-in-circle motif.[197][84] They carried pottery technology as far as Tonga in Polynesia. Pottery technology in Tonga, however, became reduced to undecorated plainware within only two centuries before abruptly disappearing completely by around 400 BCE. The reasons for this are still unknown. Pottery was absent in subsequent migrations to the rest of Remote Oceania, being replaced instead with carved wooden or bamboo containers, bottle gourds, and baskets.[198][192][199][197] However, the geometric designs and stylized figures used in the pottery are still present in other surviving art forms, such as tattooing, weaving, and barkcloth patterns.[200][197]
A common practice among Austronesians in a large area of Island Southeast Asia is the use of burial jars, which emerged during the Late Neolithic and flourished in the first millennium CE. They are characteristic of a region bordered by the Philippines to the north, southern Sumatra in the southwest, and Sumba and the Maluku Islands in the southeast. However, these didn't comprise a single tradition but can be grouped into at least fourteen different traditions scattered across the islands. In most cases, the earliest burial jars used were large indigenous earthenware jars, followed by indigenous or imported stoneware jars (martaban), and finally imported porcelain jars acquired from the burgeoning maritime trade with China and Mainland Southeast Asia around the 14th century CE.[201]
Slit drums are indigenous Austronesian musical instruments invented and used by Southeast Asian-Austronesian and Oceanic-Austronesian ethnic groups.
Gong ensembles are also a common musical heritage of Island Southeast Asia. The casting of gong instruments is believed to have originated from the Bronze Age cultures of Mainland Southeast Asia. It spread to the Austronesian islands initially through trade as prestige goods. However, mainland Asian gongs were never used in ensembles; the innovation of using gong sets is uniquely Austronesian. Gong ensembles are found in western Malayo-Polynesian groups, though they never penetrated much further east. There are roughly two gong ensemble traditions among Austronesians, which also produced gongs in ancient times.[138]
In western Island Southeast Asia, these traditions are collectively known as gamelan, being centred on the island of Java in Indonesia. They include the celempung of the Malay Peninsula, talempung of northern Sumatra, caklempung of central Sumatra, chalempung of southern Sumatra, bonang of Java, kromong of western Kalimantan, engkromong of Sarawak, and trompong of western Nusa Tenggara.[138]
In eastern Island Southeast Asia, these traditions are known as kulintang and are centred in Mindanao and the Sulu archipelago of the southern Philippines. They include the kulintangan of Sabah and Palawan, kolintang of northern Sulawesi, kulintang of Halmahera and Timor, and totobuang of the southern Maluku Islands.[138]
The ancestral pre-Austronesian Liangzhu culture (3400–2250 BCE) of the Yangtze River delta was one of the ancient centers of Neolithic jade carving. Jade was spread to Taiwan by around 3,000 BCE, then further into the Philippines at 2,000 BCE and Vietnam at 1,800–1,500 BCE. All of them began to produce various tools and ornaments in indigenous jade workshops, including adzes, bracelets, beads, and rings.[202][203]
The most notable jade products of these regions were the vast amounts of penannular (in the form of an incomplete circle) and double-headed earrings and pendants known as lingling-o, primarily produced in the Philippines and the Sa Huỳnh culture of Vietnam, mostly with raw jade material sourced from eastern Taiwan. These typically depicted two-headed animals or were ring-shaped with side projections. They were indicative of a very active ancient maritime trading region, known as the Sa Huynh-Kalanay Interaction Sphere, that imported and exported raw jade and finished jade ornaments. They were produced during a period from 500 BCE to as late as 1000 CE, although later examples were replaced with metal, wood, bone, clay, green mica, black nephrite, or shell materials, rather than green jade.[204][202][205][203]
Polished and ground stone adzes, gouges, and other implements, some of which are made from jade-like stone, have also been recorded in areas of Island Melanesia and eastern New Guinea associated with the Lapita culture. These were considered valuable currency and were primarily used to trade for goods.[206][207] In 2012, a Lapita jadeite gouge used for wood carving was found on Emirau Island in the Bismarck Archipelago. It was dated to around 3,300 BCE, but the origin of the jade material is unknown.[208][209] Similar stone tools have also been found in New Caledonia.[210]
Jade was absent in most of Remote Oceania, due to the lack of deposits. However, there is putative evidence that Polynesians may have remained familiar with jade and may acquired it through prehistoric trade contacts with New Caledonia, Island Melanesia, and/or New Zealand.[206][211]
Jade-carving traditions reappeared among the Māori people of New Zealand. These were produced from locally sourced pounamu (greenstone) and were used to produce taonga (treasure). They include various tools and weapons like adzes, scrapers, fishing hooks, and mere, as well as ornaments like the hei-tiki and hei matau. Certain ornaments like the pekapeka (double-headed animal pendant) and the kākā pōria (bird leg ring) bear remarkably strong resemblances to the double-headed and ring-type lingling‑o.[205][212] Bellwood et al. (2011) has suggested that the reappearance of these motifs might be evidence of a preserved tradition of Southeast Asian jade motifs (perhaps carved in perishable wood, bone, or shell by Polynesians prior to the reacquisition of a jade source), or they might even be the result of later Iron Age contact between eastern Polynesia and the Philippines.[205]
There are approximately six to seven hundred rock art sites discovered in Southeast Asia and Island Melanesia, as well as over eight hundred megalithic sites. The sites specifically associated with the Austronesian expansion contain examples of indigenous pictograms and petroglyphs. Within Southeast Asia, the sites associated with Austronesians can be divided into three general rock art traditions: the Megalithic Culture of Borneo, Sulawesi, and the Greater Sunda Islands; the Austronesian Painting Tradition (APT) of the Lesser Sunda Islands, coastal New Guinea, and Island Melanesia; and the Austronesian Engraving Style (AES) of Papua New Guinea and Island Melanesia.[213] Despite proximity, these traditions can be distinguished readily from the Australo-Melanesian rock art traditions of Australia (except the Torres Strait Islands) as well as the interior highlands of New Guinea, indicating the borders of the extent of the Austronesian expansion.[197]
Dating rock art is difficult, but some of the sites subjected to direct dating pre-date Austronesian arrival, like the Lene Hara paintings of East Timor, which have an age range of 6,300 to 26,000 BP. Conversely, others are more recent and can be dated indirectly by their subjects. The depictions of pottery, ships, and metal objects, for example, put certain rock art sites at a range of 2,000 to 4,000 BP. Some hunter-gatherer groups have also continued to produce rock art well into the present period, as evidenced by their modern subjects.[213][214][215]
The Megalithic Culture is mostly limited to western Island Southeast Asia, with the greatest concentration being western Indonesia. While most sites are not dated, the age ranges of dating sites are between the 2nd and 16th centuries CE. They are divided into two phases: The first is an older megalithic tradition associated with the Neolithic Austronesian rectangular axe culture (2,500 to 1,500 BCE), while the second is the 3rd- or 4th-century BCE megalithic tradition associated with the (non-Austronesian) Dong Son culture of Vietnam. Prasetyo (2006) suggests that the megalithic traditions are not originally Austronesian but rather innovations acquired through trade with India and China, but this has little to no evidence in the intervening regions in Thailand, Vietnam, and the Philippines.[213][217]
The Austronesian Painting Traditions are the most common types of rock art in Island Southeast Asia. They consist of scenes and pictograms typically found in rock shelters and caves near coastal areas. They are characteristically rendered in red ocher pigments for the earlier forms, later sometimes superseded by paintings done in black charcoal pigments. Their sites are mostly clustered in Eastern Indonesia and Island Melanesia, although a few examples can be found in the rest of Island Southeast Asia. Their occurrence has a high correlation to Austronesian-speaking areas, further evidenced by the appearance of bronze artifacts in the paintings. They are mostly found near the coastlines. Their common motifs include hand stencils, "sun-ray" designs, boats, and active human figures with headdresses or weapons and other paraphernalia. They also feature geometric motifs similar to those of the Austronesian Engraving Style.[213][218] Some paintings are also associated with traces of human burials and funerary rites, including ship burials. The representations of boats themselves are believed to be connected to the widespread "ship of the dead" Austronesian funerary practices.[218][219]
The earliest APT site dated is from Vanuatu and was found to be around 3,000 BP, corresponding to the initial migration wave of the Austronesians. These early sites are largely characterized by face motifs and hand stencils. Later sites, from 1,500 BP onwards, however, begin to show regional divergence in their art styles. APT can be readily distinguished from older Pleistocene-era Australo-Melanesian cave paintings by their motifs, color, and composition, though they can often be found in the same locality. The most recognizable motifs of APT (like boats) do not occur in cave paintings (or engravings) that definitely pre-date the Austronesian arrival—the sole exception being the stenciled hand motif. Some APT examples are also characteristically found in relatively inaccessible locations, including very high up in cliffsides overlooking the sea. No traces of APT have been found in Taiwan or the Philippines, though there is continuity in the motifs of spirals and concentric circles found in ancestral petroglyphs.[213][218]
AES, which consists of petroglyphs carved into rock surfaces, is far less common than APT. The majority of these sites are in coastal New Guinea and Island Melanesia. AES sites, which can be tentatively traced back to the similar Wanshan petroglyphs of Taiwan, are believed to be largely correlated to the prehistoric extent of the Lapita culture. The common motif of this tradition is curvilinear geometric engravings like spirals, concentric circles, and face-like forms. These resemble the geometric motifs in APT, though they are considered to be two separate artistic traditions.[213][218] AES is particularly dominant in the Solomon Islands and New Caledonia, where engravings are far more abundant than painted sites.[197]
O'Connor et al. (2015) proposes that APT developed during the initial rapid southward Austronesian expansion, and not before, possibly as a response to the communication challenges brought about by the new maritime mode of living. Along with AES, these material symbols and associated rituals and technologies may have been manifestations of "powerful ideologies" spread by Austronesian settlers that were central to the "Neolithization" and rapid assimilation of the various non-Austronesian indigenous populations of ISEA and Melanesia.[218]
The easternmost islands of Island Melanesia (Vanuatu, Fiji, and New Caledonia) are considered part of Remote Oceania, as they are beyond the inter-island visibility threshold. These island groups begin to show divergence from the APT and AES traditions of Near Oceania. While their art traditions show a clear continuation of the APT and AES traditions, they also feature innovations unique to each island group, like the increasing use of black charcoal, rectilinear motifs, and being more commonly found inside sacred caves rather than on open cliffsides.[197]
In Micronesia, the rock art traditions can be divided into three general regions: western, central, and eastern. The divisions reflect the various major migration waves from the Philippines into the Mariana Islands and Palau in 3,500 BP; a Lapita culture back-migration from Island Melanesia into central and eastern Micronesia around 2,200 BP; and finally, a back-migration from western Polynesia into eastern Micronesia around 1,000 BP.[197]
In western Micronesia (Palau, Yap, Guam, and the Northern Mariana Islands), rock art primarily consists of paintings on high cave ceilings and sea-facing cliffs. It is very similar to APT in terms of its motifs as well as its placement in relatively inaccessible locations. Common motifs include hand stencils, faces, turtles and fish, concentric circles, and characteristic four-pointed stars. Petroglyphs are rare and mainly consist of human forms with triangular bodies without heads or arms. This is believed to be connected to the funerary rite of removing the heads from the bodies of deceased relatives.[197] A notable megalithic tradition in western Micronesia are the haligi stone pillars of the Chamorro people. These are capped stone pillars that are believed to have served as supports for raised buildings. They are associated with the Latte period (900 to 1700 CE), when a new wave of migrants from Southeast Asia reintroduced rice cultivation to the islands. Another megalithic tradition is that of the rai stones, massive doughnut-shaped discs of rock that were used as currency on Yap.[220][221][222]
Rock art in central Micronesia (Chuuk, Pohnpei, and Kosrae), in contrast, is dominated by rock engravings with motifs tying it to the rock art traditions of Island Melanesia. They include curvilinear shapes like spirals and concentric circles, tree-like shapes, and the distinctive "enveloped cross" motif. The Pohnpaid petroglyphs are the largest assemblage of rock engravings in the region, with motifs dominated by footprints, enveloped crosses, and outlined "sword-paddles".[197] Central Micronesia also hosts the ruins of the stone cities of Nan Madol (1,180–1,200 CE) and Leluh (1,200–1,800 CE), on the islands of Pohnpei and Kosrae, respectively.[197][223][224]
In the low-lying atolls of eastern Micronesia, rock art is rare-to-nonexistent, due to the absence of suitable rock surfaces for painting or engraving.[197]
In Polynesia, rock art is dominated by petroglyphs, rather than paintings, and they show less variation than the rock art of Near Oceania and ISEA. In the western Polynesian islands nearest to Island Melanesia, rock art is rare (like in Tonga and Samoa) or absent entirely (like in the Cook Islands). However, petroglyphs are abundant on the islands in the further reaches of the Polynesian triangle, particularly on Hawaii, the Marquesas, and Rapa Nui. Rapa Nui has the densest concentration of engravings in Polynesia as a whole, while the Puʻuloa petroglyphs site on Hawaii has the largest number of petroglyphs in a single site, at over 21,000 engravings.[197] Polynesia also features megalithic sacred ceremonial centers generally known as marae.
On Tonga and Samoa, the existing rock art sites consist mostly of engravings with motifs including curvilinear shapes, human figures, "jellyfish", turtles, birds, and footprints. These are typically carved in natural rock formations or marae sites.[197]
In the central-eastern Polynesian islands, which include the Marquesas and the Society Islands, petroglyphs are more numerous. They show the archetypal Polynesian motifs of turtles, faces, cup-like depressions (cupules), stick-like human figures, boats, fish, curvilinear shapes, and concentric circles. Like in western Polynesia, they are typically carved into marae sites or in rocks beside streams. The existing rock paintings also display the same motifs but are rendered in different styles.[197]
On the Hawaiian islands, the abundant petroglyphs are remarkably all similar in execution. Their common subjects include stick-like human figures, dogs, boats, sails, paddles, footprints, and ceremonial headdresses. Depictions of marine life, however, are rare, unlike in the rest of Polynesia. They are typically carved into boulders, lava rock formations, and cliffsides. Red paintings of dogs on cliffsides and caves can also be found on Kauʻai and Maui.[197] The megalithic traditions of Hawaii can be exemplified by the heiau sacred sites, which can range from simple earth terraces to standing stones.
On Rapa Nui, the engravings are distinctive but still show similarities to the techniques and motifs of the Marquesas. Their motifs commonly include disembodied parts of the human body (vulvae in particular), animals, plants, ceremonial objects, and boats. A prominent motif is also that of the "birdman" figure, which is associated with the tangata manu cult of Makemake. The best-known rock art assemblage of Rapa Nui, however, are the moai megaliths. A few paintings, mostly of birds and boats, have also been discovered, which are associated with the engravings, rather than being separate artforms.[197]
The rock art in New Zealand can be divided into two regions. North Island features more engravings than paintings, while South Island is unique in that it is the only Polynesian island where there are more paintings than engravings. New Zealand rock paintings are done in red and black pigments and can sometimes be found at inaccessible heights. They typically depict human figures (particularly a front-facing human figure with flexed arms), birds, lizards, dogs, fish, and what has been identified as "birdmen". Engravings in open spaces like cliffsides are generally of spirals and curvilinear shapes, while engravings in enclosed caves and shelters depict faces and boats. The same motifs can also be seen in dendroglyphs on living trees.[197]
Body art among Austronesian peoples is common, especially elaborate tattooing, which is one of the most well-known pan-Austronesian traditions.[226]
In modern times, tattoos are usually associated with Polynesian culture, due to the highly influential accounts of James Cook in his explorations of the Pacific in the 18th century. Cook introduced the word "tattoo" (archaic: "tattaow", "tattow") into the English vocabulary from Tahitian and Samoan tātau ("to tap"). However, tattoos existed prominently in various other Austronesian groups prior to contact with other cultures.[227][228][229]
Tattoos had various functions among Austronesian societies. Among men, they were strongly linked to the widespread practice of head-hunting raids. In head-hunting societies, tattoos were records of how many heads the warrior had taken in battle, and they were part of the initiation rites into adulthood. The number and location of tattoos, therefore, were indicative of a warrior's status and prowess.[230]
Among the Indigenous Taiwanese, tattoos were present for both men and women. Among the Tayal, facial tattoos were dominant. They indicated maturity and skill in weaving and farming for women and skill in hunting and battle for men. As in most of Austronesia, tattooing traditions in Taiwan have largely disappeared due to the Sinicization of native peoples after the Chinese colonization of the island in the 17th century, as well as conversion to Christianity. Most of the remaining tattoos are only found among elders.[citation needed]
One of the earliest descriptions of Austronesian tattoos by Europeans was during the 16th-century Spanish expeditions to the Philippines, beginning with the first voyage of circumnavigation by Ferdinand Magellan. The Spanish encountered the heavily tattooed Visayan people in the Visayas Islands, whom they named the Pintados (Spanish for "the painted ones").[231][232] However, Philippine tattooing traditions (batok) have mostly been lost as the natives of the islands converted to Christianity and Islam, though they are still practiced in isolated groups in the highlands of Luzon and Mindanao. Philippine tattoos were usually geometric patterns or stylized depictions of animals, plants, and human figures.[233][234][235] Some of the few remaining traditional tattoos in the Philippines are among elders of the Igorot peoples. Most of these were records of war exploits against the Japanese during World War II.[236]
Among the Māori of New Zealand, tattoos (moko) were originally carved into the skin using bone chisels (uhi) rather than through puncturing, as in usual practice.[237] In addition to being pigmented, the skin was also left raised into ridges of swirling patterns.[238][239]
Teeth blackening was the custom of dyeing one's teeth black with various tannin-rich plant dyes. It was practiced throughout almost the entire range of Austronesia, including Island Southeast Asia, Madagascar, Micronesia, and Island Melanesia, reaching as far east as Malaita. However, it was absent in Polynesia. It also existed in non-Austronesian populations in Mainland Southeast Asia and Japan. The practice was primarily preventative, as it reduced the chances of developing tooth decay, similar to modern dental sealants. It also had cultural significance and was seen as beautiful. A common sentiment was that blackened teeth separated humans from animals.[240][241][242][243]
Teeth blackening was often done in conjunction with other modifications to the teeth associated with beauty standards, including dental evulsion and filing.[244]
The religious traditions of the Austronesian people focus mostly on ancestral spirits, nature spirits, and gods, making it a complex animistic religion. Mythologies vary by culture and geographical location but share common basic aspects, such as ancestor worship, animism, shamanism, and the belief in a spirit world and powerful deities.[245] There is also a great amount of shared mythology and a common belief in Mana.[246]
Many of these beliefs have gradually been replaced. Examples of native religions include: Indigenous Philippine folk religions (including beliefs in Anito), Sunda Wiwitan, Kejawen, Kaharingan, and Māori religion. Many Austronesian religious beliefs have been incorporated into foreign religions, such as Hinduism, Buddhism, Christianity, and Islam, which Austronesian peoples were introduced to later.[247]
With the possible exception of rongorongo on Rapa Nui, Austronesians did not have an indigenous writing system but rather adopted or developed writing systems after contact with various non-Austronesian cultures.[248] There existed various forms of symbolic communication using pictograms and petroglyphs, but these did not encode language.[citation needed]
Rongorongo, said to have originally been called kohau motu mo rongorongo ("lines of inscriptions for chanting out"), is the only pre-contact indigenous Austronesian system of glyphs that appear to be true writing or at least proto-writing. They consist of around 120 glyphs, ranging from representations of plants to animals, celestial objects, and geometric shapes. They were inscribed into wooden tablets about 12 to 20 in (30 to 51 cm) long using shark teeth and obsidian flakes. The wood allegedly came from toromiro and makoʻi trees, which is notable given that Rapa Nui was completely deforested at the time of European contact. Of the surviving two dozen tablets, a few were made from trees introduced after European contact, as well as wood originating from European ships and driftwood.[249][248][250] Rapa Nui also has a rich assemblage of petroglyphs largely associated with the tangata manu ("birdman") cult of Makemake. Although some rongorongo glyphs may have been derived from these petroglyphs, rongorongo does not appear in any of the abundant rock carvings in Rapa Nui and seems to be restricted to the wooden tablets.[251]
The tablets were first described by an outsider in 1864 by the Catholic missionary Eugène Eyraud, who said they were found "in all the houses". However, he paid them little attention, and they remained unnoticed by the outside world. It wasn't until 1869 that one of the tablets came into the possession of Florentin-Étienne Jaussen, the Bishop of Tahiti. He brought the tablets to the world's attention and instructed the Rapa Nui mission to gather more information about them. But by then, most of the tablets were allegedly already destroyed, presumed to have been used as fuel by the natives on the deforested island.[249]
At the time of discovery of the tablets, Rapa Nui had undergone severe depopulation. This was largely due to the loss of the island's last trees and the Peruvian and Chilean slave raids in the early 1860s. The literate ruling classes of the Rapa Nui people (including the royal family and the religious caste) and the majority of the island's population were kidnapped or killed in the slave raids. Most of those taken died after only one or two years in captivity from harsh working conditions and European diseases. Succeeding epidemics of smallpox and tuberculosis further decimated the island's population to the point that there were not enough people to bury the dead. The last remnants of the Rapa Nui people were assimilated by the Tahitians who were later brought to the island in an effort to repopulate it, further resulting in the loss of most of the Old Rapa Nui language.[248]
Oral tradition holds that the ruling classes were the only ones who could read the tablets, and the ability to decipher the tablets was lost along with them. Numerous attempts have been made to read the tablets, starting from a few years after their discovery. But to this day, none have proven successful. Some authors have proposed that rongorongo may have been an attempt to imitate European script after the idea of writing was introduced during the "signing" of the 1770 Spanish Treaty of Annexation or through knowledge of European writing acquired elsewhere. They cite various reasons, including the lack of attestation of rongorongo prior to the 1860s, the clearly more recent provenance of some of the tablets, the lack of antecedents, and the lack of additional archaeological evidence since its discovery. Others argue that it was merely a mnemonic list of symbols meant to guide incantations. Whether rongorongo is merely an example of trans-cultural diffusion or a true indigenous Austronesian writing system (and one of the few independent inventions of writing in human history) remains unknown.[249][248][252]
In Southeast Asia, the first true writing systems of pre-modern Austronesian cultures were all derived from the Grantha and Pallava Brahmic scripts, all of which are abugidas from South India. Various forms of abugidas spread throughout Austronesian cultures in Southeast Asia as kingdoms became Indianized through early maritime trading. The oldest use of abugida scripts in Austronesian cultures are 4th-century stone inscriptions written in Cham, from Vietnam. There are numerous other Brahmic-derived writing systems among Southeast Asian Austronesians, usually specific to a certain ethnic group. Notable examples include Balinese, Batak, Baybayin, Buhid, Hanunó'o, Javanese, Kulitan, Lontara, Old Kawi, Rejang, Rencong, Sundanese, and Tagbanwa. They vary from having letters with rounded shapes to characters with sharp cuneiform-like angles, as a result of the difference in writing mediums, with the former being ideal for writing on soft leaves and the latter on bamboo panels. The use of the scripts ranged from mundane records to encoding esoteric knowledge on magico-religious rituals and folk medicine.[253]
In regions that converted to Islam, abjads derived from the Arabic script started replacing the earlier abugidas at around the 13th century in Southeast Asia. Madagascar adopted the Arabic script in the 14th century. Abjads, however, have an even greater inherent problem with encoding Austronesian languages than abugidas, because Austronesian languages have more varied and salient[clarification needed] vowels that the Arabic script usually cannot encode. As a result, the Austronesian adaptations such as the Jawi and the Pegon scripts have been modified with a system of diacritics that encode sounds, both vowels and consonants, native to Austronesian languages but absent in Semitic ones.[253] With the advent of the Colonial Era, almost all of these writing systems have been replaced with alphabets adapted from the Latin, as in the Hawaiian, Filipino, and Malay alphabet. However, several Formosan languages had been written in zhuyin, and Cia-Cia off Sulawesi has experimented with hangul.
On Woleai and surrounding islands, a script was developed for the Woleaian language in the early 20th century. Approximately 20% of the script's letterforms were borrowed from Latin letters; the remaining characters seem to have been derived from indigenous iconography. Despite this heavy Latin influence, the script was a syllabary.[citation needed]
Vanuatu has a unique tradition of sand drawing, in which images are created by a single continuous line drawn in the sand. It is believed to have functioned as a means of symbolic communication in pre-contact Island Melanesia, especially between travelers and ethnic groups that do not speak the same language. The sand drawings consist of around 300 different designs and seem to be shared across language groups.[254] In the 1990s, elements of the drawings were adapted into a modern constructed script called Avoiuli by the Turaga indigenous movement on Pentecost Island.[255]
Genetic studies have been conducted on Austronesian peoples.[256] Haplogroup O1a, marked by the M119 SNP, is frequently detected in Native Taiwanese and people of the northern Philippines as well as some people in Indonesia, Malaysia, and non-Austronesian populations in southern China.[257]
A 2007 analysis of the DNA recovered from human remains in archaeological sites of prehistoric peoples along the Yangtze River in China also shows high frequencies of Haplogroup O1 in the Neolithic Liangzhu culture, linking them to Austronesian and Tai-Kadai peoples. The Liangzhu culture existed in coastal areas around the mouth of the Yangtze. Haplogroup O1 was absent in other archaeological sites inland. The authors of the study suggest that this may be evidence of two different human migration routes during the peopling of Eastern Asia; one coastal and the other inland, with little gene flow between them.[129]
An important breakthrough in studies in Austronesian genetics was the identification of the "Polynesian motif" (haplogroup B4a1a1) in 1989, a specific nine-base-pair deletion mutation in mitochondrial DNA. Several studies have shown that it is shared by Polynesians and Island Southeast Asians,[258] with a sub-branch also identified in Madagascar, indicating shared maternal ancestry among Austronesians.[259] Austronesian-speaking regions also have high-to-moderate frequencies of Haplogroup O1 of the Y-DNA (including Madagascar), indicating shared paternal ancestry, with the exception of Polynesia where the Papuan-derived Haplogroup C2a1 predominates (although lower frequencies of Austronesian Haplogroup O-M122 also exist). This indicates that the Lapita people, the direct ancestors of Polynesians, were likely matrilocal, assimilating Papuan men from outside the community by marriage in Near Oceania, prior to the Polynesian expansion into Remote Oceania.[258][259][260][57]
Moodley et al. (2009) identified two distinct populations of the gut bacteria Helicobacter pylori that accompanied human migrations into Island Southeast Asia and Oceania, called hpSahul and hspMāori. The study sampled Native Australians, Native Taiwanese, highlanders in New Guinea, and Melanesians and Polynesians in New Caledonia, which were then compared with other H. pylori haplotypes from Europeans, Asians, Pacific Islanders, and others. They found that hpSahul diverged from mainland Asian H. pylori populations approximately 31,000 to 37,000 years ago and have remained isolated for 23,000 to 32,000 years, confirming the Australo-Melanesian substratum in Island Southeast Asia and New Guinea. hspMāori, on the other hand, is a subpopulation of hpEastAsia, previously isolated from Polynesians (Māori, Tongans, Samoans) in New Zealand, and three individuals from the Philippines and Japan. The study found hspMāori from Native Taiwanese, Melanesians, Polynesians, and two inhabitants from the Torres Strait Islands, all of which are Austronesian sources. As expected, hspMāori showed greatest genetic diversity in Taiwan, while all non-Taiwanese hspMāori populations belonged to a single lineage they called the "Pacific clade". They also calculated the isolation-with-migration model (IMa), which showed that the divergence of the Pacific clade of hspMāori was unidirectional from Taiwan to the Pacific. This is consistent with the Out-of-Taiwan model of Austronesian expansion.[261]
On 16 January 2020, the personal genomics company 23andMe added the category "Filipino & Austronesian" after customers with no known Filipino ancestors were getting false positives for 5% or more "Filipino" ancestry in their ancestry composition report (the proportion was as high as 75% in Samoa, 71% in Tonga, 68% in Guam, 18% in Hawaii, and 34% in Madagascar). The company's scientists surmised that this was due to the shared Austronesian genetic heritage being incorrectly identified as Filipino ancestry.[262]
A 2020 study showed that the core Austronesian population in southern China derived most of their ancestry from a Late Neolithic Fujian source (66.9%–74.3%). This ancestry is also significant in Kra-Dai groups and southeastern Han Chinese.[263]A 2021 study showed strong affinities between Qihe3, a ~12,000 year old individual from Fujian, and island Austronesian populations from Southeast Asia. There are also strong affinities with coastal southern East Asians and Oceanian Vanuatu. Qihe3 can be modeled as an admixture of East Asian-related ancestry and populations of deeper ancestry.[264]
According to a 2021 study, Cordillerans are the best modern surrogate for the least admixed genetic signal for the Austronesian expansion. Compared to Ami and Atayal, they do not exhibit admixture with Austroasiatic-related and Northeast Asian-related groups although Northeast Asian ancestry was later introduced to the Batanes Islands and coastal regions of Luzon. Central Cordillerans also show no admixture with indigenous Negritos despite extensive interaction with their neighbors. However, Ami, Atayal and Cordillerans all share strong affinities with Malaysians, Indonesians, Oceanians and even ancient individuals from peninsular Malaysia and Oceanian Lapita. They are also closely related to the ~7,000- to 8,000-y-old Liangdao-2 individual.[265]
The Austronesian migrations were accompanied by a set of domesticated, semi-domesticated, and commensal plants and animals transported via outrigger ships and catamarans that enabled early Austronesians to thrive in their mostly island environments.[266][267] These include crops and animals believed to have originated from the Hemudu and Majiabang cultures in the hypothetical pre-Austronesian homelands in mainland China,[122] as well as other plants and animals believed to have been first domesticated from within Taiwan, Maritime Southeast Asia, and New Guinea.[268][269] Some of these plants are sometimes also known as "canoe plants", especially in the context of Polynesian migrations.[270][271][272] They provide another source of evidence for Austronesian population movements.[266]
Notable examples of these crops include coconuts,[273][86][85] bananas,[269][274] rice,[111] sugarcane,[275][276] paper mulberry (tapa tree),[277][278] breadfruit,[279] taro,[269] ube,[280] areca nut (including the practice of betel chewing),[139] ginger,[281] turmeric,[282] candlenut,[283] pandan,[175][176] and citruses.[283] The cultivation of sweet potatoes in Polynesia may also be evidence of prehistoric Austronesian contact with the Americas, though this remains disputed.[284] The domesticated animals carried in Austronesian voyages include dogs,[285][286] pigs,[287] and chickens.[287][288]
Austronesians also introduced these crops and domesticated animals westward via trade links. Island Southeast Asians established spice trade links with the Dravidian-speaking regions in Sri Lanka and Southern India by around 1500 to 600 BCE.[139][289][290][167] These early contacts resulted in the introduction of Austronesian crops and material culture to South Asia,[289] including betel nut chewing, coconuts, sandalwood, domesticated bananas,[289][139] sugarcane,[291] cloves, and nutmeg.[292] South Asian crops like the mung bean and horsegram were also present in Southeast Asia by 400–100 BCE, indicating the exchange was reciprocal.[289][293]
There is also indirect evidence of very early Austronesian contacts with Africa, based on the presence and spread of Austronesian domesticates like bananas, taro, chickens, and purple yam in Africa in the first millennium BCE.[289][294]
A genomic analysis in 2020 showed Austronesian contact with South America around 1150–1200 CE, the earliest one being between Fatu Hiva and Colombia.[295]
Africa
Eurasia
North America
Oceania
South America

For other species or subspecies suggested, see below.
Homo (from Latin  homō 'human') is a genus of great ape (family Hominidae) that emerged from the genus Australopithecus and encompasses only a single extant species, Homo sapiens (modern humans), along with a number of extinct species (collectively called archaic humans) classified as either ancestral or closely related to modern humans; these include Homo erectus and Homo neanderthalensis. The oldest member of the genus is Homo habilis, with records of just over 2 million years ago.[a] Homo, together with the genus Paranthropus, is probably most closely related to the species Australopithecus africanus within Australopithecus.[4] The closest living relatives of Homo are of the genus Pan (chimpanzees and bonobos), with the ancestors of Pan and Homo estimated to have diverged around 5.7–11 million years ago during the Late Miocene.[5]
H. erectus appeared about 2 million years ago and spread throughout Africa (debatably as another species called Homo ergaster) and Eurasia in several migrations. The species was adaptive and successful, and persisted for more than a million years before gradually diverging into new species around 500,000 years ago.[b][6]
Anatomically modern humans (H. sapiens) emerged close to 300,000 to 200,000 years ago[7] in Africa, and H. neanderthalensis emerged around the same time in Europe and Western Asia. H. sapiens dispersed from Africa in several waves, from possibly as early as 250,000 years ago, and certainly by 130,000 years ago, with the so-called Southern Dispersal, beginning about 70–50,000 years ago,[8][9][10] leading to the lasting colonisation of Eurasia and Oceania by 50,000 years ago. H. sapiens met and interbred with archaic humans in Africa and in Eurasia.[11][12] Separate archaic (non-sapiens) human species including Neanderthals are thought to have survived until around 40,000 years ago.
The Latin noun homō (genitive hominis) means "human being" or "man" in the generic sense of "human being, mankind".[c] The binomial name Homo sapiens was coined by Carl Linnaeus (1758).[d][15] Names for other species of the genus were introduced from the second half of the 19th century (H. neanderthalensis 1864, H. erectus 1892).
The genus Homo has not been strictly defined, even today.[16][17][18] Since the early human fossil record began to slowly emerge from the earth, the boundaries and definitions of the genus have been poorly defined and constantly in flux. Because there was no reason to think it would ever have any additional members, Carl Linnaeus did not even bother to define Homo when he first created it for humans in the 18th century. The discovery of Neanderthal brought the first addition.
The genus Homo was given its taxonomic name to suggest that its member species can be classified as human. And, over the decades of the 20th century, fossil finds of pre-human and early human species from late Miocene and early Pliocene times produced a rich mix for debating classifications. There is continuing debate on delineating Homo from Australopithecus—or, indeed, delineating Homo from Pan. Even so, classifying the fossils of Homo coincides with evidence of: (1) competent human bipedalism in Homo habilis inherited from the earlier Australopithecus of more than four million years ago, as demonstrated by the Laetoli footprints; and (2) human tool culture having begun by 2.5 million years ago to 3 million years ago.[19]
From the late-19th to mid-20th centuries, a number of new taxonomic names, including new generic names, were proposed for early human fossils; most have since been merged with Homo in recognition that Homo erectus was a single species with a large geographic spread of early migrations. Many such names are now regarded as "synonyms" with Homo, including Pithecanthropus,[20] Protanthropus,[21] Sinanthropus,[22] Cyphanthropus,[23] Africanthropus,[24] Telanthropus,[25] Atlanthropus,[26] and Tchadanthropus.[27][28]
Classifying the genus Homo into species and subspecies is subject to incomplete information and remains poorly done. This has led to using common names ("Neanderthal" and "Denisovan"), even in scientific papers, to avoid trinomial names or the ambiguity of classifying groups as incertae sedis (uncertain placement)—for example, H. neanderthalensis vs. H. sapiens neanderthalensis, or H. georgicus vs. H. erectus georgicus.[29] Some recently extinct species in the genus have been discovered only lately and do not as yet have consensus binomial names (see Denisova hominin).[30] Since the beginning of the Holocene, it is likely that Homo sapiens (anatomically modern humans) has been the only extant species of Homo.
John Edward Gray (1825) was an early advocate of classifying taxa by designating tribes and families.[31] Wood and Richmond (2000) proposed that Hominini ("hominins") be designated as a tribe that comprised all species of early humans and pre-humans ancestral to humans back to after the chimpanzee–human last common ancestor, and that Hominina be designated a subtribe of Hominini to include only the genus Homo — that is, not including the earlier upright walking hominins of the Pliocene such as Australopithecus, Orrorin tugenensis, Ardipithecus, or Sahelanthropus.[32] Designations alternative to Hominina existed, or were offered: Australopithecinae (Gregory & Hellman 1939) and Preanthropinae (Cela-Conde & Altaba 2002);[33][34][35] and later, Cela-Conde and Ayala (2003) proposed that the four genera Australopithecus, Ardipithecus, Praeanthropus, and Sahelanthropus be grouped with Homo within Hominini (sans Pan).[34]
Several species, including Australopithecus garhi, Australopithecus sediba, Australopithecus africanus, and Australopithecus afarensis, have been proposed as the ancestor or sister of the Homo lineage.[36][37] These species have morphological features that align them with Homo, but there is no consensus as to which gave rise to Homo.
Especially since the 2010s, the delineation of Homo in Australopithecus has become more contentious. Traditionally, the advent of Homo has been taken to coincide with the first use of stone tools (the Oldowan industry), and thus by definition with the beginning of the Lower Palaeolithic. But in 2010, evidence was presented that seems to attribute the use of stone tools to Australopithecus afarensis around 3.3 million years ago, close to a million years before the first appearance of Homo.[38] LD 350-1, a fossil mandible fragment dated to 2.8 Mya, discovered in 2013 in Afar, Ethiopia, was described as combining "primitive traits seen in early Australopithecus with derived morphology observed in later Homo.[39] Some authors would push the development of Homo close to or even past 3 Mya.[e] This finds support in a recent phylogenetic study in hominins that by using morphological, molecular and radiometric information, dates the emergence of Homo at 3.3 Ma (4.30 – 2.56 Ma).[40] Others have voiced doubt as to whether Homo habilis should be included in Homo, proposing an origin of Homo with Homo erectus at roughly 1.9 Mya instead.[41]
The most salient physiological development between the earlier australopithecine species and Homo is the increase in endocranial volume (ECV), from about 460 cm3 (28 cu in) in A. garhi to 660 cm3 (40 cu in) in H. habilis and further to 760 cm3 (46 cu in) in H. erectus, 1,250 cm3 (76 cu in) in H. heidelbergensis and up to 1,760 cm3 (107 cu in) in H. neanderthalensis. However, a steady rise in cranial capacity is observed already in Autralopithecina and does not terminate after the emergence of Homo, so that it does not serve as an objective criterion to define the emergence of the genus.[42]
Homo habilis emerged about 2.1 Mya. Already before 2010, there were suggestions that H. habilis should not be placed in the genus Homo but rather in Australopithecus.[43][44] The main reason to include H. habilis in Homo, its undisputed tool use, has become obsolete with the discovery of Australopithecus tool use at least a million years before H. habilis.[38] Furthermore, H. habilis was long thought to be the ancestor of the more gracile Homo ergaster (Homo erectus). In 2007, it was discovered that H. habilis and H. erectus coexisted for a considerable time, suggesting that H. erectus is not immediately derived from H. habilis but instead from a common ancestor.[45] With the publication of Dmanisi skull 5 in 2013, it has become less certain that Asian H. erectus is a descendant of African H. ergaster which was in turn derived from H. habilis. Instead, H. ergaster and H. erectus appear to be variants of the same species, which may have originated in either Africa or Asia[46] and widely dispersed throughout Eurasia (including Europe, Indonesia, China) by 0.5 Mya.[47]
Homo erectus has often been assumed to have developed anagenetically from H. habilis from about 2 million years ago. This scenario was strengthened with the discovery of Homo erectus georgicus, early specimens of H. erectus found in the Caucasus, which seemed to exhibit transitional traits[example needed] with H. habilis. As the earliest evidence for H. erectus was found outside of Africa, it was considered plausible that H. erectus developed in Eurasia and then migrated back to Africa. Based on fossils from the Koobi Fora Formation, east of Lake Turkana in Kenya, Spoor et al. (2007) argued that H. habilis may have survived beyond the emergence of H. erectus, so that the evolution of H. erectus would not have been anagenetically, and H. erectus would have existed alongside H. habilis for about half a million years (1.9 to 1.4 million years ago), during the early Calabrian.[45] On 31 August 2023, researchers reported, based on genetic studies, that a human ancestor population bottleneck (from a possible 100,000 to 1000 individuals) occurred "around 930,000 and 813,000 years ago ... lasted for about 117,000 years and brought human ancestors close to extinction."[48][49]
Weiss (1984) estimated that there have been about 44 billion (short scale) members of the genus Homo from its origins to the evolution of H. erectus, about 56 billion individuals from H. erectus to the Neolithic, and another 51 billion individuals since the Neolithic. This provides the opportunity for an immense amount of new mutational variation to have arisen during human evolution.[50]
A separate South African species Homo gautengensis has been postulated as contemporary with H. erectus in 2010.[51]
A taxonomy of Homo within the great apes is assessed as follows, with Paranthropus and Homo emerging within Australopithecus (shown here cladistically granting Paranthropus, Kenyanthropus, and Homo).[a][f][6][53][52][4][54][55][56][57][58][59][60][excessive citations] The exact phylogeny within Australopithecus is still highly controversial. Approximate radiation dates of daughter clades are shown in millions of years ago (Mya).[40][57] Sahelanthropus and Orrorin, possibly sisters to Australopithecus, are not shown here. The naming of groupings is sometimes muddled as often certain groupings are presumed before any cladistic analysis is performed.[55]
Hylobatidae (gibbons)
Ponginae (orangutans)
Gorillini (gorillas)
Panina (chimpanzees)
Australopithecines (incl. Australopithecus, Kenyanthropus, Paranthropus, Homo)
Cladogram based on Dembo et al. (2016):[57]
Ardipithecus ramidus (†)
Australopithecus anamensis s.s. (†3.8)
Australopithecus afarensis (†)
Australopithecus garhi (†)
Australopithecus deyiremeda (†3.4)
Kenyanthropus platyops (†3.3)
Australopithecus africanus (†2.1)
Paranthropus (†1.2)
Homo habilis (†1.5)
Homo rudolfensis (†1.9)
Homo ergaster (†1.4)
African Homo erectus s.s. (†)
Asian Homo erectus s.s. (†0.1)
Homo naledi (†0.2)
Homo antecessor  (†0.8)
H. neanderthalensis (†0.05)
Denisova people (†0.05)
Homo sapiens
Australopithecus sediba (†2.0)
Homo floresiensis (†0.05)
Cladogram based on Feng et al. (2024):[61]
Homo habilis (†1.7 Mya)
Stw53 (†1.9)
Dmanisi (†1.8)
Turkana (†1.7)
Olduvai Hominids (†1.5)
Homo naledi (†0.2)
Homo floresiensis (†0.05)
Sangiran (†1.4)
Hexian (†0.5)
Nanjing Man (†0.6)
Peking Man (†0.5)
Sambungmacan (†0.2)
Ngandong (†0.1)
H. neanderthalensis (†0.05)
Denisova people (†0.05)
Homo sapiens
Several of the Homo lineages appear to have surviving progeny through introgression into other lines. Genetic evidence indicates an archaic lineage separating from the other human lineages 1.5 million years ago, perhaps H. erectus, may have interbred into the Denisovans about 55,000 years ago.[62][54][63] Fossil evidence shows H. erectus s.s. survived at least until 117,000 yrs ago, and the even more basal H. floresiensis survived until 50,000 years ago. A 1.5-million-year H. erectus-like lineage appears to have made its way into modern humans through the Denisovans and specifically into the Papuans and aboriginal Australians.[54] The genomes of non-sub-Saharan African humans show what appear to be numerous independent introgression events involving Neanderthal and in some cases also Denisovans around 45,000 years ago.[64][63] The genetic structure of some sub-Saharan African groups seems to be indicative of introgression from a west Eurasian population some 3,000 years ago.[58][65]
Some evidence suggests that Australopithecus sediba could be moved to the genus Homo, or placed in its own genus, due to its position with respect to e.g. H. habilis and H. floresiensis.[56][66]
By about 1.8 million years ago, H. erectus is present in both East Africa (H. ergaster) and in Western Asia (H. georgicus). The ancestors of Indonesian H. floresiensis may have left Africa even earlier.[g][56]
Homo erectus and related or derived archaic human species over the next 1.5 million years spread throughout Africa and Eurasia[67][68] (see: Recent African origin of modern humans). Europe is reached by about 0.5 Mya by Homo heidelbergensis.
Homo neanderthalensis and H. sapiens develop after about 300 kya. Homo naledi is present in Southern Africa by 300 kya.
H. sapiens soon after its first emergence spread throughout Africa, and to Western Asia in several waves, possibly as early as 250 kya, and certainly by 130 kya. In July 2019, anthropologists reported the discovery of 210,000 year old remains of a H. sapiens and 170,000 year old remains of a H. neanderthalensis in Apidima Cave, Peloponnese, Greece, more than 150,000 years older than previous H. sapiens finds in Europe.[69][70][71]
Most notable is the Southern Dispersal of H. sapiens around 60 kya, which led to the lasting peopling of Oceania and Eurasia by anatomically modern humans.[11] H. sapiens interbred with archaic humans both in Africa and in Eurasia, in Eurasia notably with Neanderthals and Denisovans.[72][73]
Among extant populations of H. sapiens, the deepest temporal division is found in the San people of Southern Africa, estimated at close to 130,000 years,[74] or possibly more than 300,000 years ago.[75] Temporal division among non-Africans is of the order of 60,000 years in the case of Australo-Melanesians. Division of Europeans and East Asians is of the order of 50,000 years, with repeated and significant admixture events throughout Eurasia during the Holocene.
Archaic human species may have survived until the beginning of the Holocene, although they were mostly extinct or absorbed by the expanding H. sapiens populations by 40 kya (Neanderthal extinction).
The species status of H. rudolfensis, H. ergaster, H. georgicus, H. antecessor, H. cepranensis, H. rhodesiensis, H. neanderthalensis, Denisova hominin, and H. floresiensis remain under debate. H. heidelbergensis and H. neanderthalensis are closely related to each other and have been considered to be subspecies of H. sapiens.
There has historically been a trend to postulate new human species based on as little as an individual fossil. A "minimalist" approach to human taxonomy recognizes at most three species, H. habilis (2.1–1.5 Mya, membership in Homo questionable), H. erectus (1.8–0.1 Mya, including the majority of the age of the genus, and the majority of archaic varieties as subspecies,[76][77][78] including H. heidelbergensis as a late or transitional variety[79][80][81]) and Homo sapiens (300 kya to present, including H. neanderthalensis and other varieties as subspecies). Consistent definitions and methodology of species delineation are not generally agreed upon in anthropology or paleontology. Indeed, speciating populations of mammals can typically interbreed for several million years after they begin to genetically diverge,[82][83] so all contemporary "species" in the genus Homo would potentially have been able to interbreed at the time, and introgression from beyond the genus Homo can not a priori be ruled out.[84] It has been suggested that H. naledi may have been a hybrid with a late surviving Australipith (taken to mean beyond Homo, ed.),[53] despite the fact that these lineages generally are regarded as long extinct. As discussed above, many introgressions have occurred between lineages, with evidence of introgression after separation of 1.5 million years.

Prehistoric Asia refers to events in Asia during the period of human existence prior to the invention of writing systems or the documentation of recorded history. This includes portions of the Eurasian land mass currently or traditionally considered as the continent of Asia. The continent is commonly described as the region east of the Ural Mountains, the Caucasus Mountains, the Caspian Sea, Black Sea and Red Sea, bounded by the Pacific, Indian, and Arctic Oceans.[1] This article gives an overview of the many regions of Asia during prehistoric times.
About 1.8 million years ago, Homo erectus left the African continent.[2] This species, whose name means "upright man", is believed to have lived in East and Southeast Asia from 1.8 million to 40,000 years ago.[3] Their regional distinction is classified as Homo erectus stricto.[4] The females weighed an average of 52 kilograms (115 lb) and were on average 1.5 metres (4.9 ft) tall. The males weighed an average of 58 kilograms (128 lb) and were on average 1.7 metres (5.6 ft) tall. They are believed to have had a vegetarian diet with some meat.[3] They had small brains, when compared to the later Homo sapiens and used simple tools.[2]
The earliest human fossils found outside of Africa are skulls and mandibles of the Asian Homo erectus  from Dmanisi (modern Republic of Georgia) in Caucasus, which is a land corridor that led to North Asia from Africa and Near East or Middle East. They are approximately 1.8 Ma (Megaannum, or million years) old.  Archaeologists have named these fossils Homo erectus georgicus.[2][5][6] There were also some remains that looked similar to the Homo ergaster, which may mean that there were several species living about that time in Caucasus. Bones of animals found near the human remains included short-necked giraffes, ostriches, ancient rhinoceroses from Africa and saber-toothed cats and wolves from Eurasia.[2] Tools found with the human fossils include simple stone tools like those used in Africa: a cutting flake, core and a chopper.[2]
The oldest Southeast Asian Homo fossils, known as the Homo erectus Java Man, were found between layers of volcanic debris in Java, Indonesia.[7] Fossils representing 40 Homo erectus individuals, known as Peking Man, were found near Beijing at Zhoukoudian that date to about 400,000 years ago. The species was believed to have lived for at least several hundred thousand years in China,[3] and possibly until 200,000 years ago in Indonesia. They may have been the first to use fire and cook food.[8]
Skulls were found in Java of Homo erectus that dated to about 300,000 years ago.[7] A skull was found in Central China that was similar to the Homo heidelbergensis remains that were found in Europe and Africa and are dated  between 200,000 and 50,000 years ago.[9]
Between 60,000 and 100,000 years ago, Homo sapiens came to Southeast Asia and Australia by migrating from Africa, known as the "Out of Africa" model.[3][7][nb 1] Homo sapiens are believed to have migrated through the Middle East on their way out of Africa about 100,000 years ago.[10][11] Near Nazareth, remains of skeletons, including a double grave of a mother and child, dating to about 93,000 years ago were found in a Jebel Qafzeh cave. Included among the remains was a skeleton of another species which was not Homo sapiens; it had a "distinct and undivided browridge that is continuous across the eye sockets" and other discrepancies.[10]
Researchers believe that the modern human, or Homo sapiens, migrated about 60,000 years ago to South Asia along the Indian Ocean, because people living in the most isolated areas of the Indian Ocean have the oldest non-African DNA markers. Humans migrated into inland Asia, likely by following herds of bison and mammoth and arrived in southern Siberia by about 43,000 years ago and some people moved south or east from there.[12][13] By about 40,000 years ago Homo sapiens made it to Malaysia, where a skull was found on Borneo in Niah Cave.[11] Modern humans interbred with an archaic human species called Denisovans on the islands of Southeast Asia.[14]
Homo sapiens females weighed an average of 54 kilograms (119 lb) and were on average 1.6 metres (5.2 ft) tall. The males weighed an average of 65 kilograms (143 lb) and were on average 1.7 metres (5.6 ft) tall. They were omnivorous. As compared to earlier hominids, Homo sapiens had larger brains and used more complex tools, including, blades, awls, and microliths out of antlers, bones and ivory. They were the only hominids to develop language, make clothes, create shelters, and store food underground for preservation. In addition, language was formed, rituals were created, and art was made.[15]
Above China is North Asia, in which Siberia,[16] and Russian Far East are extensive geographical regions which has been part of Russia since the seventeenth century.
At the southwestern edge of North Asia is Caucasus. It is a region at the border of Europe and Asia, situated between the Black and the Caspian seas. Caucasus is home to the Caucasus Mountains, which contain Europe's highest mountain, Mount Elbrus. The southern part of the Caucasus consists of independent sovereign states, whereas the northern parts are under the jurisdiction of the Russian Federation.
Evidence from full genomic studies suggests that the first people in the Americas diverged from Ancient East Asians about 36,000 years ago and expanded northwards into Siberia, where they encountered and interacted with a different Paleolithic Siberian population (known as Ancient North Eurasians), giving rise to both Paleosiberian peoples and Ancient Native Americans, which later migrated towards the Beringian region, became isolated from other populations, and subsequently populated the Americas.[17][18]
The Armenian Highland, in Prehistoric Armenia, shows traces of settlement from the Neolithic era. The Shulaveri-Shomu culture of the central Transcaucasus region is one of the earliest known prehistoric culture in the area, carbon-dated to roughly 6000–4000 BC.  Another early culture in the area is the Kura-Araxes culture, assigned to the period of ca. 3300–2000 BC, succeeded by the Georgian Trialeti culture (ca. 3000–1500 BC).
The prehistory of Georgia is the period between the first human habitation of the territory of modern-day nation of Georgia and the time when Assyrian and Urartian, and more firmly, the Classical accounts, brought the proto-Georgian tribes into the scope of recorded history.
Central Asia is the core region of the Asian continent and stretches from the Caspian Sea in the west to China in the east and from Afghanistan in the south to Russia in the north. It is also sometimes referred to as Middle Asia, and, colloquially, "the 'stans" (as the six countries generally considered to be within the region all have names ending with the Persian suffix "-stan", meaning "land of")[20] and is within the scope of the wider Eurasian continent. The countries are Kazakhstan, Kyrgyzstan, Tajikistan, Turkmenistan, Uzbekistan, and Afghanistan.
East Asia, for the purpose of this discussion, includes the prehistoric regions of China, Taiwan, Tibet, Xinjiang and Korea. Study of Prehistoric China includes its paleolithic sites,  neolithic cultures, Chalcolithic cultures, the Chinese Bronze Age, and the Bronze Age sites.
Ancestors of East Asians split from other human populations possibly as early as 70,000 to 50,000 years ago.[21][22] Ancestral East Asians, which gave rise to modern East/Southeast Asians, Polynesians, Siberians and Native Americans, expanded in multiple waves outgoing from Southern China northwards and southwards respectively. Population genomic data suggest that Paleolithic East Asian show continuity to modern East Asians and related groups.[23]
The earliest traces of early humans, Homo erectus, in East Asia have been found in China. Fossilized remains of Yuanmou Man were found in Yunnan province in southwest China and have been dated to 1.7 Ma. Stone tools from the Nihewan Basin of the Hebei province in northern China are 1.66 million years old.[24]
Early humans were attracted to what was the warm, fertile climate of Central China more than 500,000 years ago.[25] Skeletal remains of about 45 individuals, known collectively as Peking Man were found in a limestone cave in Yunnan province at Zhoukoudian. They date from 400,000 to 600,000 years ago and some researchers believe that evidence of hearths and artifacts means that they controlled fire, although this is challenged by other archaeologists. About 800 miles west of this site, near Xi'an in the Shaanxi province are remains of a hominid who lived earlier than Peking Man.[25]
Between 100,000 and 200,000 years ago, humans lived in various places in China, such as Guanyindong[26] in Guizhou, where they made Levallois stone artefacts. After 100,000 BCE, Homo sapiens lived in China and by 25,000 BCE the modern humans lived in isolated locations on the North China Plain, where they fished and hunted for food. They made artifacts of bone and shell.[25]
Starting about 5000 BCE humans lived in Yellow River valley settlements where they farmed, fished, raised pigs and dogs for food, and grew millet and rice. Begun during the late Neolithic period, they were the earliest communities in China. Its artifacts include ceramic pots, fishhooks, knives, arrows and needles. In the northwest Shaanxi, Gansu and Henan provinces two cultures were established by about the sixth millennium BCE. They produced red pottery. Other cultures that emerged, that also made pottery, include the Bao-chi and Banpo people of Shaanxi and the Chishan people of Hebei.[25]
The Yangshao people, who existed between 5000 and 2500 BCE, were farmers who lived in distinctive dwelling which were partly below the surface. Their pottery included designs which may have been symbols that later evolved into written language. Their villages were in western Henan, southwestern Shanxi and central Shaanxi. Between 2500 and 1000 BCE the Longshan culture existed in southern, eastern and northeastern China and into Manchuria. They had superior farming and ceramic making techniques to that of the Yangshao people and had ritualistic burial practices and worshiped their ancestors.[27] Subsequent dynasties include the Xia, Shang, and Zhou dynasties, when the Old Chinese language developed.[28]
The Prehistory of Taiwan ended with the arrival of the Dutch East India Company in 1624, and is known from archaeological finds throughout the island.  The earliest evidence of human habitation dates back 50,000 years or more,[29] when the Taiwan Strait was exposed by lower sea levels as a land bridge. Around 5000 years ago farmers from mainland China settled on the island.  These people are believed to have been speakers of Austronesian languages, which dispersed from Taiwan across the islands of the Pacific and Indian Oceans.  The current Taiwanese aborigines are believed to be their descendants.
Prehistoric Korea is the era of human existence in the Korean Peninsula for which written records did not exist. It, however, constitutes the greatest segment of the Korean past and is the major object of study in the disciplines of archaeology, geology, and palaeontology.
The study of Prehistoric Japan includes Japanese Paleolithic and Jōmon.
The Near East is a geographical term that roughly encompasses Western Asia. Despite having varying definitions within different academic circles, the term was originally applied to the maximum extent of the Ottoman Empire, but has since been gradually replaced by the term Middle East. The region is sometimes called the Levant.
At 1.4 million years, Ubeidiya in the northern Jordan River Valley is the earliest Homo erectus site in the Levant.[30]
South Asia is the southern region of the Asian continent, which comprises the sub-Himalayan countries and, for some authorities, also includes the adjoining countries to the west and the east. Topographically, it is dominated by the Indian Plate, which rises above sea level as the India south of the Himalayas and the Paropamisadae. South Asia is bounded on the south by the Indian Ocean and on land (clockwise, from west) by West Asia, Central Asia, East Asia, and Southeast Asia.
The Riwat site in Pakistan contains a few artifacts – a core and two flakes – that might date human activity there to 1.9 million years ago, but these dates are still controversial.[31]
The South Asian prehistory is explored in the articles about Prehistoric Sri Lanka, India and Tamil Nadu
Southeast Asia is a subregion of Asia, consisting of the countries that are geographically south of China, east of India, west of New Guinea and north of Australia.[32] The region lies on the intersection of geological plates, with heavy seismic and volcanic activity. Southeast Asia consists of two geographic regions: (1) Mainland Southeast Asia, also known as Indochina, comprising Cambodia, Laos, Myanmar (Burma), Thailand, and Vietnam; and (2) Maritime Southeast Asia, comprising Brunei, Malaysia, East Timor, Indonesia, Philippines, and Singapore.[33]
The rich Sangiran Formation in Central Java (Indonesia) has yielded the earliest evidence of hominin presence in Southeast Asia. These Homo erectus fossils date to more than 1.6 Ma.[34] Remains found in Mojokerto have been dated to 1.49 Ma.[35]
Its history is told by region, including the Early history of Burma and Cambodia, as well as the articles about Prehistoric Philippines, Thailand, Malaysia and Indonesia.
Skeleton remains were found of a hominid that was only 3 feet (0.91 m) tall as an adult in Indonesia on the island of Flores. It had a small brain and, nicknamed "the Hobbit" for its diminutive structure, was classified distinctly as Homo floresiensis. Evidence of H. floresiensis has been dated to be from 50,000 to 190,000 years ago,[36] after early publications suggested the small hominid persisted until as recently as 12,000 years ago.[37] Ancestral East Asians are suggested to have originated in Mainland Southeast Asia, before expanding northwards.[38]
The Negritos form the indigenous population of Southeast Asia, but were largely absorbed by Austroasiatic- and Austronesian-speaking groups that migrated from southern East Asia into Mainland and Insular Southeast Asia with the Neolithic expansion. The remainders form minority groups in geographically isolated regions.[39]

This is a timeline of German history, comprising important legal and territorial changes and political events in Germany and its predecessor states. To read about the background to these events, see History of Germany.  See also the list of German monarchs and list of chancellors of Germany and the list of years in Germany.

Archaeology and geology continue to reveal the secrets of prehistoric Scotland, uncovering a complex past before the Romans brought Scotland into the scope of recorded history. Successive human cultures tended to be spread across Europe or further afield, but focusing on this particular geographical area sheds light on the origin of the widespread remains and monuments in Scotland, and on the background to the history of Scotland.
The extent of open countryside untouched by intensive farming, together with past availability of stone rather than timber, has given Scotland a wealth of accessible sites where the ancient past can be seen.
Scotland is geologically alien to Europe, comprising a sliver of the ancient continent of Laurentia (which later formed the bulk of North America). During the Cambrian period the crustal region which became Scotland formed part of the continental shelf of Laurentia, then still south of the equator. Laurentia was separated from the continent of Baltica (which later became Scandinavia and the Baltic region) by the diminishing Iapetus Ocean. The two ancient continents moved toward one another through the Cambrian and Ordovician periods, with tectonic folding during the Silurian pushing the first Scottish land above water. The final collision occurred during the Devonian period, with the Scottish segment of the Laurentian plate smashing into Avalonia (which contained what is now most of England and Wales), a motile subcontinent which had previously joined with Baltica. This impact threw up a massive chain of mountains (at least as tall as the present-day Alps) and saw the formation of the granitic West Highland and Grampian mountain chains and (through the Carboniferous) a period of volcanic activity in central and eastern Scotland. During the Permian and Triassic periods, with the Iapetus Ocean entirely closed, Scotland lay near the centre of the Pangaean supercontinent. At the start of the Tertiary, a constructive plate boundary (at which tectonic plates move apart) became active between Laurentia and Eurasia, pushing the two apart (and parting Scotland from Laurentia). This recession opened the Atlantic Ocean for the first time, and the consequent subduction zone at the western plate margin led to a renewed period of volcanism, this time on Scotland's west coast, producing fresh mountains on Skye, Jura, Mull, Rùm, and Arran.
This tectonic activity produced the basis of Scotland's topography: ancient mountains in the North and South of the country, partially eroded by 400 million years of water and ice with a wide fertile valley between them, and a newer, wilder western terrain. With Scotland now in the northern temperate zone, it was subjected to numerous glaciations in the Neogene and Quaternary periods, the ice sheets and their attendant glaciers carving the landscape into a typical postglacial one, overdeepening river valleys into the characteristic U-shape and leaving the upland areas covered with glacial corries and dramatic pyramidal peaks. In lowland areas the ice deposited rich fields of fertile glacial till and eroded the softer material surrounding the extinct volcanoes (particularly the older Carboniferous ones), leaving many crags.
During the last interglacial, around 130,000–70,000 BC, there were times when the climate in Europe was warmer than it is today, and after the Neanderthals came to prominence[clarification needed] there was another mild spell around 40,000 BC. Neanderthal sites have been found in the south of England from this era, though no traces of early modern humans have been found. Repeated glaciations, which covered the entire land mass of modern Scotland, may have destroyed traces of human habitation that existed before the Mesolithic period.
Glaciers then scoured their way across most of Britain, and it was only after the ice retreated about 15,000 years ago that Scotland again became habitable.
As the climate improved, mesolithic hunter-gatherers extended their range into Scotland. The earliest evidence to date is the flint artefacts found at Howburn Farm, near Elsrickle in 2005. This is the first and so far the only evidence of Upper Paleolithic human habitation in Scotland, around 12,000 BC, which appears to fall between the Younger Dryas and Lomond Stadial periods when cold conditions returned relatively briefly.[1][2][3]
An early settlement at Cramond, near what is today Edinburgh, has been dated to around 8500 BC. Pits and stakeholes suggest a hunter-gatherer encampment, and microlith stone tools made at the site predate finds of similar style in England. Although no bones or shells had survived in the acidic soils, numerous carbonised hazelnut shells indicate cooking in a similar way to finds at other Mesolithic period sites, including the slightly earlier Star Carr and the Howick house in Northumberland, dated to 7600 BC ("Britain's oldest house"), where post holes indicate a very substantial construction, interpreted as a permanent residence for hunting people. This suggests that hunter-gatherers could also have settled down in Scotland.
Other sites on the east coast and at lochs and rivers, and large numbers of rock shelters and shell middens around the west coast and islands, build up a picture of highly mobile people, often using sites seasonally and having boats for fishing and for transporting stone tools from sites where suitable materials were found. Finds of flint tools on Ben Lawers and at Glen Dee (a mountain pass through the Cairngorms) show that these people were capable of travelling well inland across the hills.
At a rock shelter and shell midden at Sand, Applecross in Wester Ross facing Skye, excavations have shown that around 7500 BC people had tools of bone, stone and antlers, were living off shellfish, fish, and deer using "pot boiler" stones as a cooking method, were making beads from seashells, and had ochre pigment and used shellfish which can produce purple dye.
Neolithic farming brought permanent settlements. At Balbridie in Aberdeenshire crop markings were investigated, and ditches and post holes found, revealing a massive timber-framed building dating to about 3600 BC. An almost identical building, with evidence of pottery, was excavated at Claish near Stirling.[4] On the islet of Eilean Domhnuill, in Loch Olabhat on North Uist, Unstan ware pottery suggests a date of 3200–2800 BC for what may be the earliest crannog. Neolithic habitation, burial, and ritual sites are particularly common and well preserved in the Northern Isles and Western Isles, where a lack of trees led to most structures being built of local stone.[5]
The remainder of this section focuses mainly on the Orkney Islands, where there is a Neolithic landscape rich in sites amazingly preserved by prevalent use of the local stone which appears on the shore ready-split into convenient building slabs. There are many other examples across the country, many under the care of Historic Scotland.
At the stone house at Knap of Howar on the Orkney island of Papa Westray (occupied from 3500 BC to 3100 BC) the walls stand to a low eaves height, and the stone furniture is intact. Evidence from middens shows that the inhabitants kept cattle, sheep and pigs, farmed barley and wheat and gathered shellfish, as well as fishing for species which must be caught from boats using lines. Finely made and decorated Unstan ware pottery links the inhabitants to chambered cairn tombs nearby and to sites far afield, including Balbrindi and Eilean Domhnuill.
The houses at Skara Brae on the Mainland of the Orkney Islands are very similar, but are grouped into a village linked by low passageways. This settlement was occupied from about 3000 BC to 2500 BC. Pottery found here is of the grooved ware style which is found across Britain as far away as Wessex.
About 6 miles (10 km) from Skara Brae, grooved ware pottery was found at the Standing Stones of Stenness (originally a circle) which lie centrally in a close group of three major monuments. Maeshowe, the finest example of the passage grave type of chambered cairn (radiocarbon dated to before 2700 BC) lies just to the east. The Ring of Brodgar circle of standing stones is across a bridge immediately to the north. This circle was one of the first to be analysed by Professor Alexander Thom to establish the likely use of standing stones as astronomical observatories. Another Neolithic village has been found nearby at Barnhouse Settlement, and the inference is that these farming people were the builders and users of these mysterious structures.
Like the standing stones at Callanish on Lewis and other standing stones across Scotland, these monuments form part of the Europe-wide Megalithic culture which also produced Stonehenge in Wiltshire and the stone rows at Carnac in Brittany.
Further evidence can be found in Kilmartin Glen with its Stone Circles, Standing Stones and Rock Art.
The widespread connections of these people are shown by offerings imported from Cumbria and Wales and left on the sacred hilltop at Cairnpapple Hill, West Lothian, as early as 3500 BC.
The cairns and megalithic monuments continued into the Bronze Age, though there was a decline in both the building of large new structures and the total area under cultivation.[6]
The Clava cairns and standing stones near Inverness show complex geometries and astronomical alignments, with smaller, perhaps individual, tombs instead of the communal Neolithic tombs.[7]
Mummies dating from 1600 to 1300 BC have been discovered at Cladh Hallan on South Uist.
Hill forts were introduced, such as Eildon Hill near Melrose in the Scottish Borders, which goes back to around 1000 BC and which accommodated several hundred houses on a fortified hilltop. Excavation at Edinburgh Castle found late Bronze Age material from about 850 BC.
From around 700 BC and extending into Roman times, the Iron Age was an age of forts and defended farmsteads, which support the image of quarrelsome tribes and petty kingdoms recorded by the Romans. Evidence that at times occupants neglected the defences might suggest that symbolic power was as significant as warfare.[10]
Brythonic (or "Pritennic") Celtic culture and language spread into southern Scotland at some time after the 8th century BC, possibly through cultural contact rather than mass invasion, and systems of kingdoms developed.
Larger fortified settlements expanded, such as the Votadini stronghold of Traprain Law, East Lothian, which was the size of a town. Huge numbers of small duns, hill forts and ring forts were built on any suitable crag or hillock. The spectacular brochs were built, most impressively the nearly complete Mousa Broch, Shetland. Many Souterrain underground passageways were constructed, though their purpose is obscure. Island settlements linked with land by a causeway, the crannogs, became common; it is thought that their function was defensive.
Scotland has a long tradition of volunteer archaeological research and in general (outside Scheduled Monuments) you only need the permission of the landowner to conduct an archaeological dig. There are however, a series of ethical and legal frameworks surrounding reporting and post-excavation analysis (for example Treasure Trove) and advice should always sought. A key organisation for such advice is the Association of Local Government Archaeological Officers. There are also a series of organisation who support volunteers research in Scotland including Archaeology Scotland which covers the whole of Scotland and then there are a series of regional groups eg Clutha, NOSAS,  Peeblesshire Archaeological Society and Stirling Archaeology.

Cognitive archaeology is a theoretical perspective in archaeology that focuses on the ancient mind. It is divided into two main groups: evolutionary cognitive archaeology (ECA), which seeks to understand human cognitive evolution from the material record, and ideational cognitive archaeology (ICA), which focuses on the symbolic structures discernable in or inferable from past material culture.
ECA infers change in ancestral human cognition from the archaeological record, often drawing on the theories, methods, and data of other disciplines: cognitive science, comparative cognition, paleoneurology, experimental replication, and hands-on participation in the manufacture and use of traditional technologies.[1] For example, the 3.3-million-year history[2] of stone tool use is broadly informative of change in cognitive capacities like intelligence, spatial reasoning,[3][4] working memory, and executive functioning,[5][6] as defined by and understood through cognitive psychology and as operationalized to permit their detection in the archaeological record.[1] Other ECA investigations have focused on the development of domain-specific abilities, including theory of mind,[7] visual perception and visuospatial abilities,[8][9] technological reasoning,[10][11] language,[12] numeracy,[13][14] and literacy.[15][16][17] ECA is broadly analogous to Steven Mithen's categories of cognitive-processual and evolutionary-cognitive archaeology.[18]
Within ECA, there are two main schools of thought. The North American ECA school began in the mid-1970s with the pioneering work of archaeologist Thomas G. Wynn[3][4] and biological anthropologist Sue Taylor Parker working with evolutionary neurobiologist Kathleen Gibson.[19] It focuses on understanding human cognitive evolution, either from the artifactual record of forms like stone tools, comparisons of ancestral tool use with that of contemporary species (typically but not exclusively, non-human primates), or both. It often involves descriptive pattern analysis: analyzing change in a form like stone tools over millions of years and interpreting that change in terms of its cognitive significance using theories, constructs, and paradigms from cognitive psychology and neuroscience.[1]
East of the Atlantic, the British ECA school also began in the mid-1970s with the work of archaeologists Colin Renfrew[20][21] and John Gowlett[22][23] and evolutionary primatologist William McGrew.[24][25] Renfrew's work in particular, as well as that of his student, Lambros Malafouris, has taken a philosophical approach to the study of the ancient mind, drawing on concepts from the philosophy of mind and ecological psychology to examine the role of material structures in human cognition more fundamentally.[26][27] Renfrew and Malafouris coined the term neuroarchaeology to describe their approach.[28][29] ECA is concerned with how humans think through material structures, with the ability to leverage and exploit material structures for cognitive purposes perhaps being what truly sets human cognition apart from that of all other species.[30] Pottery making is a typical example of this approach. Malafouris does not see the vase as a form created by the potter imposing an internal mental concept on external clay. Instead, the potter’s brain and body interact with his materials, the clay and the wheel; the form assumed by the clay is ultimately produced by the complex interaction between the potter’s perception of the feel of the clay, the pressure of his fingers on it, and its reactions of texture, moisture content, color, balance, and form.[31]
Other early ECA pioneers include Glynn Isaac,[32][33] archaeologist Iain Davidson, and psychologist William Noble.[34][35] Today, ECA integrates interdisciplinary data from human psychology and neurophysiology, social anthropology, physical anthropology, comparative cognition, and artificial intelligence. As a vibrant and expanding field of inquiry,
"[ECA continues to] develop many of the same themes raised in the formative decade of cognitive archaeology: the validity and use of ethnoarchaeological and experimental methods; the question of continuities and discontinuities between humans and non-human species; the selection and application of theoretical frameworks, including the displacement of Piagetian theory by contemporary psychological and neuroscientific approaches to brain function and form; the incorporation of interdisciplinary data; the origin of language; the ability of construing intentionality from artifactual form; the philosophical turn in cognitive archaeology; and the riddle of intergenerational accumulation and transmission."[1]: 6
Between 2018 and 2020, cognitive archaeologists Thomas Wynn and Lambros Malafouris headed a collaboration between the University of Colorado, Colorado Springs and the University of Oxford to examine the archaeology of the Lower Paleolithic through the lens of the extended mind; the results were published in the journal Adaptive Behavior in 2021.[36]
Archaeologist Thomas Huffman defined ideational cognitive archaeology as the study of prehistoric ideology: the ideals, values, and beliefs that constitute a society's worldview.[37] It is analogous to the category Mithen called postprocessual cognitive archaeology.[18]
"Archaeologists can tell from which mountain source a stone axe came, what minerals there are in a bronze bracelet, how old a dug-out canoe is. They can work out the probable cereal-yield from the fields of a Late Bronze Age farm. These are objective matters. But the language, laws, morals, religion of dead societies are different. They belong to the minds of man. Unless they were written down, and even then only if they were recorded accurately, we shall find it hard to recapture them."
ICA scholars often study the role that ideology and differing organizational approaches would have had on ancient peoples. The way that these abstract ideas are manifested through the remains these peoples have left can be investigated and debated often by drawing inferences and using approaches developed in fields such as semiotics, psychology and the wider sciences.
ICA uses the principles of sociocultural anthropology to investigate such diverse things as material symbols, the use of space, political power, and religion. For example, Huffman uses oral history sources from Zimbabwe and Portuguese documents to attempt to explain symbols discovered in the ruins of Great Zimbabwe, specifically connecting the Shona people's historical association of the right with men and the left with women to the placement of entrances to stone structures. Historian David Beach has pointed out that this ICA may be problematic in its logical leaps and incomplete use of archaeological sources, demonstrating the care that must be used when attempting to explain deep-time intentionality using archaeological evidence.[39]
ICA also works with constructs such as the cognitive map. Humans do not behave under the influence of their senses alone but also through their past experiences such as their upbringing. These experiences contribute to each individual's unique view of the world, a kind of cognitive map that guides them. Groups of people living together tend to develop a shared view of the world and similar cognitive maps, which in turn influence their group material culture.
The multiple interpretations of an artifact, archaeological site or symbol are affected by the archaeologist's own experiences and ideas as well as those of the distant cultural tradition that created it. Cave art, for example, may not have been art in the modern sense at all, but was perhaps the product of ritual. Similarly, it would likely have described activities that were perfectly obvious to the people who created it, but the symbology employed will be different from that used today or at any other time.
Archaeologists have always tried to imagine what motivated people, but early efforts to understand how they thought were unstructured and speculative. Since the rise of processualism, these approaches have become more scientific, paying close attention to the archaeological context of finds and all possible interpretations. For example, a prehistoric bâton de commandement served an unknown purpose, but using ICA to interpret it would involve evaluating all its possible functions using clearly defined procedures and comparisons. By applying logic and experimental evidence, the most likely functions can be isolated.
It can also be argued that the material record shows behavioral traces that are the product of human thought, and thus would have been governed by a multitude of experiences and perspectives with the potential to influence behavior. The combination of material culture and actions can be further developed into a study of the ideas that drove action and used objects. This method attempts to avoid the pitfalls of post-processual archaeology by retaining the 'scientific' aspects of processual archaeology, while reaching for the higher social levels of ideas.
Cognitive archaeology began in the 1970s as a reaction to the insistence of processual archaeology that the past be interpreted strictly according to the material evidence.[1] This rigid materialism tended to limit archaeology to finding and describing artifacts, excluding broader interpretations of their possible cognitive and cultural significance as something beyond the reach of inferential reasoning.[40] As social anthropologist Edmund Leach once put it, "all the ingenuity in the world will not replace the evidence that is lost and gone for ever," and "you should recognize your guesses for what they are."[41]: 768
However, processual archaeology also opened up the possibility of investigating the lifestyle of those who made and used material culture. An initial approach was proposed by Lewis Binford, who suggested that ancient lifestyles could be understood by studying the traditional lifestyles of contemporary peoples.[42][43] While this approach was subject to legitimate criticism, Binford's efforts nonetheless inspired further development of the idea that material forms could be informative about lifestyle, and as the product of intelligent behavior, might provide insight into how and perhaps even what their makers had thought.[1] Archaeologists like Binford have also critiqued cognitive archaeology, stating it is only people's actions rather than their thoughts that are preserved in the archaeological record. ECA has responded to this criticism by stressing that it seeks to understand "how" ancient peoples thought using material structures, not "what" they thought.[27]
Several early books helped popularize the idea that the ancient mind could be investigated and characterized, including Merlin Donald's Origins of the Modern Mind (1991),[44] Steven Mithen's The Prehistory of Mind (1996),[45] and David Lewis-Williams's The Mind in the Cave (2002).[46]

The following outline is provided as an overview of and topical guide to prehistoric technology.
Prehistoric technology – technology that predates recorded history. History is the study of the past using written records; it is also the record itself. Anything prior to the first written accounts of history is prehistoric (meaning "before history"), including earlier technologies. About 2.5 million years before writing was developed, technology began with the earliest hominids who used stone tools, which they may have used to start fires, hunt, cut food, and bury their dead.
Prehistoric technology can be described as:
The New World periods began with the crossing of the Paleo-Indians, Athabaskan, Aleuts, Inuit, and Yupik peoples along the Bering Land Bridge onto the North American continent.[36] In their book, Method and Theory in American Archaeology, Gordon Willey and Philip Phillips defined five cultural stages for the Americas, including the three prehistoric Lithic, Archaic and Formative stages. The historic stages are the Classic and Post-Classic stages.[37][38]

Nubia (/ˈnjuːbiə/, Nobiin: Nobīn,[2] Arabic: النُوبَة, romanized: an-Nūba) is a region along the Nile river encompassing the confluence of the Blue and White Niles (in Khartoum in central Sudan), and the area between the first cataract of the Nile (south of Aswan in southern Egypt) or more strictly, Al Dabbah.[3][4][5][6] It was the seat of one of the earliest civilizations of ancient Africa, the Kerma culture, which lasted from around 2500 BC until its conquest by the New Kingdom of Egypt under Pharaoh Thutmose I around 1500 BC, whose heirs ruled most of Nubia for the next 400 years. Nubia was home to several empires, most prominently the Kingdom of Kush, which conquered Egypt in the eighth century BC during the reign of Piye and ruled the country as its 25th Dynasty.
From the 3rd century BC to 3rd century AD, northern Nubia was invaded and annexed to Egypt, ruled by the Greeks and Romans. This territory was known in the Greco-Roman world as Dodekaschoinos.
Kush's collapse in the fourth century AD was preceded by an invasion from the Ethiopian Kingdom of Aksum and the rise of three Christian kingdoms: Nobatia, Makuria and Alodia. Makuria and Alodia lasted for roughly a millennium. Their eventual decline started not only the partition of Nubia, which was split into the northern half conquered by the Ottomans and the southern half by the Sennar sultanate, in the sixteenth century, but also a rapid Islamization and partial Arabization of the Nubian people. Nubia was reunited with the Khedivate of Egypt in the nineteenth century. Today, the region of Nubia is split between Egypt and Sudan.
The primarily archaeological science dealing with ancient Nubia is called Nubiology.
Historically, the people of Nubia spoke at least two varieties of Nubian languages, a subfamily that includes Nobiin (the descendant of Old Nubian), Dongolawi, Midob and several related varieties in the northern part of the Nuba Mountains in South Kordofan. The Birgid language was spoken north of Nyala in Darfur, but became extinct as late as 1970. However, the linguistic identity of the ancient Kerma culture of southern and central Nubia (also known as Upper Nubia), is uncertain; some research suggests that it belonged to the Cushitic branch of the Afroasiatic languages,[9][10] while more recent studies indicate that the Kerma culture belonged to the Eastern Sudanic branch of Nilo-Saharan languages instead, and that other peoples of northern or Lower Nubia north of Kerma (such as the C-Group culture and the Blemmyes) spoke Cushitic languages before the spread of Eastern Sudanic languages from southern or Upper Nubia.[11][12][13][14]
Nubia was divided into three major regions: Upper, Middle, and Lower Nubia, in reference to their locations along the Nile. "Lower" referred to regions downstream (further north) and "upper" to regions upstream (further south). Lower Nubia lay between the First and the Second Cataracts within the current borders of Egypt, Middle Nubia lay between the Second and the Third Cataracts, and Upper Nubia lay south of the Third Cataract.[15]
Archaeological evidence attests to long histories of fishing-hunting-gathering, and later herding, throughout the Nile Valley.[16]
Affad 23 is an archaeological site located in the Affad region of southern Dongola Reach in northern Sudan,[17] which hosts "the well-preserved remains of prehistoric camps (relics of the oldest open-air hut in the world) and diverse hunting and gathering loci some 50,000 years old".[18][19][20]
In southern Nubia (near modern Khartoum) from the ninth to the sixth millennia cal BC, Khartoum Mesolithic fisher-hunter-gatherers produced sophisticated pottery.[21]
By 5000 BC, the people who inhabited what is now called Nubia participated in the Neolithic Revolution. The Sahara became drier and people began to domesticate sheep, goats, and cattle.[22] Saharan rock reliefs depict scenes that have been thought to suggest the presence of a cattle cult, typical of those seen throughout parts of Eastern Africa and the Nile Valley even to this day.[23] Nubian rock art depicts hunters using bows and arrows in the Neolithic period, which is a precursor to Nubian archer culture in later times.
Megaliths discovered at Nabta Playa are early examples of what seems to be one of the world's first astronomical devices, predating Stonehenge by almost 2,000 years.[24] This complexity as expressed by different levels of authority within the society there likely formed the basis for the structure of both the Neolithic society at Nabta and the Old Kingdom of Egypt.[25]
American anthropologist, Joseph Vogel wrote that:
"The period when sub-Saharan Africa was most influential in Egypt was a time when neither Egypt, as we understand it culturally, nor the Sahara, as we understand it geographically, existed. Populations and cultures now found south of the desert roamed far to the north. The culture of Upper Egypt, which became dynastic Egyptian civilization, could fairly be called a Sudanese transplant."[26]
British Africanist Basil Davidson outlined that "The ancient Egyptians belonged, that is, not to any specific Egyptian region or Near Eastern heritage but to that wide community of peoples who lived between the Red Sea and the Atlantic Ocean, shared a common "Saharan-Sudanese culture", and drew their reinforcements from the same great source, even though, as time went by, they also absorbed a number of wanderers from the Near East".[27]
Biological anthropologists Shomarka Keita and A.J. Boyce have stated that the "Studies of crania from southern predynastic Egypt, from the formative period (4000-3100 B.C.), show them usually to be more similar to the crania of ancient Nubians, Kushites, Saharans, or modern groups from the Horn of Africa than to those of dynastic northern Egyptians or ancient or modern southern Europeans."[28]
Archaeological evidence has attested that population settlements occurred in Nubia as early as the Late Pleistocene era and from the 5th millennium BC onwards, whereas there is "no or scanty evidence" of human presence in the Egyptian Nile Valley during these periods, which may be due to problems in site preservation.[29] Several scholars have argued that the African origins of the Egyptian civilization derived from pastoral communities which emerged in both the Egyptian and Sudanese regions of the Nile Valley in the fifth millennium BCE.[30][31]
Dietrich Wildung (2018) examined Eastern Saharan pottery styles and Sudanese stone sculptures and suggested these artefacts were transmitted across the Nile Valley and influenced the pre-dynastic Egyptian culture in the Neolithic period.[32]
The poorly known "pre-Kerma" culture existed in Upper (Southern) Nubia on a stretch of fertile farmland just south of the Third Cataract.
Nubia has one of the oldest civilizations in the world. This history is often intertwined with Egypt to the north.[33]: 16  Around 3500 BC, the second "Nubian" culture, termed the Early A-Group culture, arose in Lower Nubia.[34] They were sedentary agriculturalists,[22]: 6  traded with the Egyptians and exported gold.[35] This trade is supported archaeologically by large amounts of Egyptian commodities deposited in the A-Group graves. The imports consisted of gold objects, copper tools, faience amulets and beads, seals, slate palettes, stone vessels, and a variety of pots.[36] During this time, the Nubians began creating distinctive black topped, red pottery. The A-Group population have been described as ethnically “very similar” to the pre-dynastic Egyptians in physical characteristics.[37]
Around 3100 BC, the A-group transitioned from the Early to Classical phases. "Arguably royal burials are known only at Qustul and possibly Sayala."[35]: 8  During this period, the wealth of A-group kings rivaled Egyptian kings. Royal A-group graves contained gold and richly decorated pottery.[33]: 19  Some scholars believe Nubian A-Group rulers and early Egyptian pharaohs used related royal symbols; similarities in A-Group Nubia and Upper Egypt rock art support this position. Scholars from the University of Chicago Oriental Institute excavated at Qustul (near Abu Simbel in Sudan), in 1960–64, and found artifacts which incorporated images associated with Egyptian pharaohs.
Archeologist Bruce Williams studied the artifacts and concluded that "Egypt and Nubia A-Group culture shared the same official culture", "participated in the most complex dynastic developments", and "Nubia and Egypt were both part of the great East African substratum".[38] Williams also wrote that Qustul "could well have been the seat of Egypt's founding dynasty".[39][40] David O'Connor wrote that the Qustul incense burner provides evidence that the A-group Nubian culture in Qustul marked the "pivotal change" from predynastic to dynastic "Egyptian monumental art".[41] However, "most scholars do not agree with this hypothesis",[42] as more recent finds in Egypt indicate that this iconography originated in Egypt instead of Nubia, and that the Qustul rulers adopted or emulated the symbols of Egyptian pharaohs.[43][44][45][46]
According to David Wengrow, the A-Group polity of the late 4th millenninum BCE is poorly understood since most of the archaeological remains are submerged underneath Lake Nasser.[47] Frank Yurco also remarked that depictions of pharonic iconography such as the royal crowns, Horus falcons and victory scenes were concentrated in the Upper Egyptian Naqada culture and A-Group Nubia. He further elaborated that "Egyptian writing arose in Naqadan Upper Egypt and A-Group Nubia, and not in the Delta cultures, where the direct Western Asian contact was made, further vitiates the Mesopotamian-influence argument".[48]
The archaeological cemeteries at Qustul are no longer available for excavations since the flooding of Lake Nasser.[49] The earliest representations of pharaonic iconography have been excavated from Nag el-Hamdulab in Aswan, the extreme southern region of Egypt which borders the Sudan, with an estimated dating range between 3200 and 3100 BC.[50]
Writing developed in Egypt around 3300 BC. In their writings, Egyptians referred to Nubia as "Ta-Seti", or "The Land of the Bow," as the Nubians were known to be expert archers.[51] More recent and broader studies have determined that the distinct pottery styles, differing burial practices, different grave goods, and site distribution all indicate that the Naqada people and the Nubian A-Group people were from different cultures.
Kathryn Bard states that "Naqada cultural burials contain very few Nubian craft goods, which suggests that while Egyptian goods were exported to Nubia and were buried in A-Group graves, A-Group goods were of little interest further north."[52] According to anthropologist Jane Hill, there is no evidence that the pharaohs of the First Dynasty of Egypt buried at Abydos were of Nubian origin.[53] However, several biological anthropological studies have shown the Badarian and Naqada people to be closely related to the Nubian and other, tropical African populations.[54][55][56][57][58][59] Also, the proto-dynastic kings emerged from the Naqada region.[60][61]
A uniform culture of nomadic herders, called the Gash group, existed from 3000 to 1500 BC to the east and west of Nubia.[22]: 8
In Lower Nubia, the A-group moved from the Classical to Terminal phase. At this time, kings at Qustul likely ruled all of Lower Nubia and demonstrated the political centralization of Nubian society.[22]: 21  The A-Group culture came to an end sometime between 3100 and 2900 BC, when it was apparently destroyed by the First Dynasty rulers of Egypt.[62] There are no records of settlement in Lower Nubia for the next 600 years. Old Kingdom Egyptian dynasties (4th to 6th) controlled uninhabited Lower Nubia and raided Upper Nubia.
The pre-Kerma developed into the Middle phase Kerma group. Some A-group people (transitioning to C-group) settled the area and co-existed with the pre-Kerma group.[22]: 25  Like other Nubian groups, the two groups made an abundance of red pottery with black tops, though each group made different shapes.[22]: 29  Traces of the C-group in Upper Nubia vanish by 2000 BC and Kerma culture began to dominate Upper Nubia.[22]: 25  The power of an independent Upper Nubia increased around 1700 BC and Upper Nubia dominated Lower Nubia.[22]: 25  An Egyptian official, Harkhuf, mentions that Irtjet, Setjet, and Wawat all combined under a single ruler. By 1650 BC, Egyptian texts started to refer to only two kingdoms in Nubia: Kush and Shaat.[22]: 32, 38  Kush was centered at Kerma and Shaat was centered on Sai island.[22]: 38  Bonnet posits that Kush actually ruled all of Upper Nubia, since "royal" graves were much larger in Kush than Shaat and Egyptian texts other than the Execration lists only refer to Kush (and not Shaat).[22]: 38–39
C-group Nubians resettled Lower Nubia by 2400 BC.[22]: 25  As trade between Egypt and Nubia increased, so did wealth and stability. Nubia was divided into a series of small kingdoms. There is debate over whether the C-group people,[63] who flourished from 2500 BC to 1500 BC, were another internal evolution or invaders. O'Connor states "a transition from A group into a later culture, the C-group, can be traced" and the C-group culture was typical of Lower Nubia from 2400 to 1650 BC.[22]: 25  Although they lived in close proximity to each other, Nubians did not acculturate much to Egyptian culture. Notable exceptions include C-group Nubians during the 15th Dynasty, isolated Nubian communities in Egypt, and some bowmen communities.[22]: 56  C-Group pottery is characterized by all-over incised geometric lines with white infill and impressed imitations of basketry. Lower Nubia was controlled by Egypt from 2000 to 1700 BC and Upper Nubia from 1700 to 1525 BC.
From 2200 to 1700 BC, the Pan Grave culture appeared in Lower Nubia.[33]: 20  Some of the people were likely the Medjay (mḏꜣ,[64]) arriving from the desert east of the Nile river. One feature of Pan Grave culture was shallow grave burial. The Pan Grave and C-Group definitely interacted: Pan Grave pottery is characterized by more limited incised lines than the C-Group's and generally have interspersed undecorated spaces within the geometric schemes.[65]
In 2300 BC, Nubia was first mentioned in Old Kingdom Egyptian accounts of trade missions. The Egyptians referred to Lower Nubia as Wawat, Irtjet, and Setju, while they referred to Upper Nubia as Yam. Some authors believe that Irtjet and Setju could also have been in Upper Nubia.[22]: 32  They referred to Nubians dwelling near the river as Nehasyu.[22]: 26
From Aswan, the southern limit of Egyptian control at the time, Egyptians imported gold, incense, ebony, copper, ivory, and exotic animals from tropical Africa through Nubia. Relations between the Egyptians and Nubians showed peaceful cultural interchange, cooperation, and mixed marriages. Nubian bowmen that settled at Gebelein during the First Intermediate Period married Egyptian women, were buried in Egyptian style, and eventually could not be distinguished from Egyptians.[22]: 56
Older scholarship noted that some Egyptian pharaohs may have had Nubian ancestry.[66][67] Richard Loban expressed the view that Mentuhotep II of the 11th Dynasty "was quite possibly of Nubian origin" and cited historical evidence which mentioned that Amenemhet I, founder of the 12th Dynasty, "had a Ta Seti or Nubian mother".[68][69][70] Dietrich Wildung has argued that Nubian features were common in Egyptian iconography since the pre-dynastic era and that several pharaohs such as Khufu and Mentuhotep II were represented with these Nubian features.[71]
Frank Yurco wrote that "Egyptian rulers of Nubian ancestry had become Egyptians culturally; as pharaohs, they exhibited typical Egyptian attitudes and adopted typical Egyptian policies". Yurco noted that some Middle Kingdom rulers, particularly some pharaohs of the Twelfth Dynasty had strong Nubian features, due to the origin of the dynasty in the Aswan region of southern Egypt. He also identified the pharaoh Sequenre Tao of the Seventeenth Dynasty, as having Nubian features.[72] Many scholars in recent years have argued that the mother of Amenemhat I, founder of the Twelfth Dynasty was of Nubian origin.[73][74][69][75][76][77][78]
After a period of withdrawal, the Middle Kingdom of Egypt conquered Lower Nubia from 2000 to 1700 BC.[22]: 8, 25  By 1900 BC, King Sesostris I began building a series of towns below the Second Cataract with heavy fortresses that had enclosures and drawbridges.[33]: 19  Sesotris III relentlessly expanded his kingdom into Nubia (from 1866 to 1863 BC) and erected massive river forts including Buhen, Semna, Shalfak and Toshka at Uronarti to gain more control over the trade routes in Lower Nubia. They also provided direct access to trade with Upper Nubia, which was independent and increasingly powerful during this time. These Egyptian garrisons seemed to peacefully coexist with the local Nubian people, though they did not interact much with them.[79]
Medjay was the name given by ancient Egypt to nomadic desert dwellers from east of the Nile River. The term was used variously to describe a location, the Medjay people, or their role/job in the kingdom. They became part of the Egyptian military as scouts and minor workers before being incorporated into the Egyptian army.[citation needed] In the army, the Medjay served as garrison troops in Egyptian fortifications in Nubia and patrolled the deserts as a kind of gendarmerie,[80] or elite paramilitary police force,[81] to prevent their fellow Medjay tribespeople from further attacking Egyptian assets in the region.[81]
The Medjay were often used to protect valuable areas, especially royal and religious complexes. Although they are most notable for their protection of the royal palaces and tombs in Thebes and the surrounding areas, the Medjay were deployed throughout Upper and Lower Egypt; they were even used during Kamose's campaign against the Hyksos and became instrumental in turning the Egyptian state into a military power.[82][83] After the First Intermediate Period of Egypt, the Medjay district was no longer mentioned in written records.[84]
From the Middle Kerma phase, the first Nubian kingdom to unify much of the region arose. The Classic Kerma culture, named for its royal capital at Kerma, was one of the earliest urban centers in the Nile region and oldest city in Africa outside of Egypt.[85][22]: 50–51  The Kerma group spoke either languages of the Cushitic branch[9][10] or, according to more recent research, Nilo-Saharan languages of the Eastern Sudanic branch.[11][12][13][14]
By 1650 BC (Classic Kerma phase), the kings of Kerma were powerful enough to organize the labor for monumental town walls and large mud brick structures, such as the Eastern and Western Deffufas (50 by 25 by 18 meters). They also had rich tombs with possessions for the afterlife and large human sacrifices. George Andrew Reisner excavated sites at the royal city of Kerma and found distinctive Nubian architecture, such as large pebble covered tombs (90 meters in diameter), a large circular dwelling, and a palace-like structure.[22]: 41  Classic Kerma rulers employed "a good many Egyptians", according to the Egyptian Execration texts.[22]: 57
Kerma culture was militaristic, as attested by many archers' burials and bronze daggers/swords found in their graves.[22]: 31  Other signs of Nubia's military prowess are the frequent use of Nubians in Egypt's military and Egypt's need to construct numerous fortresses to defend their southern border from the Nubians.[22]: 31  Despite assimilation, the Nubian elite remained rebellious during Egyptian occupation. There were numerous rebellions and "military conflict occurred almost under every reign until the 20th dynasty".[86]: 102–103  At one point, Kerma came very close to conquering Egypt: Egypt suffered a serious defeat at the hands of the Kingdom of Kush.[87][88]
According to Davies, head of the joint British Museum and Egyptian archaeological team, the attack was so devastating that, if the Kerma forces had chosen to stay and occupy Egypt, they might have permanently eliminated the Egyptians and brought the nation to extinction. During Egypt's Second Intermediate period, the Kushites reached the height of their Bronze Age power and completely controlled southern trade with Egypt.[22]: 41  They maintained diplomatic ties with the Thebans and Hyksos until the New Kingdom pharaohs brought all of Nubia under Egyptian rule from 1500 to 1070 BC.[22]: 41  After 1070 BC, there were continued hostilities with Egypt, which led Nubians to concentrate in Upper Nubia.[22]: 58  Within 200 years, a fully formed Kushite state, based at Napata, began to exert its influence on Upper (Southern) Egypt.[22]: 58–59
When the Middle Kingdom Egyptians pulled out of the Napata region around 1700 BC, they left a lasting legacy that was merged with indigenous C-group customs. Egyptians remaining at the garrison towns started to merge with the C-group Nubians in Lower Nubia. The C-group quickly adopted Egyptian customs and culture, as attested by their graves, and lived together with the remaining Egyptians in garrison towns.[22]: 41  After Upper Nubia annexed Lower Nubia around 1700 BC, the Kingdom of Kush began to control the area. At this point, C-group Nubians and Egyptians began to proclaim their allegiance to the Kushite King in their inscriptions.[22]: 41  Egypt conquered Lower and Upper Nubia from 1500 to 1070 BC. However, the Kingdom of Kush survived longer than Egypt.
After the Theban 17th Dynasty New Kingdom of Egypt (c. 1532–1070 BC) expelled the Canaanite Hyksos from Egypt, they turned their imperial ambitions to Nubia. By the end of Thutmose I's reign (1520 BC), all of Lower Nubia had been annexed. After a long campaign, Egypt also conquered the Kingdom of Kerma in Upper Nubia and held both areas until 1070 BC.[86]: 101–102 [22]: 25  The Egyptian empire expanded into the Fourth Cataract, and a new administrative center was built at Napata, which became a gold and incense production area.[89][90] Egypt became a prime source of gold in the Middle East. The primitive working conditions for the slaves are recorded by Diodorus Siculus.[91] One of the oldest maps known is of a gold mine in Nubia: the Turin Papyrus Map dating to about 1160 BC; it is also one of the earliest characterized road maps in existence.[92]
Nubians were an integral part of New Kingdom Egyptian society. Some scholars state that Nubians were included in the 18th Dynasty of Egypt's royal family.[93] Ahmose-Nefertari, "arguably the most venerated woman in Egyptian history",[94] was thought by some scholars such as Flinders Petrie to be of Nubian origin because she is most often depicted with black skin.[66][95]: 17 [96] The mummy of Ahmose-Nefertari's father, Seqenenre Tao, has been described as presenting "tightly curled, woolly hair", with "a slight build and strongly Nubian features".[97] Some modern scholars also believe that in some depictions, her skin color is indicative of her role as a goddess of resurrection, since black is both the color of the fertile land of Egypt and that of the underworld.[98][99]: 90 [100][94][101]: 125  However, there is no known depiction of her painted during her lifetime (she is represented with the same light skin as other represented individuals in tomb TT15, before her deification); the earliest black skin depiction appears in tomb TT161, c. 150 years after her death.: 11–12, 23, 74–5 [101]: 125  Egyptologist Barbara Lesko wrote in 1996 that Ahmose-Nefertari was "sometimes portrayed by later generations as having been black, although her coffin portrait gives her the typical light yellow skin of women."[102] In 2009, 
Egyptologist Elena Vassilika, noting that in a wooden statuette of the queen (now at the Museo Egizio) the face is painted black but the arms and feet are light in color, argued that the reason for the black coloring in that case was religious and not genetic.[103]: 78–9
In 1098–1088 BC, Thebes was "the scene of a civil war-like conflict between the High Priest of Amun of Thebes Amenhotep and the Viceroy of Kush Panehesy (= the Nubian)". It was chaotic and many tombs were plundered. Instead of sending soldiers to restore order, Ramesses XI put Panehesy in control of that area's military and appointed him Director of Granaries. Panehesy stationed his troops in Thebes to protect the city from thieves, but it resembled a military occupation of Thebes to the High Priest, which later led to the Civil war in Thebes.[86]: 104–105  By 1082 BC, Ramesses XI finally sent help to the High Priest. Panehesy continued his revolt and the city of Thebes suffered from "war, famine, and plunderings".[86]: 106  Panehesy initially succeeded and the High Priest fled Thebes. Panehesy pursued the High Priest as far as Middle Egypt before Egyptian forces pushed Panehesy and his troops out of Egypt and into Lower Nubia.[86]: 106  Ramesses sent new leadership to Thebes: Herihor was named the new High Priest of Thebes (and effectively King of Southern Egypt) and Paiankh was named the new Viceroy of Kush. Paiankh recaptured former Egyptian holdings in Lower Nubia as far as the second Nile cataract, but could not defeat Panehesy in Lower Nubia, who ruled the area until his death.[86]: 106  Herihor's descendants became rulers of Egypt's 21st and 22nd Dynasties.
There are competing theories on the origins of the Kushite kings of the 25th Dynasty:[104] some scholars believe they were Nubian officials that learned "state level organization" by administering Egyptian-held Nubia from 1500 to 1070 BC,[22]: 59  such as the rebel Viceroy of Kush, Panehesy, who ruled Upper Nubia and some of Lower Nubia after Egyptian forces withdrew.[86]: 110  Other scholars believe they are descended from families of the Egyptianized Nubian elite supported by Egyptian priests or settlers.[105][106][107][104] Children of elite Nubian families were sent to be educated in Egypt then returned to Kush to be appointed in bureaucratic positions to ensure their loyalty. During the Egyptian occupation of Nubia, there were temple towns with Egyptian cults, but "production and redistribution" was based mostly on indigenous social structures.[86]: 111
The El Kurru chiefdom likely played a major role in the development of the Kingdom of Kush due to its access to gold producing areas, control of caravan routes,[86]: 112  more arable land, and participation in international trade.[86]: 121  "There can be no doubt that el-Kurru was the burial place of the ancestors of the Twenty-Fifth Dynasty."[86]: 112  The early el-Kurru burials resemble Nubian Kerma/C-group traditions (contracted body, circular stone structures, burial on a bed).[86]: 121  However, by 880–815 BC, Nubian burials at el-Kurru became more Egyptian in style with "mastabas, or pyramid on mastabas, chapels, and rectangular enclosures".[86]: 117, 121–122  Alara, the first el-Kurru prince, and his successor, Kashta, were buried at el-Kurru.[86]: 123
Later documents mention Alara as the 25th Dynasty's founder and "central to a myth of the origins of the kingdom".[86]: 124–126  Alara's sister was the priestess of Amun, which created a system of royal secession and an "ideology of royal power in which Kushite concepts and practice were united with contemporary Egyptian concepts of kingship".[86]: 144  Later, Kashta's daughter, the Kushite princess Amenirdis, was installed as God's Wife of Amun Elect and later Divine Adoratrice (effectively governor of Upper Egypt), which signaled the Kushite conquest of Egyptian territories.[86]: 148
The Napatan Empire ushered in the age of Egyptian archaism, or a return to a historical past, which was embodied by a concentrated effort at religious renewal and restoration of Egypt's holy places.[86]: 169  Piye expanded the Temple of Amun at Jebel Barkal[35] by adding "an immense colonnaded forecourt".[86]: 163–164  Shabaka restored the great Egyptian monuments and temples, "unlike his Libyan predecessors".[86]: 167–169  Taharqa enriched Thebes on a monumental scale."[86] At Karnak, the Sacred Lake structures, the kiosk in the first court, and the colonnades at the temple entrance are all built by Taharqa and Mentuemhet. In addition to architecture, the Kingdom of Kush was deeply influenced by Egyptian culture.[108][109][110] By 780 BC, Amun was the main god of Kush and "intense contacts with Thebes" were maintained.[86]: 144  Kush used the methods of Egyptian art and writing.[111] The Nubian elite adopted many Egyptian customs and gave their children Egyptian names. Although some Nubian customs and beliefs (e.g. burial practices) continued to be practiced,[86]: 111  Egyptianization dominated in ideas, practices, and iconography.[112] The cultural Egyptianization of Nubia was at its highest levels at the times of both Kashta and Piye.[113]
Kashta peacefully became King of Upper and Lower Egypt with his daughter Amendiris as Divine Adoratrice of Amun in Thebes.[86]: 144–146  Rulers of the 23rd Dynasty withdrew from Thebes to Heracleopolis, which avoided conflict with the new Kushite rulers of Thebes. Under Kashta's reign, the Kushite elite and professional classes became significantly Egyptianized.
The city-state of Napata was the spiritual capital of Kush and it was from there that Piye (spelled Piankhi or Piankhy in older works) invaded and took control of Egypt.[116] Piye personally led the attack on Egypt and recorded his victory in a lengthy hieroglyphic filled stele called the "Stele of Victory".[86]: 166  Piye's success in achieving the double kingship after generations of Kushite planning resulted from "Kushite ambition, political skill, and the Theban decision to reunify Egypt in this particular way", and not Egypt's utter exhaustion, "as frequently suggested in Egyptological studies."[35] Due to archaism, Piye mostly used the royal titulary of Tuthmosis III, but changed the Horus name from "Strong bull appearing (crowned) in Thebes" to "Strong bull appearing in Napata" to announce that the Kushites had reversed history and conquered their former Thebaid Egyptian conquerors.[86]: 154  He also revived one of the greatest features of the Old and Middle Kingdoms: pyramid construction. As an energetic builder, he constructed the oldest known pyramid at the royal burial site of El-Kurru.
According to the revised chronology, Shebitku "brought the entire Nile Valley as far as the Delta under the empire of Kush and is 'reputed' to have had Bocchoris, dynast of Sais, burnt to death".[117][86]: 166–167  Shabaka "transferred the capital to Memphis".[86]: 166  Shebitku's successor, Taharqa, was crowned in Memphis in 690 BC[86][33] and ruled Upper and Lower Egypt as Pharaoh from Tanis in the Delta.[118][117] Excavations at el-Kurru and studies of horse skeletons indicate the finest horses used in Kushite and Assyrian warfare were bred in and exported from Nubia. Horses and chariots were key to the Kushite war machine.[86]: 157–158
Taharqa's reign was a prosperous time in the empire with a particularly large Nile river flood and abundant crops and wine.[119][86] Taharqa's inscriptions indicate that he gave large amounts of gold to the temple of Amun at Kawa.[120] His army undertook successful military campaigns, as attested by the "list of conquered Asiatic principalities" from the Mut temple at Karnak and "conquered peoples and countries (Libyans, Shasu nomads, Phoenicians?, Khor in Palestine)" from Sanam temple inscriptions.[86] László Török mentions the military success was due to Taharqa's efforts to strengthen the army through daily training in long-distance running and Assyria's preoccupation with Babylon and Elam.[86] Taharqa also built military settlements at the Semna and Buhen forts and the fortified site of Qasr Ibrim.[86]
Imperial ambitions of the Mesopotamian-based Assyrian Empire made war with the 25th Dynasty inevitable. Taharqa conspired with Levantine kingdoms against Assyria:[121] in 701 BC, Taharqa and his army aided Judah and King Hezekiah in withstanding a siege by King Sennacherib of the Assyrians (2 Kings 19:9; Isaiah 37:9).[122] There are various theories (Taharqa's army,[123] disease, divine intervention, Hezekiah's surrender, Herodotus' mice theory) as to why the Assyrians failed to take Jerusalem and withdrew to Assyria.[124] Sennacherib's annals record Judah was forced into tribute after the siege and Sennacherib became the ruler of the region.[125] However, this is contradicted by Khor's frequent utilization of an Egyptian system of weights for trade and the twenty-year cessation in Assyria's pattern of repeatedly invading Khor (as Assyrians had before 701 and after Sennacherib's death).[126][127] In 681 BC, Sennacherib was murdered by his own sons in Babylon.
In 679 BC, Sennacherib's successor, King Esarhaddon, campaigned in Khor, destroyed Sidon, and forced Tyre into tribute in 677–676 BC. Esarhaddon invaded Egypt proper in 674 BC, but according to Babylonian records, Taharqa and his army outright defeated the Assyrians.[128] In 672 BC, Taharqa brought reserve troops from Kush, as mentioned in rock inscriptions.[86] Taharqa's Egypt still had influence in Khor during this period as Tyre's King Ba'lu "put his trust upon his friend Taharqa". Further evidence was Ashkelon's alliance with Egypt and Esarhaddon's inscription asking "if the Kushite-Egyptian forces 'plan and strive to wage war in any way' and if the Egyptian forces will defeat Esarhaddon at Ashkelon".[129] However, Taharqa was defeated in Egypt in 671 BC when Esarhaddon conquered Northern Egypt, captured Memphis, and imposed tribute before withdrawing.[118] Pharaoh Taharqa escaped to the south, but Esarhaddon captured the Pharaoh's family, including "Prince Nes-Anhuret and the royal wives",[86] and sent them to Assyria. In 669 BC, Taharqa reoccupied Memphis and the Delta, and recommenced intrigues with the king of Tyre.[118]
Esarhaddon led his army to Egypt again and, after his death in 668 BC, command passed to Ashurbanipal. Ashurbanipal and the Assyrians defeated Taharqa again and advanced as far south as Thebes, but direct Assyrian control was not established.[118] The rebellion was stopped and Ashurbanipal appointed Necho I, who had been king of the city Sais, as his vassal ruler in Egypt. Necho's son, Psamtik I, was educated at the Assyrian capital of Nineveh during Esarhaddon's reign.[citation needed] As late as 665 BC, the vassal rulers of Sais, Mendes, and Pelusium were still making overtures[a] to Taharqa in Kush.[86] The vassals' plot was uncovered by Ashurbanipal and all rebels but Necho of Sais were executed.[86]
Taharqa's successor, Tantamani, sailed north from Napata with a large army to Thebes, where he was "ritually installed as the king of Egypt".[86]: 185  From Thebes, Tantamani began his reconquest and regained control of Egypt as far north as Memphis.[86]: 185 [118] Tantamani's dream stele states that he restored order from the chaos, where royal temples and cults were not being maintained.[86]: 185  After conquering Sais and killing Assyria's vassal, Necho I, in Memphis, "some local dynasts formally surrendered, while others withdrew to their fortresses".[86]: 185
The Kushites had influence over their northern neighbors for nearly 100 years until they were repelled by the invading Assyrians. The Assyrians installed the native 26th Dynasty of Egypt under Psamtik I and they permanently forced the Kushites out of Egypt around 590 BC.[130]: 121–122  The heirs of the Kushite empire established their new capital at Napata, which was also sacked by the Egyptians in 592 BC. The Kushite kingdom survived for another 900 years after being pushed south to Meroë. The Egyptianized culture of Nubia grew increasingly Africanized after the fall of the 25th Dynasty until Queen Amanishakhete acceded in 45 BC.[citation needed] She temporarily arrested the loss of Egyptian culture, but then it continued unchecked.[113]
Due to pressure from Assyrians and Egyptians, Meroë (800 BC – c. 350 AD) became the southern capital of the Kingdom of Kush.[86] According to partially deciphered Meroitic texts, the name of the city was Medewi or Bedewi. Meroë was in southern Nubia by the east bank of the Nile, about 6 km north-east of the Kabushiya station near Shendi, Sudan, and about 200 km northeast of Khartoum. Meroë is mentioned in first-century AD Periplus of the Erythraean Sea: "farther inland, in the country towards the west, there lies a city called Meroe". In fifth-century BC, Greek historian Herodotus described it as "a great city...said to be the mother city of the other Ethiopians."[131][132] Together, Musawwarat es-Sufra, Naqa, and Meroë formed the Island of Meroe.
The town's importance gradually increased from the beginning of the Meroitic Period, especially from the reign of Arakamani (c. 280 BC) when the royal burial ground was transferred to Meroë from Napata (Jebel Barkal). Excavations revealed evidence of important, high ranking Kushite burials, from the Napatan Period (c. 800 – c. 280 BC) in the vicinity of the settlement called the Western cemetery. They buried their kings in small pyramids with steeply sloped sides that were based on New Kingdom Viceroy designs.[105] At its peak, the rulers of Meroë controlled the Nile Valley over a north–south straight-line distance of more than 1,000 km (620 mi).[133]
People of the Meroitic period preserved many ancient Egyptian customs but were unique in many respects. The Meroitic language was spoken in Meroë and Sudan during the Meroitic period (attested from 300 BC) before becoming extinct around 400 AD. They developed their own form of writing by using Egyptian hieroglyphs before switching to a cursive alphabetic script with 23 signs.[134] It was split into two types: Meroitic Cursive, which was written with a stylus and used for general record-keeping; and Meroitic Hieroglyphic, which was carved in stone or used for royal or religious documents. It is not well understood due to the scarcity of bilingual texts.[clarification needed] The earliest inscription in Meroitic writing dates from between 180 and 170 BC. These hieroglyphics were found engraved on the temple of Queen Shanakdakhete. Meroitic Cursive is written horizontally, and is read from right to left like all Semitic orthographies.[135] The Meroitic people worshiped the Egyptian gods as well as their own, such as Apedemak and the lion-son of Sekhmet (or Bast).
Meroë was the base of a flourishing kingdom whose wealth was centered around a strong iron industry and international trade with India and China.[136] Metalworking is believed to have happened in Meroë, possibly through bloomeries and blast furnaces.[137] The centralized control of production within the Meroitic empire and distribution of certain crafts and manufactures may have been politically important. Other important sites were Musawwarat es-Sufra and Naqa. Musawwarat es-Sufra, which is now a UNESCO World Heritage Site, was constructed in sandstone. Its main features were the Great Enclosure, the Lion Temple of Apedemak (14×9×5 meters), and the Great Reservoir. The Great Enclosure is the main structure of the site. Much of the large labyrinth-like building complex, which covers approximately 45,000 m2, was erected in third-century BC.[138]
The scheme of the site is, so far, without parallel in Nubia and ancient Egypt. According to Hintze, "the complicated ground plan of this extensive complex of buildings is without parallel in the entire Nile valley".[139] The maze of courtyards includes three (possible) temples, passages, low walls that prevent any contact with the outside world, about 20 columns, ramps and two reservoirs.[140][141] There is some debate about the purpose of the buildings, with earlier suggestions including a college, a hospital, and an elephant-training camp.[142] The Lion Temple was constructed by Arnekhamani and bears inscriptions in Egyptian hieroglyphs, representations of elephants and lions on the rear inside wall, and reliefs of Apedemak depicted as a three-headed god on the outside walls.[143] The Great Reservoir is a hafir to retain as much as possible of the rainfall of the short, wet season. It is 250 m in diameter and 6.3 m deep.[144]
Kandake, often Latinised as Candace, was the Meroitic term for the sister of the king of Kush who, due to matrilineal succession, would bear the next heir, making her a queen mother. According to scholar Basil Davidson, at least four Kushite queens — Amanirenas, Amanishakheto, Nawidemak and Amanitore — probably spent part of their lives in Musawwarat es-Sufra.[145] Pliny writes that the "Queen of the Ethiopians" bore the title Candace, and indicates that the Ethiopians had conquered ancient Syria and the Mediterranean.[146] In 25 BC the Kush kandake Amanirenas, as reported by Strabo, attacked the city of Syene (known as Aswan today) within the territory of the Roman Empire; Emperor Augustus destroyed the city of Napata in retaliation.[147][148] In the New Testament biblical account, a treasury official of "Candace, queen of the Ethiopians", returning from a trip to Jerusalem, met with Philip the Evangelist and was baptized.[149][150]
The Achaemenids occupied the Kushan kingdom, possibly from the time of Cambyses (c. 530 BC), and more probably from the time of Darius I (550–486 BC), who mentions the conquest of Kush (Kušiya) in his inscriptions.[151][152]
Herodotus mentioned an invasion of Kush by the Achaemenid ruler Cambyses II, however, he mentions that "his expedition failed miserably in the desert".[118]: 65–66  Derek Welsby states "scholars have doubted that this Persian expedition ever took place, but... archaeological evidence suggests that the fortress of Dorginarti near the second cataract served as Persia's southern boundary."[118]: 65–66
The Greek Ptolemaic Kingdom under Ptolemy II Philadelphus invaded Nubia in 275 BC and annexed the northern twelve miles of this territory, subsequently known as the Dodekaschoinos ('twelve-mile land').[153] Throughout the 160s and 150s BC, Ptolemy VI has also reasserted Ptolemaic control over the northern part of Nubia.[154][155]
There is no record of conflict between the Kushites and Ptolemies. However, there was a serious revolt at the end of Ptolemy IV's reign and the Kushites likely tried to interfere in Ptolemaic affairs.[118]: 67  It is suggested that this led to Ptolemy V defacing the name of Arqamani on inscriptions at Philae.[118]: 67  "Arqamani constructed a small entrance hall to the temple built by Ptolemy IV at Pselchis and constructed a temple at Philae to which Ptolemy contributed an entrance hall."[118]: 66  There is evidence of Ptolemaic occupation as far south as the Second Cataract, but recent finds at Qasr Ibrim, such as "the total absence of Ptolemaic pottery", have cast doubts on the effectiveness of the occupation.[118]: 67  Dynastic struggles led to the Ptolemies abandoning the area, so "the Kushites reasserted their control...with Qasr Ibrim occupied" (by the Kushites) and other locations perhaps garrisoned.[118]: 67
According to Welsby, after the Romans assumed control of Egypt, they negotiated with the Kushites at Philae and drew the southern border of Roman Egypt at Aswan.[118]: 67  Theodore Mommsen and Welsby state the Kingdom of Kush became a client Kingdom, which was similar to the situation under Ptolemaic rule of Egypt. Kushite ambition and excessive Roman taxation are two theories for a revolt supported by Kushite armies.[118]: 67–68  The ancient historians, Strabo and Pliny, give accounts of the conflict with Roman Egypt.
Strabo describes a war with the Romans in first-century BC. He stated that the Kushites "sacked Aswan with an army of 30,000 men and destroyed imperial statues...at Philae."[118]: 68  A "fine over-life-size bronze head of the emperor Augustus" was found buried in Meroe in front of a temple.[118]: 68  After the initial victories of Kandake (or "Candace") Amanirenas against Roman Egypt, the Kushites were defeated and Napata was sacked.[156] Napata's fall was not a crippling blow to the Kushites and did not frighten Candace enough to prevent her from again engaging in combat with the Roman military. In 22 BC, a large Kushite force moved northward with the intention of attacking Qasr Ibrim.[157]
Alerted to the advance, Petronius again marched south and managed to reach Qasr Ibrim and bolster its defences before the invading Kushites arrived. Welsby states after a Kushite attack on Primis (Qasr Ibrim),[118]: 69–70  the Kushites sent ambassadors to negotiate a peace settlement with Petronius, which succeeded on favourable terms.[156] Trade between the two nations increased and the Roman Egyptian border being extended to "Hiera Sykaminos (Maharraqa)."[157]: 149 [118]: 70  This arrangement "guaranteed peace for most of the next 300 years" and there is "no definite evidence of further clashes."[118]: 70
During this time, different parts of the region divided into smaller groups with individual leaders (or generals), each commanding small armies of mercenaries. They fought for control of what is now Nubia and its surrounding territories, leaving the entire region weak and vulnerable to attack. Meroë would eventually be defeated by the new rising Kingdom of Aksum to their south ruled by King Ezana. A stele of Ge'ez of an unnamed ruler of Aksum thought to be Ezana was found at the site of Meroë. From his description, in Greek, he was "King of the Aksumites and the Omerites" (i.e. of Aksum and Himyar). It is likely this king ruled sometime around 330 AD. While some authorities interpret these inscriptions as proof that the Axumites destroyed the kingdom of Meroe, others note that archeological evidence points to an economic and political decline in Meroe around 300.[158] Moreover, some view the stele as military aid from Aksum to Meroe to quell the revolt and rebellion. From then on, the Romans referred to the area as Nobatia.
Around 350 AD, the area was invaded by the Kingdom of Aksum and the Meroitic kingdom collapsed. Three smaller Christian kingdoms replaced it: the northernmost was Nobatia (capital Pachoras; now modern-day Faras, Egypt) between the first and second cataract of the Nile River; in the middle was Makuria (capital Old Dongola), and southernmost was Alodia (capital Soba). King Silky of Nobatia defeated the Blemmyes and recorded his victory in a Greek language inscription carved in the wall of the temple of Talmis (modern Kalabsha) around 500 AD.
Christianity had been introduced to the region by the fourth century: Bishop Athanasius of Alexandria consecrated Marcus as bishop of Philae before his death in 373 AD. John of Ephesus records that a Miaphysite priest named Julian converted the king and his nobles of Nobatia around 545 AD. He also writes that the kingdom of Alodia was converted around 569. However, John of Biclarum wrote that the kingdom of Makuria converted to Catholicism the same year, suggesting that John of Ephesus might be mistaken. Further doubt is cast on John's[clarification needed] testimony by an entry in the chronicle of the Greek Orthodox Patriarch of Alexandria Eutychius of Alexandria, which states that in 719 AD the church of Nubia transferred its allegiance from the Greek to the Coptic Orthodox Church. After the official Christianization of Nubia, the Isis cult of Philae remained for the sake of the Nubians. The edict of Theodosius I (390 AD) was not enforced at Philae. Later attempts to suppress the cult of Isis led to armed clashes between the Nubians and Romans. Finally, in 453 AD, a treaty recognizing the traditional religious rights of Nubians at Philae was signed.
By the seventh century, Makuria expanded and became the dominant power in the region. It was strong enough to halt the southern expansion of Islam after the Arabs had taken Egypt. After several failed invasions the new Muslim rulers agreed to a treaty with Dongola, called Baqt, to allow peaceful coexistence and trade, contingent on the Nubians making an annual payment consisting of slaves and other tributes to the Islamic Governor at Aswan; it guaranteed that any runaway slaves were returned to Nubia.[159] The treaty was kept for six hundred years.[159] Throughout this period, Nubia's main exports were dates and slaves, though ivory and gold were also exchanged for Egyptian ceramics, textiles, and glass.[160] Over time the influx of Arab traders introduced Islam to Nubia and it gradually supplanted Christianity. After an interruption in the annual tribute of slaves, the Egyptian Mamluk ruler invaded in 1272 and declared himself sovereign over half of Nubia.[159] While there are records of a bishop Timothy at Qasr Ibrim in 1372, his see included Faras. It is also clear that the cathedral of Dongola had been converted to a mosque in 1317.[161]
The influx of Arabs and Nubians to Egypt and Sudan had contributed to the suppression of the Nubian identity following the collapse of the last Nubian kingdom around 1504. A vast majority of the Nubian population is currently Muslim, and the Arabic language is their main medium of communication in addition to their indigenous Nubian language. The unique characteristic of Nubian is shown in their culture (dress, dances, traditions, and music).
In the fourteenth century, the Dongolan government collapsed and the region was divided and dominated by Arabs. Several Arab invasions into the region and the establishment of smaller kingdoms occurred over the next few centuries. Northern Nubia was brought under Egyptian control, while the south was controlled by the Kingdom of Sennar in the sixteenth century. The entire region came under Egyptian control during Muhammad Ali's rule in the early nineteenth century, and later became a joint Anglo-Egyptian condominium.
The paleo-demography of Nubians from the upper paleolithic to late 16th century BC were analyzed by Aleksandra Pudlo. Mesolithic period inhabitants were characterized as robust and tall, with strong alveolar prognathism. During the Neolithic, Nubians were less robust and shorter, retaining some prognathism, but having facial shape changes and a narrower nasal index. Over a period of 8,500 years, the features had shifted a considerable degree. The variety of morphological forms which occurred was considered a result of the combination of two distinct traits, with Pudlo concluding: "Nubians were hardly a homogeneous population. Neither the climate nor the specific geographic conditions in the region they inhabited were conducive of such homogeneity." The populations were possibly influenced by migration waves coming from the north, but these movements did not prevent repeated contacts of the people of Nubia with other regions further south in Africa.[162]
Findings were recorded in 2016 on Nubian remains over a period of 11,000 years: "Taken together, our results suggest a dramatic shift in cranial morphology between the Mesolithic and the A-group cultural group, with little perceptible change in cranial shape between A-group and the later farming groups. In the case of the mandible, we observe the largest morphological change between the Mesolithic and the A-group, but also see morphological differentiation between the early farmers (A and C-group) and the later farming groups (Pharaonic and Meroitic specimens)." Changes were attributed to factors such as in situ adaptation or influxes of people, as well as migration of farmers. Nonetheless, authors concluded further studies with larger samples and a combination of morphometric analyses and ancient DNA are needed.[163]
In 2003, archaeologist Charles Bonnet led a team of Swiss archaeologists to excavate near Kerma and discovered a cache of monumental black granite statues of the Pharaohs of the 25th Dynasty of Egypt, now displayed at the Kerma Museum. Among the sculptures are ones belonging to the dynasty's last two pharaohs, Taharqa and Tanoutamon, whose statues are described as "masterpieces that rank among the greatest in art history".[164] Craniometric analysis of Kerma fossils that compared them to various other early populations inhabiting the Nile Valley and Maghreb found that they were morphologically close to Predynastic Egyptians from Naqada (4000–3200 BC).[165] Dental trait analysis of Kerma fossils found affinities with various populations inhabiting the Nile Valley, Horn of Africa, and Northeast Africa, especially to other ancient populations from the central and northern Sudan. Among the sampled populations, the Kerma people were overall nearest to the Kush populations in Upper Nubia, the A-Group culture bearers of Lower Nubia, and Ethiopians.[166]
Nubia was divided between Egypt and Sudan after colonialism ended and the Republic of Egypt was established in 1953, and the Republic of Sudan seceded from Egypt in 1956.
In the early-1970s, many Egyptian and Sudanese Nubians were forcibly relocated to make room for Lake Nasser after dams were constructed at Aswan.[167] Nubian villages can be found north of Aswan on the west bank of the Nile and on Elephantine Island. Many Nubians now live in large cities like Cairo and Khartoum.[167]
In 2014, a male infant skeleton was recovered during an excavation in what is present-day Wadi Halfa from the Christian Period (500-1400 C.E.), located near the Second Cataract of the Nile in the Republic of the Sudan. The results from the Principal component analysis (PCA) had the individual placed between African and European clusters. Furthermore, the individual was assigned to mitochondrial haplogroup L5a1a, a branch of the ancient L5 haplogroup with origins in East Africa.[168]
Another analysis in 2015 studied a Nubian individual from an archeological site in Kulubnarti. The geographic ancestry of the individual was estimated to be closer to Middle Eastern, and Central and South Asians, rather than to any African populations.[169]
Sirak et al. 2021 obtained and analyzed the whole  genomes of 66 individuals from the site of Kulubnarti situated in northern Nubia between the 2nd and 3rd cataract, near the modern Egyptian border, and dated to the Christian period between 650 and 1000 CE. The samples were obtained from two cemeteries. The samples' genetic profile was found to be a mixture between West Eurasian and Sub Saharan Dinka-related ancestries, with ~60% West Eurasian related ancestry that likely came from ancient Egyptians but ultimately resembles that found in Bronze or Iron Age Levantines, and ~40% Dinka-related ancestry.
The two cemeteries showed minimal differences in their West Eurasian/Dinka ancestry proportions. These findings in addition to multiple cross cemetery relatives that the analyses have revealed indicate that people of both the R and S cemeteries were part of the same population despite the archaeological and anthropological differences between the two burials showing social stratification.
Modern Nubians, despite their superficial resemblance to the Kulubnarti Nubians on the PCA, were not found to be descended from Kulubnarti Nubians without additional later admixtures. Modern Nubians were found to have an increase in Sub-Saharan ancestry along with a change in their west Eurasian ancestry from that which was found in the ancient samples.[170]
A sample from historic Lower Nubia, in the Nubian site of Sayala during the 3rd-6th Century AD, belonged to mtDNA haplogroup J1c.[171]
In 2022, DNA was sequenced from the hair of a Kerma period individual (4000 BP), and the results revealed close genetic affinity to early pastoralists from the Rift Valley in eastern Africa during the Pastoral Neolithic.[172]
Media related to Nubia at Wikimedia Commons

This is a list of Neolithic cultures of China that have been unearthed by archaeologists. They are sorted in chronological order from earliest to latest and are followed by a schematic visualization of these cultures.
It would seem that the definition of Neolithic in China is undergoing changes. The discovery in 2012 of pottery about 20,000 years BC indicates that this measure alone can no longer be used to define the period.[1] It will fall to the more difficult task of determining when cereal domestication started.
These cultures existed during the period from 8500 to 1500 BC. Neolithic cultures remain unmarked and Bronze Age cultures (from 2000 BC) are marked with *. There are many differences in opinion on the dating for these cultures, so the dates chosen here are tentative:
For this schematic outline of its neolithic cultures China has been divided into the following nine parts:

This is a descriptive list of Stone Age art, the period of prehistory characterised by the widespread use of stone tools. This article contains, by sheer volume of the artwork discovered, a very incomplete list of the works of the painters, sculptors, and other artists who created what is now called prehistoric art. For fuller lists see Art of the Upper Paleolithic, Art of the Middle Paleolithic, and Category:Prehistoric art and its many sub-categories.
The oldest undisputed figurative art appears with the Aurignacian, about 40,000 years ago, which is associated with the earliest presence of Cro-Magnon artists in Europe. Figurines with date estimates of 40,000 years are the so-called Lion-man and Venus of Hohle Fels, both found in the Southern Germany caves of the Swabian Jura.
The Gravettian spans the Last Glacial Maximum, ca. 33–21 kya. The Solutrean (c. 22–17 kya) may or may not be included as the final phase of the Gravettian.
Australia and parts of Southeast Asia remained in the Paleolithic stage until European contact. 
The oldest firmly dated rock-art painting in Australia is a charcoal drawing on a rock fragment found during the excavation of the Nawarla Gabarnmang rock shelter in south western Arnhem Land in the Northern Territory. Dated at 28,000 years, it is one of the oldest known pieces of rock art on Earth with a confirmed date.

The Ahmarian culture[1][2][3][4][5][6] was a Paleolithic archeological industry in the Levant dated at 46,000–42,000 years before present (BP) and thought to be related to Levantine Emiran and younger European Aurignacian cultures.
The word "Ahmarian" was adopted from the archaeological site of Erq el-Ahmar (also written Erk el Ahmar), West Bank, Palestine, a rockshelter in the Judean Desert in the northern Dead Sea Rift.[7] It was explored and excavated by French Prehistorian René Neuville in 1951.[8] The "Ahmarian" category had only been recognized since the 1980s, and was previously designated as "Phase II Upper Paleolithic" or "Ksar Akil Phase B".[9][10]
The Ahmarian period together with the Emiran period, both from the Levant, are among the first periods of the Upper Paleolithic, corresponding to the first stages of the expansion of Homo sapiens out of Africa. From this stage, the first modern humans migrated to Europe to form the beginning of the European Upper Paleolithic, including the Aurignacian culture, where they become known as the Cro-Magnons.[11]
The European Bohunician culture, probably linked to the Emiran and Ahmarian, may slightly predate the Ahmarian at 48,000 BP.[12] There is also a claim that it is roughly contemporary with the Aurignacian and the Gravettian cultures of Europe, all emerging prior to the Atlitian, which was also contemporary with the Solutrean and Magdalenian cultures of Western Europe.[13]
Ahmarian technology, which included the complex of blade/bladelet-knapping techniques is also linked to the tools used by the hunter-gatherers of southwestern Asia.[14]
Late Ahmarian is called Masraqan.[15]
Ahmarian blades are usually elongated with some curves.[8] The Levallois technique is still in use, but only sparsely, thereby making Ahmarian the first fully Upper Paleolithic period.[8]
Ahmarian assemblages can be found throughout the Levant, including Syria, Lebanon, Israel, Palestine, and Jordan.[8] The Lagaman industry in the Sinai can be considered as derivative to the Ahmarian culture.[8]
"Levantine Aurignacian", from the Levant, is a type of blade technology very similar to the European Aurignacian, immediately following chronologically the Emiran and Early Ahmarian in the same area of the Near East, and closely related to them.[8]
Fertile Crescent:
Europe:
Africa:
Siberia:

The Proto-Afroasiatic homeland is the hypothetical place where speakers of the Proto-Afroasiatic language lived in a single linguistic community, or complex of communities, before this original language dispersed geographically and divided into separate distinct languages. Afroasiatic languages are today mostly distributed in parts of  Western Asia and North Africa.
The contemporary Afroasiatic languages are spoken in West Asia, North Africa, the Horn of Africa, parts of the Sahara and Sahel, and Malta in Europe. The various hypotheses for the Afroasiatic homeland are distributed throughout this territory;[1][2][3][4] that is, it is generally assumed that proto-Afroasiatic was spoken in some region where Afroasiatic languages are still spoken today, although this is not unanimous. However, there is disagreement as to which part of the contemporary Afroasiatic speaking areas corresponds with the original homeland. The majority of scholars today contend that Afroasiatic languages arose somewhere in Northeast Africa or Western Asia[5]
There is no consensus as to when Proto-Afroasiatic was spoken.[6] The absolute latest date for when Proto-Afroasiatic could have been extant is c. 4000 BC, after which Egyptian and the Semitic languages are firmly attested. However, in all likelihood these languages began to diverge well before this hard boundary.[7] The estimations offered by scholars as to when Proto-Afroasiatic was spoken vary widely, ranging from 18,000 BC to 8,000 BC. According to Igor M. Diakonoff (1988: 33n), proto-Afroasiatic was spoken c. 10,000 BC. According to Christopher Ehret (2002: 35–36), proto-Afroasiatic was spoken c. 11,000 BC at the latest, and possibly as early as c. 16,000 BC. These dates are older than dates associated with most other proto-languages.[6] An estimate at the youngest end of this range still makes Afroasiatic the oldest proven language family.[8] Contrasting proposals of an early emergence, Tom Güldemann has argued that less time may have been required for the divergence than is usually assumed, as it is possible for a language to rapidly restructure due to areal contact, with the evolution of Chadic (and likely also Omotic) serving as pertinent examples.[9]
No consensus exists as to where proto-Afroasiatic originated. Scholars have proposed locations for the Afroasiatic homeland across parts of Africa and western Asia.[6][10] A complicating factor is the lack of agreement on the subgroupings of Afroasiatic (see Further subdivisions) – this makes associating archaeological evidence with the spread of Afroasiatic particularly difficult.[11] Nevertheless, there is a long-accepted link between the speakers of Proto-Southern Cushitic languages and the East African Savanna Pastoral Neolithic (3000 BC), and archaeological evidence associates the Proto-Cushitic speakers with economic transformations in the Sahara dating c. 8,500 years ago, as well as the speakers of the Proto-Zenati variety of the Berber languages with an expansion across the Maghreb in the 5th century AD.[12] More hypothetical links associate the proto-Afroasiatic-speakers with the Kebaran and the Mushabian culture,[13] while others argue for a possible affiliation between proto-Afroasiatic and the Natufian culture.[6][14][15][16]
The linguistic view on the location of the homeland of Afroasiatic languages is largely divided into proponents for a homeland within Northeast Africa, and proponents for a homeland in Western Asia. A homeland within Northeast Africa is favored by some scholars, while other scholars support a homeland in western Asia.[6][7]
Pagani and Crevecoeur (2019) argue that, given the still open debate on the origin of Afroasiatic, the consensus will probably settle on an intermediate "across-the-Sinai" solution. They also note that the very early interactions between North African and Eurasian cultures, point "to a geographical shrinking of what can currently be defined as 'strictly African' in a long term perspective."[17]
Supporters of a western Asian origin for Afroasiatic are particularly common among those with a background in Semitic or Egyptological studies,[11] and amongst archaeological proponents of the "farming/language dispersal hypothesis" according to which major language groups dispersed with early farming technology in the Neolithic, the birthplace of the Neolithic Revolution being in West Asia.[18][19] The leading linguistic proponent of this idea in recent times is Alexander Militarev, who argues that Proto-Afroasiatic was spoken by early agriculturalists in the Levant, Mesopotamia and Anatolia and subsequently spread to North Africa and Horn of Africa. Militarev associates the speakers of Proto-Afroasiatic with the Levantine Post-Natufian Culture, arguing that the reconstructed lexicon of flora and fauna, as well as farming and pastoralist vocabulary indicates that Proto-AA must have been spoken in this area. Scholar Jared Diamond and archaeologist Peter Bellwood have taken up Militarev's arguments as part of their general argument that the spread of linguistic macrofamilies (such as Afroasiatic, Bantu, and Austroasiatic) can be associated with the development of agriculture; they argue that there is clear archaeological support for farming and associated tools and domesticated animals spreading from the Levant into Africa via the Nile valley.[14][16]
Militarev, who linked proto-Afroasiatic to the Levantine Natufian culture, that preceded the spread of farming technology, believes the language family to be about 10,000 years old. He wrote (Militarev 2002, p. 135) that the "Proto-Afrasian language, on the verge of a split into daughter languages", meaning, in his scenario, into "Cushitic, Omotic, Egyptian, Semitic and Chadic-Berber", "should be roughly dated to the ninth millennium BC". Support for the migration of agricultural populations, according to linguists, are the word for dog (an Asian domesticate) reconstructed to Proto-Afroasiatic[20] as well as words for bow and arrow,[21] which according to some archaeologists spread rapidly across North Africa once they were introduced to North Africa from the Near East, viz. Ounan points.[22]
Lexicon linked to a pastoralist society (cattle-breeding) reconstructed for proto-Afroasiatic also support a western Asian homeland, possibly indicating an earlier pastoralist migration.[23]
A Northeast African homeland has been proposed by linguists as the origin of the language group largely because it includes the geographic center of its present distribution and the majority of the diversity observed among the Afroasiatic language family, sometimes considered a telltale sign for a linguistic geographic origin.[5][25] Within this hypothesis there are a number of competing variants:
Christopher Ehret has proposed the western Red Sea coast from Eritrea to southeastern Egypt. While Ehret disputes Militarev's proposal that Proto-Afroasiatic shows signs of a common farming lexicon, he suggests that early Afroasiatic languages were involved in the even earlier development of intensive food collection in the areas of Ethiopia and Sudan. In other words, he proposes an even older age for Afroasiatic than Militarev, at least 11,000 years old, and believes farming lexicon can only be reconstructed for branches of Afroasiatic. Ehret argues that Proto-Afroasiatic speakers in Northeast Africa developed subsistence patterns of intensive plant collection and pastoralism, giving the population an economic advantage which impelled the expansion of the Afroasiatic languages. He suggests that a Proto-Semitic or Proto-Semito-Berber-speaking population migrated from Northeast Africa to the Levant during the late Paleolithic.[26][27][28][29]
In the next phase, unlike many other authors Ehret proposed an initial split between northern, southern and Omotic. The northern group includes Semitic, Egyptian and Berber (agreeing with others such as Diakonoff). He proposed that Chadic stems from Berber (some other authors group it with southern Afroasiatic languages such as Cushitic ones).
Roger Blench has proposed a region in the adjacent Horn of Africa, specifically in modern day Ethiopia, arguing that Omotic represents the most basal branch and displays high diversity.[1] Others have however pointed out that Omotic displays strong signs of contact with non-Afroasiatic languages, with some arguing that Omotic should be regarded as an independent language family.[30][31] Like Ehret, Blench accepts that Omotic is part of the Afroasiatic grouping and sees the split of northern languages from Omotic as an important early development. Güldemann (2018) does not accept Omotic as unified group, but argues for at least four distinct groupings.[9]
Igor Diakonoff proposed the Eastern Saharan region, specifically the southern fringe of the Sahara as possible location of the Afroasiatic homeland.[3][32] Lionel Bender proposed the area near Khartoum, Sudan, at the confluence of the Blue Nile and White Nile.[3][32] The details of his theory are widely cited but controversial, as it involves the proposal that Semitic originated in Ethiopia and crossed to Asia directly from there over the Red Sea.[33]
Scholars, such as Hodgson et al., present archaeogenetic evidence in favor for a place of dispersion within Africa, but argue that the speakers of Proto-Afroasiatic can ultimately be linked to a Paleolithic and pre-agricultural migration wave into Africa from Western Asia, and that the Semitic-branch represents a later back-migration to the Levant.[34]
According to an autosomal DNA research in 2014 on ancient and modern populations, the Afroasiatic languages likely spread across Africa and the Near East by an ancestral population(s) carrying a newly identified "non-African" (Western Eurasian) genetic component, which the researchers dub the "Ethio-Somali" component. This genetic component is most closely related to the "Maghrebi" component and is believed to have diverged from other "non-African" (Western Eurasian) ancestries at least 23,000 years ago. The "Ethio-Somali" genetic component is prevalent among modern Afroasiatic-speaking populations, and found at its highest levels among Cushitic peoples in the Horn of Africa. On this basis, the researchers suggest that the original Ethio-Somali carrying population(s) probably arrived in the pre-agricultural period (12–23 ka) from the Near East, having crossed over into northeast Africa via the Sinai Peninsula and then split into two, with one branch continuing west across North Africa and the other heading south into the Horn of Africa. They suggest that a descendant population migrated back to the Levant prior to 4000 BC and developed the Semitic branch of Afroasiatic. Later migration from Arabia into the HOA beginning around 3 ka would explain the origin of the Ethio-Semitic languages at this time.[37] A similar view has already been proposed earlier, suggesting that the ancestors of Afroasiatic speakers could have been a population originating in the Near East that migrated to Northeast Africa during the Late Palaeolithic with a subset later moving back to the Near East.[38]
Subsequent archaeogenetic studies have corroborated the migrations of Western Eurasian ancestry during the Paleolithic into Africa, becoming the dominant component of Northern Africa since at least 15,000 BC. The "Maghrebi" component, which gave rise to the Iberomaurusian culture, is described as autochthonous to Northern Africa, related to the Paleolithic Eurasian migration wave, and the characteristic ancestry components of modern Northern Africans along a West-to-East cline, with Northeastern Africans having an additionally higher frequency of a Neolithic Western Asian component associated with the Neolithic expansion.[39]
Genetic research on Afroasiatic-speaking populations revealed strong correlation between the distribution of Afroasiatic languages and the frequency of Northern African/Natufian/Arabian-like ancestry. In contrast, Omotic speakers display ancestry mostly distinct from other Afroasiatic-speakers, indicating language shift, or support for the exclusion of Omotic from the Afroasiatic group.[40]
Genetic studies on a specimen of the Savanna Pastoral Neolithic excavated at the Luxmanda site in Tanzania, which has been associated with migrations of Cushitic-speaking peoples and the spread of pastoralism, found that the specimen carried a large proportion of ancestry related to the Pre-Pottery Neolithic culture of the Levant (Natufian), similar to that borne by modern Afroasiatic-speaking populations inhabiting the Horn of Africa. It is suggested that a population related to the Pre-Pottery Neolithic culture of the Levant contributed significantly to historical Eastern African populations represented by the c. 5,000 year old Luxmanda specimen, while modern Cushitic-speaking populations have additional contributions from Dinka-related and "Neolithic Iranian-related" sources. This type of ancestry was later partially replaced by following migration events associated with the Bantu expansion, with Bantu-speaking Eastern Africans having only little ancestry associated with the Pre-Pottery Neolithic culture of the Levant.[41][42]
Keita (2008) examined a published Y-chromosome dataset on Afro-Asiatic populations and found that a key lineage E-M35/E-M78, sub-clade of haplogroup E, was shared between the populations in the locale of Egyptian and Libyan speakers and modern Cushitic speakers from the Horn. These lineages are present in Egyptians, Berbers, Cushitic speakers from the Horn of Africa, and Semitic speakers in the Near-East. He noted that variants are also found in the Aegean and Balkans, but the origin of the M35 subclade was in Egypt or Libya, and its clades were dominant in a core portion of Afro-Asiatic speaking populations which included Cushitic, Egyptian and Berber groups, in contrast Semitic speakers showed a decline in frequency going west to east in the Levantine-Syria region. Keita identified high frequencies of M35 (>50%) among Omotic populations, but stated that this derived from a small, published sample of 12. Keita also wrote that the PN2 mutation was shared by M35 and M2 lineages and this paternal clade originated from East Africa. He concluded that "the genetic data give population profiles that clearly indicate males of African origin, as opposed to being of Asian or European descent" but acknowledged that the biodiversity does not indicate any specific set of skin colors or facial features as populations were subject to microevolutionary pressures.[43]
Fregel summarized that the Y-chromosome diversity of North Africans was compatible with a demic expansion from the Middle East, because the age of common lineages in North Africa (E-M78 and J-304) were relatively recent. The North African pattern of Y-chromosome variation was mostly shaped during the Neolithic period.[44]
Ehret cited genetic evidence which had identified the Horn of Africa as a source of a genetic marker “M35/215” Y-chromosome lineage for a significant population component which moved north from that region into Egypt and the Levant. Ehret argued that this genetic distribution paralleled the spread of the Afrasian language family with the movement of people from the Horn of Africa into Egypt and added a new demic component to the existing population of Egypt 17,000 years ago.[45]

Human migration is the movement by people from one place to another, particularly different countries, with the intention of settling temporarily or permanently in the new location. It typically involves movements over long distances and from one country or region to another. The number of people involved in every wave of immigration differs depending on the specific circumstances.
Historically, early human migration includes the peopling of the world, i.e. migration to world regions where there was previously no human habitation, during the Upper Paleolithic. Since the Neolithic, most migrations (except for the peopling of remote regions such as the Arctic or the Pacific), were predominantly warlike, consisting of conquest or Landnahme on the part of expanding populations.[citation needed] Colonialism involves expansion of sedentary populations into previously only sparsely settled territories or territories with no permanent settlements. In the modern period, human migration has primarily taken the form of migration within and between existing sovereign states, either controlled (legal immigration) or uncontrolled and in violation of immigration laws (illegal immigration).
Migration can be voluntary or involuntary. Involuntary migration includes forced displacement (in various forms such as deportation, the slave trade, flight (war refugees and ethnic cleansing), all of which could result in the creation of diasporas.
Studies show that the pre-modern migration of human populations begins with the movement of Homo erectus out of Africa across Eurasia about 1.75 million years ago. Homo sapiens appeared to have occupied all of Africa about 150,000 years ago; some members of this species moved out of Africa 70,000 years ago (or, according to more recent studies, as early as 125,000 years ago into Asia,[1][2] and even as early as 270,000 years ago).[3][4] It is suggested that modern non-African populations descend mostly from a later migration out of Africa between 70,000 and 50,000 years ago,[5][6] which spread across Australia, Asia and Europe by 40,000 BCE. Migration to the Americas took place 20,000 to 15,000 years ago. West-Eurasian back-migrations into Africa happened between 30,000 to 15,000 years ago, as well as pre-Neolithic and Neolithic back-migrations, followed by the Arab expansion in medieval times.
By 2000 years ago humans had established settlements in most of the Pacific Islands. Major population-movements notably include those postulated as associated with the Neolithic Revolution and with Indo-European expansion. The Early Medieval Great Migrations including Turkic expansion have left significant traces. In some places, such as Turkey and Azerbaijan, there was a substantial cultural transformation after the migration of relatively small elite populations.[8] Historians see elite-migration parallels in the Roman and Norman conquests of Britain, while "the most hotly debated of all the British cultural transitions is the role of migration in the relatively sudden and drastic change from Romano-Britain to Anglo-Saxon Britain", which may be explained by a possible "substantial migration of Anglo-Saxon Y chromosomes into England (contributing 50–100% to the gene pool at that time)."[9]
Early humans migrated due to many factors, such as changing climate and landscape and inadequate food-supply for the levels of population. The evidence indicates that the ancestors of the Austronesian peoples spread from the South Chinese mainland to the island of Taiwan around 8,000 years ago. Evidence from historical linguistics suggests that seafaring peoples migrated from Taiwan, perhaps in distinct waves separated by millennia, to the entire region encompassed by the Austronesian languages. Scholars believe that this migration began around 6,000 years ago.[10] Indo-Aryan migration from the Indus Valley to the plain of the River Ganges in Northern India is presumed[by whom?] to have taken place in the Middle to Late Bronze Age, contemporary with the Late Harappan phase in India (around 1700 to 1300 BCE). From 180 BCE a series of invasions from Central Asia followed in the northwestern Indian subcontinent, including those led by the Indo-Greeks, Indo-Scythians, Indo-Parthians and Kushans.[11][12][13]
From 728 BCE, the Greeks began 250 years of expansion, settling colonies in several places, including Sicily and Marseille. Classical-era Europe provides evidence of two major migration movements: the Celtic peoples in the first millennium BCE, and the later Migration Period of the first millennium CE from the North and East. A smaller migration (or sub-migration) involved the Magyars moving into Pannonia (modern-day Hungary) in the 9th century CE. Turkic peoples spread from their homeland in modern Turkestan across most of Central Asia into Europe and the Middle East between the 6th and 11th centuries CE. Recent research suggests that Madagascar was uninhabited until Austronesian seafarers from present-day Indonesia arrived during the 5th and 6th centuries CE. Subsequent migrations both from the Pacific and from Africa further consolidated this original mixture[which?], and Malagasy people emerged.[14]
Before the expansion of the Bantu languages and their speakers, the southern half of Africa is believed[by whom?] to have been populated by Pygmies and Khoisan-speaking people, whose descendants today occupy the arid regions around the Kalahari Desert and the forests of Central Africa. By about 1000 CE Bantu migration had reached modern-day Zimbabwe and South Africa. The Banu Hilal and Banu Ma'qil, a collection of Arab Bedouin tribes from the Arabian Peninsula, migrated westwards via Egypt between the 11th and 13th centuries. Their migration strongly contributed to the Arabisation and Islamisation of the western Maghreb, until then dominated by Berber tribes. Ostsiedlung was the medieval eastward migration and settlement of Germans – following in the footsteps of East Germanic Goths and North Germanic Varangians. The 13th century was the time of the great Mongol and Turkic migrations across Eurasia,[15] where the Eurasian steppe has time and again provided a ready migration-path – for (for example) Huns, Bulgars, Tatars and Slavs.
Between the 11th and 18th centuries, numerous migrations took place in Asia. The Vatsayan Priests migrated from the eastern Himalaya hills to Kashmir during the Shan invasion in the 13th century. They settled in the lower Shivalik Hills in the 13th century to sanctify the manifest goddess.[clarification needed] In the Ming occupation, the Vietnamese started expanding southward in the 11th century; this is known in Vietnamese as nam tiến (southward expansion).[16] The early Qing dynasty (1644-1912) separated Manchuria from China proper with the Inner Willow Palisade, which restricted the movement of the Han Chinese into Manchuria, as the area was off-limits to the Han until the Qing started colonizing the area with them (late 18th century) later on in the dynasty's rule.[17]
The Age of Exploration and European colonialism has led to an accelerated pace of migration since Early Modern times. In the 16th century, perhaps 240,000 Europeans entered American ports.[18] In the 19th century over 50 million people left Europe for the Americas alone.[19] The local populations or tribes, such as the Aboriginal people in Canada, Brazil, Argentina, Australia, and the United States, were often numerically overwhelmed by incoming settlers and by those settlers' indentured laborers and imported slaves.
When the pace of migration had accelerated since the 18th century already (including the involuntary slave trade), it would increase further in the 19th century. Manning distinguishes three major types of migration: labor migration, refugee migrations, and urbanization. Millions of agricultural workers left the countryside and moved to the cities causing unprecedented levels of urbanization. This phenomenon began in Britain in the late 18th century and spread around the world and continues to this day in many areas.
Industrialization encouraged migration wherever it appeared. The increasingly global economy globalized the labor market. The Atlantic slave trade diminished sharply after 1820, which gave rise to self-bound contract labor migration from Europe and Asia to plantations. Overcrowding, open agricultural frontiers, and rising industrial centers attracted voluntary migrants. Moreover, migration was significantly made easier by improved transportation techniques.
Romantic nationalism also rose in the 19th century, and, with it, ethnocentrism. The great European industrial empires also rose. Both factors contributed to migration, as some countries favored their own ethnicity over outsiders and other countries appeared to be considerably more welcoming. For example, the Russian Empire identified with Eastern Orthodoxy, and confined Jews, who were not Eastern Orthodox, to the Pale of Settlement and imposed restrictions. Violence was also a problem. The United States was promoted as a better location, a "golden land" where Jews could live more openly.[20] Another effect of imperialism, colonialism, led to the migration of some colonizing parties from "home countries" to "the colonies", and eventually the migration of people from "colonies" to "home countries".[21]
Transnational labor migration reached a peak of three million migrants per year in the early twentieth century. Italy, Norway, Ireland and the Guangdong region of China were regions with especially high emigration rates during these years.  These large migration flows influenced the process of nation state formation in many ways. Immigration restrictions have been developed, as well as diaspora cultures and myths that reflect the importance of migration to the foundation of certain nations, like the American melting pot. The transnational labor migration fell to a lower level from the 1930s to the 1960s and then rebounded.
The United States experienced considerable internal migration related to industrialization, including its African American population.
From 1910 to 1970, approximately 7 million African Americans migrated from the rural Southern United States, where black people faced both poor economic opportunities and considerable political and social prejudice, to the industrial cities of the Northeast, Midwest and West, where relatively well-paid jobs were available.[22] This phenomenon came to be known in the United States as its own Great Migration, although historians today consider the migration to have two distinct phases. The term "Great Migration", without a qualifier, is now most often used to refer the first phase, which ended roughly at the time of the Great Depression.
The second phase, lasting roughly from the start of U.S. involvement in World War II to 1970, is now called the Second Great Migration. With the demise of legalised segregation in the 1960s and greatly improved economic opportunities in the South in the subsequent decades, millions of blacks have returned to the South from other parts of the country since 1980 in what has been called the New Great Migration.
The First and Second World Wars, and wars, genocides, and crises sparked by them, had an enormous impact on migration. Muslims moved from the Balkan to Turkey, while Christians moved the other way, during the collapse of the Ottoman Empire. I Four hundred thousand Jews had already moved to Palestine in the early twentieth century, and numerous Jews to America, as already mentioned. The Russian Civil War caused some three million Russians, Poles, and Germans to migrate out of the new Soviet Union. Decolonization following the Second World War also caused migrations.[23][24]
The Jewish communities across Europe, the Mediterranean and the Middle East were formed from voluntary and involuntary migrants. After the Holocaust (1938 to 1945), there was increased migration to the British Mandate of Palestine, which became the modern state of Israel as a result of the United Nations Partition Plan for Palestine.
Provisions of the Potsdam Agreement from 1945 signed by victorious Western Allies and the Soviet Union led to one of the largest European migrations, and the largest in the 20th century. It involved the migration and resettlement of close to or over 20 million people. The largest affected group were 16.5 million Germans expelled from Eastern Europe westwards. The second largest group were Poles, millions of whom were expelled westwards from eastern Kresy region and resettled in the so-called Recovered Territories (see Allies decide Polish border in the article on the Oder-Neisse line). Hundreds of thousands of Poles, Ukrainians (Operation Vistula), Lithuanians, Latvians, Estonians and some Belarusians were expelled eastwards from Europe to the Soviet Union. Finally, many of the several hundred thousand Jews remaining in Eastern Europe after the Holocaust migrated outside Europe to Israel and the United States.
In 1947, upon the Partition of India, large populations moved from India to Pakistan and vice versa, depending on their religious beliefs. The partition was created by the Indian Independence Act 1947 as a result of the dissolution of the British Indian Empire. The partition displaced up to 17 million people in the former British Indian Empire,[25] with estimates of loss of life varying from several hundred thousand to a million.[26] Muslim residents of the former British India migrated to Pakistan (including East Pakistan, now Bangladesh), whilst Hindu and Sikh residents of Pakistan and Hindu residents of East Pakistan (now Bangladesh) moved in the opposite direction.
In modern India, estimates based on industry sectors mainly employing migrants suggest that there are around 100 million circular migrants in India. Caste, social networks and historical precedents play a powerful role in shaping patterns of migration.
Research by the Overseas Development Institute identifies a rapid movement of labor from slower- to faster-growing parts of the economy. Migrants can often find themselves excluded by urban housing policies, and migrant support initiatives are needed to give workers improved access to market information, certification of identity, housing and education.[27]
In the riots which preceded the partition in the Punjab region, between 200,000 and 500,000 people were killed in the retributive genocide.[28][29] U.N.H.C.R. estimates 14 million Hindus, Sikhs and Muslims were displaced during the partition.[30] Scholars call it the largest mass migration in human history:[31] Nigel Smith, in his book Pakistan: History, Culture, and Government, calls it "history's greatest migration."[25]
Books
Journals
Online Books

The Capsian was an Epipalaeolithic tradition in Algeria and Tunisia from ca. 9000 to 5400 cal BC.[1] It is named after the town of Gafsa, Tunisia (Capsa in Latin).
The Capsian is traditionally divided into the Typical Capsian and the Upper Capsian, which are sometimes found in chronostratigraphic sequence. In terms of lithics, the differences between these divisions are both typological and technological.[2][3][4]
During this period, the environment of the Maghreb was open savanna, much like modern East Africa, with Mediterranean forests at higher altitudes;[5] where the initial phase overlaps with the African humid period.[6] The Capsian diet included a wide variety of animals, ranging from aurochs and hartebeest to hares and snails; there is little evidence concerning plants eaten.[7][8] During the succeeding Neolithic of Capsian Tradition, there is evidence from one site, for domesticated, probably imported, ovicaprids.[9]
Given the Capsian culture's timescale, widespread occurrence in the Sahara, and geographic association with modern speakers of the Afroasiatic languages, historical linguists have tentatively associated the industry with the Afroasiatic family's earliest speakers on the continent.[10]
Decorative art can be found at Capsian sites, including figurative and abstract rock art. Ochre is found on both tools and corpses. Ostrich eggshells were used to make beads and containers; seashells were used for necklaces. The Iberomaurusian practice of extracting the central incisors continued sporadically but became rarer.
Anatomically, Capsian populations were modern Homo sapiens, traditionally classed into two variegate types: Proto-Mediterranean and Mechta-Afalou on the basis of cranial morphology and anthropological traits. Some have argued that they were associated with Mediterranean immigrants from the east such as the Natufians/Pre-Pottery Neolithic,[11] whereas others argue for a population continuity based on physical skeletal characteristics and other criteria.[12][7][13] In 1950, 3 skulls from the Upper Capsian of the Maghreb were measured, and based on indicators of the craniofacial form, considered to have been mixed in traits. The overall anthropological investigation highlighted that their dominant characteristics were conforming to a Mediterranean type, while the minority characteristics conformed to Mechta-Afalou (Iberomarusian) and "Negroid" type. It was suggested that this population was the product of Pre-Neolithic Mectha-Afalous, "White" immigrants from the east, and African migrants from the south.[14]
The Eburran industry which dates between 13,000 and 9,000 BC[is this date calibrated?] in East Africa, was formerly known as the "Kenya Capsian" due to similarities in the stone blade shapes.
Recent genetic studies have further illuminated the origins and diversity of Capsian populations. In 2025, a study by researchers from Harvard University analyzed the DNA of nine late Stone Age individuals from Tunisia and Algeria.
[15] The findings suggest that the inhabitants of North Africa during this period were predominantly of local North African origin, resembling the Stone Age populations from Taforalt and Ifri N'Ammar in Morocco. This indicates a broad geographical and temporal distribution of a distinctive genetic component in the region.
Some of these genomes had contributions from European farmers (~7,000 BP) and Levantine groups (~6,800 BP). Moreover, one sample from Djebba, Tunisia, revealed European hunter-gatherer ancestry dating back to around ~8,000 BP, likely due to human migrations across the Sicilian Straits. Other samples from the Greater East of Morocco demonstrated minimal genetic contributions from European farmers or Eastern groups, reflecting a relatively isolated genetic profile compared to southern Europe and other parts of the Mediterranean.
In terms of paternal haplogroups, the study identified the following lineages among the analyzed individuals:
The majority belonged to Haplogroup E-M215
Two specimens carried the E-Z1902 lineage (a subclade of E-V65).
One sample belonged to the E-M78 haplogroup.
Two individuals were assigned to the Haplogroup T-M184 on distinct branches.
Maternal haplogroups included:
Two individuals with U6a.
Two with U6d.
One with U6b.
Two with R.
One with U5b.
One with L3f1b-a.
These findings align with earlier genetic data from Taforalt and Ifri n'Amr Ou Moussa caves in Morocco. At Taforalt, all samples belonged to the E-M78* haplogroup, while samples from Ifri n'Amr Ou Moussa included E-L19 and a unique E-PF2545 subclade within the E-M81 lineage.[16]

This glossary of history is a list of definitions of terms and concepts relevant to the study of history and its related fields and sub-disciplines, including both prehistory and the period of human history.
Also eon.
Also called the Age of Exploration.
Also antiquity.
Also antiquary.
Also artefact.
Also ancillary sciences of history.
Also Beringia.
Also sometimes classical era, classical period, or classical civilization.
Also called Cimmeria.
Variously abbreviated c., ca., circ., or cca.
Also called classical studies.
(pl.) codices
Also referred to simply as the Western Interior Seaway.
Also called pseudo-folklore.
Also impress, heraldic badge, livery badge, personal device, and cognizance.
(pl.) interregna
Also called landscape archaeology.
Also called the medieval period.
Also called the modern period or modernity.
Also onomatology.
Also palaeography.
Also spelled Pangea.
Also called the Panthalassa.
Also called history from below.
Also pre-literary history.
Also called Angaraland, Angara or Angarida.
Also simply called war.

Human evolutionary genetics studies how one human genome differs from another human genome, the evolutionary past that gave rise to the human genome, and its current effects. Differences between genomes have anthropological, medical, historical and forensic implications and applications. Genetic data can provide important insights into human evolution.
Biologists classify humans, along with only a few other species, as great apes (species in the family Hominidae). The living Hominidae include two distinct species of chimpanzee (the bonobo, Pan paniscus, and the chimpanzee, Pan troglodytes), two species of gorilla (the western gorilla, Gorilla gorilla, and the eastern gorilla, Gorilla graueri), and two species of orangutan (the Bornean orangutan, Pongo pygmaeus, and the Sumatran orangutan, Pongo abelii). The great apes with the family Hylobatidae of gibbons form the superfamily Hominoidea of apes.
Apes, in turn, belong to the primate order (>400 species), along with the Old World monkeys, the New World monkeys, and others. Data from both mitochondrial DNA (mtDNA) and nuclear DNA (nDNA) indicate that primates belong to the group of Euarchontoglires, together with Rodentia, Lagomorpha, Dermoptera, and Scandentia.[1] This is further supported by Alu-like short interspersed nuclear elements (SINEs) which have been found only in members of the Euarchontoglires.[2]
A phylogenetic tree is usually derived from DNA or protein sequences from populations. Often, mitochondrial DNA or Y chromosome sequences are used to study ancient human demographics. These single-locus sources of DNA do not recombine and are almost always inherited from a single parent, with only one known exception in mtDNA.[3] Individuals from closer geographic regions generally tend to be more similar than individuals from regions farther away. Distance on a phylogenetic tree can be used approximately to indicate:
The separation of humans from their closest relatives, the non-human African apes (chimpanzees and gorillas), has been studied extensively for more than a century. Five major questions have been addressed:
As discussed before, different parts of the genome show different sequence divergence between different hominoids. It has also been shown that the sequence divergence between DNA from humans and chimpanzees varies greatly. For example, the sequence divergence varies between 0% to 2.66% between non-coding, non-repetitive genomic regions of humans and chimpanzees.[8] The percentage of nucleotides in the human genome (hg38) that had one-to-one exact matches in the chimpanzee genome (pantro6) was 84.38%. Additionally gene trees, generated by comparative analysis of DNA segments, do not always fit the species tree. Summing up:
The divergence time of humans from other apes is of great interest. One of the first molecular studies, published in 1967 measured immunological distances (IDs) between different primates.[10] Basically the study measured the strength of immunological response that an antigen from one species (human albumin) induces in the immune system of another species (human, chimpanzee, gorilla and Old World monkeys). Closely related species should have similar antigens and therefore weaker immunological response to each other's antigens. The immunological response of a species to its own antigens (e.g. human to human) was set to be 1.
The ID between humans and gorillas was determined to be 1.09, that between humans and chimpanzees was determined as 1.14. However the distance to six different Old World monkeys was on average 2.46, indicating that the African apes are more closely related to humans than to monkeys. The authors consider the divergence time between Old World monkeys and hominoids to be 30 million years ago (MYA), based on fossil data, and the immunological distance was considered to grow at a constant rate. They concluded that divergence time of humans and the African apes to be roughly ~5 MYA. That was a surprising result. Most scientists at that time thought that humans and great apes diverged much earlier (>15 MYA).
The gorilla was, in ID terms, closer to human than to chimpanzees; however, the difference was so slight that the trichotomy could not be resolved with certainty. Later studies based on molecular genetics were able to resolve the trichotomy: chimpanzees are phylogenetically closer to humans than to gorillas. However, some divergence times estimated later (using much more sophisticated methods in molecular genetics) do not substantially differ from the very first estimate in 1967, but a recent paper[11] puts it at 11–14 MYA.
Current methods to determine divergence times use DNA sequence alignments and molecular clocks. Usually the molecular clock is calibrated assuming that the orangutan split from the African apes (including humans) 12-16 MYA. Some studies also include some old world monkeys and set the divergence time of them from hominoids to 25-30 MYA. Both calibration points are based on very little fossil data and have been criticized.[12]
If these dates are revised, the divergence times estimated from molecular data will change as well. However, the relative divergence times are unlikely to change. Even if we cannot tell absolute divergence times exactly, we can be fairly sure that the divergence time between chimpanzees and humans is about sixfold shorter than between chimpanzees (or humans) and monkeys.
One study (Takahata et al., 1995) used 15 DNA sequences from different regions of the genome from human and chimpanzee and 7 DNA sequences from human, chimpanzee and gorilla.[13] They determined that chimpanzees are more closely related to humans than gorillas. Using various statistical methods, they estimated the divergence time human-chimp to be 4.7 MYA and the divergence time between gorillas and humans (and chimps) to be 7.2 MYA.
Additionally they estimated the effective population size of the common ancestor of humans and chimpanzees to be ~100,000. This was somewhat surprising since the present day effective population size of humans is estimated to be only ~10,000. If true that means that the human lineage would have experienced an immense decrease of its effective population size (and thus genetic diversity) in its evolution. (see Toba catastrophe theory)
Another study (Chen & Li, 2001) sequenced 53 non-repetitive, intergenic DNA segments from human, chimpanzee, gorilla and orangutan.[8] When the DNA sequences were concatenated to a single long sequence, the generated neighbor-joining tree supported the Homo-Pan clade with 100% bootstrap (that is that humans and chimpanzees are the closest related species of the four). When three species are fairly closely related to each other (like human, chimpanzee and gorilla), the trees obtained from DNA sequence data may not be congruent with the tree that represents the speciation (species tree).
The shorter the internodal time span (TIN), the more common are incongruent gene trees. The effective population size (Ne) of the internodal population determines how long genetic lineages are preserved in the population. A higher effective population size causes more incongruent gene trees. Therefore, if the internodal time span is known, the ancestral effective population size of the common ancestor of humans and chimpanzees can be calculated.
When each segment was analyzed individually, 31 supported the Homo-Pan clade, 10 supported the Homo-Gorilla clade, and 12 supported the Pan-Gorilla clade. Using the molecular clock the authors estimated that gorillas split up first 6.2-8.4 MYA and chimpanzees and humans split up 1.6-2.2 million years later (internodal time span) 4.6-6.2 MYA. The internodal time span is useful to estimate the ancestral effective population size of the common ancestor of humans and chimpanzees.
A parsimonious analysis revealed that 24 loci supported the Homo-Pan clade, 7 supported the Homo-Gorilla clade, 2 supported the Pan-Gorilla clade and 20 gave no resolution. Additionally they took 35 protein coding loci from databases. Of these 12 supported the Homo-Pan clade, 3 the Homo-Gorilla clade, 4 the Pan-Gorilla clade and 16 gave no resolution. Therefore, only ~70% of the 52 loci that gave a resolution (33 intergenic, 19 protein coding) support the 'correct' species tree. From the fraction of loci which did not support the species tree and the internodal time span they estimated previously, the effective population of the common ancestor of humans and chimpanzees was estimated to be ~52 000 to 96 000. This value is not as high as that from the first study (Takahata), but still much higher than present day effective population size of humans.
A third study (Yang, 2002) used the same dataset that Chen and Li used but estimated the ancestral effective population of 'only' ~12,000 to 21,000, using a different statistical method.[14]
Humans and chimpanzees are 99.1% identical at the codingl level, with 99.4% similarity at the nonsynonymous level and 98.4% at the synonymous level.[15] The alignable sequences within genomes of humans and chimpanzees differ by about 35 million single-nucleotide substitutions. Additionally about 3% of the complete genomes differ by deletions, insertions and duplications.[16]
Since mutation rate is relatively constant, roughly one half of these changes occurred in the human lineage. Only a very tiny fraction of those fixed differences gave rise to the different phenotypes of humans and chimpanzees and finding those is a great challenge. The vast majority of the differences are neutral and do not affect the phenotype.[citation needed]
Molecular evolution may act in different ways, through protein evolution, gene loss, differential gene regulation and RNA evolution. All are thought to have played some part in human evolution.
Many different mutations can inactivate a gene, but few will change its function in a specific way. Inactivation mutations will therefore be readily available for selection to act on. Gene loss could thus be a common mechanism of evolutionary adaptation (the "less-is-more" hypothesis).[17]
80 genes were lost in the human lineage after separation from the last common ancestor with the chimpanzee. 36 of those were for olfactory receptors. Genes involved in chemoreception and immune response are overrepresented.[18] Another study estimated that 86 genes had been lost.[19]
A gene for type I hair keratin was lost in the human lineage. Keratins are a major component of hairs. Humans still have nine functional type I hair keratin genes, but the loss of that particular gene may have caused the thinning of human body hair. Based on the assumption of a constant molecular clock, the study predicts the gene loss occurred relatively recently in human evolution—less than 240 000 years ago, but both the Vindija Neandertal and the high-coverage Denisovan sequence contain the same premature stop codons as modern humans and hence dating should be greater than 750 000 years ago. [20]
Stedman et al. (2004) stated that the loss of the sarcomeric myosin gene MYH16 in the human lineage led to smaller masticatory muscles. They estimated that the mutation that led to the inactivation (a two base pair deletion) occurred 2.4 million years ago, predating the appearance of Homo ergaster/erectus in Africa. The period that followed was marked by a strong increase in cranial capacity, promoting speculation that the loss of the gene may have removed an evolutionary constraint on brain size in the genus Homo.[21]
Another estimate for the loss of the MYH16 gene is 5.3 million years ago, long before Homo appeared.[22]
Segmental duplications (SDs or LCRs) have had roles in creating new primate genes and shaping human genetic variation.
When the human genome was compared to the genomes of five comparison primate species, including the chimpanzee, gorilla, orangutan, gibbon, and macaque, it was found that there are approximately 20,000 human-specific insertions believed to be regulatory. While most insertions appear to be fitness neutral, a small amount have been identified in positively selected genes showing associations to neural phenotypes and some relating to dental and sensory perception-related phenotypes. These findings hint at the seemingly important role of human-specific insertions in the recent evolution of humans.[23]
Human accelerated regions are areas of the genome that differ between humans and chimpanzees to a greater extent than can be explained by genetic drift over the time since the two species shared a common ancestor. These regions show signs of being subject to natural selection, leading to the evolution of distinctly human traits. Two examples are HAR1F, which is believed to be related to brain development and HAR2 (a.k.a. HACNS1) that may have played a role in the development of the opposable thumb.
It has also been hypothesized that much of the difference between humans and chimpanzees is attributable to the regulation of gene expression rather than differences in the genes themselves. Analyses of conserved non-coding sequences, which often contain functional and thus positively selected regulatory regions, address this possibility.[24]
When the draft sequence of the common chimpanzee (Pan troglodytes) genome was published in the summer 2005, 2400 million bases (of ~3160 million bases) were sequenced and assembled well enough to be compared to the human genome.[16] 1.23% of this sequenced differed by single-base substitutions. Of this, 1.06% or less was thought to represent fixed differences between the species, with the rest being variant sites in humans or chimpanzees. Another type of difference, called indels (insertions/deletions) accounted for many fewer differences (15% as many), but contributed ~1.5% of unique sequence to each genome, since each insertion or deletion can involve anywhere from one base to millions of bases.[16]
A companion paper examined segmental duplications in the two genomes,[25] whose insertion and deletion into the genome account for much of the indel sequence. They found that a total of 2.7% of euchromatic sequence had been differentially duplicated in one or the other lineage.
The sequence divergence has generally the following pattern: Human-Chimp < Human-Gorilla << Human-Orangutan, highlighting the close kinship between humans and the African apes. Alu elements diverge quickly due to their high frequency of CpG dinucleotides which mutate roughly 10 times more often than the average nucleotide in the genome. The mutation rate is higher in the male germ line, therefore the divergence in the Y chromosome—which is inherited solely from the father—is higher than in autosomes. The X chromosome is inherited twice as often through the female germ line as through the male germ line and therefore shows slightly lower sequence divergence. The sequence divergence of the Xq13.3 region is surprisingly low between humans and chimpanzees.[26]
Mutations altering the amino acid sequence of proteins (Ka) are the least common. In fact ~29% of all orthologous proteins are identical between human and chimpanzee. The typical protein differs by only two amino acids.[16]
The measures of sequence divergence shown in the table only take the substitutional differences, for example from an A (adenine) to a G (guanine), into account. DNA sequences may however also differ by insertions and deletions (indels) of bases. These are usually stripped from the alignments before the calculation of sequence divergence is performed.
An international group of scientists completed a draft sequence of the Neanderthal genome in May 2010. The results indicate some breeding between modern humans (Homo sapiens) and Neanderthals (Homo neanderthalensis), as the genomes of non-African humans have 1–4% more in common with Neanderthals than do the genomes of subsaharan Africans. Neanderthals and most modern humans share a lactose-intolerant variant of the lactase gene that encodes an enzyme that is unable to break down lactose in milk after weaning. Modern humans and Neanderthals also share the FOXP2 gene variant associated with brain development and with speech in modern humans, indicating that Neanderthals may have been able to speak. Chimps have two amino acid differences in FOXP2 compared with human and Neanderthal FOXP2.[27][28][29]
Homo sapiens is thought to have emerged about 300,000 years ago. It dispersed throughout Africa, and after 70,000 years ago throughout Eurasia and Oceania.
A 2009 study identified 14 "ancestral population clusters", the most remote being the San people of Southern Africa.[30][31]
With their rapid expansion throughout different climate zones, and especially with the availability of new food sources with the domestication of cattle and the development of agriculture, human populations have been exposed to significant selective pressures since their dispersal. For example, the ancestors of East Asians are thought to have undergone processess of selection for a number of alleles, including variants of the EDAR, ADH1B, ABCC1, and ALDH2 genes.
The East Asian types of ADH1B in particular are associated with rice domestication and would thus have arisen after the development of rice cultivation roughly 10,000 years ago.[32] Several phenotypical traits of characteristic of East Asians are due to a single mutation of the EDAR gene, dated to c. 35,000 years ago.[33]
As of 2017[update], the Single Nucleotide Polymorphism Database (dbSNP), which lists SNP and other variants, listed a total of 324 million variants found in sequenced human genomes.[34]
Nucleotide diversity, the average proportion of nucleotides that differ between two individuals, is estimated at between 0.1% and 0.4% for contemporary humans (compared to 2% between humans and chimpanzees).[35][36]
This corresponds to genome differences at a few million sites; the 1000 Genomes Project similarly found 
that "a typical [individual] genome differs from the reference human genome at 4.1 million to 5.0 million sites … affecting 20 million bases of sequence."[37]
In February 2019, scientists discovered evidence, based on genetics studies using artificial intelligence (AI), that suggest the existence of an unknown human ancestor species, not Neanderthal, Denisovan or human hybrid (like Denny (hybrid hominin)), in the genome of modern humans.[38][39]
In March 2019, Chinese scientists reported inserting the human brain-related MCPH1 gene into laboratory rhesus monkeys, resulting in the transgenic monkeys performing better and answering faster on "short-term memory tests involving matching colors and shapes", compared to control non-transgenic monkeys, according to the researchers.[40][41]
In May 2023, scientists reported, based on genetic studies, a more complicated pathway of human evolution than previously understood. According to the studies, humans evolved from different places and times in Africa, instead of from a single location and period of time.[42][43]
On 31 August 2023, researchers reported, based on genetic studies, that a human ancestor population bottleneck occurred "around 930,000 and 813,000 years ago ... lasted for about 117,000 years and brought human ancestors close to extinction."[44][45]

Andrew Colin Renfrew, Baron Renfrew of Kaimsthorn, FBA, FSA, Hon FSA Scot (25 July 1937 – 24 November 2024) was a British archaeologist, paleolinguist and Conservative peer noted for his work on radiocarbon dating, the prehistory of languages, archaeogenetics, neuroarchaeology, and the prevention of looting at archaeological sites.
Renfrew was also the Disney Professor of Archaeology at the University of Cambridge and Director of the McDonald Institute for Archaeological Research and was a Senior Fellow of the McDonald Institute for Archaeological Research.
Renfrew was educated at St Albans School, Hertfordshire (where one of the houses is named after him) and from 1956 to 1958 did National Service in the Royal Air Force. He then went up to St John's College, Cambridge, where he read Natural Sciences then Archaeology and Anthropology, graduating in 1962. He was elected president of Cambridge Union in 1961 and was a member of the University of Cambridge Archaeological Field Club (AFC).[1] He had run against and lost an election to Barry Cunliffe to become president of the AFC. In 1965, he completed his PhD thesis Neolithic and Bronze Age cultures of the Cyclades and their external relations; in the same year he married Jane M. Ewbank.
In 1965, Renfrew was appointed to the post of lecturer in the Department of Prehistory and Archaeology at the University of Sheffield. Between 1968 and 1970, he directed excavations at Sitagroi, Greece. In the 1968 Sheffield Brightside by-election he unsuccessfully contested this parliamentary constituency on behalf of the Conservative Party. In that year he was elected a Fellow of the Society of Antiquaries, in 1970 was elected Fellow of the Society of Antiquaries of Scotland and in 2000 elected an Honorary Fellow of the Society of Antiquaries of Scotland.
In 1972, Renfrew became Professor of Archaeology at the University of Southampton, succeeding Barry Cunliffe. During his time at Southampton he directed excavations at Quanterness in Orkney and Phylakopi on the island of Milos, Greece. In 1973, Renfrew published Before Civilisation: The Radiocarbon Revolution and Prehistoric Europe in which he challenged the assumption that prehistoric cultural innovation originated in the Near East and then spread to Europe. He also excavated with Marija Gimbutas at Sitagroi.
In 1980, Renfrew was elected a Fellow of the British Academy. In 1981 he was elected to the Disney Professorship of Archaeology in the University of Cambridge, a post he held until his retirement. In 1990 Renfrew was appointed the founding Director of the McDonald Institute for Archaeological Research.
In 1987, he published Archaeology and Language: The Puzzle of the Indo-European Origins, a book on the Proto-Indo-Europeans. His "Anatolian hypothesis" posited that this group lived 2,000 years before the Kurgans, in Anatolia, later diffusing to Greece, then Italy, Sicily, Corsica, the Mediterranean coast of France, Spain, and Portugal. Another branch migrated along the fertile river valleys of the Danube and Rhine into central and northern Europe.
He developed the Anatolian hypothesis, which argues that Proto-Indo-European, the reconstructed ancestor of the Indo-European languages, originated approximately 9,000 years ago in Anatolia and moved with the spread of farming throughout the Mediterranean and into central and northern Europe. This hypothesis contradicted Marija Gimbutas's Kurgan hypothesis, which states that Proto-Indo-European was spread by a migration of peoples from the Pontic–Caspian steppe approximately 6,000 years ago.
From 1987 to 1991, he co-directed excavations at Markiani on Amorgos and at Dhaskalio Kavos, Keros, Greece.
Renfrew's work in using the archaeological record as the basis for understanding the ancient mind was foundational to the field of evolutionary cognitive archaeology.[2][3] Renfrew and his student, Lambros Malafouris, coined the phrase neuroarchaeology to describe an archaeology of mind.[4][5]
In 1996, Renfrew formulated a sapient paradox, that can be formulated as ""why there was such a long gap between emergence of genetically and anatomically modern humans and the development of complex behaviors?"[6][7]
Renfrew served as Master of Jesus College, Cambridge from 1986 until 1997. In 2004, he retired from the Disney Professorship and was a Senior Fellow at the McDonald Institute. From 2006 to 2008 he directed new excavations on the Cycladic Island of Keros and was recently co-director of the Keros Island Survey. He died on 24 November 2024, at the age of 87.[8]

Doctor Honoris Causa of Université catholique de Louvain (Belgium)
Oded Galor (born 1953) is an Israeli-American[1] economist who is currently Herbert H. Goldberger Professor of Economics at Brown University. He is the founder of unified growth theory.
Galor has contributed to the understanding of development over the entire course of human history and prehistory, and the role of deep-rooted factors in the transition from stagnation to growth and in the emergence of global inequality. He also pioneered the exploration of the impact of human evolution, population diversity, and inequality on the process of development over most of human existence.
Galor completed his BA and MA at the Hebrew University of Jerusalem and his PhD at Columbia University. He served as a Chilewich Professor of Economics at the Hebrew University, and he is currently the Herbert H. Goldberger Professor of Economics at Brown University.
He was awarded Doctor Honoris Causa from Poznań University of Economics & Business[2] and from UCLouvain. He is an Elected Foreign Member of Academia Europaea (honoris causa), and an Elected Fellow of the Econometric Society.  He has led the NBER research group on Income Distribution and Macroeconomics and he is a Research Fellow of the CEPR and IZA, a Research Associate of the NBER and CESifo, a Sackler Fellow at Tel-Aviv University, a Fellow of the Economics Department at the Hebrew University. Furthermore, he is the editor in chief of the Journal of Economic Growth, editor of the Journal of Population Economics, co-editor of Macroeconomic Dynamics. He was recently among the 5 candidates for the Nobel of Frankfurter Allgemeine.[3]
Oded Galor is the founder of unified growth theory, which explores the process of development over the entire course of human history and identifies the historical and prehistorical forces behind the differential transition timing from stagnation to growth and the divergence in income per capita across countries and regions.
He has made significant contributions to the understanding of process of development over the entire course of human history and the role of deep-rooted factors in the transition from stagnation to growth and in the emergence of the vast inequality across the globe. Moreover, he has pioneered the exploration of the impact of human evolution, population diversity, and inequality on the process of development over most of human existence.
His interdisciplinary research has redirected research in the field of economic growth to the exploration of the long shadow of history and to the role of biogeographical forces in comparative economic development. It has spawned the influential literatures studying the impact of inequality on the process of development, the interaction between human evolution and economic development, the transition from stagnation to growth, and the impact of human diversity on comparative economic development.
He is a co-author of the Galor–Zeira model—the first macroeconomic model to explore the role of heterogeneity in the determination of macroeconomic behavior. In contrast to the representative agent approach that dominated the field of macroeconomics until the early 1990s and argued that heterogeneity has no impact on macroeconomic activity, the model demonstrates that in the presence of capital markets imperfections and local non-convexities in the production of human capital, income distribution affects the long run level of income per-capita as well as the growth process.[4] The Review of Economic Studies named the paper among the 11 most path-breaking papers published in the journal in the past 60 years.[5]
Galor and his colleagues have used evolutionary approach in order to explain the origins of more particular elements of economic and social behavior.
This book provides an introduction to discrete dynamical systems—a framework of analysis commonly used in the fields of biology, demography, ecology, economics, engineering, finance, and physics. The book characterizes the fundamental factors that govern the qualitative and quantitative trajectories of a variety of deterministic, discrete dynamical systems, providing solution methods for systems that can be solved analytically and methods of qualitative analysis for systems that do not permit or necessitate an explicit solution. The analysis focuses initially on the characterization of the factors the govern the evolution of state variables in the elementary context of one-dimensional, first-order, linear, autonomous systems. The fundamental insights about the forces that affect the evolution of these elementary systems are subsequently generalized, and the determinants of the trajectory of multi-dimensional, nonlinear, higher-order, non-autonomous dynamical systems are established.
Backdrop
Throughout most of human existence, economic growth has been all but absent across the globe. But two centuries ago, some regions of the world began to emerge from this epoch of economic stagnation into a period of sustained economic growth, profoundly altering the level and distribution of wealth and health around the world. In his book Unified Growth Theory, Galor provides a global theory explaining what has triggered this remarkable transformation in human history.
Synopsis
Unified Growth Theory is the first theory that sheds light on the determinants of the process of development since the emergence of Homo sapiens. Galor, who founded the field of unified growth theory, identifies the historical and prehistorical forces behind the differential transition timing from stagnation to growth and the emergence of income disparity around the world. He unveils the mechanisms that have trapped the world economy in millennia of near-stagnation but ultimately has induced the remarkable transition to an era of sustained economic growth characterized by vast inequality across countries and regions. Unified Growth Theory suggests that during most of human existence, technological progress was counterbalanced by population growth and living standards were near subsistence across time and space. However, the reinforcing interaction between the rate of technological progress and the size and composition of the population has gradually increased the pace of technological progress, enhancing the importance of education in the ability of individuals to adapt to the changing technological environment. The rise in the allocation of resources towards education triggered reductions in fertility rates, enabling economies to divert a larger share of the fruits of technological progress to the growth of income per capita, rather than towards the growth of population, paving the way for the emergence of sustained economic growth. The theory further suggests that variations in biogeographical characteristics, as well as cultural and institutional characteristics, have generated a differential pace of transition from stagnation to growth across countries and consequently divergence in their income per capita over the past two centuries.
Reception
Robert Solow described Galor's project as "breathtakingly ambitious". He added that "Galor proposes a fairly simple, intensely human-capital-oriented model that will accommodate the millennia of Malthusian near-stagnation, the Industrial Revolution and its aftermath of rapid growth, the accompanying demographic transition, and the emergence of modern human-capital-based growth. And the model is supposed to generate endogenously the transitions from one era to the next. The resulting book is a powerful mixture of fact, theory, and interpretation."[12]
According to Daron Acemoglu, "Unified Growth Theory is a work of unusual ambition" that "will inspire, motivate, and challenge economists."
Steven N. Durlauf declared that "Unified Growth Theory is Big Science at its best. It grapples with some of the broadest questions in social science, integrating state-of-the-art economic theory with a rich exploration of a wide range of empirical evidence." He considers that Galor's ideas "will have a lasting effect on economics."
In The Journey of Humanity, Galor wrote a book which wraps his life's studies into one volume, this time intended for a popular audience.[13][14]

Several species of humans have intermittently occupied Great Britain for almost a million years.  The earliest evidence of human occupation around 900,000 years ago is at Happisburgh on the Norfolk coast, with stone tools and footprints probably made by Homo antecessor. The oldest human fossils, around 500,000 years old, are of Homo heidelbergensis at Boxgrove in Sussex. Until this time Britain had been permanently connected to the Continent by a chalk ridge between South East England and northern France called the Weald-Artois Anticline, but during the Anglian Glaciation around 425,000 years ago a megaflood broke through the ridge, and Britain became an island when sea levels rose during the following Hoxnian interglacial.
Fossils of very early Neanderthals dating to around 400,000 years ago have been found at Swanscombe in Kent, and of classic Neanderthals about 225,000 years old at Pontnewydd in Wales. Britain was unoccupied by humans between 180,000 and 60,000 years ago, when Neanderthals returned. By 40,000 years ago they had become extinct and modern humans had reached Britain. But even their occupations were brief and intermittent due to a climate which swung between low temperatures with a tundra habitat and severe ice ages which made Britain uninhabitable for long periods. The last of these, the Younger Dryas, ended around 11,700 years ago, and since then Britain has been continuously occupied.
Traditionally it was claimed by academics that a post-glacial land bridge existed between Britain and Ireland, however this conjecture began to be refuted by a consensus within the academic community starting in 1983, and since 2006 the idea of a land bridge has been disproven based upon conclusive marine geological evidence. It is now concluded that an ice bridge existed between Britain and Ireland up until 16,000 years ago, but this had melted by around 14,000 years ago.[1][2] Britain was at this time still joined to the Continent by a land bridge known as Doggerland, but due to rising sea levels this causeway of dry land would have become a series of estuaries, inlets and islands by 7000 BC,[3] and by 6200 BC, it would have become completely submerged.[4][5]
Located at the fringes of Europe, Britain received European technological and cultural developments much later than Southern Europe and the Mediterranean region did during prehistory. By around 4000 BC, the island was populated by people with a Neolithic culture. This neolithic population had significant ancestry from the earliest farming communities in Anatolia, indicating that a major migration accompanied farming. The beginning of the Bronze Age and the Bell Beaker culture was marked by an even greater population turnover, this time displacing more than 90% of Britain's neolithic ancestry in the process. This is documented by recent ancient DNA studies which demonstrate that the immigrants had large amounts of Bronze-Age Eurasian Steppe ancestry, associated with the spread of Indo-European languages and the Yamnaya culture.[6]
No written language of the pre-Roman inhabitants of Britain is known; therefore, the history, culture and way of life of pre-Roman Britain are known mainly through archaeological finds. Archaeological evidence demonstrates that ancient Britons were involved in extensive maritime trade and cultural links with the rest of Europe from the Neolithic onwards, especially by exporting tin that was in abundant supply.  Although the main evidence for the period is archaeological, available genetic evidence is increasing, and views of British prehistory are evolving accordingly. Julius Caesar's first invasion of Britain in 55 BC is regarded as the start of recorded protohistory although some historical information is available from before then.[7]
Palaeolithic (Old Stone Age) Britain is the period of the earliest known occupation of Britain by humans. This huge period saw many changes in the environment, encompassing several glacial and interglacial episodes greatly affecting human settlement in the region. Providing dating for this distant period is difficult and contentious. The inhabitants of the region at this time were bands of hunter-gatherers who roamed Northern Europe following herds of animals, or who supported themselves by fishing.
There is evidence from animal bones and flint tools found in coastal deposits near Happisburgh in Norfolk that early humans were present in Britain over 800,000 years ago.[8] The archaeological site at Happisburgh lies underneath glacial sediments from the Anglian glaciation of 450,000 years ago.[9] Paleo magnetic analysis shows that the sediments in which the stone tools were found have a reversed polarity- which means they are at least 780,000 years old.[9] Plant remains as well as the presence of extinct species of vole, mammoth, red deer, horse and elk indicate a date between 780,000 and 990,000 years old.[9] The evidence is that the early humans were there towards the end of an interglacial during that date range. There are two candidate interglacials - one between 970,000 and 935,000 years ago and the second from 865,000 and 815,000 years ago.[9] Numerous footprints dating to more than 800,000 years ago were found on the beach at Happisburgh in 2013 of a mixed group of adult males, females and children.[9] However there are no human fossils found. Homo antecessor is the most likely candidate species of ancient human as there are remains of roughly the same age at Gran Dolina at Atapuerca.[9] Homo antecessor  lived before the ancestors of Neanderthals split from the ancestors of Homo sapiens 600,000 years ago.
Summer temperatures at Happisburgh were an average of 16-17 degrees C (60.8-61.6 degrees F) and average winter temperatures were slightly colder than present day temperatures, around freezing point or just below. Conditions were comparable to present-day southern Scandinavia.[9] It is not established how early humans at Happisburgh would have been able to deal with the cold winters. It is possible that they migrated southwards during the winter but the distances are large. No evidence has been found for the use of fire during that period.[9]
At this time, Britain was a peninsula of  Europe, connected by a chalk ridge running across to northern France and the English Channel did not yet exist.[9] There were two main rivers in eastern Britain: the Bytham River, flowing east from the English Midlands and then across the north of East Anglia, and the River Thames, which then flowed further north than today. Early humans may have followed the Rhine and thence around the huge north-facing bay into which the Thames and Bytham also flowed.[9] Humans in Happisburgh were in a great valley downstream from the joining of the two great rivers.
Reconstructing this ancient environment has provided clues to the route first visitors took to arrive at what was then a peninsula of the Eurasian continent. Archaeologists have found a string of early sites located close to the route of a now lost watercourse named the Bytham River which indicate that it was exploited as the earliest route west into Britain.
Chronologically, the next evidence of human occupation is at Pakefield on the outskirts of Lowestoft in Suffolk 48 kilometres south of Happisburgh. They were in the lower Bytham river, and not the Thames which had now moved further south. Pakefield had mild winters and warm summers with average July temperatures of between 18 and 23 degrees C (64.4 and 73.4 degrees F). There were wet winters and drier summers. Animal bones found in the area include those of rhinos, hippos, extinct elephants, giant deer, hyaenas, lions, and sabre-toothed cats.[9]
Sites such as Boxgrove in Sussex illustrate the later arrival in the archaeological record of an archaic Homo species called Homo heidelbergensis around 500,000 years ago. These early peoples made Acheulean flint tools (hand axes) and hunted the large native mammals of the period. One hypothesis is that they drove elephants, rhinoceroses and hippopotamuses over the tops of cliffs or into bogs to more easily kill them.
The extreme cold of the following Anglian Stage is likely to have driven humans out of Britain altogether and the region does not appear to have been occupied again until the ice receded during the Hoxnian Stage.[citation needed] This warmer time period lasted from around 424,000 until 374,000 years ago and saw the Clactonian flint tool industry develop at sites such as Swanscombe in Kent. The period has produced a rich and widespread distribution of sites by Palaeolithic standards, although uncertainty over the relationship between the Clactonian and Acheulean industries is still unresolved.
Britain was populated only intermittently, and even during periods of occupation may have reproduced below replacement level and needed immigration from elsewhere to maintain numbers. According to Paul Pettitt and Mark White:
This period also saw Levallois flint tools introduced, possibly by humans arriving from Africa. However, finds from Swanscombe and Botany Pit in Purfleet support Levallois technology being a European rather than African introduction. The more advanced flint technology permitted more efficient hunting and therefore made Britain a more worthwhile place to remain until the following period of cooling known as the Wolstonian Stage, 352,000–130,000 years ago. Britain first became an island about 350,000 years ago.[11]
230,000 years BP the landscape was reachable and Early Neanderthal remains discovered at the Pontnewydd Cave in Wales have been dated to 230,000 BP,[12] and are the most north westerly Neanderthal remains found anywhere in the world.
The next glaciation closed in and by about 180,000 years ago Britain no longer had humans.[13] About 130,000 years ago there was an interglacial period even warmer than today, which lasted 15,000 years. There were lions, elephants hyenas and hippos as well as deer. There were no humans. Possibly humans were too sparse at that time. Until c.60,000 years ago there is no evidence of human occupation in Britain, probably due to inhospitable cold in some periods, Britain being cut off as an island in others, and the neighbouring areas of north-west Europe being unoccupied by hominins at times when Britain was both accessible and hospitable.[14]
This period is often divided into three subperiods: the Early Upper Palaeolithic (before the main glacial period), the Middle-Upper Palaeolithic (the main glacial period) and the Late Upper Palaeolithic (after the main glacial period). There was limited Neanderthal occupation of Britain in marine isotope stage 3 between about 60,000 and 42,000 years BP. Britain had its own unique variety of late Neanderthal handaxe, the bout-coupé, so seasonal migration between Britain and the continent is unlikely, but the main occupation may have been in the now submerged area of Doggerland, with summer migrations to Britain in warmer periods.[15] La Cotte de St Brelade in Jersey is the only site in the British Isles to have produced late Neanderthal fossils.[16]
The earliest evidence for modern humans in North West Europe is a jawbone discovered in England at Kents Cavern in 1927, which was re-dated in 2011 to between 41,000 and 44,000 years old.[17][18] The most famous example from this period is the burial of the "Red Lady of Paviland" (actually now known to be a man) in modern-day coastal South Wales, which was dated in 2009 to be 33,000 years old. The distribution of finds shows that humans in this period preferred the uplands of Wales and northern and western England to the flatter areas of eastern England. Their stone tools are similar to those of the same age found in Belgium and far north-east France, and very different from those in north-west France. At a time when Britain was not an island, hunter gatherers may have followed migrating herds of reindeer from Belgium and north-east France across the giant Channel River.[19]
The climatic deterioration which culminated in the Last Glacial Maximum, between about 26,500 and 19,000–20,000 years ago,[20] drove humans out of Britain, and there is no evidence of occupation for around 18,000 years after c.33,000 years BP.[21] Sites such as Cathole Cave in Swansea County dated at 14,500BP,[22] Creswell Crags on the border between Derbyshire and Nottinghamshire at 12,800BP and Gough's Cave in Somerset 12,000 years BP, provide evidence suggesting that humans returned to Britain towards the end of this ice age during a warm period from 14,700 to 12,900 years ago (the Bølling-Allerød interstadial known as the Windermere Interstadial in Britain), although further extremes of cold right before the final thaw may have caused them to leave again and then return repeatedly. The environment during this ice age period would have been largely treeless tundra, eventually replaced by a gradually warmer climate, perhaps reaching 17 degrees Celsius (62.6 Fahrenheit) in summer, encouraging the expansion of birch trees as well as shrub and grasses.
The first distinct culture of the Upper Palaeolithic in Britain is what archaeologists call the Creswellian industry, with leaf-shaped points probably used as arrowheads. It produced more refined flint tools but also made use of bone, antler, shell, amber, animal teeth, and mammoth ivory. These were fashioned into tools but also jewellery and rods of uncertain purpose. Flint seems to have been brought into areas with limited local resources; the stone tools found in the caves of Devon, such as Kent's Cavern, seem to have been sourced from Salisbury Plain, 100 miles (161 km) east. This is interpreted as meaning that the early inhabitants of Britain were highly mobile, roaming over wide distances and carrying 'toolkits' of flint blades with them rather than heavy, unworked flint nodules, or else improvising tools extemporaneously. The possibility that groups also travelled to meet and exchange goods or sent out dedicated expeditions to source flint has also been suggested.
The dominant food species were equines (Equus ferus) and red deer (Cervus elaphus), although other mammals ranging from hares to mammoth were also hunted, including rhino and hyena. From the limited evidence available, burial seemed to involve skinning and dismembering a corpse with the bones placed in caves. This suggests a practice of excarnation and secondary burial, and possibly some form of ritual cannibalism. Artistic expression seems to have been mostly limited to engraved bone, although the cave art at Creswell Crags and Mendip caves are notable exceptions.
Between about 12,890 and 11,650 years ago Britain returned to glacial conditions during the Younger Dryas, and may have been unoccupied for periods.[23]
(c. 9,000 to 4,300 BC)
The Younger Dryas was followed by the Holocene, which began around 9,700 BC,[24] and continues to the present. There was then limited occupation by Ahrensburgian hunter gatherers, but this came to an end when there was a final downturn in temperature which lasted from around 9,400 to 9,200 BC. Mesolithic people occupied Britain by around 9,000 BC, and it has been occupied ever since.[25] By 8000 BC temperatures were higher than today, and birch woodlands spread rapidly,[26] but there was a cold spell around 6,200 BC which lasted about 150 years.[27] The plains of Doggerland were thought to have finally been submerged around 6500 to 6000 BC,[28] but recent evidence suggests that the bridge may have lasted until between 5800 and 5400 BC, and possibly as late as 3800 BC.[29]
The warmer climate changed the arctic environment to one of pine, birch and alder forest; this less open landscape was less conducive to the large herds of reindeer and wild horse that had previously sustained humans. Those animals were replaced in people's diets by pig and less social animals such as elk, red deer, roe deer, wild boar and aurochs (wild cattle),[30] which would have required different hunting techniques.[31] Tools changed to incorporate barbs which could snag the flesh of an animal, making it harder for it to escape alive. Tiny microliths were developed for hafting onto harpoons and spears. Woodworking tools such as adzes appear in the archaeological record, although some flint blade types remained similar to their Palaeolithic predecessors. The dog was domesticated because of its benefits during hunting, and the wetland environments created by the warmer weather would have been a rich source of fish and game. Wheat of a variety grown in the Middle East was present on the Isle of Wight at the Bouldnor Cliff Mesolithic Village dating from about 6,000 BC.[32]
It is likely that these environmental changes were accompanied by social changes. Humans spread and reached the far north of Scotland during this period.[33] Sites from the British Mesolithic include the Mendips, Star Carr in Yorkshire and Oronsay in the Inner Hebrides. Excavations at Howick in Northumberland uncovered evidence of a large circular building dating to c. 7600 BC which is interpreted as a dwelling. A further example has also been identified at Deepcar in Sheffield, and a building dating to c. 8500 BC was discovered at the Star Carr site.  A group of 25 pits, aligned with a watercourse, laid out in straight lines, up to 500 metres long, has been found at Linmere, Bedfordshire.[30] The older view of Mesolithic Britons as nomadic is now being replaced with a more complex picture of seasonal occupation or, in some cases, permanent occupation. Travel distances seem to have become shorter, typically with movement between high and low ground.
In 1997, DNA analysis was carried out on a tooth of Cheddar Man, human remains dated to c.  7150 BC found in Gough's Cave at Cheddar Gorge.  His mitochondrial DNA (mtDNA) belonged to Haplogroup U5. Within modern European populations, U5 is now concentrated in North-East Europe, among members of the Sami people, Finns, and Estonians. This distribution and the age of the haplogroup indicate that individuals belonging to U5 were among the first people to resettle Northern Europe, following the retreat of ice sheets from the Last Glacial Maximum, about 10,000 years ago. It has also been found in other Mesolithic remains in Germany, Lithuania, Poland, Portugal, Russia,[34] Sweden,[35] France[36] and Spain.[37] Members of U5 may have been one of the most common haplogroups in Europe, before the spread of agriculture from the Middle East.[38]
Though the Mesolithic environment was bounteous, the rising population and the ancient Britons' success in exploiting it eventually led to local exhaustion of many natural resources. The remains of a Mesolithic elk found caught in a bog at Poulton-le-Fylde in Lancashire show that it had been wounded by hunters and escaped on three occasions, indicating hunting during the Mesolithic.  A few Neolithic monuments overlie Mesolithic sites but little continuity can be demonstrated.
Farming of crops and domestic animals was adopted in Britain around 4500 BC, at least partly because of the need for reliable food sources.
The climate had been warming since the later Mesolithic and continued to improve, replacing the earlier pine forests with woodland.
(c. 4,300 to 2,000 BC)
The Neolithic was the period of domestication of plants and animals, but the arrival of a Neolithic package of farming and a sedentary lifestyle is increasingly giving way to a more complex view of the changes and continuities in practices that can be observed from the Mesolithic period onwards. For example, the development of Neolithic monumental architecture, apparently venerating the dead,[citation needed] may represent more comprehensive social and ideological changes involving new interpretations of time, ancestry, community and identity.
In any case, the Neolithic Revolution, as it is called, introduced a more settled way of life and ultimately led to societies becoming divided into differing groups of farmers, artisans and leaders. Forest clearances were undertaken to provide room for cereal cultivation and animal herds. Native cattle and pigs were reared whilst sheep and goats were later introduced from the continent, as were the wheats and barleys grown in Britain. However, only a few actual settlement sites are known in Britain, unlike the continent. Cave occupation was common at this time.
The construction of the earliest earthwork sites in Britain began during the early Neolithic (c. 4400 BC – 3300 BC) in the form of long barrows used for communal burial and the first causewayed enclosures, sites which have parallels on the continent.  The former may be derived from the long house, although no long house villages have been found in Britain — only individual examples. The stone-built houses on Orkney — such as those at Skara Brae — are, however, indicators of some nucleated settlement in Britain. Evidence of growing mastery over the environment is embodied in the Sweet Track, a wooden trackway built to cross the marshes of the Somerset Levels and dated to 3807 BC. Leaf-shaped arrowheads, round-based pottery types and the beginnings of polished axe production are common indicators of the period. Evidence of the use of cow's milk comes from analysis of pottery contents found beside the Sweet Track. According to archaeological evidence from North Yorkshire, salt was being produced by evaporation of seawater around this time, enabling more effective preservation of meat.[39]
Pollen analysis shows that woodland was decreasing and grassland increasing, with a major decline of elms. The winters were typically 3 degrees colder than at present but the summers some 2.5 degrees warmer.[citation needed]
The Middle Neolithic (c. 3300 BC – c. 2900 BC) saw the development of cursus monuments close to earlier barrows and the growth and abandonment of causewayed enclosures, as well as the building of impressive chamber tombs such as the Maeshowe types. The earliest stone circles and individual burials also appear.
Different pottery types, such as grooved ware, appear during the later Neolithic (c. 2900 BC – c. 2200 BC). In addition, new enclosures called henges were built, along with stone rows and the famous sites of Stonehenge, Avebury and Silbury Hill, which building reached its peak at this time. Industrial flint mining begins, such as that at Cissbury and Grimes Graves, along with evidence of long-distance trade. Wooden tools and bowls were common, and bows were also constructed.
Changes in Neolithic culture could have been due to the mass migrations that occurred in that time. A 2017 study showed that British Neolithic farmers had formerly been genetically similar to contemporary populations in the Iberian peninsula, but from the Beaker culture period onwards, all British individuals had high proportions of Steppe ancestry and were genetically more similar to Beaker-associated people from the Lower Rhine area. The study argues that more than 90% of Britain's Neolithic gene pool was replaced with the coming of the Beaker people.[6]
Analysis of the mitochondrial DNA of modern European populations shows that over 80% are descended in the female line from European hunter-gatherers.[citation needed] Less than 20% are descended in the female line from Neolithic farmers from Anatolia and from subsequent migrations. The percentage in Britain is smaller at around 11%. Initial studies suggested that this situation is different with the paternal Y-chromosome DNA, varying from 10 to 100% across the country, being higher in the east. This was considered to show a large degree of population replacement during the Anglo-Saxon invasion and a nearly complete masking over of whatever population movement (or lack of it) went before in these two countries.[40] However, more widespread studies have suggested that there was less of a division between Western and Eastern parts of Britain with less Anglo-Saxon migration.[41] Looking from a more Europe-wide standpoint, researchers at Stanford University have found overlapping cultural and genetic evidence that supports the theory that migration was at least partially responsible for the Neolithic Revolution in Northern Europe (including Britain).[42] The science of genetic anthropology is changing very fast and a clear picture across the whole of human occupation of Britain has yet to emerge.[43]
(Around 2200 to 750 BC)
This period can be sub-divided into an earlier phase (2300 to 1200 BC) and a later one (1200 – 700 BC). Beaker pottery appears in England around 2475–2315 cal. BC[44] along with flat axes and burial practices of inhumation. With the revised Stonehenge chronology, this is after the Sarsen Circle and trilithons were erected at Stonehenge. Several regions of origin have been postulated for the Beaker culture, notably the Iberian peninsula, the Netherlands and Central Europe.[45] Beaker techniques brought to Britain the skill of refining metal. At first the users made items from copper, but from around 2150 BCE smiths had discovered how to smelt bronze (which is much harder than copper) by mixing copper with a small amount of tin. With this discovery, the Bronze Age arrived in Britain. Over the next thousand years, bronze gradually replaced stone as the main material for tool and weapon making.
Britain had large, easily accessible reserves of tin in the modern areas of Cornwall and Devon and thus tin mining began. By around 1600 BC the southwest of Britain was experiencing a trade boom as British tin was exported across Europe, evidence of ports being found in Southern Devon at Bantham and Mount Batten. Copper was mined at the Great Orme in North Wales.
The Beaker people were also skilled at making ornaments from gold, silver and copper, and examples of these have been found in graves of the wealthy Wessex culture of central southern Britain.
Early Bronze Age Britons buried their dead beneath earth mounds known as barrows, often with a beaker alongside the body. Later in the period, cremation was adopted as a burial practice with cemeteries of urns containing cremated individuals appearing in the archaeological record, with deposition of metal objects such as daggers. People of this period were also largely responsible for building many famous prehistoric sites such as the later phases of Stonehenge along with Seahenge. The Bronze Age people lived in round houses and divided up the landscape. Stone rows are to be seen on, for example, Dartmoor. They ate cattle, sheep, pigs and deer as well as shellfish and birds. They carried out salt manufacture. The wetlands were a source of wildfowl and reeds. There was ritual deposition of offerings in the wetlands and in holes in the ground.
There has been debate amongst archaeologists as to whether the "Beaker people" were a race of people who migrated to Britain en masse from the continent, or whether a Beaker cultural "package" of goods and behaviour (which eventually spread across most of Western Europe) diffused to Britain's existing inhabitants through trade across tribal boundaries. A 2017 study suggests a major genetic shift in late Neolithic/early Bronze Age Britain, so that more than 90% of Britain's Neolithic gene pool was replaced with the coming of a people genetically related to the Beaker people of the lower-Rhine area.[6]
There is evidence of a relatively large scale disruption of cultural patterns (see Late Bronze Age collapse) which some scholars think may indicate an invasion (or at least a migration) into Southern Great Britain c. the 12th century BC. This disruption was felt far beyond Britain, even beyond Europe, as most of the great Near Eastern empires collapsed (or experienced severe difficulties) and the Sea Peoples harried the entire Mediterranean basin around this time. Some scholars consider that the Celtic languages arrived in Britain at this time,[46][47][48] but other elements of the Celtic cultural package derive from the Hallstatt culture.[49]
In an archaeogenetics study, Patterson et al. (2021) uncovered a migration into southern Britain during the 500-year period 1,300–800 BC.[50] The newcomers were genetically most similar to ancient individuals from Gaul, and had higher levels of EEF ancestry.[50] During 1,000–875 BC, their genetic marker swiftly spread through southern Britain,[51] making up around half the ancestry of subsequent Iron Age people in this area, but not in northern Britain.[50] The "evidence suggests that, rather than a violent invasion or a single migratory event, the genetic structure of the population changed through sustained contacts between Britain and mainland Europe over several centuries, such as the movement of traders, intermarriage, and small scale movements of family groups".[51] The authors describe this as a "plausible vector for the spread of early Celtic languages into Britain".[50] There was much less migration into Britain during the Iron Age, so it is likely that Celtic reached Britain before then.[50] The study also found that lactose tolerance rose swiftly in early Iron Age Britain, a thousand years before it became widespread in mainland Europe; suggesting milk became a very important foodstuff in Britain at this time.[50]
(around 750 BC – 43 AD)
In around 750 BC iron working techniques reached Britain from southern Europe. Iron was stronger and more plentiful than bronze, and its introduction marks the beginning of the Iron Age. Iron working revolutionised many aspects of life, most importantly agriculture. Iron tipped ploughs could turn soil more quickly and deeply than older wooden or bronze ones, and iron axes could clear forest land more efficiently for agriculture. There was a landscape of arable, pasture and managed woodland. There were many enclosed settlements and land ownership was important.
It is generally thought that by 500 BC most people inhabiting the British Isles were speaking Common Brythonic, on the limited evidence of place-names recorded by Pytheas of Massalia and transmitted to us second-hand, largely through Strabo. Certainly by the Roman period there is substantial place and personal name evidence which suggests that this was so; Tacitus also states in his Agricola that the British language differed little from that of the Gauls.[52] Among these people were skilled craftsmen who had begun producing intricately patterned gold jewellery, in addition to tools and weapons of both bronze and iron. It is disputed whether Iron Age Britons were "Celts", with some academics such as John Collis[53] and Simon James[54] actively opposing the idea of 'Celtic Britain', since the term was only applied at this time to a tribe in Gaul. However, place names and tribal names from the later part of the period suggest that a Celtic language was spoken.
The traveller Pytheas, whose own works are lost, was quoted by later classical authors as calling the people "Pretanoi", which is cognate with "Britanni" and is apparently Celtic in origin. The term "Celtic" continues to be used by linguists to describe the family that includes many of the ancient languages of Western Europe and modern British languages such as Welsh without controversy.[55] The dispute essentially revolves around how the word "Celtic" is defined; it is clear from the archaeological and historical record that Iron Age Britain did have much in common with Iron Age Gaul, but there were also many differences. Many leading academics, such as Barry Cunliffe, still use the term to refer to the pre-Roman inhabitants of Britain for want of a better label.
Iron Age Britons lived in organised tribal groups, ruled by a chieftain. As people became more numerous, wars broke out between opposing tribes. This was traditionally interpreted as the reason for the building of hill forts, although the siting of some earthworks on the sides of hills undermined their defensive value, hence "hill forts" may represent increasing communal areas or even 'elite areas'. However some hillside constructions may simply have been cow enclosures. Although the first had been built about 1500 BC, hillfort building peaked during the later Iron Age. There are around 3,300 structures that can be classed as hillforts or similar "defended enclosures" within Britain.[56] By about 350 BC many hillforts went out of use and the remaining ones were reinforced. Pytheas was quoted as writing that the Britons were renowned wheat farmers. Large farmsteads produced food in industrial quantities and Roman sources note that Britain exported hunting dogs, animal skins and slaves.
The last centuries before the Roman invasion saw an influx of Celtic speaking refugees from Gaul (approximately modern day France and Belgium) known as the Belgae, who were displaced as the Roman Empire expanded around 50 BC. They settled along most of the coastline of southern Britain between about 200 BC and AD 43, although it is hard to estimate what proportion of the population there they formed. A Gaulish tribe known as the Parisi, who had cultural links to the continent, appeared in northeast England.
From around 175 BC, the areas of Kent, Hertfordshire and Essex developed especially advanced pottery-making skills. The tribes of southeast England became partially Romanised and were responsible for creating the first settlements (oppida) large enough to be called towns.
The last centuries before the Roman invasion saw increasing sophistication in British life. About 100 BC, iron bars began to be used as currency, while internal trade and trade with continental Europe flourished, largely due to Britain's extensive mineral reserves. Coinage was developed, based on continental types but bearing the names of local chieftains. This was used in southeast England, but not in areas such as Dumnonia in the west.
As the Roman Empire expanded northwards, Rome began to take interest in Britain. This may have been caused by an influx of refugees from Roman occupied Europe, or Britain's large mineral reserves.  See Roman Britain for the history of this subsequent period.
The first significant written record of Britain and its inhabitants was made by the Greek navigator Pytheas, who explored the coastal region of Britain around 325 BC. However, there may be some additional information on Britain in the Ora Maritima, a text which is now lost but which is incorporated in the writing of the later author Avienius.  Julius Caesar also wrote of Britain in about 50 BC after his two military expeditions to the island in 55 and 54 BC. The failed invasion during 54 BC is thought to be an attempt to conquer at least the southeast of Britain.[57]
After some further false starts, the Roman conquest of Britain in 43 AD led to most of the island falling under Roman rule, and began the period of Roman Britain.

The concept of sexual selection was introduced by Charles Darwin as an element of his theory of natural selection.[1] Sexual selection is a biological way one sex chooses a mate for the best reproductive success. Most compete with others of the same sex for the best mate to contribute their genome for future generations. This has shaped human evolution for many years, but reasons why humans choose their mates are not fully understood. Sexual selection is quite different in non-human animals than humans as they feel more of the evolutionary pressures to reproduce and can easily reject a mate.[2] The role of sexual selection in human evolution has not been firmly established although neoteny has been cited as being caused by human sexual selection.[3] It has been suggested that sexual selection played a part in the evolution of the anatomically modern human brain, i.e. the structures responsible for social intelligence underwent positive selection as a sexual ornamentation to be used in courtship rather than for survival itself,[4] and that it has developed in ways outlined by Ronald Fisher in the Fisherian runaway model.[5][6][7][8][9] Fisher also stated that the development of sexual selection was "more favourable" in humans.[10]
Some hypotheses about the evolution of the human brain argue that it is a sexually selected trait, as it would not confer enough fitness in itself relative to its high maintenance costs (a fifth to a quarter of the energy and oxygen consumed by a human).[11] Current consensus about the evolutionary development of the human brain accepts sexual selection as a potential contributing factor but maintains that human intelligence and the ability to store and share cultural knowledge would have likely carried high survival value as well.[12]
Sexual selection's role in human evolution cannot be definitively established, as features may result from an equilibrium among competing selective pressures, some involving sexual selection, others natural selection, and others pleiotropy. Richard Dawkins argued that
Charles Darwin described sexual selection as depending on "the advantage which certain individuals have over others of the same sex and species, solely in respect of reproduction".[14] Darwin noted that sexual selection is of two kinds and concluded that both kinds had operated on humans:[15] "The sexual struggle is of two kinds; in the one it is between the individuals of the same sex, generally the male sex, in order to drive away or kill their rivals, the females remaining passive; whilst in the other, the struggle is likewise between the individuals of the same sex, in order to excite or charm those of the opposite sex, generally the females, which no longer remain passive, but select the more agreeable partners."[16]
Charles Darwin conjectured that the male beard, as well as the hairlessness of humans compared to nearly all other mammals, were results of sexual selection. He reasoned that since the bodies of females are more nearly hairless, the loss of fur was due to sexual selection of females at a remote prehistoric time when males had overwhelming selective power, and that it nonetheless affected males due to genetic correlation between the sexes. He also hypothesized that contrasts in sexual selection acting along with natural selection were significant factors in the geographical differentiation in human appearance of some isolated groups, as he did not believe that natural selection alone provided a satisfactory answer. Although not explicit, his observation that in Khoisan women "the posterior part of the body projects in a most wonderful manner" (known as steatopygia)[17] implies sexual selection for this characteristic. In The Descent of Man, and Selection in Relation to Sex, Darwin viewed many physical traits which vary around the world as being so trivial to survival[18] that he concluded some input from sexual selection was required to account for their presence. He noted that variation in these features among the various peoples of the world meant human mate-choice criteria would also have to be quite different if the focus was similar, and he himself doubted that, citing[19] reports indicating that ideals of beauty did not, in fact, vary in this way around the world.
The effects on the human brain formation during puberty is directly linked to hormones changes.  The mismatch timing between biological puberty and age of social maturity in western society has a psychological expectation on children.[20] With puberty, men are generally hairier than women, and Darwin was of the opinion that hairlessness was related to sexual selection; however, several other explanations have been advanced to explain human hairlessness; a leading one being that loss of body hair facilitated sweating.[21] This idea closely relates to that of the suggested need for increased photoprotection and is part of the most-commonly-accepted scientific explanation for the evolution of pigmentary traits.[22]
Sexual dimorphism suggests the presence of sexual selection. The earliest homininae were highly dimorphic and that this tendency lessened over the course of human evolution, suggesting humans have become more monogamous. In contrast, gorillas living in harems exhibit a much stronger sexual dimorphism (see: homininae).[23]
The theory of sexual selection has been used to explain a number of human anatomical features. These include rounded breasts, facial hair, pubic hair and penis size. The breasts of primates are flat, yet are able to produce sufficient milk for feeding their young. The breasts of non-lactating human females are filled with fatty tissue and not milk. Thus it has been suggested the rounded female breasts are signals of fertility.[24] Richard Dawkins has speculated that the loss of the penis bone in humans, when it is present in other primates, may be due to sexual selection by females looking for a clear sign of good health in prospective mates. Since a human erection relies on a hydraulic pumping system, erection failure is a sensitive early warning of certain kinds of physical and mental ill health.[25]
Homo has a thicker penis than the other great apes, though it is on average no longer than the chimpanzee's.[26] It has been suggested the evolution of the human penis towards larger size was the result of female choice rather than sperm competition, which generally favors large testicles.[27] However, penis size may have been subject to natural selection, rather than sexual selection, due to a larger penis' efficiency in displacing the sperm of rival males during sexual intercourse. A model study showed displacement of semen was directly proportional to the depth of pelvic thrusting, as an efficient semen displacement device.[28]
Several factors drive sexual selection in humans.[29][30] Selection preferences are biologically driven,[31][32] that is, by the display of phenotypic traits that can be both consciously and unconsciously evaluated by the opposite sex to determine the health and fertility of a potential mate.[33] This process can be affected, however, by social factors, including in cultures where arranged marriage is practiced, or psychosocial factors, such as valuing certain cultural traits of a mate, including a person's social status, or what is perceived to be an ideal partner in various cultures.[34]
Some of the factors that affect how females select their potential mates for reproduction include voice pitch, facial shape, muscular appearance, and particularly height;[35][36][37][38] with the possibility of a Fisherian runaway in the making.[39]
Several studies suggest that there is a link between hormone levels and partner selection among humans. In a study measuring female attraction to males with varying levels of masculinity, it was established that women had a general masculinity preference for men's voices, and that the preference for masculinity was greater in the fertile phase of the menstrual cycle than in the non-fertile phase.[36] There is further evidence from the same study that in fertile stages of the menstrual cycle, women also had a preference for other masculine traits such as body size, facial shape, and dominant behavior, which are indicators of both fertility and health.[36] This study did not exclude males with feminine traits from being selected, however, as feminine traits in men indicate a higher probability of long-term relationship commitment,[36] and may be one of several evolutionary strategies.[40] Further research also backs up the idea of using phenotypic traits as a means of assessing a potential mate's fitness for reproduction as well as assessing whether a partner has high genetic quality.[41]
One study proposed a link between Human Development Index levels and female preference for male facial appearance.[42] While women from the United Kingdom preferred the faces of men with low cortisol levels, women from Latvia did not discriminate between men with either high or low levels of cortisol.[42] It was concluded that societal-level ecological factors impact the valuation of traits by combinations of sex- and stress-hormones.[42]
A 2020 study reported that women tend to find a man more attractive if the man's previous relationships ended mutually, and less attractive if the man was dumped.[43]
Like their female counterparts, males also use visual information about a potential mate, as well as voice, body shape, and an assortment of other factors in selecting a partner. Research shows that males tend to prefer feminine women's faces and voices as opposed to women with masculine features in these categories.[44] Furthermore, males also evaluate skin coloration, symmetry, and apparent health, as a means by which they select a partner for reproductive purposes.[44] Males are particularly attracted to femininity in women's faces when their testosterone levels are at their highest, and the level of attraction to femininity may fluctuate as hormone levels fluctuate.[45] Studies on men have also been done to show the effects of exogenous testosterone and its effects on attraction to femininity, and the results concluded that throughout several studies, men have shown decreased preference for feminine female faces in the long-term context, when given exogenous testosterone, but this difference did not occur with placebo.[46]
Sexual selection is in essence a process which favors sexual displays for attraction, aggressiveness, dominance, size, and strength, and the ability to exclude competitors by force if necessary, or by using resources to win.[47] Both male and female use voice, face, and other physical characteristics[34] to assess a potential mate's ability to reproduce, as well as their health.[33] Together with visual and chemical signals, these crucial characteristics which are likely to enhance the ability to produce offspring, as well as long-term survival prospects, can be assessed and selections made.[31][48]
Contest competition is form of sexual selection in which mating is obtained by using force or the threat of force to exclude same-sex competitors from mates.[49] Male contest competition favors large body size, which is seen in the sexual dimorphism of human males and females.[50] In all living hominid species, males are more muscular, allowing them to have more strength and power. Human males have 61% more overall muscle mass compared to females.[51] This greater muscle mass allows males to gain greater acceleration, speed, and more powerful striking movements.[52] Compared to females, human males exhibit more same-sex aggression, which peaks in young adulthood.[53][54][55][56]
Male contest competition also often favors threat displays, which allow one competitor to submit without a costly fight.[57] Low vocalization fundamental frequencies (perceived as vocal pitch) increase the perception of threat among human males.[58][59][60] Controlling for body size, lower male fundamental frequency relative to females tends to evolve in polygynous anthropoid primates, where males compete more intensely for mates.[61] Chimpanzees and humans have the greatest sexual dimorphism in fundamental frequency of all hominids.[61] Males are also more likely to engage in physical risks in front of competitors, and males who take more physical risks are perceived as being stronger.[62] Status badges such as facial hair are generally related to men being perceived as more dominant.[49] Facial hair makes the jaw appear more prominent and shows emotions like anger clearly which makes a male appear more threatening.[63][64] Dominance has been associated with increased male mating success.[65][66][67]
Often contest competition produces anatomical weapons such as antlers or large canine teeth; however, hominids lack canine weaponry typical of other primates.[49] Reduced canine size may be due to bipedalism and adaptations of the hand.[68][69] Bipedalism is not a common trait, yet many species like the great apes stand on their hind legs when fighting, which increases power behind blows.[70][49] Hominin hands are adapted for gripping tools or hurling objects like stones.[71][72][73][74] Bipedalism and utilizing handheld objects such as weapons may have aided early hominins in contest competition, reducing sexual selection pressures of maintaining large canine teeth.[68][73][75]
Several other traits in human males may have been selected for contest competition. Males exhibit a more robust face compared to females.[49] This may have provided protection against blows to the face during contest competitions as the areas on the skull that have increased robusticity are parts that are more likely to suffer from injury.[76]  Additionally, there are 23% more lefthanded males than females.[77] Although left-handedness is heritable and associated with survival disadvantages, the rarity of left-handedness may have given ancestral males a fighting advantage in competitions keeping this trait in the gene pool via negative frequency-dependent selection.[49][78][79][80] Many combat sports such as boxing have higher-than-chance frequencies of left-handed individuals among the top competitors.[81] Human males are also able to tolerate pain longer than females, especially during competition.[49][82][83]  A higher pain tolerance allows for males to remain aggressive during contests along with an increased aerobic capacity.[49] Males have an oxygen capacity rate that is 25–30% higher than females.[84][85] This aerobic capacity increases during puberty when males are sexually maturing and preparing to mate.[49]
Human males engage in both within-group contest competition and coalitional aggression.[49] The latter form competition may be supported by males tending to contribute more to a group task when competing against other groups and to discriminate more strongly against outgroup members.[86][87][88][89]
Traits that evolve during contest competition, such as large body size and physical aggression, are often costly to produce and maintain.[90] These traits may therefore be indicators of male genetic quality and/or ability to provide resources and other direct benefits.[90] Consequently, human females may evolve preferences for these traits, which then comprise an additional selection pressure.  However, secondary sexual characteristics in human males do not always enhance overall attractiveness to females.[91][92][93] Some traits of human males that function in contests, such as body size, strength, and weaponry usage, may also have been selected to aid in hunting.[49] However, contest competition is observed in all great apes and thus likely preceded hunting as a selective pressure.[49]
Human female mating competition is complex and multifaceted and varies across cultures, societies, and individuals.[94] Females may compete for high-quality mates who possess traits that indicate underlying genetic quality, possibly including physical attractiveness and intelligence,[95] or material resources that can enhance the survival and reproductive success of the female and her offspring.[96][95]
Females may also compete for leadership and reputation in social alliances and networks that can provide support, protection, and mating opportunities.[97][98] Human females compete with other females, sometimes including co-wives, to obtain and retain investment from mates, while managing cooperative same-sex relationships.[99]
Over human evolution, the cost of aggressive and physical contests in females may have been high given that females were the primary caregivers and protectors of offspring, so a mother's death greatly impacts infant mortality.[97][98] Some behaviors from mothers competing with other females at a similar life stage over resources include self-promotion and competitor derogation.[98] However, maternal competition remains understudied.  
Compared to male aggression, female aggression tends to be more indirect. Females tend to engage in more subtle and indirect aggression, such as gossip, as a competitive tool to harm same-sex rivals' social opportunities[100] and partake in competitor derogation to prevent female rivals from getting male attention.[98] Gossip, derogation, and social exclusion grant the aggressor the chance to go undetected and avoid retaliation. Derogation, for example, can eliminate same-sex rivals by reducing their ability to compete; it was found that girls' suicide attempts were associated with any amount of indirect peer victimization, whereas only frequent indirect peer victimization was associated with boys' suicide attempts.[98] Furthermore, same-sex harassment in some nonhuman animals impacted females' ovulation capabilities, which suggests that human females' reproductive success could be influenced by the stress induced by indirect or direct peer victimization.[98]  
Males pursue both sexually attractive and faithful long-term partners, which might be the source of female mating competition greatly revolving around denigrating same-sex rivals' attractiveness and reputation through accusations of promiscuity and infidelity.[98] Competitive women are more likely to spread reputation-harming information about other women, suggesting that reputation manipulation is a form of female competition for romantic partners.[101] 
Women are more likely to compete for desirable mates when maternal investment levels are high, and their social groups are largely composed of mothers,[102] as more women living closer together are looking for similar resources that benefit their own survival and that of their children.[98]
Competition for mates among human females may take multiple forms. Contests tend to be less frequent, aggressive, and injurious than male-male contests.[103] This leads to a difference in the traits selected. The indirect aggression in which females engage can take the form of damaging the reputation of other women (e.g., via gossip), potentially influencing their sexual behavior and opportunities.[104] Additionally, females compete with one another through male mate choice, e.g., by enhancing their own physical attractiveness.[104] Some female anatomical traits are targets of male mate choice and possibly represent female sexual ornaments shaped by selection. Femininity in the female face and voice provide cues to female reproductive hormones and reproductive potential.[105] Males tend to have lower pitched voices than females, likely due to male intrasexual competition,[106] but some evidence suggests that high female voice pitch may also be favored by male mate choice and function in intrasexual competition among females.[93]
Deposition of fat on the hips, buttocks, and breasts in human females may also be an outcome of female sexual selection, signaling the ability to support gestation and lactation for offspring in environments where resources may be low.[107][108] However, in the Western World, women with larger breasts are seen as more likely to commit infidelity and more likely to participate in intra-sexual competition with other females.[107] Greater overall body fat percentage in human females appears to be unique among primates and may function in storing resources needed to gestate and support large-brained offspring[109] as well as in sexual selection.[110] For example, higher female body mass index (BMI) is associated with increased fertility in young women, particularly those in subsistence societies.[111] Lower WHR, lower BMI, and smaller waist sizes are also associated with lower birth weights and higher infant mortality.[112] Such traits, particularly body fat distribution, may represent sexual ornamentation, which is important in mating throughout the animal kingdom, for example, in birds.[113][114] Humans also use bodily decoration, including jewelry, tattoos, scarification, and makeup to enhance appearance and desirability to potential mates.[107][115]
It has also been suggested that women who are nearing ovulation were more likely to be judged as more attractive than their counterparts who were in different stages of their cycle.[116] Facial and vocal attractiveness have been observed to change with estradiol and progesterone in pattens consistent with fertility-related increases,[117] although some data challenge this interpretation.[118] In general, ovulatory cycle changes are more subtle than in non-human primates, perhaps representing leakage of information on fertility and hormonal status rather than signals functioning to convey this information.[119]
John Manning[120] suggests that where polygyny is common, there is also a higher disease burden, resulting in selection for antimicrobial resistance. In this view, the antimicrobial properties of melanin help mitigate the susceptibility to disease in sub-Saharan Africa. According to this argument, the anti-infective qualities of melanin were more important than protection from ultraviolet light in the evolution of the darkest skin types. Manning asserts that skin color is more correlated with the occurrence of polygyny – because melanin has an antimicrobial function – than the latitudinal gradient in intensity of ultraviolet radiation.[120][121]
Research seems to contradict Manning's explanation about skin color. The analysis of indigenous populations from more than 50 countries has shown that the strongest correlation with light skin is upper latitude.[122] Rogers et al. (2004) concluded that dark skin evolved as a result of the loss of body hair among the earliest primate ancestors of humans.[123][124][125] and protect from folate depletion due to the increased exposure to sunlight.[126] When humans started to migrate away from the tropics, where there is less-intense sunlight, lighter skin is able to generate more vitamin D than darker skin, so it would have represented a health benefit in reduced sunlight, leading to natural selection for lighter skin.[124][127]
Anthropologist Peter Frost has proposed that sexual selection for women with unusual hair or eye color was responsible for the evolution of pigmentary traits in European populations,[128] however this theory has since been refuted by data-based evidence from genetics and spectrophotometry,[129][130] and multiple studies have shown that women with the facial features and pigmentary characteristics of East Asian women are considered more attractive than European women.[131][132][133]
Geoffrey Miller, drawing on some of Darwin's largely neglected ideas about human behavior, has hypothesized that many human behaviors not clearly tied to survival benefits, such as humor, music, visual art, some forms of altruism, verbal creativity, or the fact that most humans have a far greater vocabulary than that which is required for survival, can nevertheless play a role.[134] Miller (2000) has proposed that this apparent redundancy is due to individuals using vocabulary to demonstrate their intelligence, and consequently their "fitness", to potential mates. This has been tested experimentally, and it appears that males do make greater use of lower-frequency (more unusual) words when in a romantic mindset compared to a non-romantic mindset, suggesting that vocabulary is likely to be used as a sexual display (Rosenberg & Tunney, 2008). All these qualities are considered courtship adaptations that have been favored through sexual selection.[135]
Miller is critical of theories that imply that human culture arose as accidents or by-products of human evolution. He believes that human culture arose through sexual selection for creative traits. In that view, many human artifacts could be considered subject to sexual selection as part of the extended phenotype, for instance clothing that enhances sexually selected traits.[2] During human evolution, on at least two occasions, hominid brain size increased rapidly over a short period of time followed by a period of stasis. The first period of brain expansion occurred 2.5 million years ago, when Homo habilis first began using stone tools. The second period occurred 500,000 years ago, with the emergence of archaic Homo sapiens. Miller argues that the rapid increases in brain size would have occurred by a positive feedback loop resulting in a Fisherian runaway selection for larger brains. Tor Nørretranders, in The Generous Man conjectures how intelligence, musicality, artistic and social skills, and language might have evolved as an example of the handicap principle, analogously with the peacock's tail, the standard example of that principle.
The role of sexual selection in human evolution has been considered controversial from the moment of publication of Darwin's book on sexual selection (1871). Among his vocal critics were some of Darwin's supporters, such as Alfred Wallace, a believer in spiritualism and a non-material origin of the human mind, who argued that animals and birds do not choose mates based on sexual selection, and that the artistic faculties in humans belong to their spiritual nature and therefore cannot be connected to natural selection, which only affects the animal nature.[10] Darwin was accused of looking to the evolution of early human ancestors through the moral codes of the 19th century Victorian society.

A large amount of research on prehistory has been dedicated to the role of women in prehistoric society. Tasks typically undertaken by women are thought to have formed a major sexual division of labor in relation to child-rearing, gathering, and other everyday occupations. More recent research has however suggested women also played an active role in hunting and other physical activities in place of the exclusively domestic roles traditionally occupied by women in literary civilizations.[1][2][a]
The study of prehistoric women is of particular interest to feminist and gender archeology, which seek to challenge androcentric assumptions in conventional archeology.
A major point of contention throughout anthropology from as early as the 19th century was the difference, if any, in social status between prehistoric and contemporary women. Early socialistic thinkers such as Lewis H. Morgan, Friedrich Engels or August Bebel openly equated matrilineality with primitive communism and patrilineality with individualism, oppression, and private property.[3][4] Such schools typically argued that due to the lack of a definitive line of paternal descent without socially enforced monandry, prehistoric societies instead practiced matrifocal, communal motherhood.[3][5]
Similar ideas arose during the second wave of feminism with the increased study of matrilocality and matriarchal religion, such as Marija Gimbutas's theory of a matristic, egalitarian "Old Europe" later outcompeted and conquered by the patriarchal and expansionist Proto-Indo-Europeans.[6] Such interpretations remain highly controversial due to perceptions of political bias or lack of material evidence,[7] but have been defended by notable figures such as anthropologist Chris Knight, who instead criticized what he saw as ad-hoc functionalist attempts to downplay obvious matrilineal traditions in contemporary tribal societies.[5]
From the 1970s onward, the dominant scientific perspective of gendered roles in hunter-gatherer societies was of a model termed "Man the Hunter, Woman the Gatherer". Coined by anthropologists Richard Borshay Lee and Irven DeVore in 1968, it argued that contemporary foragers displayed a clear division of labor between women and men.[1] More recent evidence compiled by researchers such as Sarah Lacy and Cara Ocobock has found a lack of conclusive preferences for gendered roles among modern hunter-gatherer societies.[1] Recent archeological research done by the anthropologist and archeologist Steven Kuhn suggests that the sexual division of labor did not exist prior to the Upper Paleolithic (50,000 and 10,000 years ago) and developed relatively recently in human history.[8] Lacy and Ocobock in particular highlighted the role of estrogen in the potential contributions of women to everyday survival in prehistoric societies; contrary to popular belief, testosterone only significantly affects the development of type 2 muscle fibers when compared to estrogen, which instead primarily affects the development of type 1 fibers. Type 2 muscles perform better in short-term "power" activities, such as weight-lifting or spear-throwing, while type 1 muscles perform better in long-term, endurance-based "marathon" activities.[1] Women's muscles are thus more energy-efficient, which implies that persistence hunting, a technique thought to have formed one of the main evolutionary advantages of hominids over their otherwise far more mobile prey, would have been easier for women to perform than men.[1] A following study found "that multiple methodological failures all bias their results in the same direction...their analysis does not contradict the wide body of empirical evidence for gendered divisions of labor in foraging societies".[9]
Notable hunter-gatherer groups in recent or contemporary eras known to lack a distinct sexual division of labor include the Ainu,[1] Agta,[10] and Ju/'hoansi,[11] in addition to significant material evidence for female involvement in hunting among prehistoric cultures such as those in what is today Peru.[2]
The Upper Paleolithic era is known for displaying a wealth of artistic representations of women, which are generally grouped together under the term of Venus figurines as some of the first works of human culture in history. Venus figurines are noted for their exaggerated sexual characteristics, commonly taken as symbols of fertility and sexuality, and include the earliest known representation of a human being; Known as the Venus of Hohle Fels, it is described by anthropologist Nicholas Conard as "about sex, reproduction... [it is] an extremely powerful depiction of the essence of being female".[12] Other notable figurines include the Willendorf, Dolní Věstonice, and Moravany Venuses, all of which are distinguished by a focus on the hips, breasts, and stomach.[13] Examples are generally centered around Europe, inhabited at the time by relatively advanced Cro-Magnon cultures, though the term has been applied as far abroad as Siberia.[14] Similar motifs from subsequent eras include the Potnia Theron, found in the ancient Mediterranean and Near East.[15][16]
Some feminist archeologists, such as Kaylea Vandewettering or Leroy McDermott, have criticized the male gaze involved in terming and categorizing the Venuses, the name of which originates from the first figurine to be recovered, the Vénus Impudique. Coined the "Immodest Venus" by its discoverer, it was named for both contemporary European views of sex and for a perceived association with the sexuality and fertility ascribed to the Roman Venus, despite the Paleolithic cultures responsible predating Greco-Roman religions by millennia and no materially substantiated consensus as to the figurines' significance ever being reached among researchers.[17]
McCoid and McDermott suggested that because of the way these figures are depicted, such as the large breasts and lack of feet and faces, these statues were made by women looking at their own bodies. They state that women during the period would not have had access to mirrors to maintain accurate proportions or depict the faces or heads of the figurines. The theory remains difficult to prove or disprove, and Michael S. Bisson suggested that alternatives, such as puddles, could have been used as mirrors.[18]
Lacy and Ocobock stated that burial sites from the Upper Paleolithic did not demonstrate any difference between the grave goods or posthumous treatment afforded to men compared to women, further suggesting a lack of "social hierarchies based on sex".[1]
The more general archeological record has found many notable examples of lavish tombs and burial practices for women, including famous cases such as the Egtved Girl and Princess of Ukok. Excavations have yielded a wealth of well-preserved grave goods including stitching awls, medicinal supplies, cosmetics, hairnets, and miscellaneous decorative items.[20][21]
Study of the human mitochondria has allowed geneticists to begin pinpointing the matrilineal most recent common ancestor of all humanity, i.e. the last-living woman from which all modern humans are descended in an unbroken mother-to-daughter line, known as the Mitochondrial Eve in reference to the Genesis creation myth. As of 2015, estimates of the age of the Y-MRCA range around 200,000 to 300,000 years ago, roughly consistent with the emergence of anatomically modern humans.[22]
Researchers also noted that the estrogen receptor seen in humans is anywhere from 1.2 billion to 600 million years older than the equivalent androgen receptor, indicating it likely had a major evolutionary role in the development of humanity's ancestors.[1]
Genetic admixture between Neanderthals and anatomically modern humans has been found to display a clear difference between sexes, with the complete lack of mitochondrial Neanderthal DNA indicating that surviving hybrid lineages originated specifically from neanderthalensis male-sapiens sapiens female couplings.[23]

Triticum monococcum subsp. monococcum
Triticum monococcum ssp. boeoticum
Einkorn wheat (from German Einkorn, literally "single grain") is either a wild species of wheat (Triticum) or its domesticated form. The wild form is T. boeoticum (syn. T. m. subsp. boeoticum), and the domesticated form is T. monococcum (syn. T. m. subsp. monococcum). Einkorn is a diploid species (2n = 14 chromosomes) of hulled wheat, with tough glumes (husks) that tightly enclose the grains. The cultivated form is similar to the wild, except that the ear stays intact when ripe[1] and the seeds are larger. The domestic form is known as petit épeautre in French, Einkorn in German, "einkorn" or "littlespelt" in English, piccolo farro in Italian and escanda menor in Spanish.[2] The name refers to the fact that each spikelet contains only one grain.
Einkorn wheat was one of the first plants to be domesticated and cultivated. The earliest clear evidence of the domestication of einkorn dates from 10,600 to 9,900 years before present (8650 BCE to 7950 BCE) from Çayönü and Cafer Höyük, two Early Pre-Pottery Neolithic B archaeological sites in southern Turkey.[3] Remnants of einkorn were found with the iceman mummy Ötzi, dated the late 4th millennium BCE.[4]
Einkorn is a short variety of wild wheat, usually less than 70 centimetres (28 in) tall and is not very productive of edible seeds.[5] The principal difference between wild einkorn and cultivated einkorn is the method of seed dispersal. In the wild variety the seed head usually shatters and drops the kernels (seeds) of wheat onto the ground.[1] This facilitates a new crop of wheat. In the domestic variety, the seed head remains intact. While such a mutation may occasionally occur in the wild, it is not viable there in the long term: the intact seed head will only drop to the ground when the stalk rots, and the kernels will not scatter but form a tight clump which inhibits germination and makes the mutant seedlings susceptible to disease. But harvesting einkorn with intact seed heads was easier for early human harvesters, who could then manually break apart the seed heads and scatter any kernels not eaten. Over time and through selection, conscious or unconscious, the human preference for intact seed heads created the domestic variety, which has slightly larger kernels than wild einkorn. Domesticated einkorn thus requires human planting and harvesting for its continuing existence.[6] This process of domestication may have taken only 20 to 200 years, resulting in a wheat that was easier to harvest.[7]
An important characteristic facilitating the domestication of einkorn and other annual grains is that the plants are largely self-pollinating. Thus, the desirable (for human management) traits of einkorn could be perpetuated at less risk of cross-fertilization with wild plants which might have traits – e.g. smaller seeds, shattering seed heads,[1] as less desirable for human management.[8]
Einkorn wheat commonly grows wild in the hill country in the northern part of the Fertile Crescent and Anatolia, although it has a wider distribution reaching into the Balkans and south to Jordan near the Dead Sea.[5]
Einkorn wheat is one of the earliest cultivated forms of wheat, alongside emmer wheat (T. dicoccum).  Hunter gatherers in the Fertile Crescent may have started harvesting einkorn as early as 30,000 years ago, according to archaeological evidence from Syria.[9][10][11]
Although gathered from the wild for thousands of years, einkorn wheat was first domesticated approximately 10,000 years BP in the Pre-Pottery Neolithic A (PPNA) or B (PPNB) periods.[12] Evidence from DNA fingerprinting suggests einkorn was first domesticated near Karaca Dağ in southeast Turkey, an area in which a number of PPNB farming villages have been found.[13] One theory by Yuval Noah Harari suggests that the domestication of einkorn was linked to intensive agriculture to support the nearby Göbekli Tepe site.[14]
From the northern part of the Fertile Crescent, the cultivation of einkorn wheat spread to the Caucasus, the Balkans, and central Europe. Einkorn wheat was more commonly grown in cooler climates than emmer wheat, the other domesticated wheat. Cultivation of einkorn in the Middle East began to decline in favor of emmer wheat around 2000 BC. Cultivation of einkorn was never extensive in Italy, southern France, and Spain. Einkorn continued to be cultivated in some areas of northern Europe throughout the Middle Ages and until the early part of the 20th century.[5]
Cultivated Einkorn was described as a taxon, Triticum monococcum, by Carl Linnaeus in 1753. Later descriptions by other taxonomists, now treated as synonyms, include Triticum pubescens by von Bieberstein in 1800; Triticum hornemanii by Clementi in 1818; Nivieria monococcum  in 1841; Triticum vulgare monococcum by Alefeld in 1866; Triticum monococcum subsp. cereale by Albert Thellung in 1918.[15] Wild Einkorn is known either as Triticum monococcum subsp. aegilopoides or as Triticum boeoticum.[15]
Wild and domesticated einkorns are diploid wheats. Unlike emmer and bread wheat, which were formed by hybridisation with Aegilops goatgrasses, einkorn is not a hybrid.[16]
Einkorn wheat is low-yielding but can survive on poor, dry, marginal soils where other varieties of wheat will not. It is primarily eaten boiled in whole grains or in porridge.[5] As with other ancient varieties of wheat such as emmer, Einkorn is a "covered wheat" as its kernels do not break free from its seed coat (glume) with threshing. This makes it difficult to separate the husk from the seed.[17]
Einkorn is a common food in northern Provence (France).[18] It is used for bulgur or as animal feed in mountainous areas of countries including France, India, Italy, Morocco, the former Yugoslavia, and Turkey.[17] It contains gluten (so is not suitable for people with gluten-related disorders[19]) and has a higher percentage of protein than modern red wheats. It is considered more nutritious because it has higher levels of fat, phosphorus, potassium, pyridoxine, and beta-carotene.[17]
Einkorn is the source of many potential introgressions for immunity – Nikolai Vavilov called it an "accumulator of complex immunities."[15] T. monococcum is the source of Sr21, a stem rust resistance gene which has been introgressed into hexaploid worldwide.[20] It is also the source of Yr34, a resistance gene for yellow rust.[21]
The salt-tolerance feature of T. monococcum has been bred into durum wheat.[22]

The Handbook of North American Indians is a series of edited scholarly and reference volumes in Native American studies, published by the Smithsonian Institution beginning in 1978. Planning for the handbook series began in the late 1960s and work was initiated following a special congressional appropriation in fiscal year 1971.[1]
To date, 16 volumes have been published. Each volume addresses a subtopic of Americanist research and contains a number of articles or chapters by individual specialists in the field coordinated and edited by a volume editor. The overall series of 20 volumes is planned and coordinated by a general or series editor. Until the series was suspended, mainly due to lack of funds, the series editor was William C. Sturtevant, who died in 2007.[2]
This work documents information about all Indigenous peoples of the Americas north of Mexico, including cultural and physical aspects of the people, language family, history, and worldviews. This series is a reference work for historians, anthropologists, other scholars, and the general reader. The series utilized noted authorities for each topic. The set is illustrated, indexed, and has extensive bibliographies. Volumes may be purchased individually.
Handbook of North American Indians / William C. Sturtevant, General Editor. Washington, DC : Smithsonian Institution: For sale by the U.S. Government Printing Office, Superintendent of Documents., 1978–.
Krupnik, Igor (2022), Introduction, Washington, D.C.: Smithsonian Institution, p. 931, ISBN 978-1-944466-53-4, https://doi.org/10.5479/si.21262173
Native American Histories in the Twenty-First Century
New Cultural Domains
Native American Experiences in the Twenty-First Century
Transitions in Native North American Research
The Smithsonian Handbook Project, 1965–2008
End Matter
Bailey, Garrick A. (2008), Indians In Contemporary Society, Washington, D.C.: Smithsonian Institution, p. 577, ISBN 978-0-16-080388-8
The Issues in the United States
The Issues in Canada
Demographic and Ethnic Issues
Social and Cultural Revitalization
Ubelaker, Douglas H. (2006), Environment, Origins, and Population, Washington, D.C.: Smithsonian Institution, pp. 1, 146, ISBN 978-0-16-077511-6
Paleo-Indian
Plant and Animal Resources
Skeletal Biology and Population Size
Human Biology
Washburn, Wilcomb E. (1988), History of Indian-White Relations, Washington, D.C.: Smithsonian Institution, p. 852, ISBN 978-0-16-004583-7
National Policies
Military Situation
Political Relations
Economic Relations
Religious Relations
Conceptual Relations
Damas, David (1984), Arctic, Washington, D.C.: Smithsonian Institution, p. 845, ISBN 978-0-16-004580-6
Western Arctic
Canadian Arctic
Greenland
The 1950-1980 Period
Helm, June (1981), Subarctic, Washington, D.C.: Smithsonian Institution, p. 853, ISBN 978-0-16-004578-3
Subarctic Shield and Mackenzie Borderlands
Subarctic Cordillera
Alaska Plateau
South of the Alaska Range
Native Settlements
Special Topics
Suttles, Wayne (1990), Northwest Coast, Washington, D.C.: Smithsonian Institution, p. 793, ISBN 978-0-16-020390-9
History of Research
History of Contact
The Peoples
Special Topics
Heizer, Robert F. (1978), California, Washington, D.C.: Smithsonian Institution, p. 816, ISBN 978-0-16-004574-5
Ortiz, Alfonso (1979), Southwest, Washington, D.C.: Smithsonian Institution, p. 701, ISBN 978-0-16-004577-6, OCLC 26140053
Volume 9 covers the Pueblo tribes of the Southwest. Volume 10 covers the non-Pueblo tribes of the Southwest.
Ortiz, Alfonso (1983), Southwest, Washington, D.C.: Smithsonian Institution, p. 884, ISBN 978-0-16-004579-0
Volume 10 covers the non-Pueblo tribes of the Southwest. Volume 9 covers the Pueblo tribes of the Southwest.
d'Azevedo, Warren L. (1986), Great Basin, Washington, D.C.: Smithsonian Institution, p. 868, ISBN 978-0-16-004581-3
Prehistory
Ethnology
History
Special Topics
Walker, Deward E. Jr. (1998), Plateau, Washington, D.C.: Smithsonian Institution, p. 807, ISBN 978-0-16-049514-4
Prehistory
History
The Peoples
Special Topics
DeMallie, Raymond J. (2001), Plains, Washington, D.C.: Smithsonian Institution, p. 1376, ISBN 978-0-87474-193-3
Volume 13 is physically bound in two volumes (Part 1 and Part 2), but page numbering is continuous between the two parts. Part 1 ends at "Plains Métis", page 676.
Prehistory
History
Prairie Plains
High Plains
Special Topics
Fogelson, Raymond D. (2004), Southeast, Washington, D.C.: Smithsonian Institution, p. 1058, ISBN 978-0-16-072300-1
Regional Prehistory
History
Florida
Atlantic Coastal Plain
Interior Southeast
Mississippi Valley and Gulf Coastal Plain
Special Topics
Trigger, Bruce G. (1978), Northeast, Washington, D.C.: Smithsonian Institution, p. 924, ISBN 978-0-16-004575-2
General Prehistory
Coastal Region
Saint Lawrence Lowlands Region
Great Lakes-Riverine Region
Goddard, Ives (1996), Languages, Washington, D.C.: Smithsonian Institution, p. 958, ISBN 978-0-87474-197-1
The map "Native Languages and Language Families of North America" compiled by Ives Goddard is included in a pocket in the inside cover along with a small photographic reproduction of John Wesley Powell's 1891 map, "Linguistic Stocks of American Indians North of Mexico". A wall size version of the former is available separately (ISBN 978-0-8032-9269-7).
Grammatical Sketches
With the suspension of publication, the following volumes remain unpublished.

This timeline of prehistory covers the time from the appearance of Homo sapiens approximately 315,000 years ago in Africa to the invention of writing, over 5,000 years ago, with the earliest records going back to 3,200 BC. Prehistory covers the time from the Paleolithic (Old Stone Age) to the beginning of ancient history.
All dates are approximate and subject to revision based on new discoveries or analyses.
"Epipaleolithic" or "Mesolithic" are terms for a transitional period between the Last Glacial Maximum and the Neolithic Revolution in Old World (Eurasian) cultures.
The terms "Neolithic" and "Bronze Age" are culture-specific and are mostly limited to cultures of select parts of the Old World, namely Europe, Western and South Asia. Chronological periodizations typically base their periods on one or more identifiable and unique markers associated with a culturally distinct era (within a given interaction sphere), but these markers are not necessarily intrinsic to the cultural evolution of the era's people.
As such, the terms become less applicable when their markers correlate less with cultural evolution. Therefore, the Neolithic and the Neolithic Revolution have little to do with the Americas, where several different chronologies are used instead depending on the area (e.g. the Andean Preceramic, the North American Archaic and Formative periods). Similarly, since there is no appreciable cultural shift between the use of stone, bronze, and iron in East and Southeast Asia, the term "Bronze Age" is not considered to apply to this region the same as western Eurasia, and "Iron Age" is essentially never used.[89][90] In sub-Saharan Africa, iron metallurgy was developed prior to any knowledge of bronze and possibly before iron's adoption in Eurasia[91] and despite Postclassic Mesoamerica developing and using bronze,[92][93][94] it did not have a significant bearing on its continued cultural evolution in the same way as western Eurasia.
Researchers deduced in a scientific review that "no specific point in time can currently be identified at which modern human ancestry was confined to a limited birthplace" and that current knowledge about long, continuous and complex – e.g. often non-singular, parallel, nonsimultaneous and/or gradual – emergences of characteristics is consistent with a range of evolutionary histories.[154][155] A timeline dating first occurrences and earliest evidence may therefore be an often inadequate approach for describing humanity's (pre-)history.

Hominization, also called anthropogenesis, refers to the process of becoming human, and is used in somewhat different contexts in the fields of paleontology and paleoanthropology, archaeology, philosophy, theology, and mythography. In the latter three fields, the alternative term anthropogony has also been used. Both anthropogenesis and anthropogony sometimes instead refer to the related subject of human evolution.
As of 2013[update], paleoanthropologists tend to regard the search for a precise point of hominization as somewhat irrelevant, seeing the process as gradual. Anatomically modern humans (AMH, or AMHS) developed within the species Homo sapiens about 200,000 years ago.
Many thinkers have attempted to explain hominization – from Classical times through Hobbes, Rousseau, Hegel, and Engels, who wrote an essay on The Part Played by Labour in the Transition from Ape to Man. The contemporary study of hominization in archeology often looks for signs that mark out human habitations from pre-human forms: for example, the use of grave goods.[1][2]
In ancient philosophy, "hominization" referred to the ensoulment of the human fetus. When the soul is said to enter the fetus at some time later than conception, this is sometimes called "delayed hominization", as in the Aristotelian belief in ensoulment 40 days after conception.[3][4]
In the context of modern theistic evolution, "hominization" refers to the theory that there was a point at which a population of hominids who had (or may have) evolved by a process of evolution acquired souls and thus (with their descendants) became fully human in theological terms.  This group might be restricted to Adam and Eve, or indeed to Mitochondrial Eve, although versions of the theory allow for larger populations. The point at which this occurred should essentially be the same as in paleoanthropology and archeology, but theological discussion of the matter tends to concentrate on the theoretical.  The term "special transformism" refers to theories of a divine intervention of some sort, achieving hominization.[5]
The process and means by which hominization occurs is a key problem in theistic evolutionary thought, at least for the Abrahamic religions, for which the belief that animals do not have souls but humans do is a core teaching.  Scientific accounts of the origin of the universe, origin of life, and subsequent evolution of pre-human life forms may not cause any difficulty (helped by the reluctance of science itself to say anything about what preceded the Big Bang) but the need to reconcile religious and scientific views of hominization and account for the addition of a soul to humans remains a problem.  Several 19th-century theologians attempted specific solutions, including the Catholics John Augustine Zahm and St. George Jackson Mivart, but tended to come under attack from both the theological and biological camps.[6] 20th-century thinking has tended to avoid proposing precise mechanisms.[7]
Origin myths of humanity and of particular peoples are a frequent hominization-related subject of study within mythography, folkloristics, and comparative religion.

The Prehistoric Period is the oldest part of Cypriot history. This article covers the period 11,000 to 800 BC and ends immediately before the documented history of Cyprus begins.
Prior to the arrival of humans in Cyprus, only four terrestrial mammal species were present on the islands, including the Cypriot pygmy hippopotamus and the Cyprus dwarf elephant, which were much smaller than their mainland ancestors as a result of insular dwarfism. The ancestors of these species arrived on Cyprus at least 200,000 years ago,[1][2] with the other species being the genet Genetta plesictoides and the still living Cypriot mouse.[3] The earliest humans to inhabit Cyprus were hunter gatherers who arrived on the island around 13–12,000 years ago (11–10,000 BC), with some of the oldest well-dated sites being Aetokremnos on the south coast, which is suggested to show evidence of hunting of the dwarf hippopotamus and dwarf elephant, and the inland site of Roudias in southeast Cyprus. These hunter-gatherers are suggested to have originated from the Natufian culture of the neighbouring Levant.[4] The last records of the endemic mammals other than the mouse date to shortly after human settlement. The hunter gatherers later introduced wild boar to the island around 12,000 years ago, likely to act as a source of food.[5]
The oldest evidence of Neolithic settlement is dated to 8800–8600 BC.[6] The first settlers were already agriculturalists (PPNB), but did not yet produce pottery (aceramic Neolithic).[7] They introduced the dog, sheep, goats and maybe cattle and pigs as well as numerous wild animals like foxes (Vulpes vulpes) and Persian fallow deer (Dama mesopotamica) that were previously unknown on the island. The PPNB settlers built round houses with floors made of terrazzo of burned lime (e.g. Kastros, Shillourokambos, Tenta) and cultivated einkorn and emmer. Pig, sheep, goat and cattle were kept, but remained morphologically wild. Evidence for cattle (attested at Shillourokambos) is rare and when they apparently died out in the course of the 8th millennium they were not reintroduced until the early Bronze Age.
In the 6th millennium BC, the aceramic Choirokoitia culture (Neolithic I) was characterized by round houses (tholoi), stone vessels and an economy based on sheep, goats and pigs. The daily life of the people in those Neolithic villages was spent in farming, hunting, animal husbandry and the lithic industry, while homesteaders (likely women) were engaged in spindling and weaving cloths, in addition to their probable participation in other activities. The lithic industry was the most individual feature of this aceramic culture and innumerable stone vessels made of grey andesite have been discovered during excavations. The houses had a foundation of river pebbles, the remainder of the building was constructed in mudbrick. Sometimes several round houses were joined together to form a kind of compound. Some of these houses reach a diameter of up to 10 m. Inhumation burials are located inside the houses.
Water wells discovered by archaeologists in western Cyprus are believed to be among the oldest in the world, dated at 9,000 to 10,500 years old, putting them in the Stone Age. They are said to show the sophistication of early settlers, and their heightened appreciation for the environment.[8]
Plant remains indicate the cultivation of cereals, lentils, beans, peas and a kind of plum called Bullace. Remains of the following animal species were recovered during excavations: Persian fallow deer, goat, sheep, mouflon and pig. More remains indicate Red deer, Roe deer, a kind of horse and a kind of dog but no cattle as yet.
Life expectancy seems to have been short; the average age at death appears to have been about 34 years, and there was a high infant mortality rate.
In 2004, the remains of an 8-month-old cat were discovered buried with its human owner at a Neolithic archeological site in Cyprus.[9] The grave is estimated to be 9,500 years old, predating Egyptian civilization and pushing back the earliest known feline-human association significantly.[10]
The aceramic civilisation of Cyprus came to an end quite abruptly around 6000 BC. It was probably followed by a vacuum of almost 1,500 years until around 4500 BC when one sees the emergence of Neolithic II (Ceramic Neolithic).
At this time newcomers arrived in Cyprus introducing a new Neolithic era. The main settlement that embodies most of the characteristics of the period is Sotira near the south coast of Cyprus. The following ceramic Sotira phase (Neolithic II) has monochrome vessels with combed decoration. It had nearly fifty houses, usually having a single room that had its own hearth, benches, platforms and partitions that provided working places. The houses were on the main free-standing, with relatively thin walls and tended to be square with rounded corners. The sub-rectangular houses had two or three rooms. In Khirokitia, the remains of the Sotira phase overlay the aceramic remains. There are Sotira-ceramics in the earliest levels of Erimi as well. In the North of the island, the ceramic levels of Troulli may be synchronous with Sotira in the South.
The Late Neolithic is characterised by a red-on white ware. The late Neolithic settlement of Kalavassos-Pamboules has sunken houses.
The Neolithic culture was destroyed by an earthquake c. 3800 BC. In the society that emerged there are no overt signs of newcomers but signs of continuity, therefore despite the violent natural catastrophe, there is an internal evolution that is formalised around 3500 BC with appearance of the first metalwork and the beginning of the Chalcolithic (copper and stone) period that lasted until about 2500/2300 BC. Very few chisels, hooks and jewellery of pure copper have survived, but in one example there is a minimal presence of tin, something which may support contact with Asia Minor, where copper-working was established earlier.
During the Chalcolithic period changes of major importance took place along with technological and artistic achievements, especially towards its end. The presence of a stamp seal and the size of the houses that was not uniform, both hint at property rights and social hierarchy. The same story is supported by the burials because some of them were deposited in pits without grave goods and some in shaft graves with relatively rich furniture, both being indications of wealth accumulation by certain families and social differentiation.
The Eneolithic or Chalcolithic period is divided into the Erimi (Chalcolithic I) and Ambelikou/Ayios Georghios (Chalcolithic II) phases.
The type site of the Neolithic I period is Erimi on the South coast of the island. The ceramic is characterised by red-on white pottery with linear and floral designs. Stone (steatite) and clay figurines with spread arms are common. In Erimi, a copper chisel has been found, this is the oldest copper find in Cyprus so far. Otherwise, copper is still rare. Another important Chalcolithic site is Lempa (Lemba).
The Chalcolithic period did not come to an end at the same time throughout Cyprus, and lingered in the Paphos area until the arrival of the Bronze Age.
The new era was introduced by people from Anatolia who came to Cyprus about 2400 BC. The newcomers are identified archaeologically because of a distinct material culture, known as the Philia Culture. This was the earliest manifestation of the Bronze Age. Philia sites are found in most parts of the island.
As the newcomers knew how to work with copper they soon moved to the so-called copperbelt of the island, that is the foothills of the Troodos Mountains. This movement reflects the increased interest in the raw material that was going to be so closely connected with Cyprus for several centuries afterwards.
The Philia phase of the Bronze Age (or Philia phases) saw a rapid transformation of technology and economy.[11] Rectilinear buildings made of mud-brick, the plough, the warp-weighted loom and clay pot stands are among the characteristic introductions. Cattle were reintroduced, together with the donkey.
The succeeding Early Bronze Age is divided into three general phases (Early Cypriot I - III) - a continuous process of development and population increase. Marki Alonia is the best excavated settlement of this period.
Marki Alonia and Sotira Kaminoudhia are excavated settlements. Many cemeteries are known, the most important of which is Bellapais Vounous on the North coast.
The Middle Bronze Age, which follows the Early Bronze Age (1900–1600 BC), is a relatively short period and its earlier part is marked by peaceful development.
The Middle Bronze Age is known from several excavated settlements: Marki Alonia, Alambra Mouttes and Pyrgos Mavroraki.  These give evidence of economy and architecture of the period.
From Alambra and Marki in central Cyprus we know that the houses were rectangular with many rooms, with lanes allowing people to move freely in the community. At the end of the Middle Bronze Age, fortresses were built in various places, a clear indication of unrest, although the cause is uncertain.
The most important cemeteries are at Bellapais, Lapithos, Kalavasos and Deneia. An extensive collection of Bronze Age pottery can be seen online from the cemeteries at Deneia.[12]
The up to now oldest copper workshops have been excavated at Pyrgos-Mavroraki, 90 km southwest of Nicosia.  Cyprus was known as Alashiya, the name is preserved in Egyptian, Hittite, Assyrian and Ugaritic documents.  The first recorded name of a Cypriot king is Kushmeshusha, as appears on letters sent to Ugarit in the 13th century BC.[13]
The beginning of the Late Bronze Age does not differ from the closing years of the previous period. Unrest, tension and anxiety mark all these years, probably because of some sort of engagement with the Hyksos, who ruled Egypt at this time but were expelled from there in the mid-1500s BC. Soon afterwards peaceful conditions prevailed in the Eastern Mediterranean that witnessed a flowering of trade relations and the growing of urban centres. Chief among them was Enkomi, near modern Famagusta, though several other harbour towns also sprang up along the southern coast of Cyprus. Around 1500 BC, Thutmose III claimed Cyprus and imposed a tax on the island.
Literacy was introduced to Cyprus with the Cypro-Minoan syllabary, a derivation from Cretan Linear A. It was first used in early phases of the late Bronze Age (LCIB, 14th century BC) and continued in use for c. 400 years into the LC IIIB, maybe up to the second half of the 11th century BC. It likely evolved into the Cypriot syllabary.
The Late Cypriot (LC) IIC (1300–1200 BC) was a time of local prosperity. Cities were rebuilt on a rectangular grid plan, like Enkomi, where the town gates now correspond to the grid axes and numerous grand buildings front the street system or newly founded. Great official buildings (constructed from ashlar-masonry) point to increased social hierarchisation and control. Some of these buildings contain facilities for processing and storing olive oil, like at Maroni-Vournes and "building X" at Kalavassos-Ayios Dhimitrios. Other ashlar-buildings are known from Palaeokastro. A Sanctuary with a horned altar constructed from ashlar-masonry has been found at Myrtou-Pigadhes, other temples have been located at Enkomi, Kition and Kouklia (Palaepaphos). Both the regular layout of the cities and the new masonry techniques find their closest parallels in Syria, especially in Ugarit (modern Ras Shamra).
Rectangular corbelled tombs point to close contacts with Syria and Canaan (probably around the emergence of ancient Israelites) as well. The practice of writing spread, and tablets in the Cypro-Minoan script have been found on the mainland as well (Ras Shamra). Ugaritic texts from Ras Shamra and Enkomi mention "Ya", the Assyrian name of Cyprus, that thus seems to have been in use already in the late Bronze Age.
Cyprus was, at some times, a part of the Hittite empire but was a client state and as such was not invaded but rather merely part of the empire by association and governed by the ruling kings of Ugarit.[14] As such Cyprus was essentially "left alone with little intervention in Cypriot affairs".[14] However, during the reign of Tudhaliya IV the island was briefly invaded by the Hittites for either reasons of securing the copper resource or as a way of preventing piracy. Shortly afterwards the island had to be reconquered again by his son Suppiluliuma II, around 1200 BC. Some towns (Enkomi, Kition, Palaeokastro and Sinda) show traces of destruction at the end of LC IIC. Originally, two waves of destruction, c. 1230 BC by the Sea Peoples and 1190 BC by Aegean refugees, or 1190 and 1179 BC according to Paul Aström had been proposed. Some smaller settlements (Ayios Dhimitrios and Kokkinokremnos) were abandoned but do not show traces of destruction.
The years of peace that brought about such a flowering of culture and civilisation did not last. During these years Cyprus reached unprecedented heights in prosperity and it played a rather neutral role in the differences of her powerful neighbours.
Rich finds from this period testify to a vivid commerce with other countries. We have jewellery and other precious objects from the Aegean along with pottery that prove the close connections of the two areas, though finds coming from Near Eastern countries are also plentiful.
In the later phase of the late Bronze Age (LCIIIA, 1200–1100 BC) great amounts of "Mycenaean" IIIC:1b pottery were produced locally. New architectural features include Cyclopean walls, found on the Greek mainland as well and a certain type of rectangular stepped capitals, endemic on Cyprus. Chamber tombs are given up in favour of shaft graves.
Cyprus was settled by Mycenaean Greeks by the end of the Bronze Age, beginning the Hellenization of the island.[15] Large amounts of IIIC:1b pottery are found in Palestine during this period as well. There are finds that show close connections to Egypt as well. In Hala Sultan Tekke Egyptian pottery has been found, among them wine jugs bearing the cartouche of Seti I and fish bones of the Nile perch.
Another Greek wave of colonization is believed to have taken place in the following century (LCIIIB, 1100–1050), indicated, among other things, by a new type of graves (long dromoi) and Mycenean influences in pottery decoration.
Most authors claim that the Cypriot city kingdoms, first described in written sources in the 8th century BC were already founded in the 11th century BC. Other scholars see a slow process of increasing social complexity between the 12th and the 8th centuries, based on a network of chiefdoms. In the 8th century (geometric period) the number of settlements increases sharply and monumental tombs, like the 'Royal' tombs of Salamis appear for the first time. This could be a better indication for the appearance of the Cypriot kingdoms. This period shows the appearance of large urban centers.
The Iron Age follows the Submycenean period (1125–1050 BC) or Late Bronze Age and is divided into the:
In the ensuing Early Iron Age Cyprus becomes predominantly Greek. Pottery shapes and decoration show a marked Aegean inspiration although Oriental ideas creep in from time to time. Pottery types also appear from other Mediterranean cultures as evidenced from in archaeological recovery on Cyprus of pottery from Cydonia, a powerful urban center of ancient Crete.[16]
New burial customs with rock-cut chamber tombs having a long "dromos" (a ramp leading gradually towards the entrance) along with new religious beliefs speak in favour of the arrival of people from the Aegean. The same view is supported by the introduction of the safety pin that denotes a new fashion in dressing and also by a name scratched on a bronze skewer from Paphos and dating between 1050–950 BC.
Foundations myths documented by classical authors connect the foundation of numerous Cypriot towns with immigrant Greek heroes in the wake of the Trojan War. For example, Teucer, the brother of Aias was supposed to have founded Salamis, and the Arcadian Agapenor of Tegea to have replaced the native ruler Kinyras and to have founded Paphos. Some scholars see this a memory of a Greek colonisation already in the 11th century. In the 11th-century tomb 49 from Palaepaphos-Skales three bronze obeloi with inscriptions in Cypriot syllabic script have been found, one of which bears the name of Opheltas. This is the first indication of Greek language use on the island, although it is written in the Cypriot syllabary that remained in use down to the 3rd century BC.
Cremation as a burial rite is seen as a Greek introduction as well. The first cremation burial in Bronze vessels has been found at Kourion-Kaloriziki, tomb 40, dated to the first half of the 11th century (LCIIIB). The shaft grave contained two bronze rod tripod stands, the remains of a shield, and a golden sceptre as well. Formerly seen as the Royal grave of first Argive founders of Kourion, it is now interpreted as the tomb of a native Cypriote or a Phoenician prince. The cloisonné enamelling of the sceptre head with the two falcons surmounting it has no parallels in the Aegean, but shows a strong Egyptian influence.
In the 8th century, numerous Phoenician colonies were founded, like Kart-Hadasht ('New Town'), present day Larnaca and Salamis. The oldest cemetery of Salamis has indeed produced children's burials in Canaanite jars, clear indication of Phoenician presence already in the LCIIIB (11th century). Similar jar burials have been found in cemeteries in Kourion-Kaloriziki and Palaepaphos-Skales near Kouklia. In Skales, many Levantine imports and Cypriote imitations of Levantine forms have been found and point to a Phoenician expansion even before the end of the 11th century.
The 8th century BC saw a marked increase of wealth in Cyprus. Communications to the east and west were on the ascent and this created a prosperous society. Testifying to this wealth are the so-called royal tombs of Salamis, which, although plundered, produced a truly royal abundance of wealth. Sacrifices of horses, bronze tripods and huge cauldrons decorated with sirens, griffins etc., chariots with all their ornamentation and the horses' gear, ivory beds and thrones exquisitely decorated were all deposited into the tombs' "dromoi" for the sake of their masters.
The late 8th century is the time of the spreading of the Homeric poems, the "Iliad" and the "Odyssey". Funerary customs at Salamis and elsewhere were greatly influenced by these poems. The deceased were given skewers and firelogs in order to roast their meat, a practice found in contemporary Argos and Crete, recalling the similar gear of Achilles when he entertained other Greek heroes in his tent. Honey and oil, described by Homer as offerings to the dead are also found at Salamis, and the flames of fire that consumed the deceased were quenched with wine as it happened to Patroclus' body after it was given to the flames. The hero's ashes were gathered carefully wrapped into a linen cloth and put into a golden urn.[citation needed]
At Salamis the ashes of the deceased are also wrapped into a cloth and deposited into a bronze cauldron[citation needed].
The Prehistoric Period came to an end with the writing of the first works that still survive, first by the Assyrians, then by Greeks and Romans.
In their archaeogenetics study, Lazaridis et al. (2022)[17] carried out principal components analysis (PCA), projecting the ancient individuals onto the variation of present-day West Eurasians. They discovered that Neolithic Cypriots genetically clustered with Neolithic Anatolians. Two main clusters emerge: an “Eastern Mediterranean” Anatolian/Levantine cluster that also includes the geographically intermediate individuals from Cyprus, and an “inland” Zagros-Caucasus-Mesopotamia-Armenia-Azerbaijan cluster. There is structure within these groupings. Anatolian individuals group with each other and with those from Cyprus, whereas Levantine individuals are distinct.[17]
During the Neolithic era the highest proportion of Anatolian Neolithic-related ancestry is observed in Neolithic Anatolian populations as well as Neolithic Aegeans and the early farmers of Cyprus. The high Anatolian-related ancestry in Cyprus revealed by their model and subsequent analyses sheds light on debates about the origins of the people who spread Pre-Pottery Neolithic culture to Cyprus. Parallels in subsistence, technology, settlement organization, and ideological indicators suggest close contacts between Pre-Pottery Neolithic B people in Cyprus and on the mainland, but the geographic source of the Cypriot Pre-Pottery Neolithic populations has been unclear, with many possible points of origin. An inland Middle Euphrates source has been suggested on the basis of architectural and artifactual similarities. However, the faunal record at Cypriot Pre-Pottery Neolithic B sites and the use of Anatolian obsidian as raw material suggest linkages with Central and Southern Anatolia, and the genetic data increase the weight of evidence in favor of this scenario of a primary source in Anatolia.[17]

The prehistory of Manipur is the period of human history between the first use of stone tools by early men and the time just preceding ancient Kangleipak.
Comparing with other regions of the world, the development process of the archaeological work in Manipur is of recent times.[1] Archaeological research in Northeast India is severely scarce, mostly limited to surface explorations, and lacking in state-of-the-art methods.[2] The pioneering work  in archeology was initiated by O. Kumar Singh. Before his presence, there was little information on the existence of the stone age culture of Manipur.[3][1]
O. Kumar Singh is of the view that "Pre-Historic people used to settle in the hills which was habitate by the meiteis and nagas during the Paleolithic and Mesolithic (Hoabinhian) culture while Neolithic people lived in both hills and valley. They came down to the valley at least by about 2000 BC."[3] On the basis of the characters of the tool industry, the prehistory of Manipur is broadly classified into three periods.
Few attempts have been made to establish the earliest human settlement in Northeast India, and it is generally thought to have been uninhabited by archaic humans prior to late Pleistocene due to unfavorable geographical conditions. This, however, is disputed, and Northeast Corridors are proposed by some scholars to have played a defining role in early hominid migrations and peopling of India.[4]
Paleolithic period is the most primitive stone culture era. The Paleolithic period varies from place to place.
In Manipur's neighbouring country Myanmar, the lower Paleolithic culture started from 750000 to 275000 BP. Homo erectus began to settle on the banks of the Ayeyawaddy river in Burma in 750000 BCE. However, in case of Manipur, Paleolithic period started from 20,000 to 10,000 BCE.[3]
Most scholars do not discuss a Paleolithic age in Manipur (and Northeast).[5] However, Manjil Hazarika, in his 2017 survey of prehistory of Northeast India, rejects that there exists any plausible ground to deny presence of Paleolithic culture in Manipur.[6]
A few Paleolithic sites (Khangkhui, Napachik, Machi, Somgu and Singtom) have been located in Manipur.[7] Though, in absence of good chrono-stratigraphic context of the founds and their cohabitation with remains of other ages, accuracy of such identifications remains open to critiques.[5] The existence of Hoabinhian-like complexes remains disputed, as well.[8]
Evidence of Paleolithic habitation was discovered in five archeological sites, Songbu, Khangkhui, Machi, Nongpok Keithel Manbi and Singtom.
The Mesolithic period is known from two remarkable archeological sites in Manipur. These are the Nongpok Keithelmanbi and the Tharon cave.[3][11]
Multiple Neolithic sites have been identified in Manipur; they include Nongpok Keithelmanbi, Napachik, Laimenai, Naran Siena, and Phunan.Considered to be part of a larger Southeast Asian complex, the identifications are primarily accorded on the basis of stone tools and pottery (esp. cord-impressed ware); characteristic cultural identifiers of the Neolithic (agriculture, animal rearing etc.) are yet to be located and their development chronology is subject of active research.[21] Hazarika notes the Neolithic culture in Northeast to have begun some four thousand years after that in the Gangetic Plains.[22]
Meiteilon, lingua-franca of Meiteis belongs to the Tibeto-Burman phylum.[23] Hazarika notes the Manipuri sites to have an abundance of three-legged pottery and cord-impressed ware, very similar to the ones found in Southern China and Thailand, and hypothesizes that Manipur might have been the melting pot of Neolithic impulses from adjoining regions.[24] Roger Blench, in agreement with George van Driem's reconstructions of archeo-linguistic history of Southeast Asia, proposes that Northeast India accommodated a diverse group of foragers since Neolithic age, who learned agriculture and animal rearing c. 4000 B.C before migrating eastwards and establishing the  TB phylum.[25]
The Neolithic period is the last of the three Stone Age periods. It has four archeological sites in Manipur.[26]
These are (1) Napachik, (2) Laimanai,> (3) Phuna (4) Nongpok Keithelmanbi.[27]
Hazarika notes the broader region to not show evidence of any significant cultural transformation, upon the dawning of Copper Age (and then, Iron Age).[29] The state has an abundance of megaliths of various shapes, serving distinct purposes.[30] Sometime before the Christian era, the valley got inhabited by distinct yeks (clans), who had probably migrated from Southern China during the late Iron Age. The hill-tribes are probably of indigenous origin.[31]

Five Billion Years of Change: A History of the Land (2003, ISBN 157230958X) is a book by Denis Wood that attempts a holistic view of reality that ranges from the Big Bang to the World Wide Web. Specifically, this books deals with the formation of various structures:
A key theme is repeated through this book: humans have a tendency to divide our understandings into "history" and "prehistory". People are shocked when some event from prehistory intrudes upon their current lives; Wood likens the shock of this intrusion to an expulsion from the Garden of Eden.  This division is a metaphor for various artificial divisions; for example:
Instead of thinking in terms of artificial divisions of "now" and "back then", readers should develop an intuitive mindset of graduated changes in which the "fossils" of the ancient past are intermingled with contemporary objects; for instance, the "oxygen holocaust" of the paleoproterozoic eon exists in today's oxygen-rich atmosphere.
Also, the heroic saga induces another faulty thinking style that obscures the true nature of the world.  To understand the real story of humanity, Wood argues that people must focus on the mass actions of people or of large impersonal forces rather than a few heroes or kings.  Hollywood movies dealing with ecological threats are especially misleading; rather than imparting an accurate image of ecological issues, movies present a villain such as a mad scientist or a greedy, evil business person.  Instead, such entertainment and much news reporting distracts us from our individual actions that are at the heart of ecological problems.
Inaccurate ways of thinking induce a passive helplessness.  Instead, by presenting a sweeping story of successive, interlinked, long term trends, the author hopes to give readers a flexible, authentic model of the world.  With that model, readers will be capable of understanding (and possibly dealing with) current global challenges.

The pre-Cabraline history of Brazil is the stage in Brazil's history before the arrival of Portuguese navigator Pedro Álvares Cabral in 1500,[1] at a time when the region that is now Brazilian territory was occupied by thousands of indigenous peoples.
Traditional prehistory is generally divided into the Paleolithic, Mesolithic, and Neolithic periods. However, in Brazil, some authors prefer to work with the geological epochs of the current Quaternary period: Pleistocene and Holocene.[2] In this sense, the most accepted periodization is divided into: Pleistocene (hunters and gatherers at least 12,000 years ago) and Holocene, the latter being subdivided into Early Archaic (between 12,000 and 9,000 years ago), Middle Archaic (between 9,000 and 4,500 years ago) and Recent Archaic (from 4,000 years ago until the arrival of the Europeans). It is believed that the first peoples began to inhabit the region where Brazil is now located 60,000 years ago.[2]
The expression "prehistory of Brazil" is also used to refer to this period, but the term has been criticized since the concept of prehistory is questioned by some scholars as being a Eurocentric worldview, in which people without writing would be people without history. In the context of Brazilian history, this nomenclature would not accept that the indigenous people had their own history.[1] For this reason, some prefer to call this period pre-Cabraline.[3]
The study of Brazilian history before 1500 is mostly done through archaeology, since the peoples who occupied the territory are not known to have writing systems.[1][4] Linguistic, ethnological, and historical studies have aided archaeological research as much as possible. However, few authors have attempted to reconstruct this history in a panoramic way (and attempts by archaeologists to establish an overview of pre-Cabraline history have not proved satisfactory).[1] A further aggravating factor is that much remains to be done at various levels of research - language records and comparisons, analysis of excavated materials, the relationship between antiquity sites and others from the colonial period.[1]
The first scholar to inquire into Brazil's past was Danish naturalist Peter Wilhelm Lund (1801-1880).[2] Lund lived much of his life in Brazil, and was responsible for studying several reminiscences of ancient plants in the caves of the Lagoa Santa region (Minas Gerais), where he settled between 1834 and 1880.[2][5] In his researches, he found human bones mixed in with these prehistoric remains, one of the first findings that contradicted Creationism. He was the first to defend the antiquity of American Man based on archaeological finds, but he failed to convince the scientific community of his time.[2]
Middens, shell mounds, and other debris accumulated by human action were archeological remains responsible for stirring scientific debate in the 19th century.[5] Ladislau Netto, director of the National Museum of Brazil - which together with the Ipiranga Museum, represented the official interest in the archeological-historical facts in the country - sent the first scientific expeditions to these regions. After years of research, these missions claimed that "shell mounds" would have anthropogenic formation, i.e. human origin. Hermann Von Ihering, however, the director of the Ipiranga Museum, first opposed this view, stating that the shell remains would have been formed by natural, intertropical phenomena.[2]
Between 1880 and 1900, the first excavations in the Amazon took place.[1][2] Extensive discoveries of Marajoara ceramics were made in this period,[1] and were analyzed in 1882 by Egyptologist Paul l'Epine, who believed he identified Egyptian and Asian spellings on the indigenous ceramics. Émil Goeldi also conducted important research in Brazil's northern region at that time.[6] The Austrian J. A. Padberg-Drenkpohl, hired after World War I by the National Museum, was another important figure in the history of Brazilian archaeology, who excavated at Lagoa Santa between 1926 and 1929. His goal was to find traces that would prove Lund's classic findings. Drenpohl, however, was not successful in his enterprise, and he went on to criticize the defenders of the antiquity of humans from Lagoa Santa. In 1934, shortly after Drenkpohl's last expedition, Angione Costa published the first manual of Brazilian archaeology.[2]
After 1950, official archaeology diminished, while the number of amateurs who began to carry out research in the country increased. One of these was Guilherme Tiburtius, a German immigrant in Curitiba, who conducted one of the most important searches for Indian antiquities in Brazil, collecting artifacts for his collection (received by Joinville's Sambaqui Museum). He studied the coast of Santa Catarina and the plateau of Paraná and was assisted in his research by the Federal University of Paraná. Harold V. Walter, the English Consul in Belo Horizonte, was responsible for research in the state of Minas Gerais, in the region of Lagoa Santa. Although he used a methodology that is not accepted nowadays, he contributed to the collection of information about the Pleistocene era. Even at this time, substantial efforts were made to preserve Brazil's historical heritage. Thanks to the efforts of several intellectuals, the Prehistory Institute of USP (currently integrated into MAE)[7] was created, while a few years later (1961), new legislation on heritage was enacted.[2] Accompanying this advance in the question of preservation of Brazilian memory, excavations were carried out at the mouth of the Amazon river by Clifford Evans and Betty J. Meggers between 1949 and 1950, discovering important ceramic artifacts, and in São Paulo and Paraná between 1954 and 1956 by Joseph Emperaire and Annette Laming - where the first Carbon-14 datings were made.[2][8]
The recent history of archaeology in Brazil includes the creation of PRONAPA (National Project for Archaeological Research)[9] with the assistance of IPHAN,[10] which aims to conduct searches to provide a more complete picture of the Brazilian historical-cultural past. Meanwhile, institutions such as the National Museum, the Ipiranga Museum, and the Prehistory Institute conducted isolated research, and the Émil Goeldi Museum launched a project called PRONAPABA (National Project of Archaeological Research in the Amazon Basin). Several studies have been conducted since then on middens, Brazilian rock painting,[1] and lithic industry. In 1980, the first Brazilian Archaeological Society was created.[11][12] Archaeology is now taught in Brazil, although in a limited way.[5]
The occupation of American territory is a topic that has generated substantial controversy, especially as many archaeologists are still reluctant to accept that humans could have arrived in the Americas by other routes than the Bering land bridge.[1][13] However, discoveries made in Brazilian archaeological sites have called into question the validity of this theory.[1] In Piauí, for example, a fossil of the roundworm Ancylostoma duodenale was found with a date of 7,750 BP. According to some archaeologists, this Old World species could not have survived the Beringian crossing, because it would have died from the cold. Thus, they believe that the existence of the fossil indicates the migration of people from warm regions of the globe. Finds in Minas Gerais and Bahia have been dated between 25,000 and 12,000 BP[1] At the Alice Boer archaeological site in São Paulo, pieces dating back to 14,200 BP have been found.[14] In São Raimundo Nonato, in Piauí, archaeologists advocate an age of 50,000 years for a shelter occupied by prehistoric man.[1] Also at this same site, archaeologists were able to find human artifacts dating back to more than 48,000 BP[15]
The discoveries in Brazil led to polemics about the traditional view of the occupation of the Americas[16] and archeologists started to defend other theories about the great migrations, among them, that humans arrived in the Americas between 150,000 and 100,000 years ago, arriving by Malay-Polynesian (from Southeast Asia) or Australian (from the South Pacific) currents, while other authors still think of a migration current originating in Africa. The similarities between the material traces found in the Americas and those found in Oceania contribute to these theories. It can be generally admitted that Brazil was occupied 60,000 years ago, as far as Piauí is concerned.[17] Migratory currents would have reached Minas Gerais 30,000 years ago and Rio Grande do Sul 15,000 years ago.[18]
Luzia is the name of the oldest human fossil found in the Americas.[19] Charcoal near the fossilized remains was dated to about 11,000 years before present. Found by French archaeologist Annette Laming-Emperaire in the 1970s at the archaeological site of Lapa Vermelha, in the municipality of Lagoa Santa (Minas Gerais), the fossil of this prehistoric woman contributes to reigniting an old debate about the origins of humans in the Americas.[19] According to paleoanthropologist Walter Neves, responsible for naming the fossil, the morphology of Luzia's skull would bring her closer to the current aborigins of Australia and natives of Africa.[19]
Neves ventured the hypothesis that the occupation of the Americas was older than previously imagined, although not going back very far in time (about 14,000 years before the present), and that it was carried out by peoples from distinct regions, such as Oceania and Africa.[20] This thesis was not well received by some scientists.[19] According to National Geographic, besides races not being a scientific way to classify human beings, the differentiations between human groups only emerged after 9.5 thousand years.[21]
Studies conducted in the region inhabited by Luzia and other Paleo-Indians show that they were unaware of pottery and that their lithic industry was relatively simple.[22] Recent research, however, claims that these humans were sedentary. Findings such as numerous burials and the use of raw materials existing only at this site have reinforced these ideas. An analysis of cavities in the teeth of these Americans shows that they, although having no agriculture, drew heavily on plant resources.[23]
Archaeologists call a "phase" a cultural complex where the elements are closely associated. By "tradition" archaeologists mean the standard practices and techniques of the ancients for making, for example, lithic industry and cave paintings. A subtradition is a split within the tradition, usually because there has been a differentiation of the original pattern into two or more new patterns.[2]
At the end of the Pleistocene period, the temperature varied widely in phases of glacier expansion and contraction. It is believed that temperatures were cooler in the Pleistocene than in the Holocene when they experienced a considerable increase. At the beginning of the Middle Archaic period, the sea level was 10 meters lower than it is today. Many regions of the country, such as Piauí, for example, were much wetter than they are today.[19]
The Brazilian Paleolithic age is usually situated between 12,000 and 4–2,000 years before the present when the agricultural practice emerged and spread in the region.[24] Before that time, humans lived by hunting, fishing, and gathering, a fact proven by archeological findings and representations in pre-Cabraline paintings. At this time, archaeologists found different types of lithic industries in several regions of Brazil. In the northeast, several archeological sites indicate the development of chipped stone, containing slugs (a slug-shaped lithic artifact used to scrape wooden supports), splinters, awls, and stoves for roasting meat. Rock art was carried out at these early sites.[19]
In the northeast region, the techniques for working with lithic material became increasingly diversified and complex over time. The number of stoves, for example, increased as dating approached the year 8,000 before the present. Campfires were also found.[24][25]
The cave paintings of this region have been instigating experts. In the Baixão da Perna 1 cave, for example, (in the archeological area of São Raimundo Nonato) cave paintings dating back 10,500 years have been found. At the site of the Boqueirão da Pedra Furada, numerous cave paintings in red pigment were found. The authors identify the painting tradition of this area as the "northeast tradition".[26] Besides the northeast tradition, subtraditions such as Várzea Grande (southeast of Piauí) and Seridó (Rio Grande do Norte) have been identified. The most abundant figures represent human beings, plants and animals, but purely abstract graphisms are also found. Some cave walls depict hunting scenes and ritual celebrations. According to some archaeologists, the themes of violence in ancient rock art would be linked to the technical development achieved in subsequent years, responsible for promoting more efficient hunting strategies. The northeastern tradition concluded about 5,000 years ago.[27]
In the central and northeastern region, an important cultural tradition has been identified by scholars: The "Itaparica tradition" (Goiás, Minas Gerais, Pernambuco, Piauí). This tradition developed lithic tools like slugs, awls, and knives, but few projectile points. The people of these regions changed their way of life about 6,500 years ago when they began to eat mollusks and fruits.[28] In the center of the country, a tradition of rock paintings called "Planalto" developed.[29]
The oldest dates in the South are attributed to the "Ibicuí tradition" (between 13,000 and 8,500 years old), composed of simple artifacts found in the Uruguay Basin.[1][2] The Uruguay phase, which succeeds the first chronologically, dates from 11,555 to 8,640 BP and is composed of scrapers, bifacial blades, and projectile points.[30] In Santa Catarina and Rio Grande do Sul, artifacts (knives, scrapers, arrowheads) from 8,500 to 6,500 years ago have been located, established as the "Vinitu tradition". The more recent "Humaitá tradition" (between 6,500 and 2,000 years ago) extends from São Paulo to Rio Grande do Sul.[2][24] People of this tradition produced scrapers, awls, and even zooliths (stone statues in the shape of animals). Another tradition identified in the south was called "Umbu"; this one would have been responsible for making stoves and projectile points.[1][19][31]
The main archaeological sites on the coast are the middens.[1][32][33][34] Usually, skeletons of ancient Americans, lithic pieces, food remains, are found near them. A large part of the Brazilian middens are covered by the sea, due to the climatic changes that occurred during the late Pleistocene and Holocene. They exist almost everywhere along the Brazilian coast. At the time of their discovery, in the 19th century, they were compared to similar structures in Scandinavia. Middens are associated with the "Itaipu tradition". The people who inhabited the coast are usually defined as semi-nomadic fishermen.[1][35]
The appearance of cultivated plants in Minas Gerais dates back 4,000 years.[36] In São Raimundo Nonato, agriculture has likely been practiced since at least 2,090 years ago. Although Amazonian pottery is older than agriculture, the same phenomenon does not occur in the rest of the country, where the oldest pottery dates back 3,000 years (also in the area of São Raimundo Nonato). Brazilian archeologists consider that the appearance of pottery in these regions is linked to sedentarism and agriculture since pottery is difficult to transport and usually had the function of storing food. The "Taquara-Itaré tradition" is probably the most studied pottery tradition in the country.[1]
The oldest records of inhabitants in the Amazon region are dated as early as 12,500 B.C. Likely, the territory had already been colonized earlier, but only further research in the Amazon will be able to confirm this hypothesis. Archeologists identified the development of stone chipping techniques, starting with percussion flaking and moving on to pressure flaking.[37] The changes in flaking techniques are associated with different types of hunting, one targeting large animals and the other small animals. Nothing, however, is certain about the hunting style of the ancient Amazonian peoples. Scholars believe that these peoples fed on mollusks (an observation based on the discovery of sites such as midden), small animals, and fruits. Middens remain the main archaeological sites of this period in the Amazon.[37]
New research in Rondônia attributes greater antiquity to the practice of agriculture in the Amazon. According to archeologist Eduardo Bespalez, Amazonian agriculture can reach 8,000 years, a date close to the first records of agriculture in the world.[38] Furthermore, the Garbin archeological site reinforces the thesis that pottery was not, in its origins, associated with agriculture.[38] Brazilian archaeologists have found only lithic industry associated with terra preta (the main indication of the practice of agriculture in the region). The new findings may shed light on the mysteries surrounding everything from the significance of complex societies in the Amazon to the origins of the Amazon rainforest, possibly anthropogenic.[38] According to archaeologist Marcos Pereira Magalhães,
"The Amazonian Neotropical Culture is the result of a long-term regional historical event, derived from the Tropical Culture developed by hunter-gatherer societies socially, culturally, and economically integrated with the resources of the Neotropical rainforest, with which they interacted objectively and subjectively."[39]
During this time, the Amazonian peoples adopted a lifestyle similar to the lifestyle adopted by many tribes in the territory today. Thus, the indigenous people would have lived in a state of relative settlement, performing horticulture. These groups developed the first elaborate pottery in the Americas, with geometric and zoomorphic themes, and paintings in white and red paint.[40] The vessels took on oval and circular shapes. The best-known groups of ceramic styles are called "Hachurado-Zonado" and "Saldóide Barrancóide". The latter is related to red and white incisions and paintings, while the former is to the preference for ornate decoration.[1] "Saldóide Barrancóide" ceramics, found in the lower and middle Orinoco, were likely created between 2,800 and 800 B.C.. The "Hachurado-Zonado" styles of Tutoshcainyo and Ananatuba date from around 2,000-800 B.C. and 1,500-500 B.C. respectively. Many scholars have admitted that this pottery was influenced by Andean cultural complexes, although it is now accepted that Amazonian indigenous people developed this elaborate pottery in the lower region themselves, and probably influenced the Andes at a later date.[1] The "Saldóide Barrancóide" style is also believed to have been created in the lower Orinoco.[1]
In addition to horticulture, these societies practiced hunting and fishing. Shellfish consumption was reduced, and these peoples began to settle in the floodplains and riverbanks. Coarse pottery bowls have been identified in these territories, so that some archaeologists hypothesize the presence of cassava. Sites of these cultural complexes have been found in the Ucaiali basin, on Marajó Island, in the Orinoco and in the Amazon.[1]
The demographic increase of Amazonian populations in late Prehistoric times, combined with other factors, gave rise to major transformations among the indigenous societies of the Amazon.[41] According to archaeologists, the societies inhabiting regions of the Amazon basin began to organize themselves in increasingly elaborate ways between 1,000 B.C. and 1,000 A.D.[1] Archaeologists define these societies as "complex cacicados."[42] These societies became increasingly hierarchical containing nobles, "commoners" and captive servants,[42] with a centralized leadership in the figure of the cacique, and adopted bellicose and expansionist attitudes. The chieftain, besides dominating large territories, continuously organized his warriors to conquer new territories. The pottery of these societies was highly elaborate, demonstrating mastery of complex production techniques. There were elaborate funerary urns (associated with the cult of the dead chiefs) and trade. Archaeological evidence points to an urban-scale population density in these civilizations.[43] It is believed that monoculture was practiced, in addition to intensive hunting and fishing, intensive root production, and food storage.[44][1] According to researcher Anna Roosevelt,
"The development of intensive agriculture in prehistoric times seems to have been correlated to the rapid expansion of the populations of complex societies. Suggestively, the displacements and depopulation of the historical period caused these economies to return to patterns of less intensive root cultivation and animal capture (...)."[42]
Chronicles from the early colonial period are employed today in the reconstruction of ancient Brazilian civilizations. Many foreign chroniclers have described indigenous elements from the period of the complex cacicados. The dissolution of these social organizations is usually related to the conquest, which affected their demographic structure.[45]
The ceramics produced by these civilizations are classified into two main groups: the "Horizonte Policrômico" and the "Horizonte Inciso Ponteado". Among the archaeological sites that presented vestiges grouped under the "Horizonte Policrômico" are: The Marajoara (mouth of the Amazon) and the Guarita (Middle Amazon), among others located outside the Brazilian Amazon. Archaeological sites associated with the "Horizonte Inciso Ponteado" include Santarém (Lower Amazon) and Itacoatiara (Middle Amazon). The first Horizon ("Horizonte") is characterized by white, black, and red paintings, geometric themes, and incisions. The second horizon is characterized by deep incisions and the dotting technique. It is believed that the "Horizonte Inciso Ponteado" was associated with the ancestors of the Cariban-speaking peoples, while the "Horizonte Policrômico" would have been produced by the ancestors of the Tupi-speaking peoples.[45]
The large Amazonian sites of the complex cacicado epoch were probably specialized regions for burial, worship, work, and warfare. The late prehistoric occupation of the territory was sedentary. The entry of maize and other seeds into the region, as well as their popularization among Americans, dates back to the first millennium before Christ.[46]
One of the Amazonian civilizations known to have developed large cities and towns during the pre-Cabraline period was Kuhikugu.[47] The archaeological site, discovered by archaeologist Michael Heckenberger, is located within the Xingu National Park (Upper Xingu region) and proved to have been a large urban complex that may have housed up to 50,000 inhabitants. Probably built by the ancestors of the current Kuikuro people, the site houses complex constructions such as roads, fortifications, and trenches for protection. As the discovery is recent, studies on the ways of life of these populations are still needed, although scholars believe that these people cultivated cassava.[48] The disappearance of this civilization, as well as other great Amazonian civilizations, is related to the entrance of European diseases into the continent, responsible for decimating the local populations at around 1,500 of the present era. The natural characteristics of the Amazon Rainforest (dense forest, etc.) would explain why the ancient Europeans did not make acquaintance with this Brazilian civilization.[49]
Teso dos Bichos or Tesos Marajoaras, located on Marajó Island, is the site where one of the most elaborate civilizations of the pre-Cabraline Amazon were born, occupying 2.5 hectares.[50] One of the distinguished features of the complex societies of Marajó Island is the "tesos", large artificial embankments built to place dwellings, probably to avoid flooding. Marajoara tesos are considered monumental structures and, for this reason, are essential for the interpretation of the Marajoara past.[50]
It was estimated that the civilization responsible for the work would have a population of 500,000 people. The inhabitants of this civilization would belong to a society of Tuxaua, lords of the mouth of the Amazon River. There would be a division of labor between men and women, a diet rich in protein (animal and vegetable), and fermented beverages (such as aluá).[51]
In October 2009, a group of geologists claimed that the tesos could be essentially natural structures, by processes similar to fluvial mound formation elsewhere, with evidence of human activity only in more superficial layers.[52] Because they required significantly less human activity for their formation, complex societies would not have been necessary for the monumental accumulation of labor.[52] This hypothesis partially invalidates interpretations about the existence of complex societies in the Amazon.[53] Archaeologists responsible for the excavation and anthropogenic hypothesis have questioned the team's methodology, pointing to sample sizes and evidence in the study that could be interpreted as signifying human structures.[53]
The Alice Boer site is located in Ipeúna, a municipality near Rio Claro, in São Paulo. It was excavated by archaeologist Maria Beltrão at the service of the National Museum from 1964 onwards. The first Brazilians in the region were ancient and produced artifacts such as projectile points, scrapers, and slugs. A charcoal sample from this site provided a date of 14,200 years.[54][55]
The Ponta de Flecha site was excavated between 1981 and 1982 by C. Barreto and E. Robrahn. The findings from the site - among them arrowheads and bones - date back to both the Pleistocene and Holocene times. The animal bones found were marked by human lithic instruments.[56]
Anna Roosevelt (great-granddaughter of U.S. President Theodore Roosevelt), a professor of anthropology at the University of Illinois, coordinated in 1996 a team that researched the Pedra Pintada Cave in Monte Alegre, Pará, on the left bank of the Amazon River, a few kilometers from what is now Santarém.[57]
The prehistoric inhabitants of that region supported themselves with a stable economy and produced culture and technologies. The Paleoindians lived in comfortable and protected caves, had a healthier diet, and produced pottery, cave paintings, and arrowheads. They were hunters of small animals and gatherers of fruit. At the height of their civilization, they might have reached as many as 300,000 individuals.[58]
Spearheads and pottery shards dating back 10,000 to 6,000 years were found there. The results concluded that the Paleoindians (the first inhabitants of the Americas) lived in the Amazon region from 11,200 to 10,000 years ago. This serves as evidence that human occupation in the continent took place more than 20,000 years ago.[58]
Still, Roosevelt's findings have not yet fully refuted the hypothesis of the arrival of the first inhabitants of the Americas through the Bering Strait. The migratory movement would have occurred in successive waves. The Amazonian populations, whose signs she found in the Pedra Pintada cave, probably migrated south without having had contact with American mammoth hunters.[59]
The archaeological site of Pedra Furada,[60] located in São Raimundo Nonato, in the Serra da Capivara National Park, Piauí, was discovered in the 1960s. It has been studied since the 1970s by Niède Guidon, a French-Brazilian archaeologist. The findings (chipped stones and traces of bonfires) date back approximately 11,000 years. According to the team, it is not unlikely that the discoveries could be up to 48,000 years old. Guidon's thesis goes much further - about 100,000 years - and assumes that humans did not arrive in America from Asia by land (via the Bering Strait as is believed until today), but by sea, using boats. However, the findings of São Raimundo Nonato remain controversial and do not yet fully refute the Clovis Theory.[60]
The site also houses the Museum of the American Man. Illuminated explanatory panels tell the story of the presence of humans in the Americas with drawings, maps, and texts. The space also holds clay funerary urns and replicas of two human skeletons found in caves in the region. One of them, a 30-year-old Indian woman, was found practically complete and dates back to 9,700 years ago.[61]
Drawings were found as well in the Toca do Boqueirão, also in Pedra Furada, which likely represent a scene of an attack by the felines that once inhabited the continent. The conceptions of the current Indians living in the northeast region of the country, like the Kiriri, although much modified, may still be a useful element to decipher such representations with a conjectural strategy. The drawings are described by the Kiriri, in general, as huge men, ferocious and implacable, with rude features and wide-open eyes, truly frightening. According to anthropologist Nascimento, who studied in his Master's thesis for the Federal University of Bahia the rituals and ethnic identification of the northeastern Indians based on the conceptions of a remnant group - the Kiriri of Mirandela (Bahia) in 1994.[62]
In Brazil, besides the remains from Piauí, there is also an ancient set found in the region of Lagoa Santa (Minas Gerais), possibly the representatives of the country's ancient linguistic group Macro Jê, whose closest descendants today would be the Kiriri and Botocudo Indians.[63][64][65]
In 1974, at Lapa Vermelha IV, during the excavation of Annette Laming-Emperaire's team, a human skeleton dated 11,500 years BP, the oldest in the Americas, was discovered, later nicknamed Luzia. She cast doubt on the Clovis Theory since she is a woman with characteristics very distinct from today's indigenous people (who are closer to the epigenetic Mongoloid group). Luzia was investigated by bioanthropologists and archaeologists Walter Alves Neves, André Prous, Joseph F. Powell, Erik G. Ozolins, and Max Blum.[66]
While most research on earlier Brazil has mainly analyzed the material remains left by these peoples, recent pre-Cabraline Brazil is usually studied through the native languages. The study of native languages allows us to understand many aspects of these cultures, as well as their historical affiliations and migrations. When European chroniclers described the ancient Brazilian peoples, they mostly used linguistic names and, thanks to the missionary work of some Jesuits, today we know the ancient Brazilian languages (which gave rise to modern indigenous languages).[67][68]
When Europeans came to occupy the eastern coast of South America, they encountered ethnic groups linked to four main language groups: The Arawak, the Tupi-Guaraní, the Jê, and the Kalinago.[67]
One of the most relevant linguistic groups in Brazil, and one which likely spread in a large scale over Brazilian territory before 1500, is the Tupi group. The main language family within this larger group is the Tupi-Guaraní. These peoples may have first inhabited the headwaters of the Madeira, Tapajós and Xingu rivers.[69][70] The Tupi-Guaraní expansion happened between 3,000 and 2,000 years ago, shortly after this group differentiated from others in the region between the Xingu and Madeira rivers, forming new language subgroups, such as the Cocama, Omágua, Guaiaqui, and Xirinó. The Cocama and Omágua peoples headed to the Amazon River, while the Guaiaqui went to Paraguay and the Xirinó to Bolivia. The Tapirapé and Teneteara moved toward the northeast. The Pauserna, Cajabi, and Kamayurá peoples moved to the extreme southern region of Brazil.[69][71]
The Wayampi-speaking peoples reached as far as the Guianas region. The last phase of dispersion of the Tupi-Guaraní peoples occurred around the year 1,000. The speakers of languages associated with the Tupi-Guaraní family were already settled in southern Brazil (Guaraní, for example), in the Amazon basin, and also on the coast of Brazil (Potiguara, Tupinambá, Tupiniquin). Thanks to an extensive network of waterways, the peoples of this linguistic group were able to spread out and at the same time maintain contact with each other.[69][71]
Many archaeological artifacts from the ceramic period are affiliated with these ancient peoples of the Tupi-Guaraní linguistic matrix. The archaeological sites associated with these peoples were extensive villages, usually located on plateau or terrace regions. In these archaeological sites, averaging between 10,000 and 2,000 square meters in width, ancient pottery was abundant.[69][72]
The chronology of the history of the Tupi-Guaraní peoples is based on archeological theories, glottochronology, and the dating of ceramics identified. As demonstrated by the history of the Tupi-Guaraní from their languages, the expansion movement occurred between 3,000 and 2,000 years ago from the Amazon region; most of the archeological artifacts of these peoples are dated between the years 500 and the year 1,500. The time of the expansion to the coast is verified by the highest concentration of artifacts in this region between the 11th and 13th centuries.[73]
The Macro-Jê expansion began 3,000 years ago in the Midwest Region of Brazil. The Jê group itself possibly originated in the headwaters regions of the São Francisco and Araguaia rivers. A large part of the Jê speaking peoples moved away from the Kaingang and Xokleng to the south of the Central Brazilian region.[1] Some groups must have separated from the latter and moved into the Amazon region at least a thousand years ago. The Jê peoples preferred to settle in plateau regions (such as the original Brazilian Plateau region), evidenced by study of their languages among the languages of the Macro-Jê trunk, which are Kayapo, Xerente, Timbira, among others.[1]
On the eve of the arrival of Europeans in America in 1500, it is estimated that the current territory of Brazil (the eastern coast of South America) was inhabited by two million indigenous people, from north to south.[74]
According to Luís da Câmara Cascudo, the Tupi were the first "indigenous grouping that had contact with the colonizer."[75]
The names of some of the main groups that inhabited Brazil on the eve of the European arrival are (among them some of non-Tupi origin): The Potiguara, Tremembé, Tabajara, Caeté, Tupiniquim, the Tupinambá, Aimoré, Goitacá, Tamoio, Carijó and Temiminó. The Potiguara inhabited the region between the Acaraú and Paraíba rivers and controlled the river navigation. During the conquest, they were allied with the French, and some accounts speak of marriages between Potiguara and French, involving anti-Portuguese war agreements.[68][1][69] The Tabajara inhabited the southern bank of the Paraíba River, in what is now the coastal region of Pernambuco. They were important allies of the Portuguese during the conquest. The Caetés inhabited the region of Pernambuco from Olinda, "the Marim of the Caetés", to where the state of Alagoas is today, dismembered from Pernambuco.[68][1][69]
The Tremembé inhabited the western bank of the Acaraú river. The Tamoio inhabited the Guanabara Bay; their leaders were also allied with the French in the fight against the Portuguese. The Carijós inhabited the southern coast of the country. The Tupiniquim inhabited the present-day region of the state of São Paulo, and the Tupinambá the southeast region of Brazil.[68][1][69] The current knowledge of ancient Tupi is mainly based on the language of the Tupinambá.[76]
The Tupi peoples lived in villages of 600 to 700 inhabitants. Some villages were fortified because of inter-tribal wars. No authority appeared with absolute or considerably strong power over the other members of the society, although there were "hierarchies" according to gender, warrior merit and shamanic powers. Pajés (payes in ancient Tupi, intermediaries between the religious world and the world of men) and caciques (morubixaba in ancient Tupi for "warrior chiefs") generally occupied the role of tribal authorities.[2][77][78] There was belief in good and bad spirits (Tupã, Anhang, among others), who influenced events in the cosmos. Each man carried a maraca, in which they believed a spirit protector of each individual dwelled. It is believed that only the sons of the most important men of the tribe were buried in the funerary urns. The religious events had a wide scope, and brought together different ethnic groups. The ancient Indians were responsible for numerous artistic manifestations, such as pottery pieces, dances, songs/poetry and, the one that impressed Westerners the most, the sophisticated and rich featherwork.[79] Tupi literature appears with the arrival of European writing, when missionaries started writing in Tupi to convert the natives,[76] and chronicles transcribed indigenous songs.[80]
Other groups, with debated theories on their origins and their languages that have inhabited Brazil since pre-Cabraline times are:[81]
On the European side, the discovery of Brazil was preceded by several treaties between Portugal and Spain, establishing limits and dividing the world already discovered from the world yet to be discovered.[84][85]
Of these agreements signed at a distance from the assigned land, the Treaty of Tordesillas (1494) is the most important, for defining the portions of the globe that would belong to Portugal during the period in which Brazil was a Portuguese colony.[86][84] Its clauses established that the lands east of an imaginary median that would pass 370 maritime leagues west of the Cape Verde Islands would belong to the kingdom of Portugal, while the lands to the west would be owned by the kings of Castile (now Spain). In the current Brazilian territory, the line crossed from north to south, from the current city of Belém to Laguna.[84]
(antiquity until 1500)
(1500 to 1822)

Fertile Crescent:
Europe:
Africa:
Siberia:
Behavioral modernity is a suite of behavioral and cognitive traits believed to distinguish current Homo sapiens from other anatomically modern humans, hominins, and primates.[1] Most scholars agree that modern human behavior can be characterized by abstract thinking, planning depth, symbolic behavior (e.g., art, ornamentation), music and dance, exploitation of large game, and blade technology, among others.[2][3]
Underlying these behaviors and technological innovations are cognitive and cultural foundations that have been documented experimentally and ethnographically by evolutionary and cultural anthropologists. These human universal patterns include cumulative cultural adaptation, social norms, language, and extensive help and cooperation beyond close kin.[4][5]
Within the tradition of evolutionary anthropology and related disciplines, it has been argued that the development of these modern behavioral traits, in combination with the climatic conditions of the Last Glacial Period and Last Glacial Maximum causing population bottlenecks, contributed to the evolutionary success of Homo sapiens worldwide relative to Neanderthals, Denisovans, and other archaic humans.[3][6]
Debate continues as to whether anatomically modern humans were behaviorally modern as well. There are many theories on the evolution of behavioral modernity. These approaches tend to fall into two camps: cognitive and gradualist. The Later Upper Paleolithic Model theorizes that modern human behavior arose through cognitive, genetic changes in Africa abruptly around 40,000–50,000 years ago around the time of the Out-of-Africa migration, prompting the movement of some modern humans out of Africa and across the world.[7]
Other models focus on how modern human behavior may have arisen through gradual steps, with the archaeological signatures of such behavior appearing only through demographic or subsistence-based changes. Many cite evidence of behavioral modernity earlier (by at least about 150,000–75,000 years ago and possibly earlier) namely in the African Middle Stone Age.[8][3][9][10][11] Anthropologists Sally McBrearty and Alison S. Brooks have been notable proponents of gradualism—challenging Europe-centered models by situating more change in the African Middle Stone Age—though this model is more difficult to substantiate due to the general thinning of the fossil record as one goes further back in time.
To classify what should be included in modern human behavior, it is necessary to define behaviors that are universal among living human groups. Some examples of these human universals are abstract thought, planning, trade, cooperative labor, body decoration, and the control and use of fire. Along with these traits, humans possess much reliance on social learning.[12][13] This cumulative cultural change or cultural "ratchet" separates human culture from social learning in animals. In addition, a reliance on social learning may be responsible in part for humans' rapid adaptation to many environments outside of Africa. Since cultural universals are found in all cultures, including isolated indigenous groups, these traits must have evolved or have been invented in Africa prior to the exodus.[14][15][16]
Archaeologically, a number of empirical traits have been used as indicators of modern human behavior. While these are often debated[17] a few are generally agreed upon. Archaeological evidence of behavioral modernity includes:[3][7]
Several critiques have been placed against the traditional concept of behavioral modernity, both methodologically and philosophically.[3][17] Anthropologist John Shea outlines a variety of problems with this concept, arguing instead for "behavioral variability", which, according to the author, better describes the archaeological record. The use of trait lists, according to Shea, runs the risk of taphonomic bias, where some sites may yield more artifacts than others despite similar populations; as well, trait lists can be ambiguous in how behaviors may be empirically recognized in the archaeological record.[17] In particular, Shea cautions that population pressure, cultural change, or optimality models, like those in human behavioral ecology, might better predict changes in tool types or subsistence strategies than a change from "archaic" to "modern" behavior.[17] Some researchers argue that a greater emphasis should be placed on identifying only those artifacts which are unquestionably, or purely, symbolic as a metric for modern human behavior.[3]
Since 2018, recent dating methods utilized on various cave art sites in Spain and France have shown that Neanderthals performed symbolic artistic expression, consisting of red "lines, dots, and hand stencils" found in caves, prior to contact with anatomically modern humans. This is contrary to previous suggestions that Neanderthals lacked these capabilities.[18][19][20]
The Late Upper Paleolithic Model, or Upper Paleolithic Revolution, refers to the idea that, though anatomically modern humans first appear around 150,000 years ago (as was once believed), they were not cognitively or behaviorally "modern" until around 50,000 years ago, leading to their expansion out of Africa and into Europe and Asia.[7][21][22] These authors note that traits used as a metric for behavioral modernity do not appear as a package until around 40–50,000 years ago. Anthropologist Richard Klein specifically describes that evidence of fishing, tools made from bone, hearths, significant artifact diversity, and elaborate graves are all absent before this point.[7][21] According to both Shea and Klein, art only becomes common beyond this switching point, signifying a change from archaic to modern humans.[7] Most researchers argue that a neurological or genetic change, perhaps one enabling complex language, such as FOXP2, caused this revolutionary change in humans.[7][22] The role of FOXP2 as a driver of evolutionary selection has been called into question following recent research results.[clarification needed][23]
Building on the FOXP2 gene hypothesis, cognitive scientist Philip Lieberman has argued that proto-language behaviour existed prior to 50,000 BP, albeit in a more primitive form. Lieberman has advanced fossil evidence, such as neck and throat dimensions, to demonstrate that so-called “anatomically modern” humans from 100,000 BP continued to evolve their SVT (supralaryngeal vocal tract), which already possessed a horizontal portion (SVTh) capable of producing many phonemes which were mostly consonants. According to his theory, Neanderthals and early Homo sapiens would have been able to communicate using sounds and gestures.[24]
From 100,000 BP, Homo sapiens necks continued to lengthen to a point, by around 50,000 BP, where Homo sapiens necks were long enough to accommodate a vertical portion to their SVT (SVTv), which is now a universal trait among humans. This SVTv enabled the enunciation of quantal vowels: [i]; [u]; and [a]. These quantal vowels could then be immediately put to use by the already sophisticated neuro-motor-control features of the FOXP2 gene to generate more nuanced sounds and in effect increase by orders of magnitude the number of distinct sounds that can be produced, allowing for fully symbolic language.[25]
Goody (1986) draws an analogy between the development of spoken language and that of writing: the shift from pictographic or ideographic symbols into a fully abstract logographic writing system (such as hieroglyphics), or from a logoprahic system into an abjad or alphabet, led to dramatic changes in human civilization.[26]
Contrasted with this view of a spontaneous leap in cognition among ancient humans, some anthropologists like Alison S. Brooks, primarily working in African archaeology, point to the gradual accumulation of "modern" behaviors, starting well before the 50,000-year benchmark of the Upper Paleolithic Revolution models.[8][3][27] Howiesons Poort, Blombos, and other South African archaeological sites, for example, show evidence of marine resource acquisition, trade, the making of bone tools, blade and microlithic technology, and abstract ornamentation at least by 80,000 years ago.[8][9] Given evidence from Africa and the Middle East, a variety of hypotheses have been put forth to describe an earlier, gradual transition from simple to more complex human behavior. Some authors have pushed back the appearance of fully modern behavior to around 80,000 years ago or earlier in order to incorporate the South African data.[27]
Others focus on the slow accumulation of different technologies and behaviors across time. These researchers describe how anatomically modern humans could have been cognitively the same, and what we define as behavioral modernity is just the result of thousands of years of cultural adaptation and learning.[8][3] Archaeologist Francesco d'Errico, and others, have looked at Neanderthal culture, rather than early human behavior exclusively, for clues into behavioral modernity.[6] Noting that Neanderthal assemblages often portray traits similar to those listed for modern human behavior, researchers stress that the foundations for behavioral modernity may in fact, lie deeper in our hominin ancestors.[28] If both modern humans and Neanderthals express abstract art and complex tools then "modern human behavior" cannot be a derived trait for our species. They argue that the original "human revolution" theory reflects a profound Eurocentric bias. Recent archaeological evidence, they argue, proves that humans evolving in Africa some 300,000 or even 400,000 years ago were already becoming cognitively and behaviourally "modern". These features include blade and microlithic technology, bone tools, increased geographic range, specialized hunting, the use of aquatic resources, long-distance trade, systematic processing and use of pigment, and art and decoration. These items do not occur suddenly together as predicted by the "human revolution" model, but at sites that are widely separated in space and time. This suggests a gradual assembling of the package of modern human behaviours in Africa, and its later export to other regions of the Old World.
Between these extremes is the view—currently supported by archaeologists Chris Henshilwood,[29] Curtis Marean,[3] Ian Watts[30] and others—that there was indeed some kind of "human revolution" but that it occurred in Africa and spanned tens of thousands of years. The term "revolution," in this context, would mean not a sudden mutation but a historical development along the lines of the industrial revolution or the Neolithic revolution.[31] In other words, it was a relatively accelerated process, too rapid for ordinary Darwinian "descent with modification" yet too gradual to be attributed to a single genetic or other sudden event. These archaeologists point in particular to the relatively explosive emergence of ochre crayons and shell necklaces, apparently used for cosmetic purposes. These archaeologists see symbolic organisation of human social life as the key transition in modern human evolution. Recently discovered at sites such as Blombos Cave and Pinnacle Point, South Africa, pierced shells, pigments and other striking signs of personal ornamentation have been dated within a time-window of 70,000–160,000 years ago in the African Middle Stone Age, suggesting that the emergence of Homo sapiens coincided, after all, with the transition to modern cognition and behaviour.[32] While viewing the emergence of language as a "revolutionary" development, this school of thought generally attributes it to cumulative social, cognitive and cultural evolutionary processes as opposed to a single genetic mutation.[33]
A further view, taken by archaeologists such as Francesco d'Errico[28] and João Zilhão,[34] is a multi-species perspective arguing that evidence for symbolic culture, in the form of utilised pigments and pierced shells, are also found in Neanderthal sites, independently of any "modern" human influence.
Cultural evolutionary models may also shed light on why although evidence of behavioral modernity exists before 50,000 years ago, it is not expressed consistently until that point. With small population sizes, human groups would have been affected by demographic and cultural evolutionary forces that may not have allowed for complex cultural traits.[10][11][12][13] According to some authors,[10] until population density became significantly high, complex traits could not have been maintained effectively. Some genetic evidence supports a dramatic increase in population size before human migration out of Africa.[22] High local extinction rates within a population also can significantly decrease the amount of diversity in neutral cultural traits, regardless of cognitive ability.[11]
Research from 2017 indicates that Homo sapiens originated in Africa between around 350,000 and 260,000 years ago.[35][36][37][38] There is some evidence for the beginning of modern behavior among early African H. sapiens around that period.[39][40][41][42]
Before the Out of Africa theory was generally accepted, there was no consensus on where the human species evolved and, consequently, where modern human behavior arose. Now, however, African archaeology has become extremely important in discovering the origins of humanity. The first Cro-Magnon expansion into Europe around 48,000 years ago is generally accepted as already "modern",[21] and it is now generally believed that behavioral modernity appeared in Africa before 50,000 years ago, either significantly earlier, or possibly as a late Upper Paleolithic "revolution" soon before which prompted migration out of Africa.
A variety of evidence of abstract imagery, widened subsistence strategies, and other "modern" behaviors have been discovered in Africa, especially South, North, and East Africa. The Blombos Cave site in South Africa, for example, is famous for rectangular slabs of ochre engraved with geometric designs. Using multiple dating techniques, the site was dated to be around 77,000 and 100,000 to 75,000 years old.[29][43] Ostrich egg shell containers engraved with geometric designs dating to 60,000 years ago were found at Diepkloof, South Africa.[44] Beads and other personal ornamentation have been found from Morocco which might be as much as 130,000 years old; as well, the Cave of Hearths in South Africa has yielded a number of beads dating from significantly prior to 50,000 years ago,[8] and shell beads dating to about 75,000 years ago have been found at Blombos Cave, South Africa.[45][46][47]
Specialized projectile weapons as well have been found at various sites in Middle Stone Age Africa, including bone and stone arrowheads at South African sites such as Sibudu Cave (along with an early bone needle also found at Sibudu) dating approximately 72,000–60,000 years ago[48][49][50][51][52] on some of which poisons may have been used,[53] and bone harpoons at the Central African site of Katanda dating to about 90,000 years ago.[54] Evidence also exists for the systematic heat treating of silcrete stone to increase its flake-ability for the purpose of toolmaking, beginning approximately 164,000 years ago at the South African site of Pinnacle Point and becoming common there for the creation of microlithic tools at about 72,000 years ago.[55][56]
In 2008, an ochre processing workshop likely for the production of paints was uncovered dating to c. 100,000 years ago at Blombos Cave, South Africa. Analysis shows that a liquefied pigment-rich mixture was produced and stored in the two abalone shells, and that ochre, bone, charcoal, grindstones, and hammer-stones also formed a composite part of the toolkits. Evidence for the complexity of the task includes procuring and combining raw materials from various sources (implying they had a mental template of the process they would follow), possibly using pyrotechnology to facilitate fat extraction from bone, using a probable recipe to produce the compound, and the use of shell containers for mixing and storage for later use.[57][58][59] Modern behaviors, such as the making of shell beads, bone tools and arrows, and the use of ochre pigment, are evident at a Kenyan site by 78,000–67,000 years ago.[60] Evidence of early stone-tipped projectile weapons (a characteristic tool of Homo sapiens), the stone tips of javelins or throwing spears, were discovered in 2013 at the Ethiopian site of Gademotta, and date to around 279,000 years ago.[39]
Expanding subsistence strategies beyond big-game hunting and the consequential diversity in tool types has been noted as signs of behavioral modernity. A number of South African sites have shown an early reliance on aquatic resources from fish to shellfish. Pinnacle Point, in particular, shows exploitation of marine resources as early as 120,000 years ago, perhaps in response to more arid conditions inland.[9] Establishing a reliance on predictable shellfish deposits, for example, could reduce mobility and facilitate complex social systems and symbolic behavior. Blombos Cave and Site 440 in Sudan both show evidence of fishing as well. Taphonomic change in fish skeletons from Blombos Cave have been interpreted as capture of live fish, clearly an intentional human behavior.[8]
Humans in North Africa (Nazlet Sabaha, Egypt) are known to have dabbled in chert mining, as early as ≈100,000 years ago, for the construction of stone tools.[61][62]
Evidence was found in 2018, dating to about 320,000 years ago, at the Kenyan site of Olorgesailie, of the early emergence of modern behaviors including: long-distance trade networks (involving goods such as obsidian), the use of pigments, and the possible making of projectile points. It is observed by the authors of three 2018 studies on the site that the evidence of these behaviors is approximately contemporary to the earliest known Homo sapiens fossil remains from Africa (such as at Jebel Irhoud and Florisbad), and they suggest that complex and modern behaviors had already begun in Africa around the time of the emergence of anatomically modern Homo sapiens.[40][41][42]
In 2019, further evidence of early complex projectile weapons in Africa was found at Aduma, Ethiopia, dated 100,000–80,000 years ago, in the form of points considered likely to belong to darts delivered by spear throwers.[63]
Olduvai Hominid 1 wore facial piercings.[64]
While traditionally described as evidence for the later Upper Paleolithic Model,[7] European archaeology has shown that the issue is more complex. A variety of stone tool technologies are present at the time of human expansion into Europe and show evidence of modern behavior. Despite the problems of conflating specific tools with cultural groups, the Aurignacian tool complex, for example, is generally taken as a purely modern human signature.[65][66] The discovery of "transitional" complexes, like "proto-Aurignacian", have been taken as evidence of human groups progressing through "steps of innovation".[65] If, as this might suggest, human groups were already migrating into eastern Europe around 40,000 years and only afterward show evidence of behavioral modernity, then either the cognitive change must have diffused back into Africa or was already present before migration.
In light of a growing body of evidence of Neanderthal culture and tool complexes some researchers have put forth a "multiple species model" for behavioral modernity.[6][28][67] Neanderthals were often cited as being an evolutionary dead-end, apish cousins who were less advanced than their human contemporaries. Personal ornaments were relegated as trinkets or poor imitations compared to the cave art produced by H. sapiens. Despite this, European evidence has shown a variety of personal ornaments and artistic artifacts produced by Neanderthals; for example, the Neanderthal site of Grotte du Renne has produced grooved bear, wolf, and fox incisors, ochre and other symbolic artifacts.[67] Although few and controversial, circumstantial evidence of Neanderthal ritual burials has been uncovered.[28] There are two options to describe this symbolic behavior among Neanderthals: they copied cultural traits from arriving modern humans or they had their own cultural traditions comparative with behavioral modernity. If they just copied cultural traditions, which is debated by several authors,[6][28] they still possessed the capacity for complex culture described by behavioral modernity. As discussed above, if Neanderthals also were "behaviorally modern" then it cannot be a species-specific derived trait.
Most debates surrounding behavioral modernity have been focused on Africa or Europe but an increasing amount of focus has been placed on East Asia. This region offers a unique opportunity to test hypotheses of multi-regionalism, replacement, and demographic effects.[68] Unlike Europe, where initial migration occurred around 50,000 years ago, human remains have been dated in China to around 100,000 years ago.[69] This early evidence of human expansion calls into question behavioral modernity as an impetus for migration.
Stone tool technology is particularly of interest in East Asia. Following Homo erectus migrations out of Africa, Acheulean technology never seems to appear beyond present-day India and into China. Analogously, Mode 3, or Levallois technology, is not apparent in China following later hominin dispersals.[70] This lack of more advanced technology has been explained by serial founder effects and low population densities out of Africa.[71] Although tool complexes comparative to Europe are missing or fragmentary, other archaeological evidence shows behavioral modernity. For example, the peopling of the Japanese archipelago offers an opportunity to investigate the early use of watercraft. Although one site, Kanedori in Honshu, does suggest the use of watercraft as early as 84,000 years ago, there is no other evidence of hominins in Japan until 50,000 years ago.[68]
The Zhoukoudian cave system near Beijing has been excavated since the 1930s and has yielded precious data on early human behavior in East Asia. Although disputed, there is evidence of possible human burials and interred remains in the cave dated to around 34–20,000 years ago.[68] These remains have associated personal ornaments in the form of beads and worked shell, suggesting symbolic behavior. Along with possible burials, numerous other symbolic objects like punctured animal teeth and beads, some dyed in red ochre, have all been found at Zhoukoudian.[68] Although fragmentary, the archaeological record of eastern Asia shows evidence of behavioral modernity before 50,000 years ago but, like the African record, it is not fully apparent until that time.

The Prehistory of South Africa (and, inseparably, the wider region of Southern Africa) lasts from the Middle Stone Age until the 17th century. Southern Africa was first reached by Homo sapiens before 130,000 years ago, possibly before 260,000 years ago.[1]
The region remained in the Late Stone Age until the first traces of pastoralism were introduced about 2,000 years ago. 
The Bantu migration reached the area now South Africa around the first decade of the 3rd century, over 1800 years ago.[2] Early Bantu kingdoms were established in the 11th century.
First European contact dates to 1488, but European colonization began in the 17th century (see History of South Africa (1652–1815)).
The Middle Stone Age covers the period from 300,000 to 50,000 years ago. 
The hunter-gatherers of Southern Africa, named San by their pastoral neighbours, the Khoikhoi, and Bushmen by Europeans, are in all likelihood direct descendants of the first anatomically modern humans to migrate to Southern Africa more than 130,000 years ago.  The term Khoisan groups the pre-Bantu populations of South Africa. It entered usage in the early-to-mid 20th century, and was originally coined by  Isaac Schapera around 1930.[3]
It entered wider usage from the 1960s, based on the proposal of a "Khoisan" language family by Joseph Greenberg. The name San in anthropological usage is a back-formation from the compound and began to replace "Bushmen" from the 1970s onward (see San people#Names). The term has gradually replaced the former term Cape Blacks or Western Cape Blacks, from which is derived the term Capoid used in 20th-century anthropological literature. Use of Khoisanid in genetic genealogy was introduced by Cavalli-Sforza, L. Luca et al., The History and Geography of Human Genes (1994).
It is thought that the Homo sapiens populations ancestral to the Khoisan of Southern Africa have represented the largest human population during the majority of the anatomically modern human timeline, from their early separation before 150 kya until the recent peopling of Eurasia some 70 kya.[4]
They were much more widespread than today, their modern habitat being reduced due to their decimation in the course of the Bantu expansion. They were dispersed throughout much of Southern and Southeastern Africa.
There was also a significant back-migration of bearers of the mitochondrial DNA haplogroup L0 towards eastern Africa between 120 and 75 kya. Rito et al. (2013) speculate that pressure from such back-migration may even have contributed to the dispersal of East African populations out of Africa at about 70 kya.[5]
During the Middle Stone Age, the climate fluctuated between glacial, rainy, and increasingly humid causing the early hunter-gatherers of South Africa to adapt their technological advancements, movements, and foraging strategies.[6]
Blombos Cave contains personal ornaments and what are presumed to be the tools used for the production of artistic imagery, as well as bone tools.[7]
Still Bay and Howieson's Poort contain variable tool technologies.[8]
The Khoisanid populations ancestral to the Khoisan were spread throughout much of Southern and Eastern Africa throughout the Late Stone Age, after about 75,000 years ago. 
A further expansion, dated to about 20,000 years ago, has been proposed based on the distribution of the L0d haplogroup. Rito et al. suggest a connection of this recent expansion with the spread of click consonants to eastern African languages (Hadza language).[5]
The Middle Stone Age  Sangoan industry occupied southern Africa in areas where annual rainfall is less than a metre (1000 mm; 39.4 in).[9] The contemporary San and Khoi peoples resemble those represented by the ancient Sangoan skeletal remains.
The Late Stone Age in South Africa corresponds to the later phase of the  Sangoan, beginning about 50,000 years ago, followed by   Lupemban culture,  from about 30,000 to 12,000 years ago (also known as "Second Intermediate", between Middle and Late Stone Age.
The term Late Stone Age was introduced for South Africa in 1929 by John Hilary Goodwin and C. van Riet Lowe[10] The Lupemban is followed by the so-called Albany industry (12,000 to 9,000 years ago). Finally, the time from 9,000 to 2,000 years ago (7th to 1st millennia BC) is accounted for by the so-called "Wilton inventory" microliths.[11]
Beginning around 2,000 years ago, there are traces of pastoralism. This reflects the arrival of the Khoikhoi herders; from this time,  hunting and gathering gradually gave way to herding as the dominant economic activity. The arrival of livestock is thought to have introduced concepts of personal wealth and property-ownership as well as the establishment of a political structure of chiefdoms.
It was long believed at around the middle of the 1st millennium, the Bantu expansion reached Southern Africa from the Niger River Delta,[12] but recent archaeological work places the presence of Bantu-speakers in the region as early as the 3rd century CE.[2] The Bantu-speakers not only had domestic animals, but also practiced agriculture, farming millet, sorghum and other crops. They also displayed skill in working iron, and lived in settled villages. The Bantu arrived in South Africa in small waves rather than in one cohesive migration. Some groups, ancestral to the  Nguni peoples (the Zulu, Xhosa, Swazi, and Ndebele), preferred to live near the coast. Others, now known as the Sotho–Tswana peoples (Tswana, Pedi, and Basotho), settled in the Highveld, while today's Venda, Lemba, and Tsonga peoples made their homes in the northeastern areas of South Africa.[citation needed]
Bantu-speakers and Khoisan mixed, as evidenced by rock paintings showing the two different groups interacting. The type of contact remains unknown, although linguistic proof of integration survives to prove interaction was well established, as several Bantu languages (notably Xhosa and Zulu) incorporated the click consonant characteristic of the Khoisan languages.[citation needed]
The Khoikhoi began to move further south, reaching the Cape. Along the way they intermarried with the hunter-gatherers, whom they referred to as San (in origin a derogatory term based on a term for "picking up from the ground", i.e. "gathering, scavenging").
It is thought that the San themselves were pushed back by both the advancing Bantu and by the Khoikhoi, retreating to the interior of the Kalahari.[citation needed]
The Bantu expansion was one of the major demographic movements in human prehistory, sweeping much of the African continent during the 2nd and 1st millennia BC. Bantu-speaking communities reached southern Africa from the Congo Basin by the early centuries AD. Some of the migrant groups, ancestral to today's Nguni peoples (the Zulu, Xhosa, Swazi, and Ndebele), preferred to live near the eastern coast of what is present-day South Africa.[13] 
Others, now known as the Sotho–Tswana peoples (Tswana, Pedi, and Sotho), settled in the interior on the plateau known as the Highveld  while today's Venda, Lemba, and Tsonga peoples made their homes in the north-eastern areas of present-day South Africa.[citation needed]
The Kingdom of Mapungubwe, which was located near the northern border of present-day South Africa, at the confluence of the Limpopo and Shashe rivers adjacent to present-day Zimbabwe and Botswana, was the first Bantu kingdom in southern Africa, established in the 11th century. 
The kingdom was the first stage in a development that would culminate in the creation of the Kingdom of Zimbabwe in the 13th century, and with gold trading links to Rhapta and Kilwa Kisiwani on the African east coast. The Kingdom of Mapungubwe lasted about 80 years, and at its height its population was about 5,000 people.[14][15]
The first European historical records about these people begin in the late 15th century, with the beginning of European exploration. The first historical record of South Africa dates to 1488, by Portuguese explorer Bartolomeu Dias. 
In November 1497, a fleet of Portuguese ships under the command of the Portuguese mariner Vasco da Gama rounded the Cape of Good Hope.[16] Historical records of the interior begin significantly later, with the foundation of the Dutch Cape Colony in 1652.

The prehistory of Africa spans from the earliest human presence in Africa until the ancient period in the history of Africa.
The first known hominids evolved in Africa. According to paleontology, the early hominids' skull anatomy was similar to that of the gorilla and the chimpanzee, great apes that also evolved in Africa, but the hominids had adopted a bipedal locomotion which freed their hands. This gave them a crucial advantage, enabling them to live in both forested areas and on the open savanna at a time when Africa was drying up and the savanna was encroaching on forested areas.
By 4 million years ago, several australopithecine hominid species had developed throughout Southern, Eastern and Central Africa. They were tool users, and makers of tools. They scavenged for meat and were omnivores.[1]
By approximately 3.3 million years ago, primitive stone tools were first used to scavenge kills made by other predators and to harvest carrion and marrow from their bones. In hunting, Homo habilis was probably not capable of competing with large predators and was still more prey than hunter. H. habilis probably did steal eggs from nests and may have been able to catch small game and weakened larger prey (cubs and older animals). The tools were classed as Oldowan.[2]
Around 1.8 million years ago, Homo ergaster first appeared in the fossil record in Africa. From Homo ergaster, Homo erectus evolved 1.5 million years ago. Some of the earlier representatives of this species were still fairly small-brained and used primitive stone tools, much like H. habilis. The brain later grew in size, and H. erectus eventually developed a more complex stone tool technology called the Acheulean. Possibly the first hunters, H. erectus mastered the art of making fire and was the first hominid to leave Africa, colonizing most of Afro-Eurasia and perhaps later giving rise to Homo floresiensis. Although some recent writers have suggested that Homo georgicus was the first and primary hominid ever to live outside Africa, many scientists consider H. georgicus to be an early and primitive member of the H. erectus species.[3]
The fossil record shows Homo sapiens (also known as "modern humans" or "anatomically modern humans") living in Africa by about 350,000-260,000 years ago. The earliest known Homo sapiens fossils include the Jebel Irhoud remains from Morocco (c. 315,000 years ago),[4] the Florisbad Skull from South Africa (c. 259,000 years ago), and the Omo remains from Ethiopia (c. 233,000 years ago).[5][6][7][8][9] Scientists have suggested that Homo sapiens may have arisen between 350,000 and 260,000 years ago through a merging of populations in East Africa and South Africa.[10][11]
Evidence of a variety of behaviors indicative of Behavioral modernity date to the African Middle Stone Age, associated with early Homo sapiens and their emergence. Abstract imagery, widened subsistence strategies, and other "modern" behaviors have been discovered from that period in Africa, especially South, North, and East Africa. The Blombos Cave site in South Africa, for example, is famous for rectangular slabs of ochre engraved with geometric designs. Using multiple dating techniques, the site was confirmed to be around 77,000 and 100–75,000 years old.[12][13] Ostrich egg shell containers engraved with geometric designs dating to 60,000 years ago were found at Diepkloof, South Africa.[14] Beads and other personal ornamentation have been found from Morocco which might be as much as 130,000 years old; as well, the Cave of Hearths in South Africa has yielded a number of beads dating from significantly prior to 50,000 years ago,[15] and shell beads dating to about 75,000 years ago have been found at Blombos Cave, South Africa.[16][17][18]
Specialized projectile weapons as well have been found at various sites in Middle Stone Age Africa, including bone and stone arrowheads at South African sites such as Sibudu Cave (along with an early bone needle also found at Sibudu) dating approximately 60,000-70,000 years ago,[19][20][21][22][23] and bone harpoons at the Central African site of Katanda dating to about 90,000 years ago.[24] Evidence also exists for the systematic heat treating of silcrete stone to increase its flake-ability for the purpose of toolmaking, beginning approximately 164,000 years ago at the South African site of Pinnacle Point and becoming common there for the creation of microlithic tools at about 72,000 years ago.[25][26] Early stone-tipped projectile weapons (a characteristic tool of Homo sapiens), the stone tips of javelins or throwing spears, were discovered in 2013 at the Ethiopian site of Gademotta, and date to around 279,000 years ago.[27]
In 2008, an ochre processing workshop likely for the production of paints was uncovered dating to ca. 100,000 years ago at Blombos Cave, South Africa. Analysis shows that a liquefied pigment-rich mixture was produced and stored in the two abalone shells, and that ochre, bone, charcoal, grindstones and hammer-stones also formed a composite part of the toolkits. Evidence for the complexity of the task includes procuring and combining raw materials from various sources (implying they had a mental template of the process they would follow), possibly using pyrotechnology to facilitate fat extraction from bone, using a probable recipe to produce the compound, and the use of shell containers for mixing and storage for later use.[28][29][30]
Modern behaviors, such as the making of shell beads, bone tools and arrows, and the use of ochre pigment, are evident at a Kenyan site by 78,000-67,000 years ago.[31]
Expanding subsistence strategies beyond big-game hunting and the consequential diversity in tool types has been noted as signs of behavioral modernity. A number of South African sites have shown an early reliance on aquatic resources from fish to shellfish. Pinnacle Point, in particular, shows exploitation of marine resources as early as 120,000 years ago, perhaps in response to more arid conditions inland.[32] Establishing a reliance on predictable shellfish deposits, for example, could reduce mobility and facilitate complex social systems and symbolic behavior. Blombos Cave and Site 440 in Sudan both show evidence of fishing as well. Taphonomic change in fish skeletons from Blombos Cave have been interpreted as capture of live fish, clearly an intentional human behavior.[15] Humans in North Africa (Nazlet Sabaha,[33] Egypt) are known to have dabbled in chert mining, as early as ≈100,000 years ago, for the construction of stone tools.[34][35]
Evidence was found in 2018, dating to about 320,000 years ago, at the Kenyan site of Olorgesailie, of the early emergence of modern behaviors including: long-distance trade networks (involving goods such as obsidian), the use of pigments, and the possible making of projectile points. It is observed by the authors of three 2018 studies on the site, that the evidence of these behaviors is approximately contemporary to the earliest known Homo sapiens fossil remains from Africa (such as at Jebel Irhoud and Florisbad), and they suggest that complex and modern behaviors began in Africa around the time of the emergence of Homo sapiens.[36][37][38] In 2019, further evidence of early complex projectile weapons in Africa was found at Adouma, Ethiopia dated 80,000-100,000 years ago, in the form of points considered likely to belong to darts delivered by spear throwers.[39]
Around 65–50,000 years ago, the species' expansion out of Africa launched the colonization of the planet by modern human beings.[40][41][42][43] By 10,000 BC, Homo sapiens had spread to most corners of Afro-Eurasia. Their dispersals are traced by linguistic, cultural and genetic evidence.[2][44][45] Eurasian back-migrations, specifically West-Eurasian backflow, started in the early Holocene or already earlier in the Paleolithic period, sometimes between 30-15,000 years ago, followed by pre-Neolithic and Neolithic migration waves from the Middle East, mostly affecting Northern Africa, the Horn of Africa, and wider regions of the Sahel zone and East Africa.[46]
Affad 23 is an archaeological site located in the Affad region of southern Dongola Reach in northern Sudan,[47] which hosts "the well-preserved remains of prehistoric camps (relics of the oldest open-air hut in the world) and diverse hunting and gathering loci some 50,000 years old".[48][49][50]
The earliest physical evidence of astronomical activity may be a lunar calendar found on the Ishango bone dated to between 23,000 and 18,000 BC from in what is now the Democratic Republic of the Congo.[51] However, this interpretation of the object's purpose is disputed.[52]
Scholars have argued that warfare was absent throughout much of humanity's prehistoric past, and that it emerged from more complex political systems as a result of sedentism, agricultural farming, etc.[53] However, the findings at the site of Nataruk in Turkana County, Kenya, where the remains of 27 individuals who died as the result of an intentional attack by another group 10,000 years ago, suggest that inter-human conflict has a much longer history.[54]
Around 16,000 BC, from the Red Sea Hills to the northern Ethiopian Highlands, nuts, grasses and tubers were being collected for food. By 13,000 to 11,000 BC, people began collecting wild grains. This spread to Western Asia, which domesticated its wild grains, wheat and barley. Between 10,000 and 8000 BC, Northeast Africa was cultivating wheat and barley and raising sheep and cattle from Southwest Asia. A wet climatic phase in Africa turned the Ethiopian Highlands into a mountain forest. Omotic speakers domesticated enset around 6500–5500 BC. Around 7000 BC, the settlers of the Ethiopian highlands domesticated donkeys, and by 4000 BC domesticated donkeys had spread to Southwest Asia. Cushitic speakers, partially turning away from cattle herding, domesticated teff and finger millet between 5500 and 3500 BC.[55]
During the 11th millennium BP, pottery was independently invented in Africa, with the earliest pottery there dating to about 9,400 BC from central Mali.[56] It soon spread throughout the southern Sahara and Sahel.[57] In the steppes and savannahs of the Sahara and Sahel in Northern West Africa, the Nilo-Saharan speakers and Mandé peoples started to collect and domesticate wild millet, African rice and sorghum between 8000 and 6000 BC. Later, gourds, watermelons, castor beans, and cotton were also collected and domesticated. The people started capturing wild cattle and holding them in circular thorn hedges, resulting in domestication.[58] They also started making pottery and built stone settlements (e.g., Tichitt, Oualata). Fishing, using bone-tipped harpoons, became a major activity in the numerous streams and lakes formed from the increased rains.[59] Mande peoples have been credited with the independent development of agriculture about 4000–3000 BC.[60]
In West Africa, the wet phase ushered in an expanding rainforest and wooded savanna from Senegal to Cameroon. Between 9000 and 5000 BC, Niger–Congo speakers domesticated the oil palm and raffia palm. Two seed plants, black-eyed peas and voandzeia (African groundnuts), were domesticated, followed by okra and kola nuts. Since most of the plants grew in the forest, the Niger–Congo speakers invented polished stone axes for clearing forest.[61]
Most of Southern Africa was occupied by pygmy peoples and Khoisan who engaged in hunting and gathering. Some of the oldest rock art was produced by them.[62]
For several hundred thousand years the Sahara has alternated between desert and savanna grassland in a 41,000 year cycle caused by changes ("precession") in the Earth's axis as it rotates around the Sun which change the location of the North African Monsoon.[63] When the North African monsoon is at its strongest annual precipitation and subsequent vegetation in the Sahara region increase, resulting in conditions commonly referred to as the "green Sahara". For a relatively weak North African monsoon, the opposite is true, with decreased annual precipitation and less vegetation resulting in a phase of the Sahara climate cycle known as the "desert Sahara". The Sahara has been a desert for several thousand years, and is expected to become green again in about 15,000 years time (17,000 AD).[64]
Just prior to Saharan desertification, the communities that developed south of Egypt, in what is now Sudan, were full participants in the Neolithic Revolution and lived a settled to semi-nomadic lifestyle, with domesticated plants and animals.[65] It has been suggested that megaliths found at Nabta Playa are examples of the world's first known archaeoastronomical devices, predating Stonehenge by some 1,000 years.[66] The sociocultural complexity observed at Nabta Playa and expressed by different levels of authority within the society there has been suggested as forming the basis for the structure of both the Neolithic society at Nabta and the Old Kingdom of Egypt.[67] By 5000 BC, Africa entered a dry phase, and the climate of the Sahara region gradually became drier. The population trekked out of the Sahara region in all directions, including towards the Nile Valley below the Second Cataract, where they made permanent or semipermanent settlements. A major climatic recession occurred, lessening the heavy and persistent rains in Central and Eastern Africa.
Evidence of the early smelting of metals –  lead, copper, and bronze –  dates from the fourth millennium BC.[68]
Egyptians smelted copper during the predynastic period, and bronze came into use after 3,000 BC at the latest[69] in Egypt and Nubia. Nubia became a major source of copper as well as of gold.[70] The use of gold and silver in Egypt dates back to the predynastic period.[71][72]
In the Aïr Mountains of present-day Niger people smelted copper independently of developments in the Nile valley between 3,000 and 2,500 BC. They used a process unique to the region, suggesting that the technology was not brought in from outside; it became more mature by about 1,500 BC.[72]
By the 1st millennium BC iron working had reached Northwestern Africa, Egypt, and Nubia.[73] Zangato and Holl document evidence of iron-smelting in the Central African Republic and Cameroon that may date back to 3,000 to 2,500 BC.[74] Assyrians using iron weapons pushed Nubians out of Egypt in 670 BC, after which the use of iron became widespread in the Nile valley.[75]
The theory that iron spread to Sub-Saharan Africa via the Nubian city of Meroe[76] is no longer widely accepted, and some researchers[which?] believe that sub-Saharan Africans invented iron metallurgy independently. Metalworking in West Africa has been dated as early as 2,500 BC at Egaro west of the Termit in Niger, and iron working was practiced there by 1,500 BC.[77] Iron smelting has been dated to 2,000 BC in southeast Nigeria.[78] Central Africa provides possible evidence of iron working as early as the 3rd millennium BC.[79] Iron smelting developed in the area between Lake Chad and the African Great Lakes between 1,000 and 600 BC, and in West Africa around 2,000 BC, long before the technology reached Egypt. Before 500 BC, the Nok culture in the Jos Plateau was already smelting iron.[80][81][82][83][need quotation to verify][84][85] Archaeological sites containing iron-smelting furnaces and slag have been excavated at sites in the Nsukka region of southeast Nigeria in Igboland: dating to 2,000 BC at the site of Lejja (Eze-Uzomaka 2009)[78][86] and to 750 BC and at the site of Opi (Holl 2009).[86] The site of Gbabiri (in the Central African Republic) has also yielded evidence of iron metallurgy, from a reduction furnace and blacksmith workshop; with earliest dates of 896–773 BC and 907–796 BC respectively.[85]

The evolution of human bipedalism, which began in primates approximately four million years ago,[1] or as early as seven million years ago with Sahelanthropus,[2][3] or approximately twelve million years ago with Danuvius guggenmosi, has led to morphological alterations to the human skeleton including changes to the arrangement, shape, and size of the bones of the foot, hip, knee, leg, and the vertebral column. These changes allowed for the upright gait to be overall more energy efficient in comparison to quadrupeds. The evolutionary factors that produced these changes have been the subject of several theories that correspond with environmental changes on a global scale.[4]
Human walking is about 75% less costly than both quadrupedal and bipedal walking in chimpanzees. Some hypotheses have supported that bipedalism increased the energetic efficiency of travel and that this was an important factor in the origin of bipedal locomotion. Humans save more energy than quadrupeds when walking but not when running. Human running is 75% less efficient than walking. A 1980 study reported that walking in living hominin bipeds is noticeably more efficient than walking in living hominin quadrupeds, but the costs of quadrupedal and bipedal travel are the same.[5]
Human feet evolved enlarged heels.[6] The human foot evolved as a platform to support the entire weight of the body, rather than acting as a grasping structure (like hands), as it did in early hominids. Humans therefore have smaller toes than their bipedal ancestors. This includes a non-opposable hallux, which is relocated in line with the other toes.[7] The push off would also require all the toes to be slightly bent up.[8]
Humans have a foot arch rather than being flat footed.[7] When non-human hominids walk upright, weight is transmitted from the heel, along the outside of the foot, and then through the middle toes while a human foot transmits weight from the heel, along the outside of the foot, across the ball of the foot and finally through the big toe. This transference of weight contributes to energy conservation during locomotion.[1][9] The muscles that work along with the hallux has evolved to provide efficient push off. The long arch has also evolved to provide efficient push-off. The stiffening of the arch would be required of an upward gait, all considered that modern bipedalism does not include grasping of tree branches, which also explains the hallux evolving to line up with the rest of the toes.[8]
Human knee joints are enlarged for the same reason as the hip – to better support an increased amount of body weight.[7]  The degree of knee extension (the angle between the thigh and shank in a walking cycle) has decreased. The changing pattern of the knee joint angle of humans shows a small extension peak, called the "double knee action," in the midstance phase.  Double knee action decreases energy lost by vertical movement of the center of gravity.[1] Humans walk with their knees kept straight and the thighs bent inward so that the knees are almost directly under the body, rather than out to the side, as is the case in ancestral hominids. This type of gait also aids balance.[7]
An increase in leg length since the evolution of bipedalism changed how leg muscles functioned in upright gait. In humans, the push for walking comes from the leg muscles acting at the ankle. A longer leg allows the use of the natural swing of the limb so that, when walking, humans do not need to use muscle to swing the other leg forward for the next step.[7] As a consequence, since the human forelimbs are not needed for locomotion, they are instead optimized for carrying, holding, and manipulating objects with great precision.[10]
This results in decreased strength in the forelimbs relative to body size for humans compared to apes.[11]
Having long hind limbs and short forelimbs allows humans to walk upright, while orangutans and gibbons had the adaptation of longer arms to swing on branches.[12] Apes can stand on their hindlimbs, but they cannot do so for long periods of time without getting tired.  This is because their femurs are not adapted for bipedalism. Apes have vertical femurs, while humans have femurs that are slightly angled medially from the hip to the knee, thus making human knees closer together and under the body's center of gravity. This adaptation lets humans lock their knees and stand up straight for long periods of time without much effort from muscles.[13] The gluteus maximus became a major role in walking and is one of the largest muscles in humans. This muscle is much smaller in chimps, which shows that it has an important role in bipedalism. When humans run, our upright posture tends to flex forward as each foot strikes the ground creating momentum forward. The gluteus muscle helps to prevent the upper trunk of the body from "pitching forward" or falling over.[14]
Modern human hip joints are larger than in quadrupedal ancestral species to better support the greater amount of body weight passing through them.[7] They also have a shorter, broader shape.  This alteration in shape brought the vertebral column closer to the hip joint, providing a stable base for support of the trunk while walking upright.[15] Because bipedal walking requires humans to balance on a relatively unstable ball and socket joint, the placement of the vertebral column closer to the hip joint allows humans to invest less muscular effort in balancing.[7]
Change in the shape of the hip may have led to the decrease in the degree of hip extension, an energy efficient adaptation.[1][14] The ilium changed from a long and narrow shape to a short and broad one and the walls of the pelvis modernized to face laterally. These combined changes provide increased area for the gluteus muscles to attach; this helps to stabilize the torso while standing on one leg. The sacrum has also become more broad, increasing the diameter of the birth canal and making birthing easier. To increase surface for ligament attachment to help support the abdominal viscera during erect posture, the ischial spines became more prominent and shifted towards the middle of the body.[16]
The vertebral column of humans takes a forward bend in the lumbar (lower) region and a backward bend in the thoracic (upper) region.  Without the lumbar curve, the vertebral column would always lean forward, a position that requires much more muscular effort for bipedal animals.  With a forward bend, humans use less muscular effort to stand and walk upright.[15]  Together the lumbar and thoracic curves bring the body's center of gravity directly over the feet.[7] Specifically, the S-shaped curve in the spine brings the center of gravity closer to the hips by bringing the torso back. Balance of the whole vertebral column over the hip joints is a major contribution for efficient bipedalism.[17] The degree of body erection (the angle of body incline to a vertical line in a walking cycle) is significantly smaller[1] to conserve energy.
The Angle of Sacral Incidence was a concept developed by G. Duval-Beaupère and his team at the University of René Descartes. It combines both the pelvic tilt and sacral slope to determine approximately how much lordosis is required for the upright gait to eliminate strain and fatigue on the torso. Lordosis, which the inward curvature of the spine, is normal for an upright gait as long as it is not too excessive or minimal. If the inward curvature of the spine is not enough, the center of balance would be offset causing the body to essentially tip forward, which is why some apes that have the ability to be bipedal require large amounts of energy to stand up. In addition to sacral angles, the sacrum has also evolved to be more flexible in comparison to the stiff sacrum that apes possess.[17]
The human skull is balanced on the vertebral column. The foramen magnum is located inferiorly under the skull, which puts much of the weight of the head behind the spine. The flat human face helps to maintain balance on the occipital condyles. Because of this, the erect position of the head is possible without the prominent supraorbital ridges and the strong muscular attachments found in, for example, apes. As a result, in humans the muscles of the forehead (the occipitofrontalis) are only used for facial expressions.[10]
Increasing brain size has also been significant in human evolution. It began to increase approximately 2.4 million years ago, but modern levels of brain size were not attained until after 500,000 years ago. Zoological analyses have shown that the size of human brains is significantly larger than what anatomists would expect for their size. The human brain is three to four times larger than its closest relative, which is the chimpanzee.[16]
Even with much modification, some features of the human skeleton remain poorly adapted to bipedalism, leading to negative implications prevalent in humans today.  The lower back and knee joints are plagued by osteological malfunction, lower back pain being a leading cause of lost working days,[18] because the joints support more weight. Arthritis has been an obstacle since hominids became bipedal: scientists have discovered its traces in the vertebrae of prehistoric hunter-gatherers.[18]  Physical constraints have made it difficult to modify the joints for further stability while maintaining efficiency of locomotion.[7]
There have been multiple theories as to why bipedalism was favored, thus leading to skeletal changes that aided the upward gait. The savannah hypothesis describes how the transition from arboreal habits to a savannah lifestyle favored an upright, bipedal gait. This would also change the diet of hominins, more specifically a shift from primarily plant-based to a higher protein, meat-based diet. This would eventually increase the size of the brain, changing the skeletal structure of the skull.[19] Transitions from the forests to the savannah meant that sunlight and heat would require major changes in lifestyle. Being a biped on an open field is also an advantage because of heat dispersal. Walking upright reduces the amount of direct sun exposure and radiation in comparison to being a quadruped which have more body surface on top for the sun to hit.[20] Increased capabilities of postural/locomotor neural control is hypothesis suggesting that the transition from quadrupedal to habitual upright bipedal locomotion was caused by qualitative changes in the nervous system that allowed controlling the more demanding type of posture/locomotion. Only after the more demanding posture was enabled by changes in the nervous system, could advantages of bipedal over quadrupedal locomotion be utilized, including better scanning of the environment, carrying food and infants, simultaneous upper extremity movements and observation of the environment, limitless manipulation of objects with upper extremities, and less space for rotating around the Z-axis.[21]

The present-day state of Goa was established in 1987.[1] Goa is India's smallest state by area. It shares a lot of similarities with Indian history, especially with regard to colonial influences and a multi-cultural aesthetic.
The Usgalimal rock engravings, belonging to the Upper Paleolithic or Mesolithic periods, exhibit some of the earliest traces of human settlement in India. The Mauryan and Satavahana Empires ruled modern-day Goa during the Iron Age.[citation needed]
During the medieval period, Goa was ruled by the Kadamba kingdom, Vijayanagara Empire, Bahmani Sultanate and Bijapur Sultanate. [citation needed]
It was ruled by the Kadamba dynasty from the 2nd century CE to 1312 and by the Deccan from 1312 to 1367. The city was then annexed by the Kingdom of Vijayanagara and was later conquered by the Bahmanī sultanate, which founded Old Goa on the island in 1440.[2]
The Portuguese invaded Goa in 1510, defeated the Bijapur Sultanate. The Portuguese rule lasted for about 450 years, and heavily influenced Goan culture, cuisine, and architecture.
In 1961, India took control over Goa after a 36-hour battle and integrated it into India. The area of Goa was incorporated into Goa, Daman and Diu, which included the Damaon territory in the north of the Konkan region. In 1987, following the Konkani language agitation Goa was granted statehood. Goa has one of the highest GDP per capita [according to whom?]and Human Development Index among Indian states.
There is evidence of the tectonic origins of Goa dating back to 10,000 BC.[3] Further, evidence of human occupation of Goa dates back at least to the Lower Paleolithic Age, indicated by the archaeological findings of Acheulean bifaces in the Mandovi-Zuari basin.[4] However, evidence suggesting the region's ancient foundation is obscured by the legend of Goa's creation by the Hindu sage Parashurama.[3]
Some parts of present-day Goa appear to have been uplifted from the sea due to geological tectonic plate movement. There is evidence to support this theory as indicated by the presence of marine fossils, buried seashells, and other features of reclaimed topography in the coastal belt.[3]  fossilized branches have been found later in many villages on the foothills of the Sahyadri dating back more than 10,000 BC. Thus the geologists concluded that Goa has risen from the seabed as a result of violent tectonic movements. At the decline of the intensity of population in the last Pleistocene age around 10000 BC, the bottom of Deccan plateau was lifted up and out of sea-waters by the tectonic movements, formed the West-coast of India, Goa being a part thereof.[5]
Until 1993 the existence of humans in Goa during the Paleolithic and Mesolithic period was highly debated. The discovery of rock art engravings on lateritic platforms and granite boulders from Usgalimal on the banks of west-flowing river Kushavati River, has shed light on the prehistory of Goa.[6] The rock shelter at Usgalimal has enough space for 25 to 30 people. The perennial stream in the vicinity which might have served Stone Age man for centuries as a source of water.[7] An anthropomorphic figure of Mother goddess and tectiforms resembling tree-like motifs have been found.[7] This site was discovered by Dr P.P.Shirodkar. Exploration of several Mesolithic sites of the Mandovi-Zuari basin, at other sites such as Keri, Thane, Anjuna, Mauxim, Kazur in Quepem, Virdi, has led to the discovery of several scrapers, points, bores, cones, etc. A hand axe has also been found at Usgalimal.[8] Further unifacial choppers were recovered on a flat-based pebble of quartzite from a pebble conglomerate at Shigaon on the Dudhsagar River.[9] Shirodakar made a detailed study of the rock engravings and dated them to Upper Paleolithic and Mesolithic phases, or to 20,000-30,000 BC.[8] These discoveries have demonstrated that the region had been supporting a population of hunter-gatherers well before the advent of agriculture.
Evidence of Palaeolithic cave existence can be seen at Dabolim, Adkon, Shigaon, Fatorpa, Arli, Maulinguinim, Diwar, Sanguem, Pilerne, Aquem-Margaon et cetera. Difficulty in carbon dating the laterite rock compounds has posed a problem in determining the exact time period.[10]
The prehistoric engravings at Usgalimal were discovered by PP Shirodkar in the early 1990s and subsequently studied by the Institute of Oceanography in Goa.[11] More than 125 forms were found scattered on the banks of river Kushavati in south-eastern Goa. According to Kamat, these are evidence of a prehistoric Goan shamanistic practice. For hundreds of years, the Kushavati rock art of Goa was known locally as goravarakhnyachi chitram, or pictures made by cowherds. But people did not know how ancient the works were, nor could anyone interpret them. After thorough study of these forms, scholars have concluded that these petroglyphs differ from those found elsewhere in Goa. Deeper studies and analysis over a period of ten years showed these petroglyphs were an exquisitely carved ocular labyrinth, one of the best in India and Asia. Its ocular nature added to the evidence of prehistoric shamanism.
The studies have shown that the Kushavati culture was a hunter-gatherer culture with deep knowledge of local natural resources and processes – water, fish, plants, game, animal breeding cycles, seasons and natural calamities. The Kushavati culture was greatly concerned with water security, so they set up camps near the streams. The Kushavati found food security in the jungle near the streams. Like every culture, its members confronted the mysteries of illness, death and birth. Kamat believes that this culture dated to 6,000 to 8,000 years ago. On basis of recent DNA-based work on human migration, Dr. Nandkumar Kamat has ruled out the possibility of Kushavati shamans belonging to the first wave of humans to arrive in Goa. They were not negritoes or austrics. Most probably they were the earliest Mediterraneans who had descended the Western Ghats, probably in their search for sea salt on Goa's coast. As the Kushavati transitioned into a Neolithic society, they began the domestication of animals and were in the last phase of using stone tools. The entire realm of shamanism underwent a radical transition. Today evidence of the metamorphosis in masked dance drama Perni jagor can be seen in the same cultural region.
Archaeological evidence in the form of polished stone axes, suggest the first settlements of Neolithic man in Goa.[12] These axes have been found in Goa Velha.[13] During this period tribes of Austric origin such as the Kols, Mundaris and Kharvis may have settled Goa, living on hunting, fishing and a primitive form of agriculture since 3500 BC.[12] According to Goan historian Anant Ramakrishna Dhume, the Gauda and Kunbi and other such castes are modern descendants of ancient Mundari tribes. Dhume notes several words of Mundari origin in the Konkani language. He describes the deities worshipped by the ancient tribes, their customs, methods of farming, and its overall effect on modern-day Goan culture.[14] The Negroids were in a Neolithic stage of primitive culture and were food-gatherers. Traces of Negroid physical characteristics can be found in parts of Goa, up to at least the middle of the first millennium.[14]
The Proto-Australoid tribe known as the Konkas, from whom is derived the name of the region, Kongvan or Konkan, with the other mentioned tribes, reportedly made up the earliest settlers in the territory.[15] Agriculture had not fully developed at this stage and was being developed. The Kol and Mundari may have been using stone and wood implements, as iron implements were used by the megalithic tribes as late as 1200 BC. The Kol tribe is believed to have migrated from Gujarat.[16]
During this period, the people began worship of a mother goddess in the form of anthill or Santer. The Anthill is called  Roen(Konkani:रोयण), which is derived from the Austric word Rono, meaning with holes. The later Indo-Aryan and Dravidian settlers also adopted anthill worship, which was translated into Prakrit Santara. They also worshipped the mother earth by the name of Bhumika in Prakrit. Anthill worship still continues in Goa.[14]
The theocratic democracy of Sumer was transformed into the oligarchic democracy of village-administration in Goa known as Gaumkari, when it overlapped with the practices of the locals. The agricultural land was jointly owned by the group of villagers, they had right to auction the land, this rent was used for development, and the remainder was distributed amongst the Gaukars. Sumerians view that the village land must belong to the village god or goddess, this was the main feature of the Gaumkari system where the village's preeminent deity's temple was the centre of all the activities.[17] It consisted of definite boundaries of land from village to village with its topographic detail, its management and social, religious and cultural interaction. Gaumkari thus were in existence long before constitution of the state of Goa itself.[18]
Thus even before any king ruled the territory, oligarchic democracy in the form of Gaumkari existed in Goa. This form of village-administration was called as Gaumponn (Konkani:गांवपण), and despite the periodic change of sovereigns, the Gaumponn always remained, hence the attachment and fidelity of the Goans to their village has always surpassed their loyalty to their rulers (most of them were extraterritorial).[19] This system for governance became further systematised and fortified, and it has continued to exist ever since. Even today 223 comunidades are still functioning in Goa, though not in the true sense.[18]
The second wave of migrants arrived sometime between 1700 and 1400 BC. This second wave migration was accompanied by southern Indians from the Deccan plateau. A wave of Kusha or Harappan people moved to Lothal probably around 1600 BC to escape submergence of their civilization which thrived on sea-trade.[16] With the admixture of several cultures, customs, religions, dialects and beliefs, led to revolutionary change in early Goan society.[20]
Chandragupta Maurya incorporated the west coast of India in his province of Aparanta, and the impact of Magadhan Prakrit, the official language of the Mauryan Empire, on the local dialects resulted in the formation of early Konkani, as was the case with other Aryan vernaculars. During this era Buddhism was introduced to Goa. Similarly a native Goan named Purna, also known as Punna in Pali, who traveled to Sarnath is considered a direct disciple of Buddha, who popularised Buddhism in Goa in the 5th century BC.[21]
The Satavahana dynasty ruled Goa through their coastal vassals, the Chutus of Karwar. This period is estimated to have lasted from around the 2nd century BC to 100 AD. The Satavahanas had established maritime power and their contacts with Roman Empire from the coastal trade from Sindh to Saurashtra, from Bharuch to Sopara to Goa, where Greek and Roman ships would halt during voyages. The Bhojas fortified themselves after the end of Satavahana Empire.[22] With the fall of the Satavahanas, the lucrative seaborne trade declined.[23] Many Greek converts to Buddhism settled in Goa during this period. Buddha statues in Greek styles have been found in Goa.[24] It can be seen that they ruled a very small part of Goa. Maharashtri prakrit was their language of administration, which influenced medieval Konkani to a great extent.[25]
In the year 150AD, Vashishtiputra Satakarni was defeated by his son-in-law, the Kshatrapa King Rudradaman I who established his rule over Goa.[26] This dynasty ruled the territory until 249AD. Thereafter the dynasty's power seems to have been weakened by their generals, the Abhiras[27]
The history of the Mauryas is almost non-existent. The existing records disclose the names of only three of the dynasty's kings, namely Suketavarman, who ruled some time in the 4th or 5th centuries, Chandravarman in the 6th century, and Ajitavarman in the 7th century, who ruled from Kumardvipa or modern Kumarjuve, but beyond that the records provide no clue as to their mutual relationship. These dates were determined by comparing the style of the Nagari script in which these records are written with the evolution of this script, which may be dated fairly accurately. It is possible to infer from the places mentioned in these records and their discovery locations that at its zenith, the Western Maurya Kingdom comprised the Lata or South Gujarat, coastal Maharashtra, Goa, and approximately half of the North Kanara district. After the Maurya Empire had passed its meridian in the 2nd century BC its satrap in Aparanta made himself independent. A scion of the imperial Mauryas, he founded a dynasty that ruled over the west coast for nearly four centuries from its capital Shurparaka or modern Sopara. This dynasty was known as the Konkan Mauryas. Goa was called Sunaparant by the Mauryas.[21]
First existing as vassals of the Mauryas and later as an independent kingdom, the Bhojas ruled Goa for more than 500 years, annexing the entirety of Goa. The earliest known record of the Bhojas from Goa dates from the 4th century, it was found in the town of Shiroda in Goa. According to Puranik, by tradition the Bhojas belonged to the clan of Yadavas, who may have migrated to Goa via Dwaraka after the Mahabharata war.[28] Two Bhoja copperplates grants dating back to the 3rd century BC were unearthed from Bandora village, written by King Prithvimallavarman. Ancient Chandrapur, modern day Chandor, was the capital of the Bhoja Empire.
From the Bhoja inscriptions found in Goa and Konkan, it is evidenced that the Bhojas used Sanskrit and Prakrit for administration.
According to Vithal Raghavendra Mitragotri, many Brahmins and Vaishyas arrived with Kshatriyas Bhojas from the north.[29] The Kshatriya Bhojas patronised Buddhism and employed many Buddhist converts of Greek and Persian origin.[30]
Goa was ruled by several dynasties of various origins from circa the beginning of the common era to 1500.[citation needed] Since Goa had been under the sway of several dynasties, there was no organised judicial or policing system in those days, except for traditional arrangements governed by absolute rulers and local chieftains. There may have been more order under Muslim rule.[31]
During this time, Goa was not ruled as a singular kingdom. Parts of this territory were ruled by several different kingdoms. The boundaries of these kingdoms were not clearly defined and the kings were content to consider their dominions as extending over many villages, which paid tribute and owed them allegiance.[32]
The Shilaharas of South Konkan ruled Goa from 755 until 1000 AD.  Sannaphulla, the founder of the dynasty, was a vassal of the Rashtrakutas. Their copper-plate inscriptions suggest that they ruled from Vallipattana (there is no unanimity amongst the scholars regarding identification of Vallipattana, some identify it with Balli in Goa, or it may either be Banda or Kharepatan in the modern-day state of Maharashtra), Chandrapura and Gopakapattana.[33] This was a tumultuous period in Goan history. As the Goa Shilahara power waned during the 11th century, the Arab traders gained increasing control of the overseas trade. They enjoyed autonomy from the Shilaharas. In order to control this decline, Kadamba King Guhalladeva I, ruling from Chandor, established secular, political, and economic partnerships with these Arab states. After the Chalukyas defeated the Rashtrakutas, exploiting this situation to their advantage, the Kadamba King, Shashthadeva II, firmly established his rule in Goa.[33]
The Kadambas ruled Goa between the 10th and 14th centuries. In the beginning, the Kadambas ruled only Sashti present day Salcette, a small part of Konkan. They ruled from Chandor, over a large part of Goa, but the port of Gopakapattana was not included in the early years.[34]
Later King Shashthadeva conquered the island of Goa, including the ports of Gopakapattana and Kapardikadvipa, and annexed a large part of South Konkan to his kingdom. He made Gopakapattana as his secondary capital. His successor, King Jayakeshi I, expanded the Goan kingdom. The Sanskrit Jain text Dvayashraya mentions the extent of his capital. Port Gopakapattana had trade contacts with Zanzibar, Bengal, Gujarat and Sri Lanka (mentioned as Zaguva, Gauda, Gurjara, and Simhala in the Sanskrit texts). The city has been described in the contemporary records not only as aesthetically pleasing, but spiritually cleansing as well. Because it was a trading city, Gopakapattana was influenced by many cultures, and its architecture and decorative works showed this cosmopolitan effect. The capital was served by an important highway called Rajvithi or Rajpath, which linked it with Ela, the ruins of which can still be seen. For more than 300 years, it remained a centre for intra-coastal and trans-oceanic trade from Africa to Malaya. Later in the 14th century, the port was looted by the Khalji general Malik Kafur. The capital was transferred to Chandor and then back to Gopakapattana because of Muhammad bin Tughluq's attack on Chandor.[34]
Guhalladeva III, Jayakeshi II, Shivachitta Paramadideva, Vinshuchitta II and Jayakeshi III dominated Goa's political scene in the 12th century. During the rule of Kadambas, the name and fame of Goapuri had reached it zenith. Goa's religion, culture, trade and arts flourished under the rule of these kings. The Kings and their queens built many Shiva temples as they were devout Shaivites. They assumed titles like Konkanadhipati, Saptakotisha Ladbha Varaveera, Gopakapura varadhishva, Konkanmahacharavarti and Panchamahashabda.[35] The Kings had matrimonial relationships with the Kings of Saurashtra, and even the local chieftains. The Kings patronised Vedic religion and performed major fire sacrifices like the horse sacrifice or Ashvamedha. They are also known for patronising Jainism in Goa.
Though their language of administration was Sanskrit and Kannada, Konkani and Marathi were also prevalent. They introduced Kannada language to Goa, which had a very profound influence on the local tongue. Nagari script, Kadamba script, Halekannada script and Goykanadi scripts were very popular. Kadamba Tribhuvanamalla, inscribed a record, dated saka 1028 or AD 1106, that he established a Brahmapuri at Gopaka.[36] Brahmapuris were ancient universities run by the Brahmins where the Vedas, astrology, philosophy, medicine, and other subjects were studied.  Such Brahampuris were found in many places in Goa such as Savoi verem and Gauli moula.
Kadambas ruled Goa for more than 400 years. On 16 October 1345[37] Goa Kadamba King Suriya Deva was assassinated by Muslim invaders.[citation needed]
From 1350 to 1370, Goa was ruled by the Bahmani Sultanate. In 1469 Goa was again conquered by the Bahmani Sultans of Gulbarga. This Sultanate broke up in 1492.
In 1370, the Vijayanagara Empire had reconquered Goa. Vijayanagara was a resurgent Hindu state controlling much of south India; its capital was located at modern day Hampi, in Karnataka. The Vijayanagara rulers then held Goa for nearly a century. During that time its harbours were important ports of arrival for Arabian horses destined for the Vijayanagara cavalry.
In 1492, Goa became a part of Adil Shah's Bijapur Sultanate, which established Goa Velha as its second capital. The former Secretariat building in Panaji is a former Adil Shahi palace. It functioned for the Portuguese as the official residence of their Viceroys.
Vasco da Gama commanded the first circumnavigation of Africa, relying on stories and maps from earlier Portuguese voyages. His fleet of four ships set off from Lisbon in 1497. After island stops at Tenerife and Cape Verde, the ships made landfall on the West African coast. They then steered southwest into the vast South Atlantic Ocean. Near Brazil, by making an eastward turn, they headed toward the southern cape of Africa which they rounded. After passing by the Rio do Infante described earlier by a fellow explorer, a northward course was set. The ships stopped at the East African ports of Mozambique, Mombasa and Malinda. An Arab pilot, or an Indian, then guided their remaining course across the Arabian Sea. A year out from Lisbon, de Gama's fleet landed in Calicut, India. Their arrival signalled the end of Muslim monopoly over the region's maritime trade.
Before the Portuguese ships came to India, the seas to the east had been dominated by the thalassocratic Chola Empire of the Tamils, followed by their Shailendra dynasty successors and other Indianized seafaring states of Java and Sumatra. "Indian ship-building had a high reputation at the time". Yet "by the fifteenth century the navigation of Indian waters was in the hands of the Arabs" both toward the east and westward toward the Gulf and the Red Sea.[38]
When Francisco de Almeida arrived to serve as the first Portuguese viceroy of the East (1505-1509), already there was a regional war on the Malabar coast. In 1505 the Estado da India was established there, in Cochin considerably south of Goa.[39] Almeida ended his tenure with a naval victory fought off Diu, far to the north in Gujarat.
The admiral Afonso de Albuquerque became second viceroy (1509-1515). In 1510 Timoji requested the Portuguese to take over Goa. The offer was welcomed. The city then was quickly seized from Ismail Adil Shah, ruler of the Bijapur Sultanate, but as quickly lost. Albuquerque, however, returned in force on 25 November.[40] In a day the gunnery of the Portuguese ships, and armed parties landing on shore, regained possession. Ismail Adil Shah and his Egyptian Mamluk allies formally surrendered Goa on 10 December. An estimate held that 6,000 of the 9,000 Muslim defenders died, in the battle on the streets or trying to flee.[41] Albuquerque gained direct support from the Hindu people, which frustrated Timoji. He had expected to take autocratic command of the city. Albuquerque appointed him instead chief Aguazil, an administrative office whose role included being the Hindu representative. Timoji was a learned interpreter of local customs.[42]
By eliminating the jizya tax, Albuquerque secured his victory. "Most of the population of Goa were Konkani-speaking Hindus [and] Albuquerque had the good sense to cut their taxes in half".[43] In spite of frequent attacks by raiders, Goa became the centre of Portuguese India. The conquest drew deference from several neighboring kingdoms: the Sultan of Gujarat and the Zamorin of Calicut dispatched embassies, offering alliances and local concessions, e.g., to build fortifications.
Albuquerque started a Portuguese mint in Goa. Local merchants and Timoji had complained about the scarcity of currency. The new coin served to announce the recent conquests.[44] Its value was pegged to existing coins.[45][46][47] An additional mint was built in Portuguese Malacca.
Albuquerque and his successors left the customs and constitutions of the thirty village communities on the island almost untouched, abolishing only the rite of sati, in which widows were burned on their husband's funeral pyre. A register of these customs (Foral de usos e costumes) was published in 1526; it is among the most valuable historical documents pertaining to Goan customs.[48]
Goa was the base for Albuquerque's conquest of Malacca in 1511 and Hormuz in 1515. Albuquerque intended it to be a colony and a naval base, distinct from the fortified factories established in certain Indian seaports. Goa was made capital of the Portuguese Vice-Kingdom in Asia, and the other Portuguese possessions in India, Malacca and other bases in Indonesia, East Timor, the Persian Gulf, Macau in China and trade bases in Japan were under the suzerainty of its Viceroy. By mid-16th century, the area under occupation had expanded to most of present-day limits.
An initial aim of the rulers of Goa was military security, especially from the threat posed by the Bijapur sultanate. Goa's head of state, often titled the Viceroy, was appointed directly by the Portuguese King. The viceroy might consult the finance council, the captain of the armed forces, the fidalgos, the Archbishop of Goa, the chief of judiciary, the Vereador da Fazenda (superintendent of finance), the merchants, and others in informal councils. Commercial success was a primary objective, the purchase in quantity of fine spices to carry back to Europe. Ancillary objectives were creation of a spice-trade monopoly with control over merchant competitors, and levying duties on the cargoes of merchant vessels. Scores of commercial posts and stations were established, not only throughout India, but from Mozambique (Africa) and Hormuz (the Gulf) to Malacca (Malaya) and Macau (China).[49]
Portuguese rule in Goa endured for four and a half centuries. Its Senate or municipal chamber maintained direct communications with the King and paid a special representative to attend to its interests at Court. In 1563 the Governor proposed to make Goa the seat of a parliament representing all parts of the Portuguese east, but this was rejected by the King. Eventually Goa was granted the same civic privileges as Lisbon.[50]
The Portuguese rulers in Goa were either Viceroys or Governors. Their original jurisdiction included those possessions of the Portuguese from east Africa to south Asia and east Asia. The first viceroy to serve located himself in Kochi to the south of Goa on the Malabar coast; in 1510 this Portuguese seat of government was then established at Velha Goa.
Chief among the rivals of Portuguese Goa were the traders of the Zamorin, ruler of Calicut (Kozhikode) on the Malabar coast (northern Kerala). The Zamorin's merchant ships regularly sailed on the Arabian Sea, also venturing in the Bay of Bengal. Other formidable sea traders were of Gujarat to the north. Opponents of the Portuguese in India could then effectively convert their merchant vessels into warships. Early naval battles were Chaul (1508), and a decisive one off Diu (1509) won by the Portuguese.[51]
Naval combat worked to decide the status of the rivals. The distinct advantage of the Portuguese was the cannon mounted on their ships. Vasco de Gama's flagship San Gabriel alone carried twenty guns of quality manufacture. Their mostly Muslim antagonists, lacking ship cannon, could not compete in the sea battles.[52][53][54][55] Although Babur's invasion of India in 1526 used cannon, their use "on ships at sea was not known" before the Portuguese. Further, the well-made sailing ships of India had hulls sewn together not nailed, better in some weather, but unable to absorb the recoil from discharge of onboard cannon. "India was, on most criteria, one of the advanced countries of the world." Yet regarding naval cannon, gunnery, ship design, and nautical skill, the Portuguese had the edge.[56][57]
The Ottoman Turks also disputed control of the Indian Ocean. At Suez overland by camel they transported Mediterranean galleys in pieces for reassembly on the Red Sea, to reinforce their naval forces. From 1538 to 1553 the Turks sent battle fleets against the Portuguese. In several key engagements, however, the transoceanic caravels and galleons outmaneuvered the Turkish galleys.[58]
Hence, from Goa the Portuguese were able to command the Indian Ocean. They instituted a system to tax its trade. Portuguese cartazes (permits for navigation) were issued to owners of merchant vessels. The cartaza obliged the captain to keep to his ship's declared route and stop at the named Portuguese fort to pay duties on merchandise. "Any ship sailing without their cartas was treated as a pirate and was liable to capture and confiscation. . . . The Arab sea trade with India... passed into the hands of the Portuguese."[59] During the sixteenth century "some eight hundred Portuguese galleons" sailed in Indian waters, which became "virtually a Portuguese monopoly."[60]
Portuguese control of the waters off South Asia enabled them to master the lucrative spice trade during the 16th century. 
They coordinated and consolidated their operations from their base at Goa. At first their merchants, called factors, were unfamiliar with the local produce markets, and with appraising the quality of different spices. They learned how not to overpay for poor quality. For storage until seasonal ships left for Portugal, they set up warehouses called factories. At strategic positions on many coasts of the Indian Ocean, the Portuguese established well-guarded, fortified factories.[61]
At the bazaars of Goa, goods from all parts of the East were displayed. Separate streets were designated for the sale of different classes of goods: Bahrain pearls and coral, Chinese porcelain and silk, Portuguese velvet and piece-goods, and drugs and spices from the Malay Archipelago. Fine peppers came from the nearby Malabar coast. Goa was then called Goa Dourada, i.e., Golden Goa.
Especially the Portuguese enjoyed the great rewards to be made by shipping spice cargoes around Africa to Lisbon. The ever increasing demand of Europe meant ready buyers willing to pay top prices. "Arab and Venetian merchants remained in the spice trade throughout the century of Portuguese power in Asia" but the "trade has shifted dramatically". The middle-merchant carriers had been short-circuited by the ships direct to Lisbon.[62]
In 1542, St. Francis Xavier mentions the architectural splendour of the city. Goa reached the height of its prosperity between 1575 and 1625. Travellers marvelled at Goa Dourada, i.e., Golden Goa. A Portuguese proverb said, "He who has seen Goa need not see Lisbon." The houses of the rich were surrounded by gardens and palm groves; they were built of stone and painted red or white. Instead of glass, their balconied windows had thin polished oyster-shells set in lattice-work. The social life of Goa's rulers befitted the capitol of the viceregal court, the army and navy, and the church; luxury and ostentation became a byword before the end of the 16th century.[63] Nonetheless, according to Portuguese records there was a Cholera epidemic in 1543, "It is said that deaths from the disposal of the disease were so numerous that the disposal of bodies was a formidable task"[64]
In the main street, African and Indian slaves were sold by auction. Almost all manual labour was performed by slaves. The common soldiers assumed high-sounding titles, and even the poor noblemen who congregated in boarding-houses subscribed for a few silken cloaks, a silken umbrella and a common man-servant, so that each could take his turn to promenade the streets, fashionably attired and with a proper escort.[63]
In 1583, Christian missionary activity in the village of Cuncolim led to conflicts, culminating in the Cuncolim Revolt. The first massacre happened when kshatriya villagers killed five Catholic priests (including an Italian nobleman) and fourteen native Christians. The Portuguese authorities then destroyed orchards and attacked the Hindu villagers. Cuncolim village had sixteen chieftains, one for each ward or vado of the village. The sixteen were called to Assolna Fort, ostensibly to discuss a peace pact. At the fort the Portuguese executed the chieftains, except for one who jumped into the Assolna River and presumably swam to Karwar. The Hindus of Cuncolim then refused to pay taxes, and the Portuguese confiscated their land. In 1560 the Goa Inquisition began, ending in 1812. The Hindu villagers who did not want to become Christian then left their villages with their idols before their temples were demolished. Most of these Hindus then settled in the neighbouring areas that were ruled by Bijapur, and again had to pay the jizya tax.[citation needed]
In 1556 a printing press was first installed India at Saint Paul's College in Goa. Through publications made on the printing press, Goa opened a window on the knowledge and customs of Europe.[65][66][67] The Jesuits brought this European-style, metal movable type technology to Macau in China in 1588 and to Japan in 1590.[68] The Jesuits also founded the University of Santo Tomas in the Philippines, the oldest existing European-style university in the Far East.[69] In the same period, Goa Medical College was established as the first European medical college in Asia.[70]
Garcia da Orta (1501-1568) wrote in Goa a treatise in Portuguese on the medicinal plants of India, Colóquios dos simples e drogas da India.[71] It was published in 1563 in Goa on the new printing press, which contained many errors in its type-setting. The author was a physician, an herbalist, a pioneer in pharmacognosy, and originally a Sephardic Jew. As a Cristão Novo (New Christian) he had escaped the Inquisition; but one of his sisters was not as fortunate.[72]
The Crown in Lisbon undertook to finance missionary activity; missionaries and priests converted large numbers of people in all spheres of society, especially in Goa.[73] St Francis Xavier in Goa, pioneered the establishment of a seminary, called Saint Paul's College. It was the first Jesuit headquarters in Asia.[74] St Francis founded the college to train Jesuit missionaries. He went to the Far East, traveling towards China. Missionaries of the Jesuit Order spread out through India, going as far north as the court of the great Mughal Emperor Jallaluddin Akbar. Having heard about the Jesuits, he invited them to come and teach him and his children about Christianity.[75]
From Goa, the Jesuit order was able to set up base almost anywhere in Asia for evangelistic missions, including the founding of Roman Catholic colleges, universities and faculties of education. Jesuits are known for their work in education, intellectual research, and cultural pursuits, and for their missionary efforts. Jesuits also give retreats, minister in hospitals and parishes, and promote social justice and ecumenical dialogue.;[citation needed] Saint Paul's College Goa was a base for their evangelisation of Macau, and then for their important missionary campaigns into China and Japan. Macau eventually superseded St Paul's College, Goa. They built St Paul College in 1594 (now the University of Macau), known in Latin as the college of Mater Dei.[76] Due to his personal enmity with the Jesuits, the Marquês de Pombal expelled the order from Portuguese territories in 1762.[77] The Macau university combined evangelisation with education.[76]
In the year 1600 António de Andrade made the long voyage from Lisbon to Goa, where he pursued his higher studies at St. Paul's College and was ordained a Jesuit priest. He eventually became rector of the same college. He made a landmark missionary expedition from Goa, across the length of India and into Tibet. He overcame incredible hardships in the journey as the first European to cross the Himalaya mountains into Tibet.[78][79] There he founded churches and a mission in 1625.[80] The body of the co-founder of the Society of Jesus, Francis Xavier, whose example many Goan missionaries tried to emulate by engaging in evangelizing work in Asia, was shipped to Goa on 11 December 1553. Goa has also produced its own saints: the martyrs of Cuncolim; St. Joseph Vaz, whose missionary exploits in Sri Lanka are remembered with gratitude in that country; and the Venerable Agnelo de Souza.[81]
The 16th-century monument, the cathedral or Sé, was constructed during Portugal's Golden Age, and is the largest church in Asia, as well as larger than any church in Portugal. The church is 250 ft in length and 181 ft in breadth. The frontispiece stands 115 ft high. The cathedral is dedicated to St. Catherine of Alexandria and is also known as St. Catherine's Cathedral.[82][83] It was on her feast day in 1510 that Afonso de Albuquerque defeated the Muslim army and took possession of the city of Goa.
The Goa Inquisition was the office of the Inquisition acting within the Indian state of Goa and the rest of the Portuguese empire in Asia. It was established in 1560, briefly suppressed from 1774 to 1778, and finally abolished in 1812. Based on the records that survive, H. P. Salomon and I. S. D. Sassoon state that between the Inquisition's beginning in 1561 and its temporary abolition in 1774, some 16,202 persons were brought to trial. Of this number, only 57 were sentenced to death and executed; another 64 were burned in effigy. Most were subjected to lesser punishments or penances.
The Inquisition was established to punish New Christians who continued practicing their ancestral religion in secret. Many Sephardic Jews (as falsely-converted Catholics) had immigrated to Goa from the Iberian peninsula. Due to persecution by the Inquisition, most left and migrated to Fort St. George (later Madras/Chennai) and Cochin, where the English and the Dutch allowed them to be openly Jewish.[84]
In Goa the Inquisition also scrutinised Indian converts from Hinduism or Islam who were thought to have returned to their original ways.[citation needed] It prosecuted non-converts who broke prohibitions against the observance of Hindu or Muslim rites, or interfered with Portuguese attempts to convert non-Christians to Catholicism.[citation needed] Goan Inquisition was abolished in 1812.
When the Portuguese arrived in Goa, they encountered the established regime of the Sultanate of Bijapur under Yusuf Adil Shah (1450-1510). The Adil Shah (written Hidalcão by the Portuguese) controlled Goa (and significant territory of the Sultanate) from his distant, inland capital. Led by Afonso de Albuquerque, in alliance with Timoji, their 1510 attack ended in Portuguese victory. Bijapur lost Goa, but continued as a large, local power.[85][86] In 1565 Bijapur and other Deccan Sultanates in a jihad destroyed the capital of the Hindu Empire Vijayanagara, an ally of the Portuguese. From the spoils Bijapur doubled its size.[87][88] In 1571 Bijapur in an alliance of mostly Muslim sultanates (Ahmadnagar, Bijapur, Calicut, Aceh) launched determined attacks on Goa, which failed. The defeat of this siege of Goa proved decisive.[89][90][91]
The Kanara coastal regions lay immediately south of Goa. Many small principalities, largely autonomous, were under Vijayanagara, then Bijapur. Timoji, who played a role in the 1510 capture of Goa, was from Kanara, e.g., Honavar. Goa traded with various Kanara rulers, which was an important source of rice for domestic consumption; other goods were pepper for export and timber for ships building. The Portuguese had built a fort and ran a factory in Kanara, and were often in effective local control. The Nayak rulers of the Keladi ruling family, however, began to dispute with Goa over the prices paid for trade goods, and other issues. Goa was not able to pay the increases demanded. A series of treaties were nonetheless negotiated. Then hostile Dutch influence increased and Arabs from Muscat began to compete with Goa for the Kanara trade.[92][93]
When Akbar (r. 1555–1605) ruled the Mughal Empire, he endeavored to harmonize the empire's conflicting religions. At Akbar's court, rival Muslim clerics had heated debates. At his new capital Fatehpur Sikri, meetings at his Ibadat Khana [House of Worship] more variously included "Muslim scholars, Hindu pandits, Parsi mobeds, and Jain sadhus". Akbar "invited Jesuits from Goa" but no Buddhists were in proximity. Conferring privately with Jesuits, Akbar discussed Christianity and Abrahamic theology.[94] In 1682 Akbar promulgated a syncretic Din-i-Ilahi [Divine Faith].[95] "The crucial question about Akbar's religious activity is whether he established a new religion or new spiritual order." Either way, his efforts came to nought.[96][97][98]
Goa enjoyed a flourishing trade with Gujarat, when Akbar annexed it in 1573. Agreeable relations were worked out, however, allowing the Portuguese at Diu to continue to issue cartazes and collect duties on the sea trade. In 1602 the English arrived in Asia and pirated a loaded Portuguese merchant ship off Malacca. In 1608 with 25,000 pieces of gold an English captain arranged for rights at Surat, the Mughal Empire's principle trading port. This led to a two-year war between the Mughals and the Portuguese, ending with a feckless treaty in 1615. The Mughals, then dominate in India but weak at sea, began to play the Europeans off against each other. Under Emperor Aurangzeb (r. 1658–1707), the Mughals became frustrated by their war against the Marathas. Goa remained neutral, but once praised Shivaji's valor.[99][100][101][102]
In 1595 there first appeared in Indian waters ships of the Dutch United East India Company (Dutch: Vereenigde Oostindische Compagnie or VOC).[103] Until then, for almost a century, the Portuguese had managed to keep secret their "more detailed information about India," especially their "priceless Portuguese navigation maps". Yet Jan Huygen van Linschoten, who had worked in Goa, in 1592 came away with the coveted knowledge which "taught the Dutch how to use the monsoon winds to their best advantage."[104] Also unfortunately for Portugal, Spain had initiated the Iberian Union, which united the two countries. Additionally, the Dutch and the Spanish were then fighting their Eighty Years' War. In 1600 against Goa the Dutch allied with regional Muslim forces (the Sultanate of Bijapur); then the Dutch made war on Goa. The long-term result of these hostilities was the undoing of Portuguese naval dominion in the Indian Ocean and a loss of its preeminence in sea trade.[105] In 1603 and 1639, the city was blockaded by Dutch fleets, though never captured. The Dutch East India Company were defeated at the Travancore–Dutch War, which stifled their influence in the region.
The Vijayanagara Empire (1336-1646) ruled vast lands in South India when the Portuguese arrived in Goa. The empire's rise as a great power was said to encompass a "mission of upholding the Hindu cause against Islam." Vijayanagara had earlier governed Goa; its ruler Vira Narasimha Raya (r. 1505–1509) contemplated retaking it, but soon died. Krishna Deva Raya (r. 1509–1529) then succeeded as ruler, said to be the empire's best. The Portuguese then were aggressively establishing control of maritime trade routes and coastal ports in Cochin and Goa. The regional political rivalries developed so that Vijayanagara and Goa remained aligned as friendly powers. The Portuguese supplied Vijayanagara with Persian horses.[106] A Portuguese engineer improved irrigation for lands of Krishna Deva Raya.[107] Vijayanagara was ultimately defeated in 1646 by an alliance of Deccan sultanates. So vital was this alliance to Goa, that Goa lost much of its importance after the fall of Vijayanagara.[108][109][110][111]
There began a gradual drop in Goa's prosperity. In 1635 Goa was ravaged by an epidemic.[vague][citation needed] Jean de Thévenot in 1666, Baldaeus in 1672, and Fryer in 1675 described Goa in decline.[citation needed]
The Maratha Empire (1674-1818) to the north grew steadily in strength, far surpassing that of the Mughal Empire, let alone Goa. After his escape from Aurangzeb in Agra, the Maratha ruler Chhatrapati Shivaji (1627-1680) started a counterattack to recoup lands lost to the Mughals through the Treaty of Purandar (1665). Against Goa, Chhatrapati Shivaji mounted an invasion that subdued the region adjoining the Old Conquestas. He captured Pernem, Bicholim, Sattari, Ponda, Sanguem, Quepem and Canacona from the Portuguese. Sawantwadi Bhonsale and Saudekar Rajas became his vassals.
The Maratha Chhatrapati Sambhaji (1657-1689), the son of C. Shivaji, tried in 1683 to conquer all of Goa. Chh. Sambhaji almost ousted the remaining Portuguese, but suddenly a Mughal army appeared which prevented the Maratha from completing their conquest, resulting in the culmination of the Deccan wars. In 1739-1740 the territory of Bardez in north Goa was attacked by the Marathas, in order to pressure the Portuguese at Vasai. The plan of conquest, however, was forestalled with "the payment of a large war indemnity."[112][113]
In June 1756 a Maratha Army invading Goa killed in action Luís Mascarenhas, Count of Alva (Conde de Alva), the Portuguese Viceroy. The Marathas, however faced an invasion from Afghan, resulting in their defeat at the Third Battle of Panipat (1761). The Maratha Peshwa's overall control slackened throughout India.[114] The Portuguese then defeated the regional Rajas of Sawantwadi and the Raja of Sunda to reconquer an area from Pernem to Canacona. This territory formed the Novas Conquistas, within the boundaries of present-day Goa. Following the Battle of Panipat, the Mughals retained their friendly relationship with the Portuguese.[citation needed]
The long Dutch war described above led Portugal to seek an alliance with the English, which proved costly. The Dutch war did finally end in 1663.[115] In 1665 the English demanded in payment the cession of Bombay. Officially it was part of the dowry of Catherine of Braganza on her ill-starred marriage to Charles II. Though at first active rivals in India after the English East India Company arrived in 1601,[116][117] the two latter attempted to coordinate against common enemies. The Maratha-derived "pirate" fleet led by the independent Kanhoji Angre inspired such an uneasy alliance. The 1721 Anglo-Portuguese naval attack on Culaba, the Angria stronghold, was repulsed. It was a fiasco that then embittered the partnership.[118]
In 1757, King Joseph I of Portugal issued a decree, developed by his minister Marquês de Pombal, granting Portuguese citizenship to all subjects in the Portuguese Indies, with the right to be represented in the Portuguese Parliament. Pombal (1699-1782), an anti-Catholic Freemason, served the King as the de facto leader of Portugal, 1750–1777.[119] The enclaves of Goa, Damão, Diu, Dadra and Nagar Haveli became collectively known as the Estado da Índia Portuguesa. The first election was held in Goa on 14 January 1822. Three local citizens were elected as members of the Portuguese parliament.[120] From their first arrival, the Portuguese intermarried among the converted natives of Goa. They produced Luso-Indian offspring, who were also Catholic.[121][122]
In 1787, some disgruntled priests attempted a rebellion against Portuguese rule. It was known as the Conspiracy of the Pintos.
Goa was peacefully occupied by the British between 1812 and 1815 in line with the Anglo-Portuguese Alliance during the Napoleonic Wars.
The viceroy transferred his residence from the vicinity of Goa city to New Goa (in Portuguese Nova Goa), today's Panaji. In 1843 this was made the official seat of government; it completed a move that had been discussed as early as 1684. Old Goa city's population fell steeply during the 18th century as Europeans moved to the new city. Old Goa has been designated a World Heritage Site by UNESCO because of its history and architecture.[123]
The Goa civil code was introduced in 1869 after Portuguese Goa and Damaon were elevated from being mere Portuguese colonies to the status of a Província Ultramarina (Overseas possession).
Goa was neutral during the conflict like Portugal.  As a result, at the outbreak of hostilities a number of Axis ships sought refuge in Goa rather than be sunk or captured by the British Royal Navy. Three German merchants ships, the Ehrenfels, the Drachenfels and the Braunfels, as well as an Italian ship, took refuge in the port of Mormugao. The Ehrenfels began transmitting Allied ship movements to the U-boats operating in the Indian Ocean, an action that was extremely damaging to Allied shipping.
But the British Royal Navy was unable to take any official action against these ships because of Goa's stated neutrality. Instead the Indian mission of SOE backed a covert raid using members from the Calcutta Light Horse, a part-time unit made up of civilians who were not eligible for normal war service.  The Light Horse embarked on an ancient Calcutta riverboat, the Phoebe, and sailed round India to Goa, where they sunk the Ehrenfels. The British then sent a decrypted radio message announcing it was going to seize the territory. This bluff made the other Axis crews scuttle their ships fearing they could be seized by British forces.
The raid was covered in the book Boarding Party by James Leasor.  Due to the potential political ramifications of the fact that Britain had violated Portuguese neutrality, the raid remained secret until the book was published in 1978.[124] In 1980 the story was made into the film, The Sea Wolves, starring Gregory Peck, David Niven and Roger Moore.
When India became independent in 1947, Goa remained under Portuguese control. The Indian government of Jawaharlal Nehru demanded that Goa, along with a few other minor Portuguese holdings, be turned over to India. However, Portugal refused due to Goa being an integral part of Portugal since 1510. By contrast, France, which also had small enclaves in India (most notably Puducherry), surrendered all its Indian possessions relatively quickly.[125][126]
In 1954, a horde of armed Indians flooded into and took over the tiny land-locked enclaves of Dadra and Nagar Haveli. This incident led the Portuguese to lodge a complaint against India in the International Court of Justice at The Hague. The final judgement on this case, given in 1960, held that the Portuguese had a right to the enclaves, but that India equally had a right to deny Portugal access to the enclaves over Indian territory.[citation needed]
In 1955 a group of unarmed civilians, the Satyagrahis, demonstrated against Portugal. At least twenty-two of them were killed by Portuguese gunfire.[citation needed]
Later the same year, these non-Goan Satyagrahis took over a fort at Tiracol and hoisted the Indian flag. They were driven out of Goa by the Portuguese with a number of casualties. On 1 September 1955, the Indian consulate in Goa was closed using this incident as an excuse; Nehru declared that his government would not tolerate the Portuguese presence in Goa. India then instituted a blockade against Goa, Damão, and Diu in an effort to force a Portuguese departure. Goa was then given its own airline by the Portuguese, the Transportes Aéreos da Índia Portuguesa, to overcome the blockade.[citation needed]
On 27 February 1950, the Government of India asked the Portuguese government to open negotiations about the future of Portuguese colonies in India.[127] Portugal asserted that its territory on the Indian subcontinent was not a colony but part of metropolitan Portugal and hence its transfer was non-negotiable, and that India had no rights to this territory because the Republic of India did not exist at the time when Goa came under Portuguese rule.[128] On 18 December 1961, Indian troops crossed the border into Goa and annexed it. Operation Vijay involved sustained land, sea and air strikes for more than thirty-six hours; it resulted in the unconditional surrender of Portuguese forces on 19 December 1961 by Manuel António Vassalo e Silva.[129][130][131]
A United Nations resolution condemning the invasion was proposed by the United States and the United Kingdom in the United Nations Security Council, but it was vetoed by the USSR.[132] Goa celebrates Liberation Day on 19 December every year, which is also a state holiday.[133]
The territory of Goa, Daman and Diu was a union territory of India from 19 December 1961 to 30 May 1987. Its official language was declared to be Marathi, much to the anger of the majority of the native Goans.[according to whom?]
After a brief period of military rule, on 8 June 1962, military rule was replaced by civilian government when the Lieutenant Governor Kunhiraman Palat Candeth nominated an informal Consultative Council of 29 nominated members to assist him in the administration of the territory. Dayanand Bandodkar of the Maharashtrawadi Gomantak Party was elected as the first Chief Minister of Goa, Daman and Diu. He attempted to merge Goa with Maharashtra by importing Marathi immigrants from the neighbouring state[citation needed] (Goa's population increased by almost 35% in the 1960s due to heavy immigration of Marathi people), but his plans were foiled by the Goa Opinion Poll.[according to whom?]
In February 1987, the Indian government finally recognized Konkani as the official language of Goa. Goa was later admitted to Indian statehood in May 1987. Pratapsingh Rane, who had previously served as Chief Minister of Goa, Daman and Diu, was elected as the first Chief Minister of the newly formed state.
Goa has a high GDP per capita and Human Development Index compared to most Indian states.[134]
15th century
16th century
15th century
16th century
17th century
18th century
19th century
16th century
17th century
15th century
16th century
Portuguese India
17th century
Portuguese India
18th century
Portuguese India
16th century
17th century
19th century
Portuguese Macau
20th century
Portuguese Macau
15th century [Atlantic islands]
16th century [Canada]
16th century
17th century
18th century
19th century

Deep history is a term for the distant past of the human species.[1]  As an intellectual discipline, deep history encourages scholars in anthropology, archaeology, primatology, genetics and linguistics to work together to write a common narrative about the beginnings of humans,[1]  and to redress what they see as an imbalance among historians, who mostly concentrate on more recent periods.[2] Deep history forms part of Big History, and looks at the portion of deep time when humans existed, going further back than prehistory, mainly based on archaeology, usually ventures, and using a wider range of approaches.
Proponents of deep history argue for a definition of history that rests not upon the invention of writing, but upon the evolution of anatomically modern humans. According to Daniel Lord Smail, perhaps the most prominent advocate of Deep History, the concept of prehistory is recast as an arbitrary boundary that limits the longue durée perspective of historians, and which rests upon assumptions that history follows a teleological path beginning with the origins of civilization in Ancient Mesopotamia.[3] For example, Smail suggests that advances in disciplines such as neurobiology, neurophysiology and genetics mean that there are more possibilities for understanding the distant past, and offer opportunities to explain how events such as biological evolution, global environmental change, and patterns of the spread of disease have affected humanity today.[4]  Proponents of Deep History generally do not acknowledge what they claim to be the traditional barrier between conventional history, generally based on written documentation such as ancient scrolls or hieroglyphs on pyramids, and unwritten prehistory, based on archaeology, in the human past.[5]
A review of Smail's book by Steven Mithen, professor of Archaeology at the University of Reading, is sympathetic to some parts of his thesis, but says  "Smail may not be as closely acquainted with the ongoing debates in prehistoric archaeology as he might be", and on Smail's critical description of historians: "I have to take Smail’s word for it that such historians still exist, as after more than a century of prehistoric archaeology they would be an astonishing throwback to another age".[6]

The Origin of the Family, Private Property and the State: in the Light of the Researches of Lewis H. Morgan (German: Der Ursprung der Familie, des Privateigenthums und des Staats) is an 1884 anthropological treatise by Friedrich Engels. It is partially based on notes by Karl Marx to Lewis H. Morgan's book Ancient Society (1877). The book is an early historical materialist work and is regarded as one of the first major works on family economics.
The Origin of the Family, Private Property and the State begins with an extensive discussion of Morgan's Ancient Society, which aims to describe the major stages of human development, and agrees with the work that the first domestic institution in human history was the matrilineal clan. Morgan was a pioneering American anthropologist and business lawyer who championed the land rights of Native Americans. Traditionally, the Iroquois had lived in communal longhouses based on matrilineal descent and matrilocal residence, giving women a great deal of power. Engels stressed the theoretical significance of Morgan's highlighting of the matrilineal clan:
The rediscovery of the original mother-right gens as the stage preliminary to the father-right gens of the civilized peoples has the same significance for the history of primitive society as Darwin’s theory of evolution has for biology, and Marx’s theory of surplus value for political economy.
Primitive communism, according to both Morgan and Engels, was based in the matrilineal clan where females lived with their classificatory sisters – applying the principle that "my sister’s child is my child". Because they lived and worked together, females in these communal households felt strong bonds of solidarity with one another, enabling them when necessary to take action against uncooperative men. Engels cites this passage from a letter to Morgan written by a missionary who had lived for many years among the Seneca Iroquois,
As to their family system, when occupying the old long-houses, it is probable that some one clan predominated, the women taking in husbands, however, from the other clans; and sometimes, for a novelty, some of their sons bringing in their young wives until they felt brave enough to leave their mothers. Usually, the female portion ruled the house, and were doubtless clannish enough about it. The stores were held in common; but woe to the luckless husband or lover who was too shiftless to do his share of the providing. No matter how many children, or whatever goods he might have in the house, he might at any time be ordered to pack up his blanket and budge; and after such orders it would not be healthful for him to attempt to disobey. The house would be too hot for him; and, unless saved by the intercession of some aunt or grandmother, he must retreat to his own clan; or, as was often done, go and start a new matrimonial alliance in some other. The women were the great power among the clans, as everywhere else. They did not hesitate, when occasion required, to "knock off the horns", as it was technically called, from the head of a chief, and send him back to the ranks of the warriors. The original nomination of the chiefs also always rested with them.
According to Morgan, the rise of alienable property disempowered women by triggering a switch to patrilocal residence and patrilineal descent:
It thus reversed the position of the wife and mother in the household; she was of a different gens from her children, as well as her husband; and under monogamy was now isolated from her gentile kindred, living in the separate and exclusive house of her husband. Her new condition tended to subvert and destroy that power and influence which descent in the female line and the joint-tenement houses had created.
Engels added political impact to Morgan's studies of women in prehistory, describing the "overthrow of mother right" as "the world-historic defeat of the female sex"; he attributed this defeat to the onset of farming and pastoralism. In reaction, most twentieth-century social anthropologists considered the theory of matrilineal priority untenable,[1][2] though feminist scholars of the 1970s-1980s (particularly socialist and radical feminists) attempted to revive it with limited success.[3] In recent years, some proponents have attempted to rehabilitate this view using the rare indications of matrilocal marriage in ancient remains as evidence. However, the vast majority of ancient remains show patrilocality and female out-marriage. This is true even of Neanderthal populations.[4]
Engels emphasizes the importance of social relations of power and control over material resources rather than supposed psychological deficiencies of "primitive" people. In the eyes of both Morgan and Engels, terms such as "savagery" and "barbarism" were respectful and honorific, not negative. Engels summarises Morgan's three main stages as follows:
In the following chapter on family, Engels seeks to connect the transition into these stages with a change in the way that family is defined and the rules by which it is governed. Much of this is still taken from Morgan, although Engels begins to intersperse his own ideas on the role of family into the text. Morgan acknowledges four stages in the family.
The consanguine family is the first stage of the family and as such a primary indicator of our superior nature in comparison with animals. In this state marriage groups are separated according to generations. The husband and wife relationship is immediately and communally assumed between the male and female members of one generation. The only taboo is a sexual relationship between two generations (i.e. father and daughter, grandmother and grandson).
The punaluan family, the second stage, extends the incest taboo to include sexual intercourse between siblings, including all cousins of the same generation. This prevents most incestuous relationships. The separation of the patriarchal and matriarchal lines divided a family into gentes. Interbreeding was forbidden within gens (anthropology), although first cousins from separate gentes could still breed.
In the pairing family, the first indications of pairing are found in families where the husband has one primary wife. Inbreeding is practically eradicated by the prevention of a marriage between two family members who were even just remotely related, while relationships also start to approach monogamy. Property and economics begin to play a larger part in the family, as a pairing family had responsibility for the ownership of specific goods and property. Polygamy is still common amongst men, but no longer amongst women since their fidelity would ensure the child's legitimacy. Women have a superior role in the family as keepers of the household and guardians of legitimacy. The pairing family is the form characteristic of the lower stages of barbarism. However, at this point, when the man died his inheritance was still given to his gens, rather than to his offspring. Engels refers to this economic advantage for men coupled with the woman's lack of rights to lay claim to possessions for herself or her children (who became hers after a separation) as the overthrow of mother-right which was "the world historical defeat of the female sex". For Engels, ownership of property created the first significant division between men and women in which the woman was inferior.
On the monogamous family, Engels writes:
It develops from the pairing family, as we have already shown, during the time of transition from the middle to the higher stage of barbarism. Its final victory is one of the signs of beginning civilization. It is founded on male supremacy for the pronounced purpose of breeding children of indisputable paternal lineage. The latter is required, because these children shall later on inherit the fortune of their father. The monogamous family is distinguished from the pairing family by the far greater durability of wedlock, which can no longer be dissolved at the pleasure of either party. As a rule, it is only the man who can still dissolve it and cast off his wife.
Engels' ideas on the role of property in the creation of the modern family and as such modern civilization begin to become more transparent in the latter part of Chapter 2 as he begins to elaborate on the question of the monogamous relationship and the freedom to enter into (or refuse) such a relationship. Bourgeois law dictates the rules for relationships and inheritances. As such, two partners, even when their marriage is not arranged, will always have the preservation of inheritance in mind and as such will never be entirely free to choose their partner. Engels argues that a relationship based on property rights and forced monogamy will only lead to the proliferation of sexual immorality and prostitution.
The only class, according to Engels, which is free from these restraints of property, and as a result from the danger of moral decay, is the proletariat, as they lack the monetary means that are the basis of (as well as threat to) the bourgeois marriage. Monogamy is therefore guaranteed by the fact that theirs is a voluntary sex-love relationship.
The social revolution which Engels believed was about to happen would eliminate class differences, and therefore also the need for prostitution and the enslavement of women. If men needed only to be concerned with sex-love and no longer with property and inheritance, then monogamy would come naturally.
Following the death of his friend and co-thinker Karl Marx in 1883, Engels served as his literary executor, organizing his various writings and preparing them for publication. While time-consuming, this activity did not fully occupy Engels's available hours, and he continued to read and write on topics of his own.[citation needed]
While Engels' 1883 manuscript Dialectics of Nature was left uncompleted and unpublished, he successfully published Der Ursprung der Familie, des Privateigenthums und des Staats: Im Anschluss an Lewis H. Morgan's Forschungen (The Origin of the Family, Private Property, and the State: in the Light of the Researches of Lewis H. Morgan) in Zürich in the spring of 1884.
The writing of The Origin of the Family began in early April 1884, and was completed on 26 May.[5] Engels began work on the treatise after reading Marx's handwritten synopsis of Lewis H. Morgan's Ancient Society; or, Researches in the Lines of Human Progress from Savagery, Through Barbarism to Civilization, first published in London in 1877.[6] Engels believed that Marx had intended to create a critical book-length treatment of the ideas suggested by Morgan, and aimed to produce such a manuscript to fulfill his late comrade's wishes.[6]
Engels acknowledged these motives, noting in the preface to the first edition that "Marx had reserved to himself the privilege of displaying the results of Morgan's investigations in connection with his own materialist conception of history", as the latter had "in a manner discovered anew" in America the theory originated by Marx decades before.[7]
Engels's first inclination was to seek publication in Germany in spite of the passage of the first of the Anti-Socialist Laws by the government of Chancellor Otto von Bismarck. On April 26, 1884, Engels wrote a letter to his close political associate Karl Kautsky, saying he sought to "play a trick on Bismarck" by writing something "that he would be positively unable to ban".[8] He felt this goal unrealizable owing to Morgan's discussions of the nature of monogamy and the relationship between private ownership of property and class struggle, however, these making it "absolutely impossible to couch in such a way as to comply with the Anti-Socialist Law".[9]
Engels viewed Morgan's findings as providing a "factual basis we have hitherto lacked" for a prehistory of contemporary class struggle.[9] He believed that it would be an important supplement to the theory of historical materialism for Morgan's ideas to be "thoroughly worked on, properly weighed up, and presented as a coherent whole".[9] This was to be the political intent behind his Origin of the Family project.
Work on the book was completed—with the exception of revisions to the final chapter—on May 22, 1884, when the manuscript was dispatched to Eduard Bernstein in Zürich.[10] The final decision of whether to print the book in Stuttgart "under a false style", hiding Engels's forbidden name, or immediately without alteration in a Swiss edition, was deferred by Engels to Bernstein.[10] The latter course of action was chosen, with the book finding print early in October.[6]
The first edition of Der Ursprung der Familie appeared in Zürich in October 1884, with the possibility of German publication forestalled by Bismarck's Anti-Socialist Law.[6] Two subsequent German editions, each following the first Zürich edition exactly, were published in Stuttgart in 1886 and 1889.[6]
The book was translated into a number of European languages and published during the decade of the 1880s, including Polish, Romanian, Italian, Danish, and Serbian.[6]
Changes to the text were made by Engels for a fourth German language edition, published in 1891, with an effort made to incorporate contemporary findings in the fields of anthropology and ethnography into the work.[6]
The first English language edition did not appear until 1902,[6] when Charles H. Kerr commissioned Ernest Untermann to produce a translation for the "Standard Socialist Series" of popularly priced pocket editions produced by his Charles H. Kerr & Co. of Chicago. The work was extensively reprinted throughout the 20th and into the 21st Centuries and is regarded as one of Engels' seminal works.[6]

The prehistory of Burma (Myanmar) spanned hundreds of millennia to about 200 BCE. Archaeological evidence shows that the Homo erectus had lived in the region now known as Burma as early as 750,000 years ago, and the Homo sapiens about 11,000 BCE, in a Stone Age culture called the Anyathian. Named after the central dry zone sites where most of the early settlement finds are located, the Anyathian period was when plants and animals were first domesticated and polished stone tools appeared in Burma. Though these sites are situated in fertile areas, evidence shows these early people were not yet familiar with agricultural methods.[1]
The Bronze Age arrived c. 1500 BCE when people in the region were turning copper into bronze, growing rice, and domesticating chickens and pigs. The Iron Age arrived around 500 BCE when iron-working settlements emerged in an area south of present-day Mandalay.[2] Evidence also shows rice growing settlements of large villages and small cities that traded with their surroundings and as far as China between 500 BCE and 200 CE.[3] Bronze-decorated coffins and burial sites filled with the earthenware remains of feasting and drinking provide a glimpse of the lifestyle of their affluent society.[2]
Evidence of trade suggests ongoing migrations throughout the prehistory period though the earliest evidence of mass migrations only points to c. 200 BCE when the Pyu people, the earliest inhabitants of Burma of whom records are extant,[4] began to move into the upper Irrawaddy valley from present-day Yunnan.[5] The Pyu went on to found settlements throughout the plains region centred on the confluence of the Irrawaddy and Chindwin rivers that had been inhabited since the Paleolithic.[6] The Pyu were followed by various groups such as the Mon, the Arakanese and the Mranma (Burmans) in the first millennium CE. By the Pagan period, inscriptions show Thets, Kadus, Sgaws, Kanyans, Palaungs, Was and Shans also inhabited the Irrawaddy valley and its peripheral regions.[7]
Some of the earliest anthropoid primate fossils in the world, dating to about 40 million years ago,[8] were found in the Pondaung Formations in Pale Township, central Myanmar. These fossils include forms from the Eosimiidae and Amphipithecidae families and challenge beliefs that these early anthropoids originated from Africa.[9]
Homo erectus began to settle in Burma in 750,000 BCE before the arrival of Homo sapiens from Africa. [disputed (for: cited source does not state the information cited)  – discuss] Archaeological evidence of Homo sapiens has been dated to about 25,000 BP in central Myanmar.[10] The pre-migration period of Burma spanned from 11,000 BCE to 4,000 BCE before the mass migration. This era is characterised by Stone Age culture which later advanced to Bronze and Iron Age cultures. The cave ritual system, which was later used for Buddhist caves, is believed to have been rooted in the earliest civilisation of this era. The effect can be seen today in many Buddhism ritual caves across Burma.[1]
Roughly polished stone implements of various sizes are often found in the Shan States of eastern Burma.[1][12] Pebble tools, including choppers and chopping tools, are found in the Pleistocene terrace deposits of the Irrawaddy Valley of Upper Myanmar. These complexes are collectively known as the Anyathians, thus, the culture is called the Anyathian culture. The Early Anyathian is characterised by single-edged core implements made on natural fragments of fossil wood and silicified tuff, which are associated with crude flake implements. However, domestications and polishing of stones, which are possible signs of Neolithic culture, are not known until the discovery of Padah Lin caves in Southern Shan State.[13]
Three caves located near Taunggyi at the edge of the Shan Plateau, depict the Neolithic age when farming, domestication, and polished stone tools first appeared.[1] They are dated between 11,000 and 6,000 BCE. The most significant of these is the Padah-Lin cave where over 1,600 of stones and cave paintings have been uncovered. These paintings lie from ten to twelve feet above the floor level depicting figures in red ochre of two human hands, a fish, bulls, bisons, a deer and probably the hind of an elephant.[14] The paintings indicate that the cave was probably used for religious rituals. If so, these caves could be one of the earliest sites used for worshiping in Burma. The use of caves for religious purposes continued into later periods. Thus, Buddhist Burmese use of cave worshiping originates from the earlier Animist period.[1]
The finding of bronze axes at Nyaunggan located in Shwebo township suggests that Bronze Age of Burma began around 1500 BC in parallel with the earlier stages of Southeast Asian bronze production.[15] This period spans from 1500 to 1000 BC during which knowledge of the smelting and casting of copper and tin seems to have spread rapidly along the Neolithic exchange routes.[16]
Another site is the area of Taungthaman, near Irrawaddy River within the walls of the 18th century capital, Amarapura, occupied from the late Neolithic through the early Iron Age, around the middle of the first millennium BCE.[1] Small trades and barters, as well as Animism had already begun in this age. The Taungthaman site was discovered in 1971 and bulldozed by the State Administration Council in 2023.[17]
Bronze and Iron Age cultures overlapped in Burma. This era saw the growth of agriculture and access to copper resources of the Shan hills, the semi-precious stone and iron resources of the Mount Popa Plateau, and the salt resources of Halin. The wealth is evident in grave items bought from Chinese kingdoms.[3] A notable characteristics of the people of this era is that they buried their dead together with decorative ceramics and common household objects such as bowls and spoons.[1]
The prehistory period came to a close c. 200 BCE when the Pyu people, the earliest inhabitants of Burma of whom records are extant, began to move into the upper Irrawaddy valley from north of present-day Yunnan.[4][5] This era marks the beginning of urbanisation when city states began to be established. Several sizeable first millennium cities were founded by the Pyu, the Mon and the Arakanese.
Pyu city states' (Burmese: ပျူ မြို့ပြ နိုင်ငံများ) were a group of city-states that existed from c. 2nd century BCE to mid-11th century CE in present-day Upper Burma (Myanmar). The city-states were founded as part of the southward migration by the Tibeto-Burman-speaking Pyu. The thousand-year period, often referred to as the Pyu millennium, linked the Bronze Age to the beginning of the classical states period when the Pagan Dynasty emerged in the late 9th century.
The city-states—five major walled cities and several smaller towns have been excavated—were all located in the three main irrigated regions of Upper Burma: the Mu valley, the Kyaukse plains and Minbu region, around the confluence of the Irrawaddy and Chindwin rivers. Part of an overland trade route between China and India, the Pyu realm gradually expanded south. Halin, founded in the 1st century CE at the northern edge of Upper Burma, was the largest and most important city until around the 7th or 8th century when it was superseded by Sri Ksetra (near modern Pyay) at the southern edge. Twice as large as Halin, Sri Ksetra was the largest and most influential Pyu centre.[4]
The Pyu culture was heavily influenced by trade with India, importing Buddhism as well as other cultural, architectural and political concepts, which would have an enduring influence on later Burmese culture and political organisation.[18] The Pyu calendar, based on the Buddhist calendar, later became the Burmese calendar. Latest scholarship, though yet not settled, suggests that the Pyu script, based on the Indian Brahmi script, may have been the source of the Burmese script.
The millennium-old civilisation came crashing down in the 9th century when the city-states were destroyed by repeated invasions from the Kingdom of Nanzhao. The Mranma (Burmans), who came down with the Nanzhao, set up a garrison town at Pagan (Bagan) at the confluence of Irrawaddy and Chindwin. Pyu settlements remained in Upper Burma for the next three centuries but the Pyu gradually were absorbed into the expanding Pagan Empire. The Pyu language still existed until the late 12th century. By the 13th century, the Pyu had assumed the Burman ethnicity. The histories/legends of the Pyu were also incorporated to those of the Burmans.[18]
The Mon people of Haribhunjaya and Dvaravati kingdoms in modern Thailand may have entered present-day Lower Burma as early as the 6th century CE. According to mainstream scholarship, the Mon had founded at least two small kingdoms (or large city-states) centred on Pegu (Bago) and Thaton by the mid 9th century. The earliest external reference to a Lower Burma "kingdom" was in 844-848 by Arab geographers.[19] The Mon practised Theravada Buddhism. The kingdoms were prosperous from trade. The Kingdom of Thaton is widely considered to be the fabled kingdom of Suvarnabhumi (or Golden Land), referred to by the tradesmen of Indian Ocean.
The Burmans who had come down with the early 9th Nanzhao raids of the Pyu states remained in Upper Burma. (Trickles of Burman migrations may have begun as early as the 7th century.[20]) Like that of the Pyu, the original home of Burmans prior to Yunnan is believed to be present-day Qinghai and Gansu provinces.[5][21] After the Nanzhao attacks had greatly weakened the Pyu city-states, large numbers of Burman warriors and their families entered the Pyu realm in the 830s and 840s and settled at the confluence of the Irrawaddy and Chindwin rivers, perhaps to help Nanzhao pacify the surrounding countryside.[22] Over the next two hundred years, the small principality gradually grew to include its immediate surrounding areas— to about 200 miles north to south and 80 miles from east to west by Anawrahta's accession in 1044. Historically verifiable Burmese history begins with Anawrahta's accession.[23]

Jeffrey Goodman is an independent American archaeologist with training in geology and archaeology. His early career was in oil exploration.
Goodman is most notable for his controversial ideas about modern humans originating in California 500,000 years ago, which are widely dismissed as unsupported by evidence.[1][2][3][4]
Goodman holds a geological engineering degree from the Colorado School of Mines, a M.A. in anthropology from the University of Arizona, and a Ph.D. in anthropology from California Coast University. He also earned a M.B.A. from Columbia University Graduate School of Business. He was accredited by the former Society of Professional Archeologists from 1978 to 1987.[5][6]
He used to be known as a proponent of psychic archaeology. Goodman is most well known for his idea that modern man was found in California 500,000 years ago. In his book American Genesis Goodman maintains that the conventional scenario is backwards, and that modern human beings originated not in Africa, but in California, where he cites the proverbial Garden of Eden, half a million years ago. He also attributes to these early humans many discoveries considered to be much later, from pottery to insulin to "the applied understanding of the physics behind Einstein's gravity waves".[7]
Later Goodman called for a Multiregional origin of modern humans.[8]  Goodman’s next book was The Genesis Mystery: the Sudden Appearance of Man and according to Paul Dean of the Los Angeles Times it is “something of an academic brush with scientific creationism, the belief that a divine surge, without explicit adherence to the Bible, created modern man… 250,000 years ago.”.[9] His more recent work has been in biblical archaeology.
Goodman began excavation at Flagstaff, Arizona in the 1970s while still a student in the archaeological graduate program at the University of Arizona. Through the help of the psychic Aron Abrahamsen, he predicted that the excavation of a 10 foot wide test pit there would find stone tools from 4 to 20 feet, a minimum date of 20,000 years at the 15 foot level, a geological disconformity at the 15 foot level, a date of 100,000 years at 20 feet, and some human and animal skeletal material at the 20 foot plus level. As predicted, except for the human skeletal material, all of these things were found.[10]
In the 1979 dig season Dr. Alan Bryan of the University of Alberta and his team excavated at the site, and they found an engraved stone at 23 feet that is called the “Flagstaff Stone.” The “Flagstaff Stone” is thought by Goodman  to be approximately one hundred thousand years old and according to Goodman, possibly “one of the most important artifacts ever found in the whole world”, citing in the last instance Alexander Marshack of Harvard’s Peabody Museum. 3 [Goodman 1981:214.] The archaeologist Stephen Williams wrote that "Marshack has said that he was badly misquoted by Goodman, and the date is arrived at by extreme extrapolation."[11] In a review of American Genesis by Dennis Stanford, Stanford quotes Marshack as having said that although he saw the grooves as intentional, “Every groove without exception had been deepened and straightened, reworked after it was dug out of the ground . . . thus the stone cannot be used as evidence that early man engraved it.”[12]
Some have claimed the stone is a piece of tuff about two and a half inches long which according to Goodman, is 100,000 years old and is Paleo-Indian art.[13]
Goodman is the author of American Genesis: The American Indian and the origins of modern man  (1981) in which he claims that homo sapiens originated in North America in California 500,000 years ago and then spread to the rest of the world. His views have been compared to the paleontologist Florentino Ameghino who believed that humanity had originated on the banks of Río de la Plata.[14]
Goodman claims that the fossils in North America are "twice as old as the oldest fully modern skull from europe". In his work he attempted to show that the American Indians were not only the first Americans but were the first humans on earth. Goodman allows that hominids did appear in Asia and Africa, though homo sapiens are completely American in origin according to his theory. Goodman's theory has been criticised as his explanation of the relationship between hominids and humans was vaguely discussed. However Goodman in a later book addressed the issue in The Genesis Mystery: the Sudden Appearance of Man (1983) in which he criticised natural selection and advocated a type of spiritual evolution by claiming that archaeological findings verify an unbridgeable gap between modern man and the last "pre-man" creature and advocates spiritual intervention as the explanation for the "sudden appearance" of modern man.[15] According to Michael Anthony Corey: Goodman "appeals to an intervening supernatural force, which would have manifested itself entirely through a "natural" series of evolutionary processes".[16]
Goodman's theories are popular amongst American Indian creationists who believe that the American Indians originated in America and had not migrated there from Asia.[17] Few scientists today give Goodman's ideas and his use of evidence much credence.[18]
Goodman has his own publishing company which in 2010 published the book The Comets of God-New Scientific Evidence for God which suggests that the Bible tells how God used comets to create disasters such as Noah's Flood.  Julia Ann Charpentier of ForeWord Reviews writes: “Past events in the bible often have a verifiable historical or archeological basis. Though Christian fundamentalists may recoil from scientific exploration of what they believe to be unfathomable, sacred words, some experts have presented convincing theories for reinterpretation of biblical occurrences and predictions. Jeffrey Goodman, along with other scientists who preceded him, proposes that comets made an appearance in the Old Testament, one of which caused the Great Flood.” [5] *[1].
Most archaeologists agree that the practice of psychic archaeology can be categorized in the realm of pseudoscience and thus Goodman has received much criticism.
In his book Fantastic Archaeology: The Wild Side of the North American Prehistory, Stephen Williams says “he [Goodman] commits an intellectual crime on the very people he seeks spiritually to uplift with his discoveries, the Native Americans.” Here Williams is referring to Goodman’s excavations of Flagstaff, Arizona and his attempted re-writing of American Indian prehistory[13]
In his review of Goodman’s book The Genesis Mystery, John R. Cole points out many ways in which Goodman succeeds at creating a book that is convincingly scientific. Cole states that “Goodman has succeeded where many before him have failed: he has produced something that mimics a scientific book very well.” He goes on further mentioning figures such as: Barry Fell, Erich von Däniken and Immanuel Velikovsky, stating that Goodman has created a work that “seems to have sound, up-to-date, wide-ranging references to support his claims” unlike the former three. Cole states that Goodman's references are "selective, misused, or misunderstood".[19]

The Hasanlu Lovers are a pair of human remains found at the Teppe Hasanlu archaeological site, located in the Naqadeh in the West Azerbaijan Province of Iran. Around 800 BCE, the city of Hasanlu, located in north-western Iran, was destroyed by an unknown invader. Inhabitants were slain and left where they fell. In 1973, the lovers were discovered by a team of archaeologists from the University of Pennsylvania led by Robert H. Dyson.[1][2]
The two human skeletons were found together in a bin during excavations, seemingly embracing at the time of death,[3] with no other objects except a stone slab under the head of one skeleton. They died together around 800 BCE, during the last destruction of the Hasanlu.[3] Approximately 246 skeletons were found at the site altogether.[4] How the lovers died and ended up in the bin is still under speculation but both skeletons lack evidence of injury near the time of death and possibly died of asphyxiation.[5] They were exhibited at the Penn Museum from 1974 until the mid-1980s.[6]
The right skeleton, referred to as HAS 73-5-799 (SK 335), is lying on its back and the left skeleton, referred to as HAS 73-5-800 (SK 336), is lying on its left side facing SK 335.[3] When excavated, the skeletons were tested to determine various characteristics. Dental evidence suggest SK 335 was a young adult, possibly 19–22 years of age. Researchers identified the skeleton as male largely based on the pelvis. The skeleton had no apparent evidence of disease or healed lifetime injuries.[1][2][3] Skeleton SK 336 appeared to have been healthy in life; the skeleton had no apparent evidence of healed lifetimes injuries, and was estimated to have been aged to about 30–35 years.[1][2][3] Sex determination of the left skeleton was less definitive.  Recent evidence has confirmed SK 336 is male[7] after being originally identified as female.[4] The skeletons have been a subject of debate since they were first excavated.[3][5]
Hasanlu is an ancient Near Eastern site located in the Qadar River valley, on the southern shore of Lake Urmia in northwest Iran.[8][9] The city of Hasanlu was occupied consistently from the sixth millennium BCE to around 800 BCE, when the site was invaded and destroyed by fire.[9] In 1934–1936 Hasanlu was commercially dug by Sir Aurel Stein, a British archaeologist.[10] Then, in 1956, the Hasanlu Project was launched by the sponsorship of the University Museum of Pennsylvania, the Metropolitan Museum of Art, and the Archaeological Service of Iran.[9] Following the launch of the Hasanlu Project, a team of archaeologists from Penn Museum led by Director Robert H. Dyson excavated the site from 1957 to 1974.[10] This team completed nine excavation campaigns, and the excavation of the site ended more than 40 years ago (as of in 2022).
Excavation of the site revealed burnt remains of huge mudbrick walls, thick layers of ash, skeletons, vessels, and more. Excavation exposed extensive destruction – evidence of the city’s invasion and arson. Archaeologists explain that the nature of the destruction resulted in the city being frozen in time, preserving buildings, artefacts, and skeletal remains.[8] Approximately 246 skeletons,[4] of a variety of ages and genders were found.  The bodies were left where they were killed in the streets and in buildings. Some victims were found in groups, with head lacerations, and dismembered limbs which suggested mass executions had taken place.[8] Among the 246 skeletons found, two of them were the remains of the Hasanlu Lovers.
Who attacked Hasanlu is still unknown; the general academic consensus is that the Iranian Empire were the invaders, but the Assyrian Empire was also very prominent in the region. There is no indication from the skeletons themselves or from the artefacts exactly who the invaders were. The city of Hasanlu itself is considered protohistoric: there was no writing around the site, unlike sites in neighboring regions.[7] Because of this, archaeologists do not know how the people of Hasanlu would have identified themselves, who they were, or the language they would have spoken.
The skeletal remains of the Hasanlu Lovers were found together in a plaster-lined brick bin with no other objects except a stone slab under the head of one skeleton.[3][11] The excavation took place in 1973, directed by Robert H. Dyson, Jr. Dr. Selinsky stated that the lovers perished together during the invasion of the site, around 800 BCE, during the last destruction of the Hasanlu,[3] but did not have any lethal wounds.[3] Archaeologist Oscar Muscarella suggests that the hole in the right skeleton's skull is not due to an injury, but the result of a blow created by a workman's pickaxe.[5] When discovered, the two skeletons were facing and embracing each other.[3] The skeleton on the left is lying on their left side, reaching with their right hand towards the skeleton on the right.
There is no definitive explanation as to how the two skeletons ended up in the bin – only assumptions. One assumption is that "they must have crawled into this bin, which was probably covered at the time, and escaped detection."[5] Since cause of death was not due to injury, archaeologists have concluded that the probable cause of death was asphyxiation,[5] when debris fell from the burning building, and sealed them in.
Anthropologists Page Selinsky and Janet Monge go into extensive detail about the DNA testing of the Hasanlu lovers and how the DNA testing[7] compares to the skeletal assessment of their biological sex.
The lovers were first sampled for specific isotopes to see if there were any differences in the skeletal series and the diets that they consumed. What the isotopic testing revealed was that the diets of the individuals were quite varied, but they were not patterned in any particular way. Isotopic signatures indicate that the diets of the residents of Hasanlu were varied, including wheat, barley, sheep and goats.[12] Isotopic signatures coming from oxygen revealed the lovers' settlement patterns; these oxygen isotopic signatures revealed that the lovers, and the other Hasanlu people, were all born and raised in the Hasanlu area.[7]
It was concluded by Selinsky and Monge that both individuals were male.[3][7] They came to this conclusion when comparing both the DNA analysis[13] and skeletal assessment. Dr Selinsky stated that the pelvis was the single best criterion for estimating the sex of the skeletons as there are distinctive features between a female and a male pelvis.[7]
The skeleton on the right (referred to as SK 335) is lying on its back. The front portion of his pelvis was lost but when examining his sciatic notch, it was evident he was a male due to the very narrow gap which is a distinctive feature of the male pelvis.[7] As such, researchers identified the skeleton as male largely based on the pelvis. The skeleton had no apparent evidence of disease or healed lifetime injuries.[1][2][3]
For the left skeleton (SK 336), lying on its left side facing SK 335, the sex estimation was less clear, but overall research suggests a male: the cranium is distinctively male, while the pelvis is more mixed in its morphology.[3] At the time of excavation, this skeleton was originally identified as female.[5] This was because his sciatic notch was quite wide, a characteristic of a female pelvis, but the front portion of his pelvis which was retrieved from the site, had an acute angle in the front and was less pulled out than a female’s, which suggested the skeleton was a male.[7] The individual appeared to have been healthy in life, and the skeleton had no apparent evidence of healed lifetime injuries.[1][2][3] The sex of the lovers was confirmed from a bone sample for an ancient DNA analysis. The genetic determination of the Hasanlu lovers was male.[7]
The age of the two skeletons was also determined. Dental evidence suggests that the right skeleton was a young adult or subadult, estimated to be aged 19–22 years old,[3] as he has third molars, and his wisdom teeth recently grew. His skull was less developed, which was attributable to the young age of the individual.[7] The left skeleton was estimated to be an older adult 30–35 years old; his skull had fully developed, and the cranium was distinctively male.[3]
Some researchers argue sensationalism about the Hasanlu Lovers, and other potential examples of non-heteronormative behaviours in the past are problematic.[14][15] The two skeletons received their sobriquet 'Hasanlu Lovers' due to the intimate position they were found in. Before the skeletons were subjected to DNA analysis one skeleton was thought to be male and the other female. Muscarella, an archaeologist who was heavily invested in the discoveries made at Hasanlu, states, "I knew at first sight who was the female,"[16] in reference to the two skeletons. However, the team from the University of Pennsylvania, assessed that the right skeleton was likely male due to its morphology. The left skeleton had less clear osteological indicators, but was later identified to be male through DNA analysis.[3] Limitations of osteological sex assessments as noted by one author is that there are many times when the biological sex can not be certain, and that these tests do not reveal anything about the culturally-constructed gender.[15]
Reasons for expecting the skeletons to be a heteronormative couple, as Killgrove and Geller explain, are because modern society is primed by culture to see this representation.[15][17] Geller states that projecting contemporary assumptions about sex, gender, and sexuality onto the past can be problematic,[17] and that the true relationship between the two skeletons is unknown and remains up to speculation, despite the implications that may be drawn from their apparently intimate pose.

Prehistoric Europe refers to Europe before the start of written records,[3] beginning in the Lower Paleolithic. As history progresses, considerable regional unevenness in cultural development emerges and grows. The region of the eastern Mediterranean is, due to its geographic proximity, greatly influenced and inspired by the classical Middle Eastern civilizations, and adopts and develops the earliest systems of communal organization and writing.[4] The Histories of Herodotus (from around 440 BC) is the oldest known European text that seeks to systematically record traditions, public affairs and notable events.[5]
Widely dispersed, isolated finds of individual fossils of bone fragments (Atapuerca, Mauer mandible), stone artifacts or assemblages suggest that during the Lower Paleolithic, spanning from 3 million until 300,000 years ago, palaeo-human presence was rare and typically separated by thousands of years. The karstic region of the Atapuerca Mountains in Spain represents the currently earliest known and reliably dated location of residence for more than a single generation and a group of individuals.[6][7]
Homo neanderthalensis emerged in Eurasia between 600,000 and 350,000 years ago as the earliest body of European people that left behind a substantial tradition, a set of evaluable historic data through a rich fossil record in Europe's limestone caves and a patchwork of occupation sites over large areas. These include Mousterian cultural assemblages.[8][9] Modern humans arrived in Mediterranean Europe during the Upper Paleolithic between 45,000 and 43,000 years ago, and both species occupied a common habitat for several thousand years. Research has so far produced no universally accepted conclusive explanation as to what caused the Neanderthal's extinction between 40,000 and 28,000 years ago.[10][11]
Homo sapiens later populated the entire continent during the Mesolithic, and advanced north, following the retreating ice sheets of the last glacial maximum that spanned between 26,500 and 19,000 years ago. A 2015 publication on ancient European DNA collected from Spain to Russia concluded that the original hunter-gatherer population had assimilated a wave of "farmers" who had arrived from the Near East during the Neolithic about 8,000 years ago.[12]
The Mesolithic era site Lepenski Vir in modern-day Serbia, the earliest documented sedentary community of Europe with permanent buildings, as well as monumental art, precedes by many centuries sites previously considered to be the oldest known. The community's year-round access to a food surplus prior to the introduction of agriculture was the basis for the sedentary lifestyle.[13] However, the earliest record for the adoption of elements of farming can be found in Starčevo, a community with close cultural ties.[14]
Belovode and Pločnik, also in Serbia, is currently the oldest reliably dated copper smelting site in Europe (around 7,000 years ago). It is attributed to the Vinča culture, which on the contrary provides no links to the initiation of or a transition to the Chalcolithic or Copper Age.[15][16][17]
The process of smelting bronze is an imported technology with debated origins and history of geographic cultural profusion. It was established in Europe about 3200 BC in the Aegean and production was centered around Cyprus, the primary source of copper for the Mediterranean for many centuries.[18]
The introduction of metallurgy, which initiated unprecedented technological progress, has also been linked with the establishment of social stratification, the distinction between rich and poor, and use of precious metals as the means to fundamentally control the dynamics of culture and society.[19]
The European Iron Age culture also originates in the East through the absorption of the technological principles obtained from the Hittites about 1200 BC, finally arriving in Northern Europe by 500 BC.[20]
During the Iron Age, Central, Western and most of Eastern Europe gradually entered the actual historical period. Greek maritime colonization and Roman terrestrial conquest form the basis for the diffusion of literacy in large areas to this day. This tradition continued in an altered form and context for the most remote regions (Greenland and Eastern Balts, 13th century) via the universal body of Christian texts, including the incorporation of East Slavic peoples and Russia into the Orthodox cultural sphere. Latin and ancient Greek languages continued to be the primary and best way to communicate and express ideas in liberal arts education and the sciences all over Europe until the early modern period.[21]
The climatic record of the Paleolithic is characterised by the Pleistocene pattern of cyclic warmer and colder periods, including eight major cycles and numerous shorter episodes. The northern maximum of human occupation fluctuated in response to the changing conditions, and successful settlement required constant adaption capabilities and problem solving. Most of Scandinavia, the North European Plain and Russia remained off limits for occupation during the Paleolithic and Mesolithic.[28] Populations were low in density and small in number throughout the Palaeolithic.[29]
Associated evidence, such as stone tools, artifacts and settlement localities, is more numerous than fossilised remains of the hominin occupants themselves. The simplest pebble tools with a few flakes struck off to create an edge were found in Dmanisi, Georgia, and in Spain at sites in the Guadix-Baza basin and near Atapuerca. The Oldowan tool discoveries, called Mode 1-type assemblages are gradually replaced by a more complex tradition that included a range of hand axes and flake tools, the Acheulean, Mode 2-type assemblages. Both types of tool sets are attributed to Homo erectus, the earliest and for a very long time the only human in Europe and more likely to be found in the southern part of the continent. However, the Acheulean fossil record also links to the emergence of Homo heidelbergensis, particularly its specific lithic tools and handaxes. The presence of Homo heidelbergensis is documented since 600,000 BC in numerous sites in Germany, Great Britain and northern France.[30]
Although palaeoanthropologists generally agree that Homo erectus and Homo heidelbergensis immigrated to Europe, debates remain about migration routes and the chronology.[31]
The fact that Homo neanderthalensis is found only in a contiguous range in Eurasia and the general acceptance of the Out of Africa hypothesis both suggest that the species has evolved locally. Again, consensus prevails on the matter, but widely debated are origin and evolution patterns.[32][33][34][35]
The Neanderthal fossil record ranges from Western Europe to the Altai Mountains in Central Asia and the Ural Mountains in the North to the Levant in the South. Unlike its predecessors, they were biologically and culturally adapted to survival in cold environments and successfully extended their range to the glacial environments of Central Europe and the Russian plains. The great number and, in some cases, exceptional state of preservation of Neanderthal fossils and cultural assemblages enables researchers to provide a detailed and accurate data on behaviour and culture.[36][9] Neanderthals are associated with the Mousterian culture (Mode 3), stone tools that first appeared approximately 160,000 years ago.[37][38]
Homo sapiens arrived in Europe around 46,000 and 43,000 years ago via the Levant and entered the continent through the Danubian corridor, as the fossils at the sites of Bacho Kiro cave and Peștera cu Oase suggest.[39] With an approximate age of 46,000 years,[40] the Homo sapiens fossils found in Bacho Kiro cave consist of a pair of fragmented mandibles including at least one molar[41][42] This site yielded the oldest known ornaments in Europe, Radiocarbon dated to over 43,000 years ago.[43]
The fossils' genetic structure indicates a recent Neanderthal ancestry and the discovery of a fragment of a skull in Israel in 2008 support the notion that humans interbred with Neanderthals in the Levant.[citation needed]
After the slow processes of the previous hundreds of thousands of years, a turbulent period of Neanderthal–Homo sapiens coexistence demonstrated that cultural evolution had replaced biological evolution as the primary force of adaptation and change in human societies.[44][45]
Generally small and widely dispersed fossil sites suggest that Neanderthals lived in less numerous and more socially isolated groups than Homo sapiens. Tools and Levallois points are remarkably sophisticated from the outset, but they have a slow rate of variability, and general technological inertia is noticeable during the entire fossil period. Artifacts are of utilitarian nature, and symbolic behavioral traits are undocumented before the arrival of modern humans. The Aurignacian culture, introduced by modern humans, is characterized by cut bone or antler points, fine flint blades and bladelets struck from prepared cores, rather than using crude flakes. The oldest examples and subsequent widespread tradition of prehistoric art originate from the Aurignacian.[46][47][48][49]
After more than 100,000 years of uniformity, around 45,000 years ago, the Neanderthal fossil record changed abruptly. The Mousterian had quickly become more versatile and was named the Chatelperronian culture, which signifies the diffusion of Aurignacian elements into Neanderthal culture. Although debated, the fact proved that Neanderthals had, to some extent, adopted the culture of modern Homo sapiens.[50] However, the Neanderthal fossil record completely vanished after 40,000 years BC. Whether Neanderthals were also successful in diffusing their genetic heritage into Europe's future population or they simply went extinct and, if so, what caused the extinction cannot conclusively be answered.
Around 32,000 years ago, the Gravettian culture appeared in the Crimean Mountains (southern Ukraine).[52][53] By 24,000 BC, the Solutrean and Gravettian cultures were present in Southwestern Europe. Gravettian technology and culture have been theorised to have come with migrations of people from the Middle East, Anatolia and the Balkans, and might be linked with the transitional cultures mentioned earlier since their techniques have some similarities and are both very different from Aurignacian ones, but this issue is very obscure. The Gravettian also appeared in the Caucasus and Zagros Mountains but soon disappeared from southwestern Europe, with the notable exception of the Mediterranean coasts of Iberia.
The Solutrean culture, extended from northern Spain to southeastern France, includes not only a stone technology but also the first significant development of cave painting and the use of the needle and possibly that of the bow and arrow. The more widespread Gravettian culture is no less advanced, at least in artistic terms: sculpture (mainly venuses) is the most outstanding form of creative expression of such peoples.
Around 19,000 BC, Europe witnesses the appearance of a new culture, known as Magdalenian, possibly rooted in the old Aurignacian one, which soon superseded the Solutrean area and also the Gravettian of Central Europe. However, in Mediterranean Iberia, Italy, the Balkans and Anatolia, Epigravettian cultures continued to evolve locally.
With the Magdalenian culture, the Paleolithic development in Europe reaches its peak and this is reflected in art, owing to previous traditions of paintings and sculpture.
Around 12,500 BC, the Würm Glacial Age ended. Slowly, through the following millennia, temperatures and sea levels rose, changing the environment of prehistoric people. Ireland and Great Britain became islands, and Scandinavia became separated from the main part of the European Peninsula. (They had all once been connected by a now-submerged region of the continental shelf known as Doggerland.) Nevertheless, the Magdalenian culture persisted until 10,000 BC, when it quickly evolved into two microlith cultures: Azilian, in Spain and southern France, and Sauveterrian, in northern France and Central Europe. Despite some differences, both cultures shared several traits: the creation of very small stone tools called microliths and the scarcity of figurative art, which seems to have vanished almost completely, which was replaced by abstract decoration of tools.[54]
In the late phase of the epi-Paleolithic period, the Sauveterrean culture evolved into the so-called Tardenoisian and strongly influenced its southern neighbour, clearly replacing it in Mediterranean Spain and Portugal. The recession of the glaciers allowed human colonisation in Northern Europe for the first time. The Maglemosian culture, derived from the Sauveterre-Tardenois culture but with a strong personality, colonised Denmark and the nearby regions, including parts of Britain.
A transition period in the development of human technology between the Paleolithic and the Neolithic, the Balkan Mesolithic began around 15,000 years ago. In Western Europe, the Early Mesolithic, or Azilian, began about 14,000 years ago, in the Franco-Cantabrian region of northern Spain and southern France. In other parts of Europe, the Mesolithic began by 11,500 years ago (the beginning Holocene) and ended with the introduction of farming, which, depending on the region, occurred 8,500 to 5,500 years ago.
In areas with limited glacial impact, the term "Epipaleolithic" is sometimes preferred for the period. Regions that experienced greater environmental effects as the Last Glacial Period ended had a much more apparent Mesolithic era that lasted millennia. In Northern Europe, societies were able to live well on rich food supplies from the marshlands, which had been created by the warmer climate. Such conditions produced distinctive human behaviours that are preserved in the material record, such as the Maglemosian and Azilian cultures. Such conditions delayed the coming of the Neolithic to as late as 5,500 years ago in Northern Europe.
As what Vere Gordon Childe termed the "Neolithic Package" (including agriculture, herding, polished stone axes, timber longhouses and pottery) spread into Europe, the Mesolithic way of life was marginalised and eventually disappeared.[56] Controversy over the means of that dispersal is discussed below in the Neolithic section. A "Ceramic Mesolithic" can be distinguished between 7,200 and 5,850 years ago and ranged from Southern to Northern Europe.
The European Neolithic is assumed to have arrived from the Near East via Asia Minor, the Mediterranean and the Caucasus. There has been a long discussion between migrationists, who claim that the Near Eastern farmers almost totally displaced the European native hunter-gatherers, and diffusionists, who claim that the process was slow enough to have occurred mostly through cultural transmission. A relationship has been suggested between the spread of agriculture and the diffusion of Indo-European languages, with several models of migrations trying to establish a relationship, like the Anatolian hypothesis, which sets the origin of Indo-European agricultural terminology in Anatolia.[57]
Apparently related with the Anatolian culture of Hacilar, the Greek region of Thessaly was the first place in Europe known to have acquired agriculture, cattle-herding and pottery. The early stages are known as pre-Sesklo culture. The Thessalian Neolithic culture soon evolved into the more coherent Sesklo culture (6000 BC), which was the origin of the main branches of Neolithic expansion in Europe. The Karanovo culture on the territory of modern day Bulgaria, was another early Neolithic culture (Karanovo I-III ca. 62nd to 55th centuries BC) which was part of the Danube civilization and it is considered the largest and most important of the Azmak River Valley agrarian settlements.[58] The Karanovo I is considered a continuation of Near Eastern settlement type.[59] The Starčevo culture is dating to the period between c. 6200 and 4500 BCE.[60][61] It originates in the spread of the Neolithic package of peoples and technological innovations including farming and ceramics from Anatolia. The Starčevo culture marks its spread to the inland Balkan peninsula as the Cardial ware culture did along the Adriatic coastline. It forms part of the wider Starčevo–Körös–Criş culture. Practically all of the Balkan Peninsula was colonized in the 6th millennium from there. The expansion, reaching the easternmost Tardenoisian outposts of the upper Tisza, gave birth to the Proto-Linear Pottery culture, a significant modification of the Balkan Neolithic that was the origin of one of the most important branches of European Neolithic: the Danubian group of cultures. In parallel, the coasts of the Adriatic and of southern Italy witnessed the expansion of another Neolithic current with less clear origins. Settling initially in Dalmatia, the bearers of the Cardium pottery culture may have come from Thessaly (some of the pre-Sesklo settlements show related traits) or even from Lebanon (Byblos). They were sailors, fishermen and sheep and goat herders, and the archaeological findings show that they mixed with natives in most places. Other early Neolithic cultures can be found in Ukraine and Southern Russia, where the Epigravettian locals assimilated cultural influxes from beyond the Caucasus (e.g. the Dniepr-Donets culture and related cultures) and in Andalusia (Spain), where the rare Neolithic of La Almagra Pottery appeared without known origins very early (c. 7800 BC).
This phase, starting 7000 years ago was marked by the consolidation of the Neolithic expansion towards western and northern Europe but also by the rise of new cultures in the Balkans, notably the Dimini (Thessaly) and related Vinca (Serbia and Romania) and Karanovo cultures (Bulgaria and nearby areas). Meanwhile, the Proto-Linear Pottery culture gave birth to two very dynamic branches: the Western and Eastern Linear Pottery Cultures. The western branch expanded quickly, assimilating Germany, the Czech Republic, Poland and even large parts of western Ukraine, historical Moldavia, the lowlands of Romania, and regions of France, Belgium and the Netherlands, all in less than 1000 years. With this expansion came diversification and a number of local Danubian cultures started forming at the end of the 5th millennium. In the Mediterranean, the Cardium pottery fishermen showed no less dynamism and colonised or assimilated all of Italy and the Mediterranean regions of France and Spain. Even in the Atlantic, some groups among the native hunter-gatherers started the slow incorporation of the new technologies. Among them, the most noticeable regions seem to be southwestern Iberia, which was influenced by the Mediterranean but especially by the Andalusian Neolithic, which soon developed the first Megalithic burials (dolmens), and the area around Denmark (Ertebölle culture), influenced by the Danubian complex.
This period occupied the first half of the 6th millennium BC. The tendencies of the previous period consolidated and so there was a fully-formed Neolithic Europe, with five main cultural regions:
Also known as "Copper Age", the European Chalcolithic was a time of significant changes, the first of which was the invention of copper metallurgy. This is first attested in the Vinca culture in the 6th millennium BC. The Balkans became a major centre for copper extraction and metallurgical production in the 5th millennium BC. Copper artefacts were traded across the region, eventually reaching eastwards across the steppes of eastern Europe as far as the Khavalynsk culture. The 5th millennium BC also saw the appearance of economic stratification and the rise of ruling elites in the Balkan region, most notably in the Varna culture (c. 4500 BC) in Bulgaria, which developed the first known gold metallurgy in the world.
The economy of the Chalcolithic was no longer that of peasant communities and tribes, since some materials began to be produced in specific locations and distributed to wide regions. Mining of metal and stone was particularly developed in some areas, along with the processing of those materials into valuable goods.[64]
Early Chalcolithic, 5500-4000 BC
From 5500 BC onwards, Eastern Europe was apparently infiltrated by people originating from beyond the Volga, creating a plural complex known as Sredny Stog culture, which substituted the previous Dnieper-Donets culture in Ukraine, pushing the natives to migrate northwest to the Baltic and to Denmark, where they mixed with the natives (TRBK A and C). The emergence of the Sredny Stog culture may be correlated with the expansion of Indo-European languages, according to the Kurgan hypothesis. Near the end of the period, around 4000 BC, another westward migration of supposed Indo-European speakers left many traces in the lower Danube area (culture of Cernavodă I) in what seems to have been an invasion.[65]
Solnitsata ("The Saltworks"), a prehistoric town located in present-day Bulgaria, is believed by archaeologists to be the oldest town in Europe - a fortified stone settlement - citadelle, inner and outer city with pottery production site and the site of a salt production facility approximately six millennia ago;[66] it flourished ca 4700–4200 BC.[67][68][69]
Meanwhile, the Danubian Lengyel culture absorbed its northern neighbours in the Czech Republic and Poland for some centuries, only to recede in the second half of the period. The hierarchical model of the Varna culture seems to have been replicated later in the Tiszan region with the Bodrogkeresztur culture. Labour specialisation, economic stratification and possibly the risk of invasion may have been the reasons behind this development.
In the western Danubian region (the Rhine and Seine basins), the Michelsberg culture displaced its predecessor, the Rössen culture. Meanwhile, in the Mediterranean basin, several cultures (most notably the Chasséen culture in southeastern France and the Lagozza culture in northern Italy) converged into a functional union of which the most significant characteristic was the distribution network of honey-coloured silex. Despite the unity, the signs of conflicts are clear, as many skeletons show violent injuries. This was the time and area of Ötzi, the famous man found in the Alps.
This period extends through the first half of the 4th millennium BC. During this period the Cucuteni-Trypillia culture in Ukraine experienced a massive expansion, building the largest settlements in the world at the time, described as the first cities in the world by some scholars. The earliest known evidence for wheeled vehicles, in the form of wheeled models, also comes from Cucuteni-Trypillia sites, dated to c. 3900 BC.
In the Danubian region the powerful Baden culture emerged circa 3500 BC, extending more or less across the region of Austria-Hungary. The rest of the Balkans was profoundly restructured after the invasions of the previous period, with the Coțofeni culture in the central Balkans showing pronounced eastern (or presumably Indo-European) traits. The new Ezero culture in Bulgaria (3300 BC), shows the first evidence of pseudo-bronze (or arsenical bronze), as does the Baden culture and the Cycladic culture (in the Aegean) after 2800 BC.[70]
In Eastern Europe, the Yamnaya culture took over southern Russia and Ukraine. In western Europe, the only sign of unity came from the Megalithic super-culture, which extended from southern Sweden to southern Spain, including large parts of southern Germany as well. However, the Mediterranean and Danubian groupings of the previous period appear to have fragmented into many smaller pieces, some of them apparently backward in technological matters. From 2800 BC, the Danubian Seine-Oise-Marne culture pushed directly or indirectly southwards and destroyed most of the rich Megalithic culture of western France. After 2600 BC, several phenomena prefigured the changes of the upcoming period:[71]
Large towns with stone walls appeared in two different areas of the Iberian Peninsula: one in the Portuguese region of Estremadura (culture of Vila Nova de Sao Pedro), strongly embedded in the Atlantic Megalithic culture; the other near Almería (southeastern Spain), centred around the large town of Los Millares, of Mediterranean character, probably affected by eastern cultural influxes (tholoi). Despite the many differences, both civilisations seem to have had friendly contact and to have maintained productive exchanges. In the area of Dordogne (Aquitaine, France), a new unexpected culture of bowmen appears: the Artenac culture soon takes control of western and even northern France and Belgium. In Poland and nearby regions, the putative Indo-Europeans reorganised and reconsolidated with the culture of the Globular Amphoras. Nevertheless, the influence of many centuries in direct contact with the still-powerful Danubian peoples had greatly modified their culture.[70][72]
Use of Bronze begins in the Aegean around 3200 BC. From 2500 BC the new Catacomb culture, whose origins were obscure but were also Indo-Europeans, displaced the Yamna peoples in the regions north and east of the Black Sea, confining them to their original area east of the Volga. Around 2400 BC, the Corded Ware culture replaced their predecessors and expanded to Danubian and Nordic areas of western Germany. One related branch invaded Denmark and southern Sweden (Scandinavian Single Grave culture), and the mid-Danubian basin, though showing more continuity, had clear traits of new Indo-European elites (Vučedol culture). Simultaneously, in the West, the Artenac peoples reached Belgium. With the partial exception of Vučedol, the Danubian cultures, which had been so buoyant just a few centuries ago, were wiped off the map of Europe. The rest of the period was the story of a mysterious phenomenon: the Beaker people, which seemed to be of a mercantile character and to have preferred being buried according to a very specific, almost invariable, ritual. Nevertheless, out of their original area of western Central Europe, they appeared only within local cultures and so they never invaded and assimilated but went to live among those peoples and kept their way of life, which is why they are believed to be merchants.
The rest of Europe remained mostly unchanged and apparently peaceful. In 2300 BC, the first Beaker Pottery appeared in Bohemia and expanded in many directions but particularly westward, along the Rhone and the seas, reaching the culture of Vila Nova (Portugal) and Catalonia (Spain) as their limits. Simultaneously but unrelatedly, in 2200 BC in the Aegean region, the Cycladic culture decayed and was substituted by the new palatine phase of the Minoan culture of Crete.
The second phase of Beaker Pottery, from 2100 BC onwards, is marked by the displacement of the centre of the phenomenon to Portugal, within the culture of Vila Nova. The new centre's influence reached to all of southern and western France but was absent in southern and western Iberia, with the notable exception of Los Millares. After 1900 BC, the centre of the Beaker Pottery returned to Bohemia, and in Iberia, a decentralisation of the phenomenon occurred, with centres in Portugal but also in Los Millares and Ciempozuelos.
Though the use of bronze started much earlier in the Aegean area (c. 3.200 BC), c. 2300 BC can be considered typical for the start of the Bronze Age in Europe in general.
Derivations of the sudden expansion were the Sea Peoples, who attacked Egypt unsuccessfully for some time, including the Philistines (Pelasgians?) and the Dorians, most likely Hellenised members of the group that ended invading Greek itself and destroying the might of Mycene and later Troy.
Simultaneously, around then, the culture of Vila Nova de Sao Pedro, which lasted 1300 years in its urban form, vanishes into a less spectacular one but finally with bronze. The centre of gravity of the Atlantic cultures (the Atlantic Bronze Age complex) was now displaced towards Great Britain. Also about then, the Villanovan culture, the possible precursor of the Etruscan civilisation, appeared in central Italy, possibly with an Aegean origin.
Though the use of iron was known to the Aegean peoples about 1100 BC, it failed to reach Central Europe before 800 BC, when it gave way to the Hallstatt culture, an Iron Age evolution of the Urnfield culture.
Around then, the Phoenicians, benefitting from the disappearance of the Greek maritime power (Greek Dark Ages) founded their first colony at the entrance of the Atlantic Ocean, in Gadir (modern Cádiz), most likely as a merchant outpost to convey the many mineral resources of Iberia and the British Isles.
Nevertheless, from the 7th century BC onwards, the Greeks recovered their power and started their own colonial expansion, founding Massalia (modern Marseilles) and the Iberian outpost of Emporion (modern Empúries). That occurred only after the Iberians could reconquer Catalonia and the Ebro valley from the Celts, separating physically the Iberian Celts from their continental neighbours.
The second phase of the European Iron Age was defined particularly by the Celtic La Tène culture, which started around 400 BC, followed by a large expansion of them into the Balkans, the British Isles, where they assimilated druidism, and other regions of France and Italy.
The decline of Celtic power under the expansive pressure of Germanic tribes (originally from Scandinavia and Lower Germany) and the forming of the Roman Empire during the 1st century BC was also that of the end of prehistory, properly speaking; though many regions of Europe remained illiterate and therefore out of reach of written history for many centuries, the boundary must be placed somewhere, and that date, near the start of the calendar, seems to be quite convenient. The remaining is regional prehistory, or, in most cases, protohistory, but no longer European prehistory, as a whole.
The genetic history of Europe has been inferred by observing the patterns of genetic diversity across the continent and in the surrounding areas. Use has been made of both classical genetics and molecular genetics.[74][75] Analysis of the DNA of the modern population of Europe has mainly been used but use has also been made of ancient DNA.
This analysis has shown that modern man entered Europe from the Near East before the Last Glacial Maximum but retreated to refuges in southern Europe in this cold period. Subsequently, people spread out over the whole continent, with subsequent limited migration from the Near East and Asia.[76]
According to a study in 2017, the early farmers belonged predominantly to the paternal Haplogroup G-M201.[77] The maternal haplogroup N1a was also frequent in the farmers.[78]
Evidence from genome analysis of ancient human remains suggests that the modern native populations of Europe largely descend from three distinct lineages: Mesolithic hunter-gatherers, derivative of the Cro-Magnon population of Europe, Early European Farmers (EEF) introduced to Europe during the Neolithic Revolution, and Ancient North Eurasians which expanded to Europe in the context of the Indo-European expansion.[79] The Early European Farmers migrated from Anatolia to the Balkans in large numbers during the 7th millennium BC.[80] 
During the Chalcolithic and early Bronze Age, the EEF-derived cultures of Europe were overwhelmed by successive invasions of Western Steppe Herders (WSHs) from the Pontic–Caspian steppe, who carried about 60% Eastern Hunter-Gatherer (EHG) and 40% Caucasus Hunter-Gatherer (CHG) admixture. These invasions led to EEF paternal DNA lineages in Europe being almost entirely replaced with EHG/WSH paternal DNA (mainly R1b and R1a). EEF maternal DNA (mainly haplogroup N) also declined, being supplanted by steppe lineages,[81][82] suggesting the migrations involved both males and females from the steppe. EEF mtDNA, however, remained frequent, suggesting admixture between WSH males and EEF females.[83][84]
The written linguistic record in Europe first begins with the Mycenaean record of early Greek in the Late Bronze Age.
Unattested languages spoken in Europe in the Bronze and Iron Ages are the object of reconstruction in historical linguistics, in the case of Europe predominantly Indo-European linguistics.
Indo-European is assumed to have spread from the Pontic steppe at the very beginning of the Bronze Age, reaching Western Europe contemporary with the Beaker culture, after about 5,000 years ago.
Various pre-Indo-European substrates have been postulated, but remain speculative; the "Pelasgian" and "Tyrsenian" substrates of the Mediterranean world, an "Old European" (which may itself have been an early form of Indo-European), a "Vasconic" substrate ancestral to the modern Basque language,[85] or a more widespread presence of early Finno-Ugric languages in northern Europe.[86]
An early presence of Indo-European throughout Europe has also been suggested ("Paleolithic continuity theory").[87]
Donald Ringe emphasizes the "great linguistic diversity" which would generally have been predominant in any area inhabited by small-scale, tribal pre-state societies.[88]
Media related to Prehistory of Europe at Wikimedia Commons
Paleolithic sanctuaries:

The earliest anatomically modern human skeleton in Peninsular Malaysia, Perak Man, dates back 11,000 years and Perak Woman dating back 8,000 years, were both discovered in Lenggong. The site has an undisturbed stone tool production area, created using equipment such as anvils and hammer stones. The Tambun rock art is also situated in Ipoh, Perak. From East Malaysia, Sarawak's Niah Caves, there is evidence of the oldest human remains in Malaysia, dating back 40,000 years.
The Niah Caves in Sarawak are an important prehistoric site where human remains dating to ca. 40,000 years ago have been found.[1] Archaeologists have claimed a much earlier date for stone tools found in the Mansuli Valley, near Lahad Datu, Sabah, starting from 235,000 to 3,000 years ago. This makes it the oldest valley in Borneo prehistory that has been dated chronometrically.[2]
Studies in Mansuli Valley and the discovery of other open sites in Sabah marked a new episode for the prehistory of Malaysia, with both open and cave sites providing the oldest dates yet for the prehistoric sites of Sabah. The cave site, Samang Buat Cave, was the oldest inhabited cave in Sabah and Borneo,[3] dating back 46,000 years. The Mansuli open site was the oldest in Sabah and Borneo, in general, dating back 235,000 years. The findings at both sites gave a chronology of their prehistory which showed repetitive habitation at both sites.[4]
Archaeological finds from the Lenggong Valley in Perak show that people were making stone tools and using jewellery. The archaeological data from this period came from cave and rock shelter sites and are associated with Hoabinhian hunter-gatherers. It is believed that Neolithic farmers arrived in this region between 3,000 and 4,000 years ago.[5]
More people arrived, including new tribes and seafaring Austronesians. The Malay Peninsula became a crossroads in the maritime trade of the ancient age. Seafarers who came to Malaysia's shores included Malayo-Polynesian people, Indians and possibly Chinese traders among others. Ptolemy named the Malay Peninsula the Golden Chersonese.
A study from Leeds University published in Molecular Biology and Evolution, examining mitochondrial DNA lineages, suggested that humans had been occupying the islands of Southeast Asia for a longer period than previously believed. Population dispersals seem to have occurred at the same time as sea levels rose, which may have resulted in migrations from the Philippine Islands to as far north as Taiwan within the last 10,000 years.[6] The population migrations were most likely to have been driven by climate change. Rising sea levels in three massive pulses may have caused flooding and the submerging of the Sunda continent, creating the Java and South China Seas and the thousands of islands that make up Indonesia and the Philippines today.
A 2009 genetic study published by the 2009 Human Genome Organization Pan-Asian SNP Consortium found that Asia was originally settled by humans via a single southern route. The migration came from Africa via India, into Southeast Asia and what are now islands in the Pacific, and then later up to the eastern and northern Asian mainland.[7]
Genetic similarities were found between populations throughout Asia and an increase in genetic diversity from northern to southern latitudes. Although the Chinese population is very large, it has less variation than the smaller number of individuals living in Southeast Asia, because the Chinese expansion occurred very recently, following the development of rice agriculture – within only the last 10,000 years.[citation needed]
Oppenheimer locates the origin of the Austronesians in Sundaland and its upper regions.[8] Genetic research reported in 2008 indicates that the islands which are the remnants of Sundaland were likely populated as early as 50,000 years ago, contrary to a previous hypothesis[by whom?] that they were populated as late as 10,000 years ago from Taiwan.[9][dubious – discuss][10]
The theory of the Proto-Malay people originating from Yunnan is supported by R.H Geldern, J.H.C Kern, J.R Foster, J.R Logen, Slametmuljana, and Asmah Haji Omar. The Proto Malay (Melayu Asli) who first arrived had agricultural skills while the second wave Deutero Malay (mixed blood)[vague] who arrived around 1500 BC and dwelled along the coastlines had advanced fishery skills. During the migration, both groups intermarried with peoples of the southern islands, such as those from Java, and also with aboriginal peoples of Australo-Melanesian, Negrito and Melanesian origin.[citation needed]
Other evidence that supports this theory includes:
Combination of the colonial Kambujas of Hindu-Buddhism faith, the Indo-Persian royalties and traders as well as traders from southern China and elsewhere along the ancient trade routes, these peoples together with the aborigine Negrito Orang Asli and native seafarers and Proto Malays intermarried each other's and thus a new group of peoples was formed and became known as the Deutero Malays, today they are commonly known as the Malays.[citation needed]
According to most scholars the Đông Yên Châu inscription from around the 4th century AD was written in Old Cham[11] is the oldest Malay text found. However, some believe the inscription to contain the oldest examples of Malay words.[citation needed] Chamic and Malayic languages are closely related; both are the two subgroups of a Malayic–Chamic group[12] within the Malayo-Polynesian branch of the Austronesian family.
The Kedukan Bukit Inscription of 682 CE was found at Palembang, Indonesia.
The similarity of the Cambodian Cham language and the Malay language can be found in names of places such as Kampong Cham, Kambujadesa, Kampong Chhnang, etc. and Sejarah Melayu clearly mentioned a Cham community in Parameswara's Malacca around the 15th century. Cham is related to the Malayo-Polynesian languages of Malaysia, Indonesia, Madagascar and the Philippines. In the mid 15th century, when Cham was heavily defeated by the Vietnamese, some 120,000 were killed and in the 17th century the Champa king converted to Islam.[13] In the 18th century the last Champa Muslim king Pô Chien gathered his people and migrated south to Cambodia while those along the coastline migrated to the nearest peninsula state Terengganu, approximately 500 km or less by boat, and Kelantan. Malaysian constitution recognises the Cham rights to Malaysian citizenship and their Bumiputera status. Now that the history is interlinked, there is a possibility that Parameswara's family were Cham refugees who fled to Palembang before he fled to Tumasik and finally to Malacca. One of the last Kings of Angkor of the Khmer Empire had the name Paramesvarapada.[14]

Wushan Man (Chinese: 巫山人; pinyin: Wūshānrén, literally "Shaman Mountain Man") is a set of fossilised remains of an extinct, undetermined non-hominin ape found in central China in 1985. The remains are dated to around 2 million years ago and were originally considered to represent a subspecies of Homo erectus (H. e. wushanensis).[1][2]
The fossils were found in 1985 in Longgupo (龙骨坡 or "Dragon Bone Slope"), Zhenlongping Village, Miaoyu Town of Wushan County, Chongqing in the Three Gorges.
The "Dragon Bone Slope" at Longgupo was discovered as a site containing fossils in 1984. From 1985 to 1988, it was excavated by a team of Chinese scientists, led by Huang Wanpo from the Institute of Vertebrate Paleontology and Paleoanthropology in Beijing and the Chongqing National Museum (Sichuan Province).
In 1986, three fore-teeth and a left mandible with two molars were unearthed along with animal fossils including teeth from the extinct large ape Gigantopithecus and pygmy giant panda Ailuropoda microta. Excavations from 1997 to 2006 have found additional stone tools and animal fossils including remains of 120 species of vertebrates, 116 of which are mammals. This suggests that the fossils originally existed in a subtropical forest environment.[3] Remains of Sinomastodon, Nestoritherium, Equus yunnanensis, Ailuropoda microta in the jaw suggested that its remains belonged to the earliest part of the Pleistocene or late Pliocene.[4]
Early reports of these excavation in Chinese journals did not garner attention outside of China.[5] In 1992, Russell Ciochon was invited to Longgupo to examine and provide a reliable age for the jaw. In 1995, Ciochon then published the findings in the journal Nature, which brought attention to the fossils on a global scale.[4]
In a 1995 Science report, several doubts about the specimens were raised. Upon seeing the specimens on a trip to China, Milford Wolpoff of the University of Michigan is not convinced that the partial jaw is a hominid. He believed that the fossils might have belonged of an orangutan or Pongo, based on the shape of the missing neighboring tooth of a preserved premolar. Jeffrey Schwartz and Ian Tattersall also published a claim in Nature that the teeth found in Longgupo were those of an orangutan. However, it was found that the teeth did not fit in the range of variation of those found in orangutans, which ruled out this possibility.
More recently, it had been argued that the jaw fragment was indistinguishable from Late Miocene-Pliocene Chinese apes of the genus Lufengpithecus. The incisor was found to be more consistent with that of an East Asian person, who may have accidentally entered the fissure of the Longgupo Cave deposits because of natural forces such as flowing water.
In the 18 June 2009 issue of Nature, Russell Ciochon who first reported the jaw fragment from Longgupo as human,[4] announced his retraction of the findings. He is convinced that the Longgupo fossil do not belong to a pre-erectus human, but rather to unknown apes that originated from primal forests in Pleistocene Southeast Asia. He mentioned that H. erectus arrived in Asia about 1.6 million years ago, but steered clear of the forest in pursuit of grassland game, which means that the pre-erectus species did not appear in southeast Asia.[1]
Russell Ciochon no longer believes that Gigantopithecus and H. erectus coexisted in the same environment[1]—an argument he had previously made in his book 1990 Other Origins: The Search for the Giant Ape in Human Prehistory.[6] He states:
Without the assumption that Gigantopithecus and H. erectus lived together, everything changed: if early humans were not part of the Stegodon–Ailuropoda fauna, I had to envision a chimpanzee-sized ape in its place — either a descendant of Lufengpithecus, or a previously unknown ape genus.[1]p. 911
A key factor in changing his opinion about the fossil was a 2005 visit to the Guangxi Natural History Museum in Nanning, where he examined a large number of primate teeth from the Pleistocene.[1] He believes that early humans hunted mammals on grasslands and did not live in sub-tropic forests that existed at Longgupo in that period, making it impossible for the set of fossils to have belonged to a human. While Russell Ciochon no longer believes the jaw to belonged to a human, he still claims that the two stone tools found with fossils were created by humans. However, according to him, "They must have been more recent additions to the site."[1]p. 911
Jeffrey Schwartz, one of the critics of the original claim, found Ciochon's retraction astonishing since it is not a common occurrence that a scientist announces a retraction after changing his mind, praising the openness as something positive. [7]
The discovery of the Wushan Man and its related materials such as stone various vertebrate fossils, and stone artifacts such as cores, points, scrapers, drilling tools, etc. show evidence of human agency. This is important because it suggests that the creators of these tools have made the change from tool-using to tool-making.[8]
According to Nature:
The new evidence suggests that hominids entered Asia before 2 Myr, coincident with the earliest diversification of genus Homo in Africa. Clearly, the first hominid to arrive in Asia was a species other than true H. erectus, and one that possessed a stone-based technology. A pre-erectus hominid in China as early as 1.9 Myr provides the most likely antecedents for the in situ evolution of Homo erectus in Asia.p. 278
This makes its status as a Homo fossil critically important to the study of human origins as it suggests that H. erectus was not the first human species to leave Africa and supports the argument made by some that H. erectus evolved in Asia instead of Africa. The discovery of Homo floresiensis is significant to this theory of pre-erectus hominin evolving in Asia. Recent research finds that its wrist and foot bones are anatomically like those of H. habilis or Australopithecus. Evidence for pre-erectus Homo in Asia would be consistent with such a possible origin.
A middle school textbook, The Chinese History (published by People's Education Press), has plans to include the discovery of "Wushan Man" as a part of its content.[9]

Haplogroup L3 is a human mitochondrial DNA (mtDNA) haplogroup. The clade has played a pivotal role in the early dispersal of anatomically modern humans.
It is strongly associated with the out-of-Africa migration of modern humans of about 70–50,000 years ago.
It is inherited by all modern non-African populations, as well as by some populations in Africa.[8][3]
Haplogroup L3 arose close to 70,000 years ago, near the time of the recent out-of-Africa event. This dispersal originated in East Africa and expanded to West Asia, and further to South and Southeast Asia in the course of a few millennia, and some research suggests that L3 participated in this migration out of Africa.
A 2007 estimate for the age of L3 suggested a range of 104–84,000 years ago.[9]
More recent analyses, including Soares et al. (2012) arrive at a more recent date, of roughly 70–60,000 years ago. Soares et al. also suggest that L3 most likely expanded from East Africa into Eurasia sometime around 65–55,000 years ago as part of the recent out-of-Africa event, as well as from East Africa into Central Africa from 60 to 35,000 years ago.[3]
In 2016, Soares et al. again suggested that haplogroup L3 emerged in East Africa, leading to the Out-of-Africa migration, around 70–60,000 years ago.[10]
Haplogroups L6 and L4 form sister clades of L3 which arose in East Africa at roughly the same time but which did not participate in the out-of-Africa migration.
The ancestral clade L3'4'6 has been estimated at 110 kya, and the L3'4 clade at 95 kya.[8]
The possibility of an origin of L3 in Asia was proposed by Cabrera et al. (2018) based on the similar coalescence dates of L3 and its Eurasian-distributed M and N derivative clades (ca. 70 kya), the distant location in Southeast Asia of the oldest known subclades of M and N, and the comparable age of the paternal haplogroup DE. According to this hypothesis, after an initial out-of-Africa migration of bearers of pre-L3 (L3'4*) around 125 kya, there would have been a back-migration of females carrying L3 from Eurasia to East Africa sometime after 70 kya. The hypothesis suggests that this back-migration is aligned with bearers of  paternal haplogroup E, which it also proposes to have originated in Eurasia. These new Eurasian lineages are then suggested to have largely replaced the old autochthonous male and female North-East African lineages.[6]
According to other research, though earlier migrations out of Africa of anatomically modern humans occurred, current Eurasian populations descend instead from a later migration from Africa dated between about 65,000 and 50,000 years ago (associated with the migration out of L3).[11][4][12]
Vai et al. (2019) suggest, from a newly discovered old and deeply-rooted branch of maternal haplogroup N found in early Neolithic North African remains, that haplogroup L3 originated in East Africa between 70,000 and 60,000 years ago, and both spread within Africa and left Africa as part of the Out-of-Africa migration, with haplogroup N diverging from it soon after (between 65,000 and 50,000 years ago) either in Arabia or possibly North Africa, and haplogroup M originating in the Middle East around the same time as N.[4]
A study by Lipson et al. (2019) analyzing remains from the Cameroonian site of Shum Laka found them to be more similar to modern-day Pygmy peoples than to West Africans, and suggests that several other groups (including the ancestors of West Africans, East Africans and the ancestors of non-Africans) commonly derived from a human population originating in East Africa between about 80,000-60,000 years ago, which they suggest was also the source and origin zone of haplogroup L3 around 70,000 years ago.[13]
L3 is common in Northeast Africa and some other parts of East Africa,[14] in contrast to others parts of Africa where the haplogroups L1 and L2 represent around two thirds of mtDNA lineages.[15] L3 sublineages are also frequent in the Arabian Peninsula.
L3 is subdivided into several clades, two of which spawned the macrohaplogroups M and N that are today carried by most people outside Africa.[15] There is at least one relatively deep non-M, non-N clade of L3 outside Africa, L3f1b6, which is found at a frequency of 1% in Asturias, Spain. It diverged from African L3 lineages at least 10,000 years ago.[16]
According to Maca-Meyer et al. (2001), "L3 is more related to Eurasian haplogroups than to the most divergent African clusters L1 and L2".[17] L3 is the haplogroup from which all modern humans outside Africa derive.[18] However, there is a greater diversity of major L3 branches within Africa than outside of it, the two major non-African branches being the L3 offshoots M and N.
L3 has seven equidistant descendants: L3a, L3b'f, L3c'd, L3e'i'k'x, L3h, M, N. Five are African, while two are associated with the Out of Africa event.
Haplogroup L3 has been observed in an ancient fossil belonging to the Pre-Pottery Neolithic B culture.[35] L3x2a was observed in a 4,500 year old hunter-gather excavated in Mota, Ethiopia, with the ancient fossil found to be most closely related to modern Southwest Ethiopian populations.[36][37] Haplogroup L3 has also been found among ancient Egyptian mummies (1/90; 1%) excavated at the Abusir el-Meleq archaeological site in Middle Egypt, with the rest deriving from Eurasian subclades, which date from the Pre-Ptolemaic/late New Kingdom and Ptolemaic periods. The Ancient Egyptian mummies bore Near eastern genomic component most closely related to modern near easterners.[38] Additionally, haplogroup L3 has been observed in ancient Guanche fossils excavated in Gran Canaria and Tenerife on the Canary Islands, which have been radiocarbon-dated to between the 7th and 11th centuries CE. All of the clade-bearing individuals were inhumed at the Gran Canaria site, with most of these specimens found to belong to the L3b1a subclade (3/4; 75%) with the rest from both islands (8/11; 72%) deriving from Eurasian subclades. The Guanche skeletons also bore an autochthonous Maghrebi genomic component that peaks among modern Berbers, which suggests that they originated from ancestral Berber populations inhabiting northwestern Affoundnat a high ncy[39]
A variety of L3 have been uncovered in ancient remains associated with the Pastoral Neolithic and Pastoral Iron Age of East Africa.[40]
This phylogenetic tree of haplogroup L3 subclades is based on the paper by Mannis van Oven and Manfred Kayser Updated comprehensive phylogenetic tree of global human mitochondrial DNA variation[7] and subsequent published research.[41]
Most Recent Common Ancestor (MRCA)
Phylogenetic tree of human mitochondrial DNA (mtDNA) haplogroups

The Pre-Dorset is a loosely defined term for a Paleo-Eskimo culture or group of cultures that existed in the Eastern Canadian Arctic from c. 3200 to 850 cal BC,[1] and preceded the Dorset culture.[2]
Due to its vast geographical expanse and to history of research, the Pre-Dorset is difficult to define. The term was coined by Collins (1956, 1957) who recognised that there seemed to be people that lived in the Eastern Canadian Arctic prior to the Dorset, but for whose culture it was difficult to give the defining characteristics.[3] Hence, for Collins and others afterward, the term is a catch-all phrase for all occupations of the Eastern Canadian Arctic that predated the Dorset. To Taylor (1968) and Maxwell (1973), however, the Pre-Dorset were a distinct cultural entity, ancestral to the Dorset, and that lived in the Low Arctic of Canada with a number of incursions into High Arctic.[1][4]
At the site of Port Refuge on the Grinnell Peninsula, Devon Island, McGhee distinguished two sets of occupations, one that he ascribed to the Independence I culture,[5] the other to Pre-Dorset.[6] Due to the often poor preservation of organic material and the fact that bones from marine mammals can appear older with radiocarbon dating than their actual age (the marine reservoir effect), it is typically difficult to date Arctic sites. But the Independence I settlement is several metres higher above sea level, and McGhee took this to mean that the Independence I settlement was roughly 300 years older than the Pre-Dorset one at Port Refuge. Indeed, assuming that settlers are always close to the water, because sea levels fell over the centuries, older sites are expected to lie higher above the sea. Most features that McGhee believed different between the Pre-Dorset and Independence I settlements of Port Refuge are problematic and cannot systematically be used to distinguish their cultural affiliation.[1] It has been suggested that Pre-Dorset and Independence I are parts of the same culture.[7]
Maxwell divided the Pre-Dorset in four phases, a scheme refined by Murray:[1][8][9]
It is typically difficult to ascribe a Pre-Dorset site to one of these four phases without relying on radiocarbon dates.[1]: 695
The Low Eastern Arctic, namely Arctic regions on Baffin Island or to the south, are usually considered the core area of the Pre-Dorset.
Most Pre-Dorset occupations are known from the Low Arctic. But the complex is known from a number of occupations in the High Arctic as well, namely to the north of Baffin Island, on the islands of Devon and Ellesmere. One important site, the Port Refuge National Historic Site of Canada, on Devon Island, hosts occupations ascribed to the Pre-Dorset and others ascribed to Independence I. At this site, Pre-Dorset dwellings are clustered and show no mid-passage feature, whereas the Independence I dwellings are arranged linearly with mid-passage features.[10][11]
The Pre-Dorset is generally restricted to the Low Arctic, and given that incursions to the High Arctic are rare, incursions into Greenland from the High Arctic are even rarer. Grønnow and Jensen (2003:42-43) ascribe one small site in Greenland to the Pre-Dorset, the only one to date. This is a mid-passage dwelling in Solbakken, Hall Land, just across from the Nares Strait, separating Canada from Greenland.[12] This occupation was identified as Pre-Dorset on the basis of the re-sharpening technique of the burins, as well as other lithic characteristics.[12] There is an Independence I occupation at the same site that the authors believe more ancient than the Pre-Dorset on grounds of altitude (21 vs. 19 m).[12] It appears probable that surveys or re-analysis of excavated material will reveal more Greenlandic Pre-Dorset occupations.[12]
A genetic study published in Science in August 2014 examined the remains of a Pre-Dorset individual buried in Rocky Point, Canada between c. 2140 BC and 1800 BC. The sample of mtDNA extracted belonged to haplogroup D4e.[13] The examined individual was found to be closely related to peoples of the Saqqaq culture and Dorset culture.[14] The ancestors of the Saqqaq, Pre-Dorset and Dorset probably migrated from Siberia to North America in a single migration around 4000 BC.[15]

The origin of the Sámi has been of research interest since at least the early 17th century. Initially, the Sámi were grouped together with ethnic Finns, due to the relative similarity between the Sámi languages and Finnish. When biological anthropology was developed in the 19th century, Sámi were seen instead to differ from surrounding peoples, which in turn led to the theory that the Sámi had developed as an isolated people during the Ice Age, when they would have overwintered by the Arctic Ocean. This theory was eventually abandoned.
New genetic research shows that the Sámi group has developed from several different directions at different times from many different hunter-gatherer peoples who moved across the Cap of the North, and that Sámi as well as Finnish-speaking and Scandinavian language-speaking farmers could mix with each other during the iron age. This has been interpreted to show that the Sámi culture has been so formed as a result of semi-nomadic reindeer husbandry which began around 2500 years ago, rather than as a strictly isolated group.[1] Nevertheless, the Sámi are a genetically unique population. By this it is meant not that the Sámi have unique genes, rather that certain gene variants are present at a different frequency in Sámi people than in other populations.
Europe has been populated by four prehistoric waves of migration, of which the first three waves helped form the Germanic and Nordic peoples, whilst a great deal of the Sámi and Finnish population have their roots in the fourth wave.[2]
The first wave consisted of hunter-gatherers. A culture which is believed to have overwintered the Ice Age in the refugium of Southern Europe reached Scandinavia from the South 13000 years ago. Traces of them appear in the Nordic population as mtDNA Haplogroup V (passed on via one's maternal grandmother's mother etc.), which is particularly common among the Sámi.[3] Another culture had overwintered in refugia in present-day Russia, and populated Scandinavia and Europe from the northeast after the end of the Ice Age, around 10000 years ago. This migration brought with it the mtDNA Haplogroup U5, which is particularly common among Sámi-speaking populations. These days it is uncommon among other European populations, where hunter-gatherers are thought to have been displaced by later farming communities to the areas on Europe's periphery. Variant U5b1b1 appears in the Nordic countries largely only among those with Sámi roots, but also occurs in North Africa (among Berbers), in Northernmost Asia and in Southern Europe.[4]
A second wave of immigration from outside of Europe consisted of Stone Age farmers from the Middle East, and a third wave consisted of Indo-European herders from the Eurasian Steppe, just before Bronze Age.
A fourth wave, from Siberia, reached Europe c.4000 years ago, constituting a significant addition to finns and Sámi.[5] The YDNA haplogroup (inherited from father to son etc.) N1c is especially common in Finland and among the Sámi, and is thought to have arrived in Fennoscandia from the east at least 1500 years ago. The N1c population brought with it metalwork from the Ananyino Culture, resulting in what is believed to be early Sámi metalwork using asbestos-ceramic, in Norrland and other areas.
During the 6th century, humans from the coastal tracts of Finland as well as central Sweden, mostly belonging to YDNA Haplogroups I1 and R1a which commonly occur among farmers, made contact with the Sámi. The Sámi numbered very few at that time, and were therefore threatened by the bottleneck effect, which resulted in uniquely autosomal DNA and an abnormal frequency of haplogroups, but the Sámi traded and, in time, mixed with the resident population of northern Fennoscandia, not least with the Proto-Germanic populations who began the colonisation of central Norrland in the Bronze Age, and northern Norrland since the Middle Ages. Among Swedes, those from Västerbotten show a somewhat higher frequency of haplogroups commonly found among speakers of Sámi and Finnish languages, and the population in Norrland shows more variety than other Germanic population groups.
According to Sámi tradition expressed by Johan Turi, the Sámi have always lived in Lappland, and did not arrive there from anywhere else. However, the origin of the Sámi has long been a matter of debate. In his book, Lapponia, from 1673, Johannes Schefferus devoted a chapter to the lineage of the Sámi. He opened by arguing at the Sámi couldn't originate from Swedes, "... When nothing is more unlike than a Lapp and a Swede. Not in body shape, temperament, clothing, why nothing between them is alike." Nor were they believed to have come from the Russians or the Norwegians. Schefferus concluded that it was most likely that the Sámi derived from the Finns, not least due to the apparent similarity between the Sámi and Finnish languages, but also as both peoples were so alike in temperament as well as appearance: "The Finns have black hair, wide faces and grim expressions, as do the Lapps". As well as question of clothing, Schefferus found that the difference between the Sámi and the Finns was insignificant.[6]
Not everyone agreed, however, on the physical similarities between the Sámi and the Finns. At the turn of the 20th century, the linguist K.B. Wiklund stated that the Sámi "are in anthropological regards just as removed from the Finns as ever from the Nordic people". (Wiklund meant in terms of physical anthropology, that is the study of phenotypic traits, which at the time was a large area of research). According to Wiklund, it was characteristic for the Sámi to have a short head and to be of short stature, to have black hair, brown eyes and a weak chin. In his opinion, nobody had been able to prove an anthropological relationship between the Sámi and any other people. He concluded that "the Lappish race" arose through a long time of total isolation from other groups of people.[7]
But how would such an isolation have occurred? When the botanist Rolf Nordhagen realised that he could show that relatively large parts of the Norwegian Arctic and Atlantic coasts were ice-free during the last Ice Age, this dovetailed remarkably with Wiklunds theory. Archaeologist Anders Nummedal had also found very old dwellings at Gurravárri in Áltá, in precisely the area Nordhagen had identified as having been ice-free. Thus Wiklund was finally able to solve "the mystery of the Lappish prehistory". The relics of the Komsa culture had to be traces of the Sámi people's forefathers, who spent the Ice Age in isolation by the coast. From there they would have spread further south as the ice melted.
As further evidence for this overwintering theory, K.B Wiklund pointed out that the modern population in Møre district and Sunnfjord in Norway were of an "anthropological type" which were astoundingly similar to the Sámi people, and that archaeologists in precisely this area had found relics of the so-called Fosna culture, which strongly resembled the Komsa culture. Wiklund argued that this was no mere coincidence. Even the so-called "pyttar" of Bohus-Malmön on the edge of the west coast of Sweden betrayed, according to Wiklund, "an unmistakable similarity to our Lapps". Wiklund claimed that right up to modern times there would have been relicts along the Norwegian and Swedish coasts of a population who survived the Ice Age.
K.B. Wiklund believed that the forefathers of the Sámi, who had overwintered on the Arctic coast, wouldn't have spoken Sámi, rather they would have spoken several unknown languages which he called "Paleo-Laplandic". He thought he found lingering traces of Paleo-Laplandic in the form of words which couldn't be attributed to other languages. For example, this included the Sámi word for water, čáhci, which is totally unlike Finnish vesi. The place-name Luleå was, according to Wiklund's understanding, also a Paleo-Laplandic relic. As it can be interpreted as "the watercourse that lies east of and underneath the mountain range" he argued that it seemed to have come from the west. Many other place names in Lappmarken, such as Sulitelma and Abisko, could, according to Wiklund, be relics from the Paleo-Laplandic era. Only later did the Sámi switch from speaking their original language to a Finno-Ugric language.
It ought to be noted that K.B. Wiklund's discourse about "the Lappish race" soon came to be considered outdated. As early as 1941 the physician Gunnar Dahlberg wrote that the thought "that Europeans originate from a number of pure races is an unsubstantiated hypothesis". Despite he himself working with the State Institute for Racial Biology, he drew the conclusion "racial biology [is] an expression of national prejudices and has nothing to do with science". According to Dahlberg, the differences between the Sámi and others probably depended on environmental factors.[8]
Nor did K.B. Wiklund's opinions last long. Only a few decades after his death, most researchers had abandoned the overwintering theory. The concept of the Paleo-Laplandic language, however, lived on to a certain extent. The ethnologist Phebe Fjellström pointed out in the 1980s the considerable differences between Northern Sámi and Southern Sámi and thought the ethnic group of the Sámi actually consisted of two different peoples. The first is suspected to have lived on the Norwegian coast after the Ice Age, and from there traversed the Scandinavian Mountains to Lapland and northern Ångermanland where they then existed as elk-hunters from 4000 BCE onwards. This group would have spoken Paleo-Laplandic. Around 2000 BCE a finno-ugric speaking people, the asbestos-ceramic culture, would have come to Fennoscandia from the east. When the two groups met, it is thought they may have merged into a single ethnic group which became the Sámi.[9]
Even scientists who do not support the idea of a double origin have connected the Sámi to the people of the asbestos-ceramic culture. In Sweden this type of discovery does not occur south of a boundary line which separates Upper Norrland from Central Norrland and Jämtland. North of this boundary the names for rivers and the oldest place names are often of Sámi or Finnish origin, while the equivalent names south of the boundary are Germanic in their origin. The people who made asbestos-ceramics would, according to this, be the forefathers of the Sámi, while those who lived further south would be the forefathers to the Scandinavians.[10]
When the father of physical anthropology, Johann Friedrich Blumenbach (1752-1840) categorised humans into five separate races, he placed all those who spoke Finno-Ugric languages into the Mongoloid race. Despite K.B. Wiklund's understanding that the Sámi were just as different from the Finns as from the Scandinavians, this classification remained long into the 1900s.[11]
When genetics was developed as a science in the mid-1950s, it became a tool in the research into the origin of the Sámi, and the results suggested that K.B. Wiklund had come closer to the truth than Blumenbach. Lars Beckman, who primarily studied blood types, called the Sámi a genetically unique population. His research indicated that the Sámi were not closely related to Asiatic-Mongoloid ethnicities. However, his studies could not explain the origin of the Sámi, but he did exclude the idea that their "urheimat" was somewhere in Asia or Eastern Europe.[12][13] His studies of the frequency of so-called Sámi marker-genes indicated that between a quarter and a third of the genetic material of the populations of Västerbotten county and Norrbotten county have Sámi origins.[14]
Later types of genetic study, particularly of mitochondrial DNA (which is only passed down by the mother) and Y chromosomes (which are only inherited from father to son) have provided new information, yet it can often be difficult to interpret. The two types of mitochondrial DNA which are dominant among the Sámi, haplogroups U5b1b and V, are believed to originate in Western Europe. Even those Y chromosome variants which are found among the Sámi indicate a European origin. However, no genetic similarity with present-day Siberian peoples has been proved. The research group who published these results in 2004 believed that the distinctive genetic character of the Sámi is best explained by their ancestors having been a small, defined group of Europeans.[15]
It has, however, clearly been demonstrated that there is a genetic relationship to Siberia, insofar as nowhere except north-easternmost Europe and northernmost Scandinavia is there so high a frequency as in western Siberia of a particular genetic marker whose very highest area of frequency is found in the border regions between Europe and Siberia.
While mapping out human mitochondrial DNA in its entirety, one of the aforementioned variants, U5b1b, was found in the mitochondrial DNA of three Sámi people, a Berber from Algeria, a Fulani person from Senegal, as well as a Yakut person from northeast Siberia. That the Fulani and Berbers have had contact with each other was already known, but the results were generally a surprise for the research group, particularly that this variant only seems to have emerged 9000 years ago. One possible explanation could be that the forefathers of all of these ethnic groups originated in Southwestern Europe, on the border between France and Spain, from where hunter-gatherer tribes spread out in different directions after the last Ice Age.
A Swedish study from 2007 has concluded that the haplogroups U5b1b and V (those which dominate mitochondrial DNA among Sámi from northernmost Sweden, Norway and Finland) likely came to the area very soon after the Ice Age ended. They may have come either from the European continent, or from the Volga-Ural region of Russia, or from both directions. Another type of mitochondrial DNA, haplogroup Z, occurs at low rates both among Sámi and North Asian ethnicities, yet is otherwise absent from Europe. Researchers have interpreted this as a sign that the North Sámi group mixed with another from the east as recently as 2700 years ago.[16]
The Swedish genetic study of 2007 can be compared with new archaeological discoveries, which are thought to show that northernmost Sweden was populated from the north immediately after the Ice Age. The Komsa culture has thus become central again as the origin of northern Sweden's earliest inhabitants. Researchers no longer believe, however, that the people who left traces at Komsa lived out the Ice Age on the Northern Norwegian coast, rather that the coastal area was quickly colonised from the south during the final stages of the Ice Age. When these people followed the melting ice southwards across the Tundra, they eventually encountered the people who had colonised Finland from the east. Thus, the genetic heritage of the Sámi, which is primarily European but is thought to have come from both east and west, can be explained.
The time of the later migration, which geneticists believe they can see traces of from 2700 years ago, can be compared with that connection which was made before by many archaeologists between the Sámi and the people who made asbestos ceramics. Asbestos ceramics are found in dwellings from circa 3900 BCE to 1300 BCE in Finland, and from 1500 BCE to 1000 CE in Scandinavia.
One theory on the origin the Sámi was that they originate from the hunter-gatherer culture known by archaeologists as the Pitted Ware culture. However, modern genetic studies have shown this not to be the case.

Paleoanthropology or paleo-anthropology is a branch of paleontology and anthropology which seeks to understand the early development of anatomically modern humans, a process known as hominization, through the reconstruction of evolutionary kinship lines within the family Hominidae, working from biological evidence (such as petrified skeletal remains, bone fragments, footprints) and cultural evidence (such as stone tools, artifacts, and settlement localities).[1][2]
The field draws from and combines primatology, paleontology, biological anthropology, and cultural anthropology. As technologies and methods advance, genetics plays an ever-increasing role, in particular to examine and compare DNA structure as a vital tool of research of the evolutionary kinship lines of related species and genera.
The term paleoanthropology derives from Greek palaiós (παλαιός) "old, ancient", ánthrōpos (ἄνθρωπος) "man, human" and the suffix -logía (-λογία) "study of".
Hominoids are a primate superfamily, the hominid family is currently considered to comprise both the great ape lineages and human lineages within the hominoid superfamily. The "Homininae" comprise both the human lineages and the African ape lineages. The term "African apes" refers only to chimpanzees and gorillas.[3] The terminology of the immediate biological family is currently in flux. The term "hominin" refers to any genus in the human tribe (Hominini), of which Homo sapiens (modern humans) is the only living specimen.[4][5]
In 1758 Carl Linnaeus introduced the name Homo sapiens as a species name in the 10th edition of his work Systema Naturae although without a scientific description of the species-specific characteristics.[6] Since the great apes were considered the closest relatives of human beings, based on morphological similarity, in the 19th century, it was speculated that the closest living relatives to humans were chimpanzees (genus Pan) and gorilla (genus Gorilla), and based on the natural range of these creatures, it was surmised that humans shared a common ancestor with African apes and that fossils of these ancestors would ultimately be found in Africa.[6][7]
The science arguably began in the late 19th century when important discoveries occurred that led to the study of human evolution. The discovery of the Neanderthal in Germany, Thomas Huxley's Evidence as to Man's Place in Nature, and Charles Darwin's The Descent of Man were both important to early paleoanthropological research.
The modern field of paleoanthropology began in the 19th century with the discovery of "Neanderthal man" (the eponymous skeleton was found in 1856, but there had been finds elsewhere since 1830), and with evidence of so-called cave men.  The idea that humans are similar to certain great apes had been obvious to people for some time, but the idea of the biological evolution of species in general was not legitimized until after Charles Darwin published On the Origin of Species in 1859.
Though Darwin's first book on evolution did not address the specific question of human evolution—"light will be thrown on the origin of man and his history," was all Darwin wrote on the subject—the implications of evolutionary theory were clear to contemporary readers.
Debates between Thomas Huxley and Richard Owen focused on the idea of human evolution. Huxley convincingly illustrated many of the similarities and differences between humans and apes in his 1863 book Evidence as to Man's Place in Nature. By the time Darwin published his own book on the subject, Descent of Man, it was already a well-known interpretation of his theory—and the interpretation which made the theory highly controversial. Even many of Darwin's original supporters (such as Alfred Russel Wallace and Charles Lyell) balked at the idea that human beings could have evolved their apparently boundless mental capacities and moral sensibilities through natural selection.
Prior to the general acceptance of Africa as the root of genus Homo, 19th-century naturalists sought the origin of humans in Asia.  So-called "dragon bones" (fossil bones and teeth) from Chinese apothecary shops were known, but it was not until the early 20th century that German paleontologist, Max Schlosser, first described a single human tooth from Beijing. Although Schlosser (1903) was very cautious, identifying the tooth only as "?Anthropoide g. et sp. indet?," he was hopeful that future work would discover a new anthropoid in China.
Eleven years later, the Swedish geologist Johan Gunnar Andersson was sent to China as a mining advisor and soon developed an interest in "dragon bones". It was he who, in 1918, discovered the sites around Zhoukoudian, a village about 50 kilometers southwest of Beijing. However, because of the sparse nature of the initial finds, the site was abandoned.
Work did not resume until 1921, when the Austrian paleontologist, Otto Zdansky, fresh with his doctoral degree from Vienna, came to Beijing to work for Andersson. Zdansky conducted short-term excavations at Locality 1 in 1921 and 1923, and recovered only two teeth of significance (one premolar and one molar) that he subsequently described, cautiously, as "?Homo sp." (Zdansky, 1927). With that done, Zdansky returned to Austria and suspended all fieldwork.
News of the fossil hominin teeth delighted the scientific community in Beijing, and plans for developing a larger, more systematic project at Zhoukoudian were soon formulated. At the epicenter of excitement was Davidson Black, a Canadian-born anatomist working at Peking Union Medical College. Black shared Andersson’s interest, as well as his view that central Asia was a promising home for early humankind. In late 1926, Black submitted a proposal to the Rockefeller Foundation seeking financial support for systematic excavation at Zhoukoudian and the establishment of an institute for the study of human biology in China.
The Zhoukoudian Project came into existence in the spring of 1927, and two years later, the Cenozoic Research Laboratory of the Geological Survey of China was formally established. Being the first institution of its kind, the Cenozoic Laboratory opened up new avenues for the study of paleogeology and paleontology in China. The Laboratory was the precursor of the Institute of Vertebrate Paleontology and Paleoanthropology (IVPP) of the Chinese Academy of Science, which took its modern form after 1949.
The first of the major project finds are attributed to the young Swedish paleontologist, Anders Birger Bohlin, then serving as the field advisor at Zhoukoudian. He recovered a left lower molar that Black (1927) identified as unmistakably human (it compared favorably to the previous find made by Zdansky), and subsequently coined it Sinanthropus pekinensis.[8] The news was at first met with skepticism, and many scholars had reservations that a single tooth was sufficient to justify the naming of a new type of early hominin. Yet within a little more than two years, in the winter of 1929, Pei Wenzhong, then the field director at Zhoukoudian, unearthed the first complete calvaria of Peking Man. Twenty-seven years after Schlosser’s initial description, the antiquity of early humans in East Asia was no longer a speculation, but a reality.
Excavations continued at the site and remained fruitful until the outbreak of the Second Sino-Japanese War in 1937. The decade-long research yielded a wealth of faunal and lithic materials, as well as hominin fossils. These included 5 more complete calvaria, 9 large cranial fragments, 6 facial fragments, 14 partial mandibles, 147 isolated teeth, and 11 postcranial elements—estimated to represent as least 40 individuals. Evidence of fire, marked by ash lenses and burned bones and stones, were apparently also present,[9] although recent studies have challenged this view.[10] Franz Weidenreich came to Beijing soon after Black’s untimely death in 1934, and took charge of the study of the hominin specimens.
Following the loss of the Peking Man materials in late 1941, scientific endeavors at Zhoukoudian slowed, primarily because of lack of funding. Frantic search for the missing fossils took place, and continued well into the 1950s. After the establishment of the People’s Republic of China in 1949, excavations resumed at Zhoukoudian. But with political instability and social unrest brewing in China, beginning in 1966, and major discoveries at Olduvai Gorge and East Turkana (Koobi Fora), the paleoanthropological spotlight shifted westward to East Africa. Although China re-opened its doors to the West in the late 1970s, national policy calling for self-reliance, coupled with a widened language barrier, thwarted all the possibilities of renewed scientific relationships. Indeed, Harvard anthropologist K. C. Chang noted, "international collaboration (in developing nations very often a disguise for Western domination) became a thing of the past" (1977: 139).
The first paleoanthropological find made in Africa was the 1921 discovery of the Kabwe 1 skull at Kabwe (Broken Hill), Zambia. Initially, this specimen was named Homo rhodesiensis; however, today it is considered part of the species Homo heidelbergensis.[11]
In 1924 in a limestone quarry at Taung, Professor Raymond Dart discovered a remarkably well-preserved juvenile specimen (face and brain endocast), which he named Australopithecus africanus  (Australopithecus meaning "Southern Ape"). Although the brain was small (410 cm3), its shape was rounded, unlike the brain shape of chimpanzees and gorillas, and more like the shape seen in modern humans. In addition, the specimen exhibited short canine teeth, and the anterior placement of the foramen magnum was more like the placement seen in modern humans than the placement seen in chimpanzees and gorillas, suggesting that this species was bipedal.
All of these traits convinced Dart that the Taung child was a bipedal human ancestor, a transitional form between ape and human. However, Dart's conclusions were largely ignored for decades, as the prevailing view of the time was that a large brain evolved before bipedality. It took the discovery of additional australopith fossils in Africa that resembled his specimen, and the rejection of the Piltdown Man hoax, for Dart's claims to be taken seriously.
In the 1930s, paleontologist Robert Broom discovered and described a new species at Kromdraai, South Africa. Although similar in some ways to Dart's Australopithecus africanus, Broom's specimen had much larger cheek teeth. Because of this difference, Broom named his specimen Paranthropus robustus, using a new genus name. In doing so, he established the practice of grouping gracile australopiths in the genus Australopithecus and robust australopiths in the genus Paranthropus. During the 1960s, the robust variety was commonly moved into Australopithecus. A more recent consensus has been to return to the original classification of Paranthropus as a separate genus.[12]
The second half of the twentieth century saw a significant increase in the number of paleoanthropological finds made in Africa. Many of these finds were associated with the work of the Leakey family in eastern Africa. In 1959, Mary Leakey's discovery of the Zinj fossin (OH 5) at Olduvai Gorge, Tanzania, led to the identification of a new species, Paranthropus boisei.[13] In 1960, the Leakeys discovered the fossil OH 7, also at Olduvai Gorge, and assigned it to a new species, Homo habilis. In 1972, Bernard Ngeneo, a fieldworker working for Richard Leakey, discovered the fossil KNM-ER 1470 near Lake Turkana in Kenya. KNM-ER 1470 has been interpreted as either a distinct species, Homo rudolfensis, or alternatively as evidence of sexual dimorphism in Homo habilis.[12] In 1967, Richard Leakey reported the earliest definitive examples of anatomically modern Homo sapiens from the site of Omo Kibish in Ethiopia, known as the Omo remains.[14] In the late 1970s, Mary Leakey excavated the famous Laetoli footprints in Tanzania, which demonstrated the antiquity of bipedality in the human lineage.[12] In 1985, Richard Leakey and Alan Walker discovered a specimen they called the Black Skull, found near Lake Turkana. This specimen was assigned to another species, Paranthropus aethiopicus.[15] In 1994, a team led by Meave Leakey announced a new species, Australopithecus anamensis, based on specimens found near Lake Turkana.[12]
Numerous other researchers have made important discoveries in eastern Africa. Possibly the most famous is the Lucy skeleton, discovered in 1973 by Donald Johanson and Maurice Taieb in Ethiopia's Afar Triangle at the site of Hadar. On the basis of this skeleton and subsequent discoveries, the researchers came up with a new species, Australopithecus afarensis.[12] In 1975, Colin Groves and Vratislav Mazák announced a new species of human they called Homo ergaster. Homo ergaster specimens have been found at numerous sites in eastern and southern Africa.[12] In 1994, Tim D. White announced a new species, Ardipithecus ramidus, based on fossils from Ethiopia.[16]
In 1999, two new species were announced. Berhane Asfaw and Tim D. White named Australopithecus garhi based on specimens discovered in Ethiopia's Awash valley. Meave Leakey announced a new species, Kenyanthropus platyops, based on the cranium KNM-WT 40000 from Lake Turkana.[12]
In the 21st century, numerous fossils have been found that add to current knowledge of existing species. For example, in 2001, Zeresenay Alemseged discovered an Australopithecus afarensis child fossil, called Selam, from the site of Dikika in the Afar region of Ethiopia. This find is particularly important because the fossil included a preserved hyoid bone, something rarely found in other paleoanthropological fossils but important for understanding the evolution of speech capacities.[11][12]
Two new species from southern Africa have been discovered and described in recent years. In 2008, a team led by Lee Berger announced a new species, Australopithecus sediba, based on fossils they had discovered in Malapa cave in South Africa.[12] In 2015, a team also led by Lee Berger announced another species, Homo naledi, based on fossils representing 15 individuals from the Rising Star Cave system in South Africa.[17]
New species have also been found in eastern Africa. In 2000, Brigitte Senut and Martin Pickford described the species Orrorin tugenensis, based on fossils they found in Kenya. In 2004, Yohannes Haile-Selassie announced that some specimens previously labeled as Ardipithecus ramidus made up a different species, Ardipithecus kadabba.[12] In 2015, Haile-Selassie announced another new species, Australopithecus deyiremeda, though some scholars are skeptical that the associated fossils truly represent a unique species.[18]
Although most hominin fossils from Africa have been found in eastern and southern Africa, there are a few exceptions. One is Sahelanthropus tchadensis, discovered in the central African country of Chad in 2002. This find is important because it widens the assumed geographic range of early hominins.[12]

The Kebaran culture, also known as the 'Early Near East Epipalaeolithic', is an archaeological culture of the Eastern Mediterranean dating to c. 23,000 to 15,000 Before Present (BP). Its type site is Kebara Cave, south of Haifa. The Kebaran was produced by a highly mobile nomadic population, composed of hunters and gatherers in the Levant and Sinai areas who used microlithic tools.
The Kebaran is the first phase of the Epipalaeolithic in the Levant.[1] Kebaran stone tool assemblages are characterized by small, geometric microliths, and are thought to have lacked the specialized grinders and pounders found in later Near Eastern cultures. Small stone tools called microliths and retouched bladelets can be found for the first time. The microliths of this culture period differ greatly from the Aurignacian artifacts.
The Kebaran is preceded by the final phase of the Upper Paleolithic Levantine Aurignacian (also known as the Athlitian or Antelian)[2][3] and followed by the proto-agrarian Natufian culture of the Epipalaeolithic. The appearance of the Kebaran culture, of microlithic type implies a significant rupture in the cultural continuity of Levantine Upper Paleolithic.  The Kebaran culture, with its use of microliths, is associated with the use of the bow and arrow and the domestication of the dog.[4]  The Kebaran is also characterised by the earliest collecting of wild cereals, known due to the uncovering of grain grinding tools. It was the first step towards the Neolithic Revolution. The Kebaran people are believed to have practiced dispersal to upland environments in the summer, and aggregation in caves and rock shelters near lowland lakes in the winter. This diversity of environments may be the reason for the variety of tools found in their kits.
Situated in the Terminal Pleistocene, the Kebaran is classified as an Epipalaeolithic society.  They are generally thought to have been ancestral to the later Natufian culture that occupied much of the same range,[5] who advanced the use of wild grains, building on the Kebaran traits to acquire some symptoms of permanent settlements, agriculture, and hints of civilization.
In the prehistoric site of Ein Gev, the skeleton of a 30-40 year old woman associated with the Kebaran was discovered. The morphological characteristics assigned the individual to a Proto-Mediterranean population, being very similar to the Natufians.[6]
Evidence for symbolic behavior of Late Pleistocene foragers in the Levant has been found in engraved limestone plaquettes from the Epipaleolithic open-air site Ein Qashish South in the Jezreel Valley, Israel.[7] The engravings were uncovered in Kebaran and Geometric Kebaran deposits (ca. 23,000 and ca. 16,500 BP), and include the image of a bird, the first figurative representation known so far from a pre-Natufian Epipaleolithic site in the region, together with geometric motifs such as chevrons, cross-hatchings and ladders.[7] Some of the engravings closely resemble roughly contemporary European finds, and may be interpreted as "systems of notations" or "artificial memory systems" related to the timing of seasonal resources and related important events for nomadic groups.[7]
Similar looking signs and patterns are well known from the context of the local Natufia, a final Epipaleolithic period when sedentary or semi-sedentary foragers started practicing agriculture.[7]
The engravings found in Ein Qashish South involve symbolic conceptualization.[7] They suggest that the figurative and non-figurative images comprise a coherent assemblage of symbols that might have been applied in order to store, share and transmit information related to the social activities and the subsistence of mobile bands.[7] They also suggest a level of social complexity in pre-Natufian foragers in the Levant.[7] The apparent similarity in graphics throughout the Late Pleistocene world and the mode of their application support the possibility that symbolic behavior has a common and much earlier origin.[7]

The Prehistory of Siberia is marked by several archaeologically distinct cultures. In the Chalcolithic, the cultures of western and southern Siberia were pastoralists, while the eastern taiga and the tundra were dominated by hunter-gatherers until the Late Middle Ages and even beyond. Substantial changes in society, economics and art indicate the development of nomadism in the Central Asian steppes in the first millennium BC.
Scholarly research of the archaeological background of the region between the Urals and the Pacific began in the reign of Peter the Great (1682-1725), who ordered the collection of Scythian gold hoards and thereby rescued the contents of several robbed graves before they were melted down. During his reign, several expeditions were charged with the scientific, anthropological and linguistic research of Siberia, including the Second Kamchatka Expedition of the Danish-born Russian Vitus Bering (1733-1743). Scholars also took an interest in archaeology and carried out the first archaeological excavations of Siberian kurgans. After a temporary reduction of interest in the first half of the nineteenth century, archaeological research in Siberia reached new heights in the late nineteenth century. Excavations were particularly intense in South Siberia and Central Asia. The results of the October Revolution 1917 created different, often restricted, conditions for archaeological research, but led to even larger projects, especially rescue excavations as a result of gigantic building projects. Eventually, even remote areas of the Soviet Union such as Sakha and Chukotka, were archaeologically explored. After the Second World War, these developments continued. Following the Collapse of the Soviet Union in 1991, much more intensive collaboration with the west became possible.
Siberia is characterised by a great deal of variety in climate, vegetation, and landscape. In the west, it is bordered by the Ural Mountains. From there, the west Siberian lowlands extend to the east, all the way to the river Yenisei. Beyond this are the central Siberian highlands which are bordered on the east by the basin of the Lena River, beyond that are the northeast Siberian highlands. Siberia is bordered on the south by a rough chain of mountains and to the southwest by the hills of the Kazakh border. The climate is very variable. Yakutia, northeast of the Lena, is among the coldest places on Earth, but every year temperatures may vary for more than 70 °C, from as low as −50 °C in winter to over +20 °C in summer. The rainfall is very low. This is true of the southwest as well, where steppes, deserts and semi-deserts border on one another.
Agriculture is only possible in Siberia without artificial irrigation today between 50° and 60° north. The climatic situation is responsible for the different biomes of the region. In the northernmost section, there is tundra with minimal vegetation. The largest part of Siberia, aside from the mountainous regions, is taiga, northern coniferous forests. In the southwest this becomes forested steppe, and even further south it transitions to grass steppes and the central Asian desert. Before the beginning of the 
Holocene about 12,000 years ago, the situation was different. During the Weichselian glaciation (from before 115,000 years ago until 15,000 years ago), the tundra extended much further south and an ice sheet covered the Urals and the area to the east of the lower Yenisei.
Late Paleolithic southern Siberians appear to be related to Paleolithic Europeans and the Paleolithic Jōmon people of Japan.[2] Various scholars point out similarities between the Jōmon and Paleolithic and Bronze Age Siberians.[3] A genetic analyses of HLA I and HLA II genes as well as HLA-A, -B, and -DRB1 gene frequencies links the Ainu people and some Indigenous peoples of the Americas, especially populations on the Pacific Northwest Coast such as Tlingit, to Paleolithic southern Siberians.[4]
Finds from the Lower Paleolithic appear to be attested between east Kazakhstan and Altai. The burial of a Neanderthal child found in 1938 shows similarities with the Mousterian of Iraq and Iran. In the Upper Palaeolithic, by contrast, most remains are found in the Urals, where, among other things, rock carvings depicting mammoths are found, in Altai, on the upper Yenissei, west of Lake Baikal and around 25,000 on the shore of the Laptev Sea, north of the Arctic Circle.[5] The remains of huts have been found in the settlement of Mal'ta near Irkutsk. Sculptures of animals and women (Venus figurines) recall the European Upper Palaeolithic.[6] The Siberian Palaeolithic continues well into the European Mesolithic. In the postglacial period, the taiga developed. Microliths, which are common elsewhere, have not been found.
In North Asia, the Neolithic (c. 5500–3400 BC)[7] is mostly a chronological term, since there is no evidence for agriculture or even pastoralism in Siberia during the central European Neolithic. However, the Neolithic cultures of North Asia are distinguished from the preceding Mesolithic cultures and far more visible as a result of the introduction of pottery.
Southwest Siberia reached a Neolithic cultural level during the Chalcolithic, which began here towards the end of the fourth millennium BC, which roughly coincided with the introduction of copper–working. In the northern and eastern regions, there is no detectable change.
In the second half of the third millennium BC, bronzeworking reached the cultures of western Siberia. Chalcolithic groups in the eastern Ural foothills developed the so-called Andronovo culture, which took various local forms. The settlements of Arkaim, Olgino and Sintashta are particularly notable as the earliest evidence for urbanisation in Siberia. In the valleys of the Ob and Irtysh the same ceramic cultures attested there during the neolithic continue; the changes in the Baikal region and Yakutia were very slight.
In the middle Bronze Age (c. 1800–1500 BC), the west Siberian Andronovo culture expanded markedly to the east and even reached the Yenissei valley. In all the local forms of the Andronovo culture, homogenous ceramics are found, which also extended to the cultures on the Ob. Here, however, unique neolithic ceramic traditions were maintained as well.
With the beginning of the late Bronze Age (c. 1500–800 BC), crucial cultural developments took place in southern Siberia. The Andronovo culture dissolved; its southern successors produced an entirely new form of pottery, with bulbous ornamental elements. At the same time the southern cultures also developed new forms of bronze working, probably as a result of influence from the southeast. These changes were especially significant in the Baikal region. There, the Chalcolithic material culture which had continued up to this time was replaced by a bronze-working pastoralist culture. There and in Yakutia, bronze was only used as a material for the first time at this point.
The Ymyakhtakh culture (c. 2200–1300 BC) was a Late Neolithic culture of Siberia, with a very large archaeological horizon. Its origins seem to be in the Lena River basin of Yakutia, and also along the Yenisei River. From there it spread both to the east and to the west.[8]
The cultural continuity on the Ob continued in the first millennium BC, as the Iron Age began in Siberia; the local ceramic style continues there even in this period. A much larger break occurred in the central Asian steppe: the sedentary, predominantly pastoralist society of the late Bronze Age is replaced by the mobile horse nomads who would continue to dominate this region until modern times. The mobility, which the new cultural form enabled, unleashed a powerful dynamic, since henceforth the people of Central Asia were able to move across the steppe in great numbers. The neighbouring sedentary cultures were not unaffected by this development. Ancient China was threatened by the Xiongnu and their neighbours, the ancient states of modern Iran were opposed by the Massagetae and Sakas, and the Roman Empire eventually was confronted by the Huns. The social changes are clearly indicated in the archaeological finds. Settlements are no longer found, members of the new elite were buried in richly furnished kurgans and completely new forms of art developed.
In the damper steppes to the north, the sedentary pastoralist culture of the late Bronze Age developed under the influence of the material culture of the nomads. Proto-urban settlements like Tshitsha form the late Irmen culture in west Siberia and the settlements in the north of the Xiongnu cultural area.
In many places the transition to later periods remains problematic due to the lack of archaeological evidence. Nevertheless, some generalisations are possible. In the Central Asian steppes, Turkic groups become detectable sometime in the 5th century; over the following centuries, they expand to the north and west until eventually they brought the whole of southern Siberia under their control. The area further north, where the speakers of Uralic and Paleosiberian languages were located is still poorly known. The next clear break in the history of Siberia is the Russian expansion into the east which began in the 16th century and only concluded in the 19th century. This process marks the beginning of modernity in Siberia
Reliable historical evidence for the area first appears at the beginning of the first millennium BC, with sources from the Near East. Greek and Chinese sources are also available from slightly later. Thus, certain statements about the peoples and languages of the region are only possible from the Iron Age. For earlier times and the northern part of Siberia, only archaeological evidence is available. Some theories, like the Kurgan hypothesis of Marija Gimbutas, attempt to relate hypothetical language families to archaeological cultures, but this is a highly uncertain procedure.
Sure statements are possible only since the first millennium BC, when neighbouring literate cultures came into contact with the people of the steppe. In the steppes north of the Black Sea and east of the Caspian Sea, Greek, Assyrian and Persian sources attest to horse nomads, which can be identified as speakers of Iranian languages. The first reports from ancient China of the nomads north of China date from the same period. Along with various unidentified groups from Shang and Zhou dynasty texts, the Xiongnu are worthy of mention. Based on personal names and titles transmitted by the Chinese sources, different scholars have attempted to identify the language of the Xiongnu as an early Turkic language, a proto-Mongolic language or a Yeniseian language. At the beginning of the early Middle Ages, the Iranian peoples disappeared and in their place Turkic peoples expanded across the region between the eastern edge of Europe and northeastern Siberia. In the areas to the north of the Asiatic steppes, speakers of Uralic and Palaeo-Siberian languages are suspected to have been settled; in the Middle Ages, Turkic peoples appear here as well, but their prehistoric extent is not clear.
The earliest known archaeological finds from Siberia date to the Lower Palaeolithic. In various places in West Siberia, the Baikal region and Yakutia, storage places from early Neolithic times have been found, which often remained in use for centuries. Alongside tent settlements which leave no traces in the ground, there were also huts, often dug slightly into the ground, whose walls and roofs were made of animal bone and reindeer antlers. Tools and weapons were mostly made from flint, slate and bone, with few discernable differences between them despite their immense chronological and geographical scope. In some settlements, early artworks have been found, which consist of human, animal and abstract sculptures and carvings. The Palaeolithic and Mesolithic inhabitants of Siberia were hunter-gatherers, whose prey consisted of mammoths and reindeer, and occasionally fish as well. In the 6th millennium BC, pottery spread across the whole of Siberia, which scholars treat as the beginning of the Siberian neolithic. Unlike Europe and the Near East, this event did not mark a major change in lifestyle, economy or culture.
The prehistoric inhabitants of the vast areas of taiga and tundra east of the Yenissei and north of Baikal differ in many ways from the prehistoric cultures of the other parts of north Asia. There is stronger evidence than usual for settlement continuity here from the Mesolithic until the second half of the first millennium AD, when the not yet entirely clear transition to the Medieval period occurred. Despite the enormous geographic extent of the area, only minor local differences are visible, indicating very mobile, nomadic inhabitants. The earliest culture in Yakutia to make ceramic was the Syalakh culture, which have been dated by radiocarbon dating to the 5th millennium BC. They are known from a type of pottery decorated with net patterns and bands of puncture marks. Their remains include weapons and tools made from flint and bone. A series of settlements, some of which were already in use in the Mesolithic, are known, at which the finds are limited to hearths and pits, while remains of buildings are entirely absent. Thus, the people responsible for the Syalakh culture were nomads who survived from hunting and fishing and inhabited certain spots on a seasonal basis.
This culture gradually transitions into the Belkachi culture (named after the Belkachi settlement in Yakutia) without any clear break. Their pottery features cord decorations, stripes, zigzag lines and such like. Their dead were buried on their backs in earthen graves. Otherwise, no major differences from the preceding culture are visible.
The Ymyyakhtakh culture (2200–1300 BC) is marked out by a new kind of "waffle ceramic", whose upper side is decorated with textile impressions and takes on a waffle-like appearance as a result. Towards the end of the 2nd millennium BC, bronzeworking reached Yakutia. Ymyyakhtakh settlements already feature bronze artifacts.
Ust-Mil culture [de] followed next. In the first millennium BC, an independent culture developed on the Taymyr Peninsula, which shared its basic features with the Ust-Mil culture. The Iron Age began in Yakutia around the 5th century BC, but apart from the adoption of iron weapons and tools it does not mark a major change in the material culture.
The cultural development in Neolithic and Chalcolithic Baikal region, where the circumstances were similar to those in Yakutia until the appearance of the late Bronze Age Slab Grave culture. Here too there were some multi-layer storage places which extended back to the Mesolithic period, with hearths, waste pits and storage pits but no remains of buildings. The pottery was similar to that in Yakutia and shows a more or less parallel course of development. The burials are mostly stretched out on their backs, but often the graves were covered by stone slabs. An exception is the area of the Onon River, where crouching graves are found. Grave goods and bone finds indicate that the inhabitants lived by hunting bears, fish, elk and beavers, as well as some fish. The importance of the hunt to their culture is indicated by carvings on bones and rock faces. Their main subjects are people hunting animals. Unlike in Yakutia, pastoralism was adopted in the Baikal region before the Middle Ages; the earliest evidence comes from the Chalcolithic Glazkov culture.
From the Neolithic or early in the Chalcolithic, sedentary groups in which pastoralism played an important economic role developed in southwestern Siberia. The transition to the new economic system and to sedentarism was very smooth. Subsequently, it spread to the Baikal region, where the influence of northern China may also have played a role.
Through the whole Siberian prehistoric period from the Neolithic until the Iron Age, there are a very limited range of ceramic types. The vast majority of ceramic finds are round bulbous vessels, often with folded edges. In the Neolithic they mostly had concave bases, while later flat bases became more common. In the eastern part of the west Siberian forest steppe, on the Ob, Irtysh and Yenissei, decoration consisted of comb patterns, puncture rows and dimples, arranged in long series or fields (right image). In the course of the dramatic growth of the Andronovo culture in the middle Bronze Age, another type spread through the region. Examples of it are decorated with meander bands, herringbone patterns and triangles (left image). These ceramic types endured even into the Iron Age in west Siberia, yet a stark decline in decoration is observable, contemporary with the entrance of the Scythian and Hunnic Sarmatian nomads. This applies even to the nomadic cultures themselves.
Excepting the abstract decoration of the pottery, which has been dealt with above, artistic products are found in south Siberia only in the early Bronze Age.
Artefacts from the Karakol culture in Altai and the Okunev culture in the middle Yenissei include anthropomorphic motifs on stone plates and steles; the Okunev culture also produced humanoid sculptures. The art of the Samus culture of the upper Ob is related to these. In addition to humanoid sculptures and human heads engraved in pottery, the Samus culture also produced ceramic phalli and animal heads. Members of the nearby Susgun culture produced humanoid figures in bone. The only artistic products of the late Bronze Age are early South Siberian deer stones, stone steles decorated with images of deer, which were subsequently imitated by Scythian art.
The early Iron Age animal style of the south Siberian horse nomads only influenced the cultures of the west Siberian lowlands a little. An entirely unique style was developed by the Kulaika culture and its neighbours in the middle and lower Ob. Here bronze figures of animals and people were manufactured, in which eagles and bears played a particularly important role.
The predominant building material in prehistoric north Asia was wood; stone was used for foundations at most. Most houses were tight structures, sunk less than 1 metre into the earth and had a rectangular or circular ground plan; oval or polygonal ground plans occur rarely. The structure of the roofs may have been pitched wooden constructions or saddle roofs. In many cultures, a small, corridor-like porch was built in front of the entrance. One or more hearths were found in the inner house.
Floodplains and lakesides were the preferred settlement locations. Settlements could take entirely different forms in different cultures; small groups of houses, large unfortified settlements, fortified city-like settlements and elevated fortress complexes are all found. Small village-like groups of houses are found in great numbers in all the sedentary cultures. In some cases, such as the chalcolithic settlement of Botai on the Ishim river,[9] settlements experienced substantial expansion. It was not unusual for larger settlements to have walls and extramural graveyards, as in the case of the west Siberian settlements of Sintashta and Tshitsha.[10] The inner space of these city-like settlements was densely and regularly packed with rectangular houses, indicating a form of town planning. The fortified settlements in elevated locations, like those located in the Minusinsk Hollow and Khakassia in the bronze and Iron Ages are usually distinguished from these settlements by their small size. Their purpose is still unclear; they may have been temporary refuges, the seats of elites, or sanctuaries.
Unlike the nomadic groups of earlier times and of northeastern Siberia, complex social structures can be detected in sedentary groups in West Siberia in the early Bronze Age. Their existence is indicated by the city-like settlements and by the social differentiation indicated by differences in their grave goods. In the middle Bronze Age, this development seems to have reversed and social differentiation is only detectable again in the late Bronze Age and the Iron Age. Since the northern part of west Siberia was unknown to ancient literate cultures and the ancient inhabitants of this region have left no literary source material themselves, it is very difficult to make detailed statements about their society. In reference to the settled populations of the Wusun, who settled in the Tianshan and Zhetysu, Chinese sources indicate the existence of a king and several nobles.[11]
The economy of the sedentary population in prehistoric Siberia was dominated by pastoralism. Cattle were intensively farmed in all cultures, as were sheep and goats. The raising of horses became very significant in western Siberia, particularly with the beginning of the Iron Age. A somewhat different image is given by the finds from the Xiongnu, who had also domesticated pigs and dogs. Hunting and fishing were initially an important supplement, but lost a lot of their significance over time.
Based on important tool remains and the possible remains of irrigation systems, a wide use of agriculture has been proposed by many researchers, but other scholars state that remains of cereals and other clear evidence are only found in the southernmost cultures, as remains of the Wusun of the Tianshan and Zhetysu. There, as in the northern parts of the Xiongnu territory, millet was cultivated and traces of wheat and rice have also been found. Millet seeds are also found in graves from Tuva, possibly indicating that a hitherto unknown population of settled agriculturalists, who might have been responsible for the area's metal-working, existed there alongside the horse nomads.[12]
From the Chalcolithic, ore mining and metallurgy also occurred. This is shown by finds of slag, tools and workshops in various cultural contexts.
The burial customs of the sedentary societies were characterised by great variation. In the west Siberian chalcolithic, simple flat graves are found, in which the corpse is laid flat on its back. In the early Bronze Age, kurgans were erected for the first time, whose inhabitants were members of a newly developed warrior class (to judge from the grave goods interred with them) and were not buried in simple pits, but in wooden or stone structures. Already in the middle Bronze Age phase of the Andronovo culture, kurgans are found, but without differentiation of their grave goods. The corpse was interred in a crouched position or cremated. In the somewhat later Karasuk culture on the middle Yenissei, the tombs include rectangular stone enclosures, which were further developed into the stone-cornered kurgans characteristic of the area by the Tagar culture in the Iron Age. A special position belongs to the early Iron Age Slab Grave culture in the Transbaikal area; their dead were sometimes interred in stone cist graves.[13] The burial of corpses lying on their backs which was practiced in west Siberia continued in the developing Scythian cultures of south Siberia, which is dealt with separately along with the other horse nomad cultures below.
Only isolated sanctuaries are known. Among them are the many burnt offering places found near the necropolis of the Chalcolithic Afanasevo culture in south Siberia. They consisted of simple stone circles containing ashes, pottery, animal bones and tools made of copper, stone and bone.[14] The many circular buildings containing wooden stakes and walls, in the necropoleis near the early Bronze Age settlement of Sintashta, are probably cult buildings.[15]
The horse nomads who were characteristic of the Asiatic steppe until modern times are a relatively recent phenomenon. Even in the late second millennium BC, settled pastoralists lived in the arid regions of Central Asia. They were replaced by the early horse nomads in the course of the first millennium BC in ways which are not entirely clear.
The transition to the sedentary groups further north was fluid in many places. The inhabitants of the Minusinsk hollow remained settled pastoralists even in the Iron Age, but their cultural development shows strong affinities to the neighbouring nomads. The Xiongnu in Transbaikal region show characteristics of both horse nomads and settled pastoralists and farmers.[16] The situation in northern Tianshan and Zhetysu is remarkable: in the early Iron Age the nomadic Sakas lived there, but the region was subsequently taken over by the sedentary Wusun.[17]
The earlier nomadic cultures are referred to collectively by archaeologists using the term "Scythian", which is the ancient Greek term for a group of horse nomads living north of the Black Sea; in a wider sense it referred to all horse nomads in the Eurasian steppe. The third century AD marks the beginning of the Hunnic-Sarmatian period, named after two nomadic groups from southern Russia, which continued until the establishment of the Khaganate of the Gokturks in the sixth century AD.
While the art of the settled cultures of the Asiatic steppe in the Bronze Age was dominated by anthropomorphic motifs, the advent of the horse nomads was accompanied by the development of the Scytho-Sarmatian animal style, which all the steppe people of Asia and eastern Europe shared. Its basic motifs were taken from a repertoire of wild animals, with a remarkable absence of animals which were significant to the daily life of the horse nomads. Thus depictions of horses and of people are extremely rare. Instead, the common motifs are deer, mostly lying down, elk, big cats (which must indicate Near Eastern influence), griffins and hybrids. Individual animals sometimes appear rolled up together as a "rolled animal",  pairs of different animal species may be interlaced in a purely ornamental way, or depicted fighting one another. A line of the members of the same species often appear in borders, while individual parts of animals, like their heads, often serve as ornaments.
Especially in the western steppes metal wares are found almost exclusively decorated with elements of the animal style; in the permafrost of south Siberia and Transbaikal, felt carpets and other textiles with elements from the animal style are also found, among which a felt swan stuffed with moss deserves special attention.[18] Stone was only used a little, mostly in the so-called "deer stele," probably anthropomorphic grave stele, which were decorated with deer and are found in south Siberia, Transbaikalia and Mongolia. Finally, the bodies of important people were tattooed with motifs from the animal style.
The origins of the animal style are unclear. Based on possible interactions with ancient eastern art, a strong influence from the south has been proposed. The early dating of some pieces from southern Siberia however, makes a local development on the steppes themselves more likely. It is certain however that especially in central Asia and the area north of the Black Sea, Greek and Persian art had a great influence on the art of the steppe peoples.
Known features, which were shared by the societies of the horse nomad cultures of the Bronze Age, include a powerful warrior elite, whose wealth and strength is clear from their elaborate grave goods. Particularly interesting in this context are the Chinese reports which provide detailed descriptions of the society of the Xiongnu. According to them, the population was divided into clan-like groups, which gathered together in large clan alliances. Their leaders stood in a strict hierarchy and were all under the authority of the Chanyu, the commander of the entire Xiongnu confederacy.[19]
The horse nomads of Inner Asia were nomadic pastoralists and probably travelled about in rather small groups. They particularly focussed on sheep, goats and horses, and in some regions other animals, such as the camel. Agriculture was undertaken by parallel settled populations, but probably did not play an important role. Ore mining and metal working which are known for some nomadic cultures, was probably undertaken by very elusive settled groups as well.[12]
All horse nomad cultures shared the burial of the dead in barrow graves which are known as kurgans. Their size is very variable, with a radius of between 2 and 50 metres and a height of less than one or more than 18 metres, evidently reflecting differences in social hierarchy.
In some regions, kurgans are surrounded by various kinds of stone enclosure. The more or less rectangular tombs of the later Tagar culture were sometimes surrounded by a row of stones at the edge of the kurgan mound, which was broken up by higher stones at regular intervals - later these were usually just at the corners.[20] In the Iron Age culture of Tuva, some but not all kurgans were surrounded by a rectangular or round stone wall. The kurgans themselves were partially built of earth and partially of stone, with regional variation.[21]
In the ground beneath the kurgan was buried one or (very often) more tombs. The corpse lay either in a wooden chamber or a stone cist. The grave goods found along with them indicate that wooden chambers were reserved for people of higher status. While in burials from the Bronze Age the corpses were usually in a crouching position, in the Iron Age they were usually laid on their backs. Evidence for the handling of the dead are only known from Altai and Tuva, were some bodies are preserved as ice mummies by the permafrost, making detailed analysis possible. In these locations, the guts and muscles were removed before burial and the resulting holes were stitched closed with tendons and horse hair. It is uncertain whether damage to the skull reflects injuries that occurred before death or were made after death. Ritual trepanation cannot be assumed. After the guts were removed, distinguished corpses were tattooed and embalmed. These traditions are described also by the Greek historian Herodotus, who included material on the Scythians north of the Black Sea in his 5th century BC work, and is the main Greek source on the Scythians. Even his report of cannabis inhalation in small groups during the funeral have been corroborated by finds from the Pazyryk burials.[22] This corroboration not only affirms the accuracy of Herodotus, but also indicates the cultural homogeneity of the steppe peoples of west Siberia, Central Asia and the region north of the Black Sea. The great kurgans of the Xiongnu present a rather different picture, however. There the burial chambers are deeper and were accessed by a ramp.[23]
Along with the corpse, the burial chambers also contained grave goods, whose richness could vary dramatically. Ordinary mounted warriors were buried with a fully equipped horse and weapons, women were buried with a horse, a knife and a mirror. The burials of higher ranking people were much richer. These could include up to twentyfive richly outfitted horses and an elaborate chariot; the actual burial chamber was built from wooden planks (often larch). The corpse, with a woman who probably accompanied him in death, lay, clothed, in a long treetrunk coffin. In Noin Ula in Mongolia, a woman's braids were interred instead of the woman herself.[24] Outstanding examples of kurgans include the necropoleis of Pazyryk in Altai, Noin Ula in Mongolia, and Arzhan in Tuva, where organic matter was preserved by the permafrost. Thus, felt carpets which decorated the inner walls of the burial chamber, decorated saddles and various kinds of clothing were also found. Although many large kurgans have been robbed of their contents by grave robbers, exceptional examples still remain, including countless gold objects.
On account of the general absence of written source material, research on the religion of the steppe people is based on parallels with later peoples and on the archaeological finds themselves. The funerary rituals leave no doubt about the belief in an afterlife, in which the dead had need of the same material items which they had in life – hence their burial with them.

Blood Relations: Menstruation and the Origins of Culture is a book by the evolutionary anthropologist Chris Knight. Published by Yale University Press in hardback 1991 and in paperback four years later, it has remained in print ever since.
The book outlines a new theory of human origins, focusing particularly on the emergence of symbolic ritual, kinship, religion and mythic belief. Previously, the main theory in currency was that of Claude Lévi-Strauss, who claimed that culture's rule over nature was established by men when they invented the incest taboo.
According to Lévi-Strauss, a point came when the human mind proved capable of distinguishing between 'sister' and 'wife'. Equipped with that ability, human males who had previously kept their sisters to themselves offered them in matrimonial exchange to other males, who reciprocated by making a return gift of their own sisters. In this way, rule-governed society became established as alliances were forged between neighboring groups of men.[1] Lévi-Strauss claims that in addition to the incest taboo, men invented a further series of critically important rules concerning such things as the timing of menstruation, cooking, romantic attachment and the wearing of personal ornaments.[2]
In making his very different case, Knight draws on evidence not only from mythology – Lévi-Strauss' primary source – but from a wide range of disciplines including human behavioral ecology, hunter-gatherer ethnography, Paleolithic archaeology, palaeontology, rock art studies, modern genetics and studies of monkeys and apes in the wild. For Knight, symbolic culture emerged through Darwinian processes of gradual evolution culminating eventually in revolutionary change.
Most theoretical accounts of the origins of rule-based social life are 'top-down' in the sense that they envisage some dominant force, typically male, as constructing and enforcing the incest taboo and other fundamental rules. Knight's model differs starkly in postulating an essentially counter-dominant, 'bottom-up' social dynamic as the factor responsible for the world's first morally authoritative rules.
Knight's model has often been termed the 'sex-strike' theory. Females band together to resist male sexual harassment or violence, drawing on assistance from supportive sons and brothers in collective self defence. As a weapon of last resort, such female-led kin-coalitions will support one another in denying sex to any abusive, lazy or unhelpful males. As this strategy becomes embedded, women go on periodic sex-strike to underline their value and motivate men to leave camp and go hunting as a condition of sex.
In attributing creative agency to females in establishing the cultural realm, Knight turns Lévi-Strauss upside-down, claiming that his new theory is more simple, parsimonious and consistent with the rest of science. In his view, the human revolution – the momentous transition from nature to culture – is eventually consummated when females successfully mount collective resistance to male sexual thoughtlessness or abuse.
These ideas suggest a radically different explanation for the initial establishment of the incest taboo. In Knight's model, it emerges as a logical consequence of going on strike. In response to the potential threat of sexual coercion or rape, women recruit sons and brothers as members of their sex-strike coalition. Once a young male has been initiated into such a defensive alliance, it would be unthinkable for him to impose himself sexually on those sisters and mothers whose sexual resistance he is upholding. Once initiated, therefore, young men must look outside their coalition (their 'clan' or 'lineage') for sex – resulting in the pattern of 'exogamy' or 'marrying out' so characteristic of traditional systems of kinship and residence. According to Knight, collective resistance to prohibited sex also explains why kinship terminology in traditional cultures is so regularly 'classificatory', meaning that women address other female coalition members of the same generation as 'sister' while men similarly use the term 'brother'.[3]
Knight describes how traditional hunters recurrently link the incest prohibition with its counterpart on the economic plane, the rule against consuming the flesh of an animal one has killed oneself. Although many anthropologists had noticed such a prohibition while working in the field, each scholar had imagined it to be a local peculiarity without any relevance to classical concepts in the study of religion – topics such as 'rites of atonement', 'guardian spirits of the game animals', 'totemism' or 'sacrifice'. Noticing a common underlying pattern, Knight suggests that all these ideas so central to religious practices across the world have their origin in a once universal 'own kill' taboo.
Largely unnoticed by academics until 'Blood Relations' drew attention to it, what Knight terms 'the hunter's own kill rule' is almost universally recognized, even under deteriorating economic conditions which have prompted it to be evaded or ignored. Among the Mountain Arapesh people of Papua New Guinea:
“The lowest man in the community, the man who is believed to be so far outside the moral pale that there is no use reasoning with him, is the man who eats his own kill – even though that kill be a tiny bird, hardly a mouthful in all.”[4]
Most significantly, from Knight's point of view, Margaret Mead stresses how in Arapesh culture "the taboo against eating one's own kill is equated with incest".[5] By this, Mead means that just as men should make their sisters available to other men as wives, so they should generously make flesh from the animals they hunt available to others as food.
After listing and describing comparable traditions from across the world, Knight concludes that hunters' varied and often ingenious strategies for getting around the own flesh rule are as interesting as the many instances of compliance. For example, the prohibition might be weakened by applying it only to hunters who have not yet reached a certain age, or only to the first animal you kill in a given season. A widespread indigenous belief is that if you apologize profusely to the animal or its spirit, you may hope to be forgiven for eating it. Frequently misunderstood as kindhearted sentimentality toward animals, Knight argues that the sense of guilt is prompted by an awareness that the animal is being killed for selfish reasons, in defiance of the own kill taboo.
Finally, according to Knight, there is 'totemism' as classically defined – the idea that although your 'own flesh' (defined as a species of animal) must be respected, it is permissible to eat such an animal provided others have killed it, or kill your totemic animal on condition it will be eaten by others.[6]
The book culminates with a new explanation of the 'Dragon' motif in world mythology. According to Knight, the roots of this mythological creature are ancient, taking us back to when an ancestral population of humans evolved in Africa and began migrating across the world, taking with them their rituals and beliefs. Knight argues that religious beliefs are among the most conservative aspects of human symbolic culture, making them potentially a rich source of information about our distant evolutionary past when all humans still lived by hunting and gathering.
During the early decades of the twentieth century there was one place in the world where the Dragon was still being acted out regularly during the course of seasonally timed rituals of initiation. This place was the continent of Australia, where the many local variants of the Dragon became known by anthropologists as 'the Rainbow Serpent.'[7][8][9]
In Knight's view, the Rainbow Snake was an imagined creature conjured up recurrently through ritual performance. It was a way of depicting the traumatic and unforgettable events and emotions experienced by boys and girls as they underwent their initiation into adult life. Since hunter-gatherer rites of passage[10] were not scheduled randomly but at auspicious times of day or night, moon phase and season, the experiences they produced reflected cyclical time – an alternating movement between darkness and light, wet season and dry, rain and sunshine, death and new life.[11] Knight points out that the Rainbow Snake or Dragon likewise moves through contraries. For example, it is the lowest of creatures – a snake – yet also the highest, since it has wings. It is a single creature, yet often with a plurality of heads. It lives in water, perhaps as the guardian of a sacred spring or waterhole, yet also spits out lightning fire. It is a non-human beast, but with human sexual appetites – it is famous for desiring maidens. It is gender-ambivalent, being sometimes male, sometimes female and sometimes both.[12]
Knight notes that all of us find it hard to accept unfamiliar ideas or internalize concepts that form no part of our own culture. For this reason, we have a history of attempts to reduce the Dragon, Plumed Serpent or Anaconda to 'water', 'fertility', 'fear of venomous snakes' or some other concept or phenomenon familiar to us from our own culture.[13][14] For Knight, such distortions of vision can best be corrected by turning to hunter-gatherer ethnography and other branches of social anthropology  – disciplines designed precisely to enable us to transcend our own cultural assumptions. In Australia, this means listening carefully to the wisdom of Aboriginal elders – including words recorded by anthropologists at a time when informants could brush aside Western moral sensitivities and speak with pride about their own sacred knowledge and traditions.[15][16]
When asked about the size of their great Snake, Knight reports, Aboriginal people might point to the horizon where earth meets sky, as if to indicate that it was that large. Asked about the creature's coloring or shape, they might refer to a rainbow. Asked about its length, they might refer to their traditional 'song-lines' or chains of extended kinship stretched for hundreds of miles across the landscape. Asked about its precise relationship to them, they would define this in terms of blood, perhaps viewing the Snake as their 'Mother's Brother', 'Mother' or the 'All-Mother' of the Aboriginal people as a whole.[17]
Knight makes much of the fact that in north-east Arnhem Land, Australia, the rainbow snake was once acted out in elaborate ritual performances as part of the initiation of young men. These boys traditionally had their flesh cut during the ceremony and were encouraged to bleed together in synchrony with one another. Knight comments that local mythology treated such synchronized bleeding as a technique stolen by men from ancestral women – known as the Two Wawilak Sisters – who once conjured up the Snake by synchronizing their menstrual flows.[18][19] As Yolngu men explain when they re-enact how these ancestral women's synchronized bleeding provoked Julunggul the Python into swallowing them alive:
“But really we have been stealing what belongs to them (the women), for it is mostly all woman’s business; and since it concerns them it belongs to them. Men have nothing to do really, except copulate, it belongs to the women. All that belonging to those Wauwelak, the baby, the blood, the yelling, their dancing, all that concerns the women; but every time we have to trick them. Women can’t see what men are doing, although it really is their own business, but we can see their side. This is because all the Dreaming business came out of women – everything; only men take ‘picture’ for that Julunggul (i.e. men make an artificial reproduction of the Snake). In the beginning we had nothing, because men had been doing nothing; we took these things from women.”[20]
Whereas most Western anthropologists dismiss such stories as fanciful, Knight believes that they can tell us a great deal. For example, they help explain why Aboriginal men traditionally considered it so important to shed their own blood together during their initiation ceremonies. Knight believes that there may be some truth in the idea that initiation was originally a joyful celebration of a girl's first menstrual onset, and that when male initiations assumed greater prominence they were initially inspired by and modeled on that women-led menstrual precedent.[21]
This would solve many puzzles, including the Snake or Dragon's legendary demand for a regular supply of maidens. The truth (according to Knight) is that for a girl to be initiated, she had to bleed with other women and become engulfed in the combined flow – 'swallowed alive by a Dragon' – just as boys undergoing initiation had to bleed together and be 'swallowed alive' before being reborn.
Knight's interpretation is that when initiated males came to establish their dominance over the female sex, they set about usurping women's former power, justifying their dominance by claiming that they, too, could synchronously menstruate and give birth. Hence when men began monopolizing ritual power in the name of the Rainbow Serpent, they were striving to turn women's mythic former sovereignty against living women and girls themselves. In China and the East, the Emperor's sovereignty was traditionally personified as a Dragon in much the same way, suggesting that no ruler could exercise legitimate sovereignty without doing so in the Dragon's name.[22]
Knight argues that if we are aware of such historical and ethnographic details, we are much more likely to comprehend the ultimate origins and world-wide significance of the Dragon. It is, he suggests, an image of the heavenly intimacy, solidarity and sense of communion out of which religious awareness first emerged. It is certainly not a representation of any physical or biological thing. Rather, according to Knight, it represents mothers and daughters, sisters and brothers acting together in solidarity during rituals that were designed to encourage women's menstrual cycles to beat in synchrony with wider periodicities governed by moon, season and tides. If male heroes had to confront and defeat a Dragon who desired menstruating maidens, it was because these dominant males recognized their adversary as a many-headed counter-dominant threat to their power.
One of Knight's findings is that taboos around menstruation are deeply rooted in every traditional culture, taking surprisingly similar forms. Knight suggests that what have now become irrational and oppressive taboos were originally established by women in order to assert their periodic inviolability, an idea that in recent years has gained some currency.[23] He attributes the ubiquity of these rules and taboos to their antiquity and to the probability that they once upheld all other moral, religious and cultural taboos.
In Knight's model, female intolerance of male harassment or abuse reaches a peak each month around menstruation, when women declare themselves to be temporarily 'set apart' or 'sacred'. It has often been noted that women who endure men's demands over most days during each month may have a lower tolerance threshold just before menstruation.[24] This lends a periodic structure to women's levels of energy and willingness to comply with male demands.[25][26]
Knight notes that the human female cycle has an average periodicity of approximately 29.5 days,[27] making it compatible in principle with the periodicity of the moon.[28] This is in contrast to our close great ape relative the chimpanzee, whose menstrual cycle length is around 36 days. Knight's point here is that female chimpanzees could not synchronize their cycles with the Moon even if they wanted to, whereas human females do seem to possess a strikingly close approximation to the necessary menstrual cycle length.
Knight remarks on the extent to which an 'ideology of blood' permeates ritual performances and cosmological beliefs among hunter-gatherers across the world,[29][30][31] noting how elements of these beliefs are preserved in diverse forms even as horticulture, cattle herding and farming replace hunting and gathering. One feature which persists into historically more recent religions is the salience of blood as core marker of sanctity and potency. When Knight was writing, his suggestion of an intimate link between incest prohibitions and menstrual taboos seemed surprising and original although in reality, this aspect of his thinking was not new, having been proposed by the founder of modern sociology, Emile Durkheim, as long ago as 1898.[32]
Knight acknowledges that his theory is at odds with mainstream thinking. He responds by arguing that because it makes surprisingly detailed and often unexpected predictions, his theory is unusual in being testable in the light of empirical evidence.
One of Knights's theoretical predictions, made in 1991, was that future archaeological research should find that the earliest fully-cultural humans were regularly using red ocher pigments as cosmetics in their ritual displays. In 2002, this prediction was confirmed by the archaeologist Ian Watts when, as the ocher specialist working in a team led by Chris Henshilwood, he announced an early date for the world's first art. Among the team's discoveries were cross-hatched patterns engraved on pieces of ocher apparently used as body paint, found in Blombos Cave, South Africa, and dated to more than 70,000 years ago.[33] According to Watts, the kinds of ocher most frequently utilized were especially valued because their color suggested that of fresh blood.[34]
Another of Knight's predictions was that future archaeological research should confirm the centrality of lunar periodicity to ancient hunting schedules and their depiction as calendars in rock and cave art. In 1991, Knight cited the Ice Age lunar notation systems described by Alexander Marshack in his 1972 book, The Roots of Civilization.[35] Unfortunately for Knight, Marshack's interpretations were then being dismissed by critics as 'wishful thinking'.[36] Since then, however, many specialists have begun changing their minds as an increasing number of Paleolithic rock art images and cave paintings have lent weight to Marshack's original interpretations. Today, there is widespread agreement that early hunter-gatherers perceived significant correspondences between menstrual and lunar periodicities, scheduling their ceremonies and hunting patterns to achieve what for them was the ultimate ideal of synchrony with the Moon.[37][38]
Another of Knight's predictions concerned structures of family, residence and kinship. In 1991, the consensus was that early human hunters must have lived in patrilocal bands.[39] Knight's model seemed difficult to accept because it presupposed strong female kin-based coalitions, early hunter-gatherer women choosing to live with and share childcare with their own mother and other female relatives. In the years since Knight's book was written, a number of developments – including Kristen Hawks' 'grandmother hypothesis', Sarah Hrdy's 'alloparenting' model and Camilla Power's 'Female Cosmetic Coalitions' hypothesis – have indicated that early human postmarital residence patterns are unlikely to have been patrilocal.[40] Paleogenetic studies over recent decades have confirmed that among African hunter-gatherers, matrilocal residence during the early years of marriage was traditionally the norm.[41]
Positive
Knight's book was favorably reviewed in The Times Higher Education Supplement, The Times Literary Supplement and The London Review of Books; it also received publicity through an interview on the BBC World Service Science Now program, a debate with Dr. Henrietta Moore on BBC Radio 4 Woman’s Hour, a front-page news report in The Independent on Sunday and Daily Telegraph and coverage in many other periodicals.[42] In April 1998, the Independent on Sunday featured a two-page article on Knight's work by science correspondent Marek Kohn, who described Knight's approach to human origins as ‘drawing together some of the most dynamic lines of argument in current British evolutionary thought’.[43]
“This book may be the most important ever written on the evolution of human social organization. It brings together observation and theory from social anthropology, primatology, and paleoanthropology in a manner never before equalled. The author, Chris Knight, is up to date on all these fields and has achieved an extraordinary synthesis. His critiques of Claude Lévi-Strauss on totemism and myth are a sheer tour de force.”
“A man writing about menstruation as empowering not polluting; a Marxist analysis in which sex solidarity and class analysis assume equal explanatory power; a fully social and revolutionary account of our human cultural origins that privileges women; an explicitly political narrative of science in the first person; an interweaving of anthropology, biology, history of ideas, and philosophy; an attempt not just to interpret the world but to change the world: Blood Relations is all this and more.”
“Ignoring this book is a mistake. It is a very readable, witty, lively treasure-trove of anthropological wisdom and insight ... Chris Knight has taken on the task of explicating not only the whys and hows of human cultural evolution, but also vast constellations of cultural behaviour covering Australia, Africa, Europe and all of the Americas. In this endeavour he is extraordinarily cross-disciplinary in his approach, utilizing insights from cultural anthropology, sociology, sociobiology and palaeo- and ethno-archaeology. In short, Knight is a complete anthropologist, one who realizes the value of exploring all corners of his field to synthesize disparate work into a cohesive whole... And his scholarship is impeccable. While many of us rarely bother to read ‘the greats’ of our field any more, Knight delves deep into Durkheim, Frazer and Lévi-Strauss and many others, coming up with long-forgotten insights and providing his readers with an enormously useful review of a century of evolutionary theory and ethnographic data. In fact, as a feminist, I would very much like it if Knight’s story turned out to be true, since it gives so much credit to women’s collective solidarity, strike power and biological and intellectual creativity... Best of all, it's a story that's ‘good to think with’. It made me review in my mind everything I ever learned about evolution and rethink it in a new way.”
In 1997, the feminist journalist and historian Barbara Ehrenreich welcomed and made use of Knight's ideas in her book, Blood Rites: The origins and history of the passions of war.[44] Among major poets, Ted Hughes[45] and Peter Redgrove[46] favourably cited Knight's insights concerning menstrual synchrony and its place in world mythology and folklore.
The sculptor Anish Kapoor drew inspiration from Knight's work, describing how his appreciation of the colour red – in, for example, Kapoor's celebrated sculpture Blood Relations – owes much to Knight's 'wonderful theory' that the world's first art was produced when women began decorating themselves with red ochre cosmetics.[47][48]
Another prominent figure inspired by Knight's book is the Chilean revolutionary activist and artist Cecilia Vicuña. Having studied Knight's work over many years, she associates the blood-red woolen quipus or 'Red Threads' central to much of her recent work with the string figures and images of menstruating goddesses in Aboriginal Australian rock-art as described and interpreted by Knight in his book.[49]
Although Knight's theory of human cultural and symbolic origins remains controversial, in the years since Blood Relations was published it has become central to an increasing body of archaeological research and debate on how symbolic culture in our species first emerged.[50][51][52][53][54]
Negative
Knight's book was negatively reviewed by Chris Harman of the Socialist Workers' Party, who dismissed it as 'Menstrual Moonshine' incompatible with Marxist theoretical premises.[55] A review published in the International Communist Current found grounds for welcoming Knight's book despite criticizing it on political grounds.[56] Feminist critic Joan Gero found Knight's book 'offensive' on somewhat different political grounds. In a harshly negative review,[57] Gero wrote:
What Knight puts forward as an 'engendered' perspective on the origins of culture is a paranoid and distorting view of 'female solidarity', featuring (all) women as sexually exploiting and manipulating (all) men. Male-female relations are characterized forever and everywhere as between victims and manipulators; exploitative women are assumed always to have wanted to trap men by one means or another, and indeed their conspiring to do so serves as the very basis of our species' development.
In the years since its publication, Knight's book has inspired a number of scholars from various disciplines to develop some of the underlying ideas.
'What, then, is the Snake?' Chris Knight asks: 'What kind of snake was it, if people could participate in its body by dancing?' Why were the Two Wawilak Sisters 'swallowed by a Snake'? Were they swallowed by 'cyclical time'?

The Dawn of Everything: A New History of Humanity is a 2021 book by anthropologist David Graeber and archaeologist David Wengrow. It was first published in the United Kingdom on 19 October 2021 by Allen Lane (an imprint of Penguin Books).[1]
Graeber and Wengrow finished the book around August 2020.[2] Its American edition is 704 pages long, including a 63-page bibliography.[2] It was a finalist for the Orwell Prize for Political Writing (2022).[3]
Describing the diversity of early human societies, the book critiques traditional narratives of history's linear development from primitivism to civilization.[4] Instead, The Dawn of Everything posits that humans lived in large, complex, but decentralized polities for millennia.[5]
The Dawn of Everything became an international bestseller, translated into more than thirty languages.[6] It was widely reviewed in the popular press and in leading academic journals, as well as in activist circles, with divided opinions being expressed across the board. Both favorable and critical reviewers noted its challenge to existing paradigms in the study of human history.
The authors open the book by suggesting that current popular views on the progress of western civilization, as presented by Francis Fukuyama, Jared Diamond, Yuval Noah Harari, Charles C. Mann, Steven Pinker, and Ian Morris, are not supported by anthropological or archaeological evidence, but owe more to philosophical dogmas inherited unthinkingly from the Age of Enlightenment. The authors refute the Hobbesian and Rousseauian view on the origin of the social contract, stating that there is no single original form of human society. Moreover, they argue that the transition from foraging to agriculture was not a civilization trap that laid the ground for social inequality, and that throughout history, large-scale societies have often developed in the absence of ruling elites and top-down systems of management.
Rejecting the "origins of inequality" as a framework for understanding human history, the authors consider where this question originated, and find the answers in a series of encounters between European settlers and the Indigenous populations of North America. They argue that the latter provided a powerful counter-model to European civilisation and a sustained critique of its hierarchy, patriarchy, punitive law, and profit-motivated behaviour, which entered European thinking in the 18th century through travellers' accounts and missionary relations, to be widely imitated by the thinkers of the Enlightenment. They illustrate this process through the historical example of the Wendat leader Kondiaronk, and his depiction in the best-selling works of the Baron Lahontan, who had spent ten years in the colonies of New France. The authors further argue that the standard narrative of social evolution, including the framing of history as modes of production and a progression from hunter-gatherer to farmer to commercial civilisation, originated partly as a way of silencing this Indigenous critique, and recasting human freedoms as naive or primitive features of social development.
Subsequent chapters develop these initial claims with archaeological and anthropological evidence. The authors describe ancient and modern communities that self-consciously abandoned agricultural living, employed seasonal political regimes (switching back and forth between authoritarian and communal systems), and constructed urban infrastructure with egalitarian social programs. The authors then present extensive evidence for the diversity and complexity of political life among non-agricultural societies on different continents, from Japan to the Americas, including cases of monumental architecture, slavery, and the self-conscious rejection of slavery through a process of cultural schismogenesis. They then examine archaeological evidence for processes that eventually led to the adoption and spread of agriculture, concluding that there was no Agricultural Revolution, but a process of slow change, taking thousands of years to unfold on each of the world's continents, and sometimes ending in demographic collapse (e.g. in prehistoric Europe). They conclude that ecological flexibility and sustained biodiversity were key to the successful establishment and spread of early agriculture.
The authors then go on to explore the issue of scale in human history, with archaeological case studies from early China, Mesoamerica, Europe (Ukraine), the Middle East, South Asia, and Africa (Egypt). They conclude that contrary to standard accounts, the concentration of people in urban settlements did not lead mechanistically to the loss of social freedoms or the rise of ruling elites. While acknowledging that in some cases, social stratification was a defining feature of urban life from the beginning, they also document cases of early cities that present little or no evidence of social hierarchies, lacking such elements as temples, palaces, central storage facilities, or written administration, as well as examples of cities like Teotihuacan, that began as hierarchical settlements, but reversed course to follow more egalitarian trajectories, providing high quality housing for the majority of citizens. They also discuss at some length the case of Tlaxcala as an example of Indigenous urban democracy in the Americas, before the arrival of Europeans, and the existence of democratic institutions such as municipal councils and popular assemblies in ancient Mesopotamia.
Synthesizing these findings, the authors move to discovering underlying factors for the rigid, hierarchical, and highly bureaucratized political system of contemporary civilization. Rejecting the category of "the State" as a trans-historical reality, they instead define three basic sources of domination in human societies: control over violence (sovereignty), control over information (bureaucracy), and charismatic competition (politics). They explore the utility of this new approach by comparing examples of early centralised societies that elude definition as states, such as the Olmec and Chavín de Huántar, as well as the Inca, China in the Shang dynasty, the Maya Civilization, and Ancient Egypt. From this they go on to argue that these civilisations were not direct precursors to our modern states, but operated on very different principles. The origins of modern states, they conclude, are shallow rather than deep, and owe more to colonial violence than to social evolution. Returning to North America, the authors then bring the story of the Indigenous critique and Kondiaronk full circle, showing how the values of freedom and democracy encountered by Europeans among the Wendat and neighbouring peoples had historical roots in the rejection of an earlier system of hierarchy, with its focus at the urban center of Cahokia on the Mississippi.
Based on their accumulated discussions, the authors conclude by proposing a reframing of the central questions of human history. Instead of the origins of inequality, they suggest that our central dilemma is the question of how modern societies have lost the qualities of flexibility and political creativity that were once more common. They ask how we have apparently "got stuck" on a single trajectory of development, and how violence and domination became normalised within this dominant system. Without offering definitive answers, the authors end the book by suggesting lines of further investigation. These focus on the loss of three basic forms of social freedom, which they argue were once common:
They emphasize the loss of women's autonomy, and the insertion of principles of violence into basic notions of social care at the level of domestic and family relations, as crucial factors in establishing more rigid political systems.
The book ends by suggesting that narratives of social development in which western civilization is self-appointed to be the highest point of achievement to date in a linear progression are largely myths, and that possibilities for social emancipation can be found in a more accurate understanding of human history, based on scientific evidence that has come to light only in the last few decades, with the assistance of the field of anthropology and archaeology.
According to Book Marks, the book received "positive" reviews based on 16 critic reviews with 5 being "rave" and 6 being "positive" and 5 being "mixed".[7]
The book entered The New York Times best-seller list at No. 2 for the week of November 28, 2021,[8] while its German translation entered Der Spiegel Bestseller list at No.1.[9] It was named a Sunday Times, Observer and BBC History Book of the Year.[10] The book was shortlisted for the Orwell Prize for Political Writing. Historian David Edgerton, who chaired the judges panel, praised the book, saying it "genuinely is a new history of humanity" and a "celebration of human freedom and possibility, based on a reexamination of prehistory, opening up the past to make new futures possible.”[11] Writing for The Hindu, G. Sampath noted that two strands run through the book: "the consolidation of a corpus of archaeological evidence, and a history of ideas." Inspired by "the rediscovery of an unknown past," he asks, "can humanity imagine a future that's more worthy of itself?"[12]
Gideon Lewis-Kraus said in The New Yorker that the book "aspires to enlarge our political imagination by revitalizing the possibilities of the distant past".[13] In The Atlantic, William Deresiewicz described the book as "brilliant" and "inspiring", stating that it "upends bedrock assumptions about 30,000 years of change."[14] The anthropologist Giulio Ongaro, stated in Jacobin and Tribune that "Graeber and Wengrow do to human history what [Galileo and Darwin] did to astronomy and biology respectively".[15][16] In Bookforum, Michael Robbins called the book both "maddening" and "wonderful."[17] Historian of science Emily Kern, writing in the Boston Review, called the book "erudite" and "funny", suggesting that "once you start thinking like Graeber and Wengrow, it's difficult to stop."[18] Kirkus Reviews described the book as "An ingenious new look at 'the broad sweep of human history' and many of its 'foundational” stories.'" and "A fascinating, intellectually challenging big book about big ideas."[19] Andrew Anthony in The Observer said the authors persuasively replace "the idea of humanity being forced along through evolutionary stages with a picture of prehistoric communities making their own conscious decisions of how to live".[20]
Historian David Priestland argued in The Guardian that Peter Kropotkin had more powerfully addressed the sorts of questions that a persuasive case for modern-day anarchism should address, but lauded the authors' historical "myth-busting" and called it "an exhilarating read".[21] Philosopher Kwame Anthony Appiah argued in The New York Review of Books that there is a "discordance between what the book says and what its sources say," while also stating that the book, which is "chockablock with archaeological and ethnographic minutiae, is an oddly gripping read".[22] NYRB subsequently published an extended exchange between Wengrow and Appiah under the title "The Roots of Inequality" in which Wengrow expanded on the book's use of archaeological sources, while Appiah concluded that "Graeber and Wengrow's argument against historical determinism—against the alluring notion that what happened had to have happened—is itself immensely valuable."[23] Another philosopher, Helen De Cruz, wrote that the book offers "a valuable exercise in philosophical genealogy by digging up the origins of our political and social dysfunction," but also criticised the book for neglecting a range of other possible methodologies.[24]
Writing in the Chicago Review, historian Brad Bolman and archaeologist Hannah Moots suggest that what makes the book so important is "its attempt to make accessible a vast array of recent anthropological and archaeological evidence; to read it against the grain; and to synthesize those findings into a novel story about what exactly happened in our long past," drawing comparisons with the work of V. Gordon Childe.[25] Reviewing for American Antiquity archaeologist Jennifer Birch called the book 'a resounding success',[26] while archaeologist and anthropologist Rosemary Joyce, reviewing for American Anthropologist, wrote that the book succeeds in providing "provocative thinking about major questions of human history" and a "convincing demonstration of new frameworks of anthropological comparison".[27]
Archaeologist Mike Pitts, reviewing for British Archaeology described the book as "glorious" and suggested that its joint authorship by an anthropologist and an archaeologist "gives the book a depth and rigour rarely seen in the genre".[28] Reviewing for Scientific American, John Horgan described the book as "both a dense, 692-page scholarly inquiry into the origins of civilization and an exhilarating vision of human possibility"[29]
In Anthropology Today, Arjun Appadurai accused the book of "swerving to avoid a host of counter-examples and counter-arguments" while also describing the book's "fable" as "compelling".[30] David Wengrow responded in the same issue.[31] Anthropology Today later published a letter to the editor, in which political ecologist Jens Friis Lund writes "Appadurai never discloses where and how exactly Graeber and Wengrow go wrong," calling the book a "monumental empirical effort" and "exemplar of interdisciplinary engagement."[32] In a subsequent issue, Anthropology Today published a full review of the book by social anthropologist Luiz Costa, who suggested it contains "a range of examples of societies drawing on their own past experiences, or those of neighbouring peoples, to shape future ways of life - not in a voluntaristic sense, but within specific social patterns, considering historical events." Costa compared The Dawn of Everything to classic works by Claude Lévi-Strauss in terms of its scope and importance.[33] Another anthropologist Thomas Hylland Eriksen called the book an "intellectual feast"[34]
The historian David A. Bell, responding solely to Graeber and Wengrow's arguments about the Indigenous origins of Enlightenment thought and Jean-Jacques Rousseau, accused the authors of coming "perilously close to scholarly malpractice."[35] Historian and philosopher Justin E. H. Smith suggested "Graeber and Wengrow are to be credited for helping to re-legitimise this necessary component of historical anthropology, which for better or worse is born out of the history of the missions and early modern global commerce."[36]
Anthropologist Durba Chattaraj claimed that the book includes "elisions, slippages, and too-exaggerated leaps" when referring to archaeology from India, but stated that its authors are "extremely rigorous and meticulous scholars", and that reading the book from India "expands our worlds and allows us to step outside of a particular postcolonial predicament."[37] Anthropologist Matthew Porges, writing in The Los Angeles Review of Books suggested the book is "provocative, if not necessarily comprehensive", and that its "great value is that it provides a much better point of departure for future explorations of what was actually happening in the past".[38] Anthropologist Richard Handler claimed that the book's endnotes "often reveal that a particularly startling interpretation of archaeological evidence depends on one or two sources taken from vast bodies of literature" while also claiming that the stories told "are stories we need and want to hear."[39]
Writing for the New York Journal of Books, another anthropologist, James H. McDonald, suggested that The Dawn of Everything "may well prove to be the most important book of the decade, for it explodes deeply held myths about the inevitability of our social lives dominated by the state".[40] Anthropologist James Suzman in the Literary Review claimed that the book doesn't "engage with the vast historical and academic literature on recent African ... small scale hunter-gatherers", but also maintained that the book is "consistently thought-provoking" in "forcing us to re-examine some of the cosy assumptions about our deep past".[41] Writing for Black Perspectives Kevin Suemnicht noted that the book develops ideas proposed by Orlando Patterson to account for the loss of human freedoms, and argued that the book confirms the "Fanonian positions within the Black Radical Tradition that this world-system is inherently anti-Black".[42] In Antiquity, archaeologist Rachael Kiddey suggested that the book arose from "playful conversations between two eminently qualified friends" and also that it contributes to "feminist revisions of the development of knowledge."[43]
In Cliodynamics various authors praised the book while also making criticisms. Gary M. Feinman accused Graeber and Wengrow of using "cherry-picked and selectively presented examples".[44] Another archaeologist Michael E. Smith criticized the book for "problems of evidence and argumentation".[45] Ian Morris claimed some of the book's arguments "run more on rhetoric than on method," but praised it as "a work of careful research and tremendous originality."[46] Historian Walter Scheidel criticized the book for its lack of "materialist perspectives", but also called it "timely and stimulating".[47]
The book's reception among the political left was polarizing. Several reviewers suggested that the book was written from an anarchist perspective.[48][49][50] Sébastien Doubinsky called the book "an important work, both as a summary of recent discoveries in the fields of archaeology and anthropology and as an eye-opener on the structures of dominant narratives".[51] In Cosmonaut Magazine, Nicolas Villarreal described the book as "a series of brilliant interventions" while criticising the authors for not appreciating that ideology and politics are "the source of our profound unfreedom."[52] CJ Sheu said the book is "simply put a masterpiece"[53] while Peter Isackson in Fair Observer described the book as "nothing less than a compelling invitation to reframe and radically rethink our shared understanding of humanity's history and prehistory."[54] Eliza Delay, writing for Resilience called the book "a revelation" and a "sweeping revision of how we see ourselves."[55] Socialist activist and anthropologist Chris Knight stated that the "core message" of the book was rejecting Engels' primitive communism, and called The Dawn of Everything "incoherent and wrong" for beginning "far too late" and "systematically side-stepping the cultural flowering that began in Africa tens of thousands of years before Homo sapiens arrived in Europe".[56] In a longer review, Knight did, however, emphasize that the book's "one important point" was "its advocacy of [political] oscillation".[57]
Reviewers in the Ecologist expressed the view that the authors "fail to engage with the enormous body of new scholarship on human evolution" while, at the same time, calling the book a "howling wind of fresh air".[58] Reviewing for The Rumpus Beau Lee Gambold calls the book "at once dense, funny, thorough, joyful, unabashedly intelligent, and infinitely readable."[59] Historian Ryne Clos claimed that the book partly relies on "a specious, exaggerated interpretation of the historical evidence" but that it is also "incredibly informative".[60]
Historian Dominic Alexander, writing for socialist organization Counterfire questioned the evidence used in the book and characterized its rejection of "the teleological habit of thought" as a "profoundly debilitating approach" to political change.[61] Market anarchist Charles Johnson noted the book's "idiosyncratic readings of sources".[62] In The Nation, historian Daniel Immerwahr characterised the book as "less a biography of the species than a scrapbook, filled with accounts of different societies doing different things," while praising its refusal "to dismiss long-ago peoples as corks floating on the waves of prehistory. Instead, it treats them as reflective political thinkers from whom we might learn something".[49]
Writing for Artforum, Simon Wu called the book a "bracing rewrite of human history".[63] Bryan Appleyard in his review for The Sunday Times called it "pacey and potentially revolutionary."[64] Reviewing for Science, Erle Ellis described The Dawn of Everything as "a great book that will stimulate discussions, change minds, and drive new lines of research".[65]

In paleoanthropology, the recent African origin of modern humans or the "Out of Africa" theory (OOA)[a] is the most widely accepted[1][2][3] model of the geographic origin and early migration of anatomically modern humans (Homo sapiens). It follows the early expansions of hominins out of Africa, accomplished by Homo erectus and then Homo neanderthalensis.
The model proposes a "single origin" of Homo sapiens in the taxonomic sense, precluding parallel evolution in other regions of traits considered anatomically modern,[4] but not precluding multiple admixture between H. sapiens and archaic humans in Europe and Asia.[b][5][6] H. sapiens most likely developed in the Horn of Africa between 300,000 and 200,000 years ago,[7][8] although an alternative hypothesis argues that diverse morphological features of H. sapiens appeared locally in different parts of Africa and converged due to gene flow between different populations within the same period.[9][10] The "recent African origin" model proposes that all modern non-African populations are substantially descended from populations of H. sapiens that left Africa after that time.
There were at least several "out-of-Africa" dispersals of modern humans, possibly beginning as early as 270,000 years ago, including 215,000 years ago to at least Greece,[11][12][13] and certainly via northern Africa and the Arabian Peninsula about 130,000 to 115,000 years ago.[20] There is evidence that modern humans had reached China around 80,000 years ago.[21] Practically all of these early waves seem to have gone extinct or retreated back, and present-day humans outside Africa descend mainly from a single expansion about 70,000–50,000 years ago,[22][23][24][7][8][25][26][excessive citations] via the so-called "Southern Route". These humans spread rapidly along the coast of Asia and reached Australia by around 65,000–50,000 years ago,[27][28][c] (though some researchers question the earlier Australian dates and place the arrival of humans there at 50,000 years ago at earliest,[29][30] while others have suggested that these first settlers of Australia may represent an older wave before the more significant out of Africa migration and thus not necessarily be ancestral to the region's later inhabitants[24]) while Europe was populated by an early offshoot which settled the Near East and Europe less than 55,000 years ago.[31][32][33]
In the 2010s, studies in population genetics uncovered evidence of interbreeding that occurred between H. sapiens and archaic humans in Eurasia, Oceania and Africa,[34][35][36] indicating that modern population groups, while mostly derived from early H. sapiens, are to a lesser extent also descended from regional variants of archaic humans.
"Recent African origin", or Out of Africa II, refers to the migration of anatomically modern humans (Homo sapiens) out of Africa after their emergence at c. 300,000 to 200,000 years ago, in contrast to "Out of Africa I", which refers to the migration of archaic humans from Africa to Eurasia from before 1.8 and up to 0.5 million years ago. Omo-Kibish I (Omo I) from southern Ethiopia is the oldest anatomically modern Homo sapiens skeleton currently known
(around 233,000 years old).[38] There are even older Homo sapiens fossils from Jebel Irhoud in Morocco which exhibit a mixture of modern and archaic features at around 315,000 years old.[39]
Since the beginning of the 21st century, the picture of "recent single-origin" migrations has become significantly more complex, due to the discovery of modern-archaic admixture and the increasing evidence that the "recent out-of-Africa" migration took place in waves over a long time. As of 2010, there were two main accepted dispersal routes for the out-of-Africa migration of early anatomically modern humans, the "Northern Route" (via Nile Valley and Sinai) and the "Southern Route" via the Bab-el-Mandeb strait.[40]
Beginning 135,000 years ago, tropical Africa experienced megadroughts which drove humans from the land and towards the sea shores, and forced them to cross over to other continents.[49][e]
Fossils of early Homo sapiens were found in Qafzeh and Es-Skhul Caves in Israel and have been dated to 80,000 to 120,000 years ago.[50][51] These humans seem to have either become extinct or retreated back to Africa 70,000 to 80,000 years ago, possibly replaced by southbound Neanderthals escaping the colder regions of ice-age Europe.[22] Hua Liu et al. analyzed autosomal microsatellite markers dating to about 56,000 years ago. They interpret the paleontological fossil as an isolated early offshoot that retracted back to Africa.[23]
The discovery of stone tools in the United Arab Emirates in 2011 at the Faya-1 site in Mleiha, Sharjah, indicated the presence of modern humans at least 125,000 years ago,[14] leading to a resurgence of the "long-neglected" North African route.[15][52][16][17] This new understanding of the role of the Arabian dispersal began to change following results from archaeological and genetic studies stressing the importance of southern Arabia as a corridor for human expansions out of Africa.[53]
In Oman, a site was discovered by Bien Joven in 2011 containing more than 100 surface scatters of stone tools belonging to the late Nubian Complex, known previously only from archaeological excavations in the Sudan. Two optically stimulated luminescence age estimates placed the Arabian Nubian Complex at approximately 106,000 years old. This provides evidence for a distinct Stone Age technocomplex in southern Arabia, around the earlier part of the Marine Isotope Stage 5.[54]
According to Kuhlwilm and his co-authors, Neanderthals contributed genetically to modern humans then living outside of Africa around 100,000 years ago: humans which had already split off from other modern humans around 200,000 years ago, and this early wave of modern humans outside Africa also contributed genetically to the Altai Neanderthals.[55] They found that "the ancestors of Neanderthals from the Altai Mountains and early modern humans met and interbred, possibly in the Near East, many thousands of years earlier than previously thought".[55] According to co-author Ilan Gronau, "This actually complements archaeological evidence of the presence of early modern humans out of Africa around and before 100,000 years ago by providing the first genetic evidence of such populations."[55] Similar genetic admixture events have been noted in other regions as well.[56]
By some 50–70,000 years ago, a subset of the bearers of mitochondrial haplogroup L3 migrated from East Africa into the Near East. It has been estimated that from a population of 2,000 to 5,000 individuals in Africa, only a small group, possibly as few as 150 to 1,000 people, crossed the Red Sea.[57][58] The group that crossed the Red Sea travelled along the coastal route around Arabia and the Persian Plateau to India, which appears to have been the first major settling point.[59] Wells (2003) argued for the route along the southern coastline of Asia, across about 250 kilometres (155 mi), reaching Australia by around 50,000 years ago.
Today at the Bab-el-Mandeb straits, the Red Sea is about 20 kilometres (12 mi) wide, but 50,000 years ago sea levels were 70 m (230 ft) lower (owing to glaciation) and the water channel was much narrower. Though the straits were never completely closed, they were narrow enough to have enabled crossing using simple rafts, and there may have been islands in between.[40][60] Shell middens 125,000 years old have been found in Eritrea,[61] indicating that the diet of early humans included seafood obtained by beachcombing.
The dating of the Southern Dispersal is a matter of dispute.[48] It may have happened either pre- or post-Toba, a catastrophic volcanic eruption that took place between 69,000 and 77,000 years ago at the site of present-day Lake Toba in Sumatra, Indonesia. Stone tools discovered below the layers of ash deposited in India may point to a pre-Toba dispersal but the source of the tools is disputed.[48] An indication for post-Toba is haplo-group L3, that originated before the dispersal of humans out of Africa and can be dated to 60,000–70,000 years ago, "suggesting that humanity left Africa a few thousand years after Toba".[48] Some research showing slower than expected genetic mutations in human DNA was published in 2012, indicating a revised dating for the migration to between 90,000 and 130,000 years ago.[62] Some more recent research suggests a migration out-of-Africa of around 50,000-65,000 years ago of the ancestors of modern non-African populations, similar to most previous estimates.[24][63][64]
Following the fossils dating 80,000 to 120,000 years ago from Qafzeh and Es-Skhul Caves in Israel there are no H. sapiens fossils in the Levant until the Manot 1 fossil from Manot Cave in Israel, dated to 54,700 years ago,[65] though the dating was questioned by Groucutt et al. (2015). The lack of fossils and stone tool industries that can be safely associated with modern humans in the Levant has been taken to suggest that modern humans were outcompeted by Neanderthals until around 55,000 years ago, who would have placed a barrier on modern human dispersal out of Africa through the Northern Route.[66][failed verification] Climate reconstructions also support a Southern Route dispersal of modern humans as the Bab-el-Mandeb strait experienced a climate more conductive to human migration than the northern landbridge to the Levant during the major human dispersal out of Africa.[67]
A 2023 study proposed that Eurasians and Africans genetically diverged ~100,000 years ago. Main Eurasians then lived in the Saudi Peninsula, genetically isolated from at least 85 kya, before expanding north 54 kya. For reference, Homo sapiens and Neanderthals diverged ~500 kya.[68]
It is thought that Australia was inhabited around 65,000–50,000 years ago. As of 2017, the earliest evidence of humans in Australia is at least 65,000 years old,[27][28] while McChesney stated that
...genetic evidence suggests that a small band with the marker M168 migrated out of Africa along the coasts of the Arabian Peninsula and India, through Indonesia, and reached Australia very early, between 60,000 and 50,000 years ago. This very early migration into Australia is also supported by Rasmussen et al. (2011).[31]
Fossils from Lake Mungo, Australia, have been dated to about 42,000 years ago.[69][70] Other fossils from a site called Madjedbebe have been dated to at least 65,000 years ago,[71][72] though some researchers doubt this early estimate and date the Madjedbebe fossils at about 50,000 years ago at the oldest.[29][30]
Phylogenetic data suggests that an early Eastern Eurasian (Eastern non-African) meta-population trifurcated somewhere in eastern South Asia, and gave rise to the Australo-Papuans, the Ancient Ancestral South Indians (AASI), as well as East/Southeast Asians, although Papuans may have also received some gene flow from an earlier group (xOoA), around 2%,[73] next to additional archaic admixture in the Sahul region.[74][75]
According to one study, Papuans could have either formed from a mixture between an East Eurasian lineage and lineage basal to West and East Asians, or as a sister lineage of East Asians with or without a minor basal OoA or xOoA contribution.[76]
A Holocene hunter-gatherer sample (Leang_Panninge) from South Sulawesi was found to be genetically in between East-Eurasians and Australo-Papuans. The sample could be modeled as ~50% Papuan-related and ~50% Basal-East Asian-related (Andamanese Onge or Tianyuan). The authors concluded that Basal-East Asian ancestry was far more widespread and the peopling of Insular Southeast Asia and Oceania was more complex than previously anticipated.[77][78]
In China, the Liujiang man (Chinese: 柳江人) is among the earliest modern humans found in East Asia.[79] The date most commonly attributed to the remains is 67,000 years ago.[80] High rates of variability yielded by various dating techniques carried out by different researchers place the most widely accepted range of dates with 67,000 BP as a minimum, but do not rule out dates as old as 159,000 BP.[80] Liu, Martinón-Torres et al. (2015) claim that modern human teeth have been found in China dating to at least 80,000 years ago.[81]
Tianyuan man from China has a probable date range between 38,000 and 42,000 years ago, while Liujiang man from the same region has a probable date range between 67,000 and 159,000 years ago. According to 2013 DNA tests, Tianyuan man is related "to many present-day Asians and Native Americans".[82][83][84][85][86] Tianyuan is similar in morphology to Liujiang man, and some Jōmon period modern humans found in Japan, as well as modern East and Southeast Asians.[87][88][89]
A 2021 study about the population history of Eastern Eurasia, concluded that distinctive Basal-East Asian (East-Eurasian) ancestry originated in Mainland Southeast Asia at ~50,000BC from a distinct southern Himalayan route, and expanded through multiple migration waves southwards and northwards respectively.[90]
Genetic studies concluded that Native Americans descended from a single founding population that initially split from a Basal-East Asian source population in Mainland Southeast Asia around 36,000 years ago, at the same time at which the proper Jōmon people split from Basal-East Asians, either together with Ancestral Native Americans or during a separate expansion wave. They also show that the basal northern and southern Native American branches, to which all other Indigenous peoples belong, diverged around 16,000 years ago.[91][92] An indigenous American sample from 16,000BC in Idaho, which is craniometrically similar to modern Native Americans as well as Paleosiberians, was found to have largely East-Eurasian ancestry and showed high affinity with contemporary East Asians, as well as Jōmon period samples of Japan, confirming that Ancestral Native Americans split from an East-Eurasian source population in Eastern Siberia.[93]
According to Macaulay et al. (2005), an early offshoot from the southern dispersal with haplogroup N followed the Nile from East Africa, heading northwards and crossing into Asia through the Sinai. This group then branched, some moving into Europe and others heading east into Asia.[32] This hypothesis is supported by the relatively late date of the arrival of modern humans in Europe as well as by archaeological and DNA evidence.[32] Based on an analysis of 55 human mitochondrial genomes (mtDNAs) of hunter-gatherers, Posth et al. (2016) argue for a "rapid single dispersal of all non-Africans less than 55,000 years ago". By 45,000 years ago, modern humans are known to have reached northwestern Europe.[94]
The first lineage to branch off from Mitochondrial Eve was L0. This haplogroup is found in high proportions among the San of Southern Africa and the Sandawe of East Africa. It is also found among the Mbuti people.[95][96] These groups branched off early in human history and have remained relatively genetically isolated since then. Haplogroups L1, L2, and L3 are descendants of L1–L6, and are largely confined to Africa. The macro haplogroups M and N, which are the lineages of the rest of the world outside Africa, descend from L3. L3 is about 70,000 years old, while haplogroups M and N are about 65–55,000 years old.[97][64] The relationship between such gene trees and demographic history is still debated when applied to dispersals.[98]
Of all the lineages present in Africa, the female descendants of only one lineage, mtDNA haplogroup L3, are found outside Africa. If there had been several migrations, one would expect descendants of more than one lineage to be found. L3's female descendants, the M and N haplogroup lineages, are found in very low frequencies in Africa (although haplogroup M1 populations are very ancient and diversified in North and North-east Africa) and appear to be more recent arrivals.[citation needed] A possible explanation is that these mutations occurred in East Africa shortly before the exodus and became the dominant haplogroups thereafter by means of the founder effect. Alternatively, the mutations may have arisen shortly afterwards.
Results from mtDNA collected from aboriginal Malaysians called Orang Asli indicate that the haplogroups M and N share characteristics with original African groups from approximately 85,000 years ago, and share characteristics with sub-haplogroups found in coastal south-east Asian regions, such as Australasia, the Indian subcontinent and throughout continental Asia, which had dispersed and separated from their African progenitor approximately 65,000 years ago. This southern coastal dispersal would have occurred before the dispersal through the Levant approximately 45,000 years ago.[32] This hypothesis attempts to explain why haplogroup N is predominant in Europe and why haplogroup M is absent in Europe. Evidence of the coastal migration is thought to have been destroyed by the rise in sea levels during the Holocene epoch.[99] Alternatively, a small European founder population that had expressed haplogroup M and N at first, could have lost haplogroup M through random genetic drift resulting from a bottleneck (i.e. a founder effect).
The group that crossed the Red Sea travelled along the coastal route around Arabia and Persia until reaching India.[59] Haplogroup M is found in high frequencies along the southern coastal regions of Pakistan and India and it has the greatest diversity in India, indicating that it is here where the mutation may have occurred.[59] Sixty percent of the Indian population belong to Haplogroup M. The indigenous people of the Andaman Islands also belong to the M lineage. The Andamanese are thought to be offshoots of some of the earliest inhabitants in Asia because of their long isolation from the mainland. They are evidence of the coastal route of early settlers that extends from India to Thailand and Indonesia all the way to eastern New Guinea. Since M is found in high frequencies in highlanders from New Guinea and the Andamanese and New Guineans have dark skin and Afro-textured hair, some scientists think they are all part of the same wave of migrants who departed across the Red Sea ~60,000 years ago in the Great Coastal Migration. The proportion of haplogroup M increases eastwards from Arabia to India; in eastern India, M outnumbers N by a ratio of 3:1. Crossing into Southeast Asia, haplogroup N (mostly in the form of derivatives of its R subclade) reappears as the predominant lineage.[citation needed] M is predominant in East Asia, but amongst Indigenous Australians, N is the more common lineage.[citation needed] This haphazard distribution of Haplogroup N from Europe to Australia can be explained by founder effects and population bottlenecks.[100]
A 2002 study of African, European, and Asian populations, found greater genetic diversity among Africans than among Eurasians, and that genetic diversity among Eurasians is largely a subset of that among Africans, supporting the out of Africa model.[102] A large study by Coop et al. (2009) found evidence for natural selection in autosomal DNA outside of Africa. The study distinguishes non-African sweeps (notably KITLG variants associated with skin color), West-Eurasian sweeps (SLC24A5) and East-Asian sweeps (MC1R, relevant to skin color). Based on this evidence, the study concluded that human populations encountered novel selective pressures as they expanded out of Africa.[103] MC1R and its relation to skin color had already been discussed by Harding et al. (2000), p. 1355. According to this study, Papua New Guineans continued to be exposed to selection for dark skin color so that, although these groups are distinct from Africans in other places, the allele for dark skin color shared by contemporary Africans, Andamanese and New Guineans is an archaism. Endicott et al. (2003) suggest convergent evolution. A 2014 study by Gurdasani et al. indicates that the higher genetic diversity in Africa was further increased in some regions by relatively recent Eurasian migrations affecting parts of Africa.[104]
Another promising route towards reconstructing human genetic genealogy is via the JC virus (JCV), a type of human polyomavirus which is carried by 70–90 percent of humans and which is usually transmitted vertically, from parents to offspring, suggesting codivergence with human populations. For this reason, JCV has been used as a genetic marker for human evolution and migration.[105] This method does not appear to be reliable for the migration out of Africa; in contrast to human genetics, JCV strains associated with African populations are not basal. From this Shackelton et al. (2006) conclude that either a basal African strain of JCV has become extinct or that the original infection with JCV post-dates the migration from Africa.
Evidence for archaic human species (descended from Homo heidelbergensis) having interbred with modern humans outside of Africa, was discovered in the 2010s. This concerns primarily Neanderthal admixture in all modern populations except for Sub-Saharan Africans but evidence has also been presented for Denisova hominin admixture in Australasia (i.e. in Melanesians, Aboriginal Australians and some Negritos).[106] The rate of Neanderthal admixture to European and Asian populations as of 2017 has been estimated at between about 2–3%.[107]
Archaic admixture in some Sub-Saharan African populations hunter-gatherer groups (Biaka Pygmies and San), derived from archaic hominins that broke away from the modern human lineage around 700,000 years ago, was discovered in 2011. The rate of admixture was estimated at 2%.[36] Admixture from archaic hominins of still earlier divergence times, estimated at 1.2 to 1.3 million years ago, was found in Pygmies, Hadza and five Sandawe in 2012.[108][35]
From an analysis of Mucin 7, a highly divergent haplotype that has an estimated coalescence time with other variants around 4.5 million years BP and is specific to African populations, it is inferred to have been derived from interbreeding between African modern and archaic humans.[109]
A study published in 2020 found that the Yoruba and Mende populations of West Africa derive between 2% and 19% of their genome from an as-yet unidentified archaic hominin population that likely diverged before the split of modern humans and the ancestors of Neanderthals and Denisovans.[110]
In addition to genetic analysis, Petraglia et al. also examines the small stone tools (microlithic materials) from the Indian subcontinent and explains the expansion of population based on the reconstruction of paleoenvironment. He proposed that the stone tools could be dated to 35 ka in South Asia, and the new technology might be influenced by environmental change and population pressure.[111]
The cladistic relationship of humans with the African apes was suggested by Charles Darwin after studying the behaviour of African apes, one of which was displayed at the London Zoo.[113] The anatomist Thomas Huxley had also supported the hypothesis and suggested that African apes have a close evolutionary relationship with humans.[114] These views were opposed by the German biologist Ernst Haeckel, who was a proponent of the Out of Asia theory. Haeckel argued that humans were more closely related to the primates of South-east Asia and rejected Darwin's African hypothesis.[115][116]
In The Descent of Man, Darwin speculated that humans had descended from apes, which still had small brains but walked upright, freeing their hands for uses which favoured intelligence; he thought such apes were African:
In each great region of the world the living mammals are closely related to the extinct species of the same region. It is, therefore, probable that Africa was formerly inhabited by extinct apes closely allied to the gorilla and chimpanzee; and as these two species are now man's nearest allies, it is somewhat more probable that our early progenitors lived on the African continent than elsewhere. But it is useless to speculate on this subject, for an ape nearly as large as a man, namely the Dryopithecus of Lartet, which was closely allied to the anthropomorphous Hylobates, existed in Europe during the Upper Miocene period; and since so remote a period the earth has certainly undergone many great revolutions, and there has been ample time for migration on the largest scale.
In 1871, there were hardly any human fossils of ancient hominins available. Almost fifty years later, Darwin's speculation was supported when anthropologists began finding fossils of ancient small-brained hominins in several areas of Africa (list of hominina fossils). The hypothesis of recent (as opposed to archaic) African origin developed in the 20th century. The "Recent African origin" of modern humans means "single origin" (monogenism) and has been used in various contexts as an antonym to polygenism. The debate in anthropology had swung in favour of monogenism by the mid-20th century. Isolated proponents of polygenism held forth in the mid-20th century, such as Carleton Coon, who thought as late as 1962 that H. sapiens arose five times from H. erectus in five places.[118]
The historical alternative to the recent origin model is the multiregional origin of modern humans, initially proposed by Milford Wolpoff in the 1980s. This view proposes that the derivation of anatomically modern human populations from H. erectus at the beginning of the Pleistocene 1.8 million years BP, has taken place within a continuous world population. The hypothesis necessarily rejects the assumption of an infertility barrier between ancient Eurasian and African populations of Homo. The hypothesis was controversially debated during the late 1980s and the 1990s.[119] The now-current terminology of "recent-origin" and "Out of Africa" became current in the context of this debate in the 1990s.[120] Originally seen as an antithetical alternative to the recent origin model, the multiregional hypothesis in its original "strong" form is obsolete, while its various modified weaker variants have become variants of a view of "recent origin" combined with archaic admixture.[121] Stringer (2014) distinguishes the original or "classic" Multiregional model as having existed from 1984 (its formulation) until 2003, to a "weak" post-2003 variant that has "shifted close to that of the Assimilation Model".[122][123]
In the 1980s, Allan Wilson together with Rebecca L. Cann and Mark Stoneking worked on genetic dating of the matrilineal most recent common ancestor of modern human populations (dubbed "Mitochondrial Eve"). To identify informative genetic markers for tracking human evolutionary history, Wilson concentrated on mitochondrial DNA (mtDNA), which is maternally inherited. This DNA material mutates quickly, making it easy to plot changes over relatively short times. With his discovery that human mtDNA is genetically much less diverse than chimpanzee mtDNA, Wilson concluded that modern human populations had diverged recently from a single population while older human species such as Neanderthals and Homo erectus had become extinct.[124] With the advent of archaeogenetics in the 1990s, the dating of mitochondrial and Y-chromosomal haplogroups became possible with some confidence. By 1999, estimates ranged around 150,000 years for the mt-MRCA and 60,000 to 70,000 years for the migration out of Africa.[125]
From 2000 to 2003, there was controversy about the mitochondrial DNA of "Mungo Man 3" (LM3) and its possible bearing on the multiregional hypothesis. LM3 was found to have more than the expected number of sequence differences when compared to modern human DNA (CRS).[126] Comparison of the mitochondrial DNA with that of ancient and modern aborigines, led to the conclusion that Mungo Man fell outside the range of genetic variation seen in Aboriginal Australians and was used to support the multiregional origin hypothesis. A reanalysis of LM3 and other ancient specimens from the area published in 2016, showed it to be akin to modern Aboriginal Australian sequences, inconsistent with the results of the earlier study.[127]
The Y chromosome, which is paternally inherited, does not go through much recombination and thus stays largely the same after inheritance. Similar to Mitochondrial Eve, this could be studied to track the male most recent common ancestor ("Y-chromosomal Adam" or Y-MRCA).[128]
The most basal lineages have been detected in West, Northwest and Central Africa, suggesting plausibility for the Y-MRCA living in the general region of "Central-Northwest Africa".[16]
A Stanford University School of Medicine study was done by comparing Y-chromosome sequences and mtDNA in 69 men from different geographic regions and constructing a family tree. It was found that the Y-MRCA lived between 120,000 and 156,000, and the Mitochondrial Eve lived between 99,000 and 148,000 years ago, which not only predates some proposed waves of migration, but also meant that both lived in the African continent around the same time period.[129]
Another study finds a plausible placement in "the north-western quadrant of the African continent" for the emergence of the A1b haplogroup.[130] The 2013 report of haplogroup A00 found among the Mbo people of western present-day Cameroon is also compatible with this picture.[131]
The revision of Y-chromosomal phylogeny since 2011 has affected estimates for the likely geographical origin of Y-MRCA as well as estimates on time depth. By the same reasoning, future discovery of presently-unknown archaic haplogroups in living people would again lead to such revisions. In particular, the possible presence of between 1% and 4% Neanderthal-derived DNA in Eurasian genomes implies that the (unlikely) event of a discovery of a single living Eurasian male exhibiting a Neanderthal patrilineal line would immediately push back T-MRCA ("time to MRCA") to at least twice its current estimate. However, the discovery of a Neanderthal Y-chromosome by Mendez et al. was tempered by a 2016 study that suggests the extinction of Neanderthal patrilineages, as the lineage inferred from the Neanderthal sequence is outside of the range of contemporary human genetic variation.[132] Questions of geographical origin would become part of the debate on Neanderthal evolution from Homo erectus.

The caveman is a stock character representative of primitive humans in the Paleolithic. The popularization of the type dates to the early 20th century, when Neanderthals were influentially described as "simian" or "ape-like" by Marcellin Boule[1] and Arthur Keith.[2]
The term "caveman" has its taxonomic equivalent in the now-obsolete binomial classification of Homo troglodytes (Linnaeus, 1758).[3]
Cavemen are typically portrayed as wearing shaggy animal hides, and capable of cave painting like behaviorally modern humans of the last glacial period. They are often shown armed with rocks, cattle bone clubs, spears, or sticks with rocks tied to them, and are portrayed as unintelligent, easily frightened, and aggressive. Typically, they have a low pitched rough voice and make vocalizations such as "ooga-booga" and grunts or speak using simple phrases. Popular culture also frequently represents cavemen as living with, or alongside of, dinosaurs, even though non-avian dinosaurs became extinct at the end of the Cretaceous period, 66 million years before the emergence of Homo sapiens. The era typically associated with the archetype is the Paleolithic Era, sometimes referred to as the Stone Age, though the Paleolithic is but one part of the Stone Age. This era extends from more than 2 million years into the past until between 40,000 and 5,000 years before the present (i.e., from around 2,000 kya to between 40 and 5 kya).[citation needed]
The image of these people living in caves arises from the fact that caves are where the preponderance of artifacts have been found from European Stone Age cultures. However, this most likely reflects the degree of preservation that caves provide over the millennia, rather than an indication of them being a typical form of shelter. Until the last glacial period, the great majority of humans did not live in caves, as nomadic hunter-gatherer tribes lived in a variety of temporary structures, such as tents[4] and wooden huts (e.g., at Ohalo). A few genuine cave dwellings did exist, however, such as at Mount Carmel in Israel.[5]
Stereotypical cavemen have traditionally been depicted wearing smock-like garments made from the skins of animals and held up by a shoulder strap on one side, or loincloths made from leopard or tiger skins. Stereotypical cavewomen are similarly depicted, but sometimes with slimmer proportions and bones tied up in their hair. They are also depicted carrying large clubs approximately conical in shape. They often have grunt-like names, such as "Ugg" and "Zog".[6]
Caveman-like heraldic "wild men" were found in European and African iconography for hundreds of years. During the Middle Ages, these beings were generally depicted in art and literature as bearded and covered in hair, and often wielding clubs and dwelling in caves. While wild men were always depicted as living outside of civilization, it was not always clearly whether they were human or non-human.[7]
In Sir Arthur Conan Doyle's The Lost World (1912), ape-men are depicted in a fight with modern humans. How the First Letter Was Written and How the Alphabet was Made are two of Rudyard Kipling's Just So Stories (1902) featuring a group of cave-people. Edgar Rice Burroughs adapted this idea for The Land That Time Forgot (1918). A genre of cavemen films emerged, typified by D. W. Griffith's Man's Genesis (1912); they inspired Charles Chaplin's satiric take[8] in His Prehistoric Past (1914), as well as Brute Force (1914), The Cave Man (1912), and later, Cave Man (1934). From the descriptions, Griffith's characters cannot talk, and use sticks and stones for weapons, while the hero of Cave Man is a Tarzanesque figure who fights dinosaurs. Captain Caveman and the Teen Angels (1977–1980) is an animated comedy depicting the titular caveman as being hairy and carrying clubs, and in one episode extends this trait to other cave dwellers from his time period.[citation needed]
Griffith's Brute Force represents one of the earliest portrayals of cavemen and dinosaurs together, with its depiction of a Ceratosaurus.[9][10] The film reinforced the incorrect notion that non-avian dinosaurs co-existed with prehistoric humans.[10] The anachronistic combination of cavemen with dinosaurs eventually became a cliché, and has often been intentionally invoked for comedic effect. The comic strips B.C., Alley Oop, the Spanish comic franchise Mortadelo y Filemón, and occasionally The Far Side and Gogs portray "cavemen" with dinosaurs. Gary Larson, in his 1989 book The Prehistory of the Far Side, stated he once felt that he needed to confess his cartooning sins in this regard: "O Father, I Have Portrayed Primitive Man and Dinosaurs In The Same Cartoon".[11] The animated series The Flintstones, a spoof on family sitcoms, portrays the Flintstones even using dinosaurs, pterosaurs and prehistoric mammals as tools, household appliances, vehicles, and construction equipment; some spinoffs of the series also feature Captain Caveman.[12]

High-altitude adaptation in humans is an instance of evolutionary modification in certain human populations, including those of Tibet in Asia, the Andes of the Americas, and Ethiopia in Africa, who have acquired the ability to survive at altitudes above 2,500 meters (8,200 ft).[1] This adaptation means irreversible, long-term physiological responses to high-altitude environments associated with heritable behavioral and genetic changes. While the rest of the human population would suffer serious health consequences at high altitudes, the indigenous inhabitants of these regions thrive in the highest parts of the world. These humans have undergone extensive physiological and genetic changes, particularly in the regulatory systems of oxygen respiration and blood circulation when compared to the general lowland population.[2][3]
Around 81.6 million humans (approximately 1.1% of the world's human population) live permanently at altitudes above 2,500 meters (8,200 ft),[4] which would seem to put these populations at risk for chronic mountain sickness (CMS).[1] However, the high-altitude populations in South America, East Africa, and South Asia have lived there for millennia without apparent complications.[5] This special adaptation is now recognized as an example of natural selection in action.[6] The adaptation of the Tibetans is the fastest known example of human evolution, as it is estimated to have occurred between 1,000 BCE[7][8][9] to 7,000 BCE.[10][11]
Humans are generally adapted to lowland environments where oxygen is abundant.[12] At altitudes above 2,500 meters (8,200 ft), such humans experience altitude sickness, which is a type of hypoxia, a clinical syndrome of severe lack of oxygen. Some humans develop the illness beginning at above 1,500 meters (5,000 ft).[13] Symptoms include fatigue, dizziness, breathlessness, headaches, insomnia, malaise, nausea, vomiting, body pain, loss of appetite, ear-ringing, blistering and purpling of the hands and feet, and dilated blood vessels.[14][15][16]
The sickness is compounded by related symptoms such as cerebral oedema (swelling of brain)  and pulmonary oedema (fluid accumulation in lungs) .[17][18] Over a span of multiple days, individuals experiencing the effects of high-altitude hypoxia demonstrate raised respiratory activity and elevated metabolic conditions which persist during periods of rest. Subsequently, afflicted people will experience slowly declining heart rate. Hypoxia is a primary contributor to fatalities within mountaineering groups, making it a significant risk factor within high-altitude related challenges.[19][20] In women, pregnancy can be severely affected, such as development of preeclampsia, which causes premature labor, low birth weight of babies, and often complicates with profuse bleeding, seizures, or death of the mother.[2][21]
An estimated 81.6 million humans live at an elevation higher than 2,500 meters (8,200 ft) above sea level, of which 21.7 million reside in Ethiopia, 12.5 million in China, 11.7 million in Colombia, 7.8 million in Peru, and 6.2 million in Bolivia.[4] Certain natives of Tibet, Ethiopia, and the Andes have been living at these high altitudes for generations and are resistant to hypoxia as a consequence of genetic adaptation.[5][14] It is estimated that at 4,000 meters (13,000 ft) altitude, every lungful of air has approximately 60% of the oxygen molecules found in a lungful of air at sea level.[22] Highlanders are thus constantly exposed to a low oxygen environment, yet they live without any debilitating problems.[23]
One of the best-documented effects of high altitude on non-adapted women is a progressive reduction in birth weight. By contrast, the women of long-resident, high-altitude populations are known to give birth to heavier-weight infants than women of the lowland. This is particularly true among Tibetan babies, whose average birth weight is 294–650g (~470) g heavier than the surrounding Chinese population, and their blood-oxygen level is considerably higher.[24]
Scientific investigation of high-altitude adaptation was initiated by A. Roberto Frisancho of the University of Michigan in the late 1960s among the Quechua people of Peru.[25][26] Paul T. Baker of Penn State University’s Department of Anthropology also conducted a considerable amount of research into human adaptation to high altitudes, and mentored students who continued this research.[27] One of these students, anthropologist Cynthia Beall of Case Western Reserve University, began conducting decades-long research on high altitude adaptation among the Tibetans in the early 1980s.[28]
Among the different native highlander populations, the underlying physiological responses to adaptation differ. For example, among four quantitative features, such as resting ventilation, hypoxic ventilatory response, oxygen saturation, and hemoglobin concentration, the levels of variations are significantly different between the Tibetans and the Aymaras.[29] Methylation also influences oxygenation.[30]
In the early 20th century, researchers observed the impressive physical abilities of Tibetans during Himalayan climbing expeditions. They considered the possibility that these abilities resulted from an evolutionary genetic adaptation to high-altitude conditions.[31] The Tibetan plateau has an average elevation of 4,000 meters (13,000 ft) above sea level and covers more than 2.5 million km2; it is the highest and largest plateau in the world. In 1990, it was estimated that 4,594,188 Tibetans live on the plateau, with 53% living at an altitude over 3,500 meters (11,500 ft). Fairly large numbers (approximately 600,000) live at an altitude exceeding 4,500 meters (14,800 ft) in the Chantong-Qingnan area.[32]
Tibetans who have been living in the Chantong-Qingnan area for 3,000 years do not exhibit the same elevated hemoglobin concentrations to cope with oxygen deficiency that are observed in other populations who have moved temporarily or permanently to high altitudes. Instead, the Tibetans inhale more air with each breath and breathe more rapidly than either sea-level populations or Andeans. Tibetans have better oxygenation at birth, enlarged lung volumes throughout life, and a higher capacity for exercise. They show a sustained increase in cerebral blood flow, lower hemoglobin concentration, and less susceptibility to chronic mountain sickness than other populations due to their longer history of high-altitude habitation.[33][34]
With the proper physical preparation, individuals can develop short-term tolerance to high-altitude conditions. However, these biological changes are temporary and will reverse upon returning to lower elevations.[35] Moreover, while lowland people typically experience increased breathing for only a few days after entering high altitudes, Tibetans maintain this rapid breathing and elevated lung capacity throughout their lifetime.[36] This enables them to inhale large amounts of air per unit of time to compensate for low oxygen levels. Additionally, Tibetans typically have significantly higher levels of nitric oxide in their blood, often double that of lowlanders. This likely contributes to enhanced blood circulation by promoting vasodilation.[37]
Furthermore, their hemoglobin level is not significantly different (average 15.6 g/dl in males and 14.2 g/dl in females)[38] from those of humans living at low altitude.  This is evidenced by mountaineers experiencing an increase of over 2 g/dl in hemoglobin levels within two weeks at the Mt. Everest base camp.[39] Consequently, Tibetans demonstrate the capacity to mitigate the effects of hypoxia and mountain sickness throughout their lives. Even when ascending extraordinarily high peaks such as Mount Everest, they exhibit consistent oxygen uptake, heightened ventilation, augmented hypoxic ventilatory responses, expanded lung volumes, increased diffusing capacities, stable body weight, and improved sleep quality compared to lowland populations.[40]
In contrast to the Tibetans, Andean highlanders show different patterns of hemoglobin adaptation. Their hemoglobin concentration is higher than those of the lowlander population, which also happens to lowlanders who move to high altitudes. When they spend some weeks in the lowlands, their hemoglobin drops to the same levels as lowland humans. However, in contrast to lowland humans, they have increased oxygen levels in their hemoglobin; that is, more oxygen per blood volume. This confers an ability to carry more oxygen in each red blood cell, meaning a more effective transport of oxygen throughout their bodies.[36] This enables Andeans to overcome hypoxia and normally reproduce without risk of death for the mother or baby. They have developmentally-acquired enlarged residual lung volume and an associated increase in alveolar area, which are supplemented with increased tissue thickness and moderate increase in red blood cells. Though Andean highlander children show delayed body growth, change in lung volume is accelerated.[41]
Among the Quechua people of the Altiplano, there is a significant variation in NOS3 (the gene encoding endothelial nitric oxide synthase, eNOS), which is associated with higher levels of nitric oxide at high altitude.[42] Nuñoa children of Quechua ancestry exhibit higher blood-oxygen content (91.3) and lower heart rate (84.8) than their peers of different ethnicities, who have an average of 89.9 blood-oxygen and 88–91 heart rate.[43] Quechua women have comparatively enlarged lung volume for increased respiration.[44]
Blood profile comparisons show that among the Andeans, Aymaran highlanders are better adapted to highlands than the Quechuas.[45][46] Among the Bolivian Aymara people, the resting ventilation and hypoxic ventilatory response were quite low (roughly 1.5 times lower) compared to those of the Tibetans. The intrapopulation genetic variation was relatively smaller among the Aymara people.[47][48] Moreover, when compared to Tibetans, blood hemoglobin levels at high altitudes among Aymaran is notably higher, with an average of 19.2 g/dl for males and 17.8 g/dl for females.[38]
The people of the Ethiopian highlands also live at extremely high altitudes, around 3,000 meters (9,800 ft) to 3,500 meters (11,500 ft). Highland Ethiopians exhibit elevated hemoglobin levels, like Andeans and lowlander humans at high altitudes, but do not exhibit the Andeans’ increase in oxygen content of hemoglobin.[49] Among healthy individuals, the average hemoglobin concentrations are 15.9 and 15.0 g/dl for males and females, respectively (which is lower than normal, similar to the Tibetans), and an average oxygen saturation of hemoglobin is 95.3% (which is higher than average, like the Andeans).[50]  Additionally, Ethiopian highlanders do not exhibit any significant change in blood circulation of the brain, which has been observed among the Peruvian highlanders and attributed to their frequent altitude-related illnesses.[51] Yet, similar to the Andeans and Tibetans, the Ethiopian highlanders are immune to the extreme dangers posed by high-altitude environment, and their pattern of adaptation is unique from that of other highland people.[22]
The underlying molecular evolution of high-altitude adaptation has been explored in recent years.[23] Depending on geographical and environmental pressures, high-altitude adaptation involves different genetic patterns, some of which have evolved not long ago. For example, Tibetan adaptations became prevalent in the past 3,000 years, an example of rapid recent human evolution. At the turn of the 21st century, it was reported that the genetic makeup of the respiratory components of the Tibetan and the Ethiopian populations were significantly different.[29]
Substantial evidence from Tibetan highlanders suggests that variation in hemoglobin and blood-oxygen levels are adaptive as Darwinian fitness. It has been documented that Tibetan women with a high likelihood of possessing one to two alleles for high blood-oxygen content (which is rare in other women) had more surviving children; the higher the oxygen capacity, the lower the infant mortality.[52] In 2010, for the first time, the genes responsible for the unique adaptive traits were identified following genome sequencing of 50 Tibetans and 40 Han Chinese from Beijing. Initially, the strongest signal of natural selection was a transcription factor involved in response to hypoxia, called endothelial Per-Arnt-Sim (PAS) domain protein 1 (EPAS1). It was found that one single-nucleotide polymorphism (SNP) at EPAS1 shows a 78% frequency difference between Tibetan and mainland Chinese samples, representing the fastest genetic change observed in any human gene to date. Hence, Tibetan adaptation to high altitude is recognized as one of the fastest processes of phenotypically observable evolution in humans,[53] which is estimated to have occurred a few thousand years ago, when the Tibetans split from the mainland Chinese population. The time of genetic divergence has been variously estimated as 2,750 (original estimate),[9] 4,725,[11] 8,000,[54] or 9,000[10] years ago.
Mutations in EPAS1 occur at a higher frequency in Tibetans than their Han neighbors and correlates with decreased hemoglobin concentrations among the Tibetans. This is known as the hallmark of their adaptation to hypoxia. Simultaneously, two genes, egl nine homolog 1 (EGLN1), which inhibits hemoglobin production under high oxygen concentration, and peroxisome proliferator-activated receptor alpha (PPARA), were also identified to be positively selected for decreased hemoglobin levels in the Tibetans.[55]
Similarly, the Sherpas, known for their Himalayan hardiness, exhibit similar patterns in the EPAS1 gene, which is further evidence that the gene is under selection pressure for adaptation to the high-altitude life of Tibetans.[56] A study in 2014 indicates that the mutant EPAS1 gene could have been inherited from archaic hominins, the Denisovans.[57] EPAS1 and EGLN1 are believed to be important genes for unique adaptive traits when compared with those of the Chinese and Japanese.[58] Comparative genome analysis in 2014 revealed that the Tibetans inherited an equal mixture of genomes from the Nepalese Sherpas and Hans, and that they acquired adaptive genes from the Sherpa lineage. Further, the population split was estimated to occur around 20,000 to 40,000 years ago, a range supported by archaeological, mitochondria DNA, and Y chromosome evidence for an initial colonization of the Tibetan plateau around 30,000 years ago.[59]
The genes EPAS1, EGLN1, and PPARA function in concert with another gene named hypoxia inducible factors (HIF), which is in turn a principal regulator of red blood cell production (erythropoiesis) in response to oxygen metabolism.[60][61][62] The genes are associated not only with decreased hemoglobin levels, but also with regulating metabolism. EPAS1 is significantly associated with increased lactate concentration, a product of anaerobic glycolysis, and PPARA is correlated with decrease in the activity of fatty acid oxidation.[63] EGLN1 codes for an enzyme, prolyl hydroxylase 2 (PHD2), involved in erythropoiesis.
Among the Tibetans, a mutation in EGLN1 (specifically at position 12, where cytosine is replaced with guanine; and at 380, where G is replaced with C) results in mutant PHD2 (aspartic acid at position 4 becomes glutamine, and cysteine at 127 becomes serine) and this mutation inhibits erythropoiesis. This mutation is estimated to have occurred approximately 8,000 years ago.[64] Further, the Tibetans are enriched for genes in the disease class of human reproduction (such as genes from the DAZ, BPY2, CDY, and HLA-DQ and HLA-DR gene clusters) and biological process categories of response to DNA damage stimulus and DNA repair (such as RAD51, RAD52, and MRE11A), which are related to the adaptive traits of high infant birth weight and darker skin tone and are most likely due to recent local adaptation.[65]
The patterns of genetic adaptation among the Andeans are largely distinct from those of the Tibetans, with both populations showing evidence of positive natural selection in different genes or gene regions. For genes in the HIF pathway, EGLN1 is the only instance where evidence of positive selection is observed in both Tibetans and Andeans.[66] Even then, the pattern of variation for this gene differs between the two populations.[6] Furthermore, there are no significant associations between EPAS1 or EGLN1 SNP genotypes and hemoglobin concentration among the Andeans, which is characteristic of the Tibetans.[67]
The Andean pattern of adaptation is characterized by selection in a number of genes involved in cardiovascular development and function (such as BRINP3, EDNRA, NOS2A).[68][69] This suggests that selection in Andeans, instead of targeting the HIF pathway like in the Tibetans, focused on adaptations of the cardiovascular system to combat chronic disease at high altitude. Analysis of ancient Andean genomes, some dating back 7,000 years, discovered selection in DST, a gene involved in cardiovascular function.[70] The whole genome sequences of 20 Andeans (half of them having chronic mountain sickness) revealed that two genes, SENP1 (an erythropoiesis regulator) and ANP32D (an oncogene) play vital roles in their weak adaptation to hypoxia.[71]
The adaptive mechanism of Ethiopian highlanders differs from those of the Tibetans and Andeans due to the fact that their migration to the highland was relatively early. For example, the Amhara have inhabited altitudes above 2,500 meters (8,200 ft) for at least 5,000 years and altitudes around 2,000 meters (6,600 ft) to 2,400 meters (7,900 ft) for more than 70,000 years.[72] Genomic analysis of two ethnic groups, Amhara and Oromo, has revealed that gene variations associated with hemoglobin difference among Tibetans or other variants at the exact gene location do not influence the adaptation in Ethiopians.[73] Several candidate genes have been identified as possible explanations for the adaptation of Ethiopians, including CBARA1, VAV3, ARNT2 and THRB. Two of these genes (THRB and ARNT2) are known to play a role in the HIF-1 pathway, a pathway implicated in previous work reported in Tibetan and Andean studies. This supports the hypothesis that adaptation to high altitude arose independently among different highlander populations as a result of convergent evolution.[74]

A zoonosis (/zoʊˈɒnəsɪs, ˌzoʊəˈnoʊsɪs/ ⓘ;[1] plural zoonoses) or zoonotic disease is an infectious disease of humans caused by a pathogen (an infectious agent, such as a bacterium, virus, parasite, or prion) that can jump from a non-human vertebrate to a human. When humans infect non-humans, it is called reverse zoonosis or anthroponosis.[2][1][3][4]
Major modern diseases such as Ebola and salmonellosis are zoonoses. HIV was a zoonotic disease transmitted to humans in the early part of the 20th century, though it has now evolved into a separate human-only disease.[5][6][7] Human infection with animal influenza viruses is rare, as they do not transmit easily to or among humans.[8] However, avian and swine influenza viruses in particular possess high zoonotic potential,[9] and these occasionally recombine with human strains of the flu and can cause pandemics such as the 2009 swine flu.[10] Zoonoses can be caused by a range of disease pathogens such as emergent viruses, bacteria, fungi and parasites; of 1,415 pathogens known to infect humans, 61% were zoonotic.[11] Most human diseases originated in non-humans; however, only diseases that routinely involve non-human to human transmission, such as rabies, are considered direct zoonoses.[12]
Zoonoses have different modes of transmission. In direct zoonosis the disease is directly transmitted from non-humans to humans through media such as air (influenza) or bites and saliva (rabies).[13] In contrast, transmission can also occur via an intermediate species (referred to as a vector), which carry the disease pathogen without getting sick. The term is from Ancient Greek: ζῷον zoon "animal" and νόσος nosos "sickness".
Host genetics plays an important role in determining which non-human viruses will be able to make copies of themselves in the human body. Dangerous non-human viruses are those that require few mutations to begin replicating themselves in human cells. These viruses are dangerous since the required combinations of mutations might randomly arise in the natural reservoir.[14]
The emergence of zoonotic diseases originated with the domestication of animals.[15] Zoonotic transmission can occur in any context in which there is contact with or consumption of animals, animal products, or animal derivatives. This can occur in a companionistic (pets), economic (farming, trade, butchering, etc.), predatory (hunting, butchering, or consuming wild game), or research context.[16][17]
Recently, there has been a rise in frequency of appearance of new zoonotic diseases. "Approximately 1.67 million undescribed viruses are thought to exist in mammals and birds, up to half of which are estimated to have the potential to spill over into humans", says a study[18] led by researchers at the University of California, Davis. According to a report from the United Nations Environment Programme and International Livestock Research Institute a large part of the causes are environmental like climate change, unsustainable agriculture, exploitation of wildlife, and land use change. Others are linked to changes in human society such as an increase in mobility. The organizations propose a set of measures to stop the rise.[19][20]
Foodborne zoonotic diseases are caused by a variety of pathogens that can affect both humans and animals. The most significant zoonotic pathogens causing foodborne diseases are:
Escherichia coli O157:H7, Campylobacter, Caliciviridae, and Salmonella.[21][22][23]
Contact with farm animals can lead to disease in farmers or others that come into contact with infected farm animals. Glanders primarily affects those who work closely with horses and donkeys. Close contact with cattle can lead to cutaneous anthrax infection, whereas inhalation anthrax infection is more common for workers in slaughterhouses, tanneries, and wool mills.[28] Close contact with sheep who have recently given birth can lead to infection with the bacterium Chlamydia psittaci, causing chlamydiosis (and enzootic abortion in pregnant women), as well as increase the risk of Q fever, toxoplasmosis, and listeriosis, in the pregnant or otherwise immunocompromised. Echinococcosis is caused by a tapeworm, which can spread from infected sheep by food or water contaminated by feces or wool. Avian influenza is common in chickens, and, while it is rare in humans, the main public health worry is that a strain of avian influenza will recombine with a human influenza virus and cause a pandemic like the 1918 Spanish flu.[29] In 2017, free-range chickens in the UK were temporarily ordered to remain inside due to the threat of avian influenza.[30] Cattle are an important reservoir of cryptosporidiosis,[31] which mainly affects the immunocompromised. Reports have shown mink can also become infected.[32] In Western countries, hepatitis E burden is largely dependent on exposure to animal products, and pork is a significant source of infection, in this respect.[24] Similarly, the human coronavirus OC43, the main cause of the common cold, can use the pig as a zoonotic reservoir,[33] constantly reinfecting the human population.
Veterinarians are exposed to unique occupational hazards when it comes to zoonotic disease. In the US, studies have highlighted an increased risk of injuries and lack of veterinary awareness of these hazards. Research has proved the importance for continued clinical veterinarian education on occupational risks associated with musculoskeletal injuries, animal bites, needle-sticks, and cuts.[34]
A July 2020 report by the United Nations Environment Programme stated that the increase in zoonotic pandemics is directly attributable to anthropogenic destruction of nature and the increased global demand for meat and that the industrial farming of pigs and chickens in particular will be a primary risk factor for the spillover of zoonotic diseases in the future.[35] Habitat loss of viral reservoir species has been identified as a significant source in at least one spillover event.[36]
The wildlife trade may increase spillover risk because it directly increases the number of interactions across animal species, sometimes in small spaces.[37] The origin of the COVID-19 pandemic[38][39] is traced to the wet markets in China.[40][41][42][43]
Zoonotic disease emergence is demonstrably linked to the consumption of wildlife meat, exacerbated by human encroachment into natural habitats and amplified by the unsanitary conditions of wildlife markets.[44] These markets, where diverse species converge, facilitate the mixing and transmission of pathogens, including those responsible for outbreaks of HIV-1,[45] Ebola,[46] and mpox,[47] and potentially even the COVID-19 pandemic.[48] Notably, small mammals often harbor a vast array of zoonotic bacteria and viruses,[49] yet endemic bacterial transmission among wildlife remains largely unexplored. Therefore, accurately determining the pathogenic landscape of traded wildlife is crucial for guiding effective measures to combat zoonotic diseases and documenting the societal and environmental costs associated with this practice.
Pets can transmit a number of diseases. Dogs and cats are routinely vaccinated against rabies. Pets can also transmit ringworm and Giardia, which are endemic in both animal and human populations. Toxoplasmosis is a common infection of cats; in humans it is a mild disease although it can be dangerous to pregnant women.[50] Dirofilariasis is caused by Dirofilaria immitis through mosquitoes infected by mammals like dogs and cats. Cat-scratch disease is caused by Bartonella henselae and Bartonella quintana, which are transmitted by fleas that are endemic to cats. Toxocariasis is the infection of humans by any of species of roundworm, including species specific to dogs (Toxocara canis) or cats (Toxocara cati). Cryptosporidiosis can be spread to humans from pet lizards, such as the leopard gecko. Encephalitozoon cuniculi is a microsporidial parasite carried by many mammals, including rabbits, and is an important opportunistic pathogen in people immunocompromised by HIV/AIDS, organ transplantation, or CD4+ T-lymphocyte deficiency.[51]
Pets may also serve as a reservoir of viral disease and contribute to the chronic presence of certain viral diseases in the human population. For instance, approximately 20% of domestic dogs, cats, and horses carry anti-hepatitis E virus antibodies and thus these animals probably contribute to human hepatitis E burden as well.[52] For non-vulnerable populations (e.g., people who are not immunocompromised) the associated disease burden is, however, small.[53][54] Furthermore, the trade of non domestic animals such as wild animals as pets can also increase the risk of zoonosis spread.[55][56]
Outbreaks of zoonoses have been traced to human interaction with, and exposure to, other animals at fairs, live animal markets,[57] petting zoos, and other settings. In 2005, the Centers for Disease Control and Prevention (CDC) issued an updated list of recommendations for preventing zoonosis transmission in public settings.[58] The recommendations, developed in conjunction with the National Association of State Public Health Veterinarians,[59] include educational responsibilities of venue operators, limiting public animal contact, and animal care and management.
Hunting involves humans tracking, chasing, and capturing wild animals, primarily for food or materials like fur. However, other reasons like pest control or managing wildlife populations can also exist. Transmission of zoonotic diseases, those leaping from animals to humans, can occur through various routes: direct physical contact, airborne droplets or particles, bites or vector transport by insects, oral ingestion, or even contact with contaminated environments.[60] Wildlife activities like hunting and trade bring humans closer to dangerous zoonotic pathogens, threatening global health.[61]
According to the Center for Diseases Control and Prevention (CDC) hunting and consuming wild animal meat ("bushmeat") in regions like Africa can expose people to infectious diseases due to the types of animals involved, like bats and primates. Unfortunately, common preservation methods like smoking or drying aren't enough to eliminate these risks.[62] Although bushmeat provides protein and income for many, the practice is intricately linked to numerous emerging infectious diseases like Ebola, HIV, and SARS, raising critical public health concerns.[61]
A review published in 2022 found evidence that zoonotic spillover linked to wildmeat consumption has been reported across all continents.[63]
Kate Jones, Chair of Ecology and Biodiversity at University College London, says zoonotic diseases are increasingly linked to environmental change and human behavior. The disruption of pristine forests driven by logging, mining, road building through remote places, rapid urbanization, and population growth is bringing people into closer contact with animal species they may never have been near before. The resulting transmission of disease from wildlife to humans, she says, is now "a hidden cost of human economic development".[64] In a guest article, published by IPBES, President of the EcoHealth Alliance and zoologist Peter Daszak, along with three co-chairs of the 2019 Global Assessment Report on Biodiversity and Ecosystem Services, Josef Settele, Sandra Díaz, and Eduardo Brondizio, wrote that "rampant deforestation, uncontrolled expansion of agriculture, intensive farming, mining and infrastructure development, as well as the exploitation of wild species have created a 'perfect storm' for the spillover of diseases from wildlife to people."[65]
Joshua Moon, Clare Wenham, and Sophie Harman said that there is evidence that decreased biodiversity has an effect on the diversity of hosts and frequency of human-animal interactions with potential for pathogenic spillover.[66]
An April 2020 study, published in the Proceedings of the Royal Society's Part B journal, found that increased virus spillover events from animals to humans can be linked to biodiversity loss and environmental degradation, as humans further encroach on wildlands to engage in agriculture, hunting, and resource extraction they become exposed to pathogens which normally would remain in these areas. Such spillover events have been tripling every decade since 1980.[67] An August 2020 study, published in Nature, concludes that the anthropogenic destruction of ecosystems for the purpose of expanding agriculture and human settlements reduces biodiversity and allows for smaller animals such as bats and rats, which are more adaptable to human pressures and also carry the most zoonotic diseases, to proliferate. This in turn can result in more pandemics.[68]
In October 2020, the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services published its report on the 'era of pandemics' by 22 experts in a variety of fields and concluded that anthropogenic destruction of biodiversity is paving the way to the pandemic era and could result in as many as 850,000 viruses being transmitted from animals – in particular birds and mammals – to humans. The increased pressure on ecosystems is being driven by the "exponential rise" in consumption and trade of commodities such as meat, palm oil, and metals, largely facilitated by developed nations, and by a growing human population. According to Peter Daszak, the chair of the group who produced the report, "there is no great mystery about the cause of the Covid-19 pandemic, or of any modern pandemic. The same human activities that drive climate change and biodiversity loss also drive pandemic risk through their impacts on our environment."[69][70][71]
According to a report from the United Nations Environment Programme and International Livestock Research Institute, entitled "Preventing the next pandemic – Zoonotic diseases and how to break the chain of transmission", climate change is one of the 7 human-related causes of the increase in the number of zoonotic diseases.[19][20] The University of Sydney issued a study, in March 2021, that examines factors increasing the likelihood of epidemics and pandemics like the COVID-19 pandemic. The researchers found that "pressure on ecosystems, climate change and economic development are key factors" in doing so. More zoonotic diseases were found in high-income countries.[72]
A 2022 study dedicated to the link between climate change and zoonosis found a strong link between climate change and the epidemic emergence in the last 15 years, as it caused a massive migration of species to new areas, and consequently contact between species which do not normally come in contact with one another. Even in a scenario with weak climatic changes, there will be 15,000 spillover of viruses to new hosts in the next decades. The areas with the most possibilities for spillover are the mountainous tropical regions of Africa and southeast Asia. Southeast Asia is especially vulnerable as it has a large number of bat species that generally do not mix, but could easily if climate change forced them to begin migrating.[73]
A 2021 study found possible links between climate change and transmission of COVID-19 through bats. The authors suggest that climate-driven changes in the distribution and robustness of bat species harboring coronaviruses may have occurred in eastern Asian hotspots (southern China, Myanmar, and Laos), constituting a driver behind the evolution and spread of the virus.[74][75]
Zoonotic diseases contribute significantly to the burdened public health system as vulnerable groups such the elderly, children, childbearing women and immune-compromised individuals are at risk.[citation needed] According to the World Health Organization (WHO), any disease or infection that is primarily ‘naturally’ transmissible from vertebrate animals to humans or from humans to animals is classified as a zoonosis.[76] Factors such as climate change, urbanization, animal migration and trade, travel and tourism, vector biology, anthropogenic factors, and natural factors have greatly influenced the emergence, re-emergence, distribution, and patterns of zoonoses.[76]
Zoonotic diseases generally refer to diseases of animal origin in which direct or vector mediated animal-to-human transmission is the usual source of human infection. Animal populations are the principal reservoir of the pathogen and horizontal infection in humans is rare. A few examples in this category include lyssavirus infections, Lyme borreliosis, plague, tularemia, leptospirosis, ehrlichiosis, Nipah virus, West Nile virus (WNV) and hantavirus infections.[77] Secondary transmission encompasses a category of diseases of animal origin in which the actual transmission to humans is a rare event but, once it has occurred, human-to-human transmission maintains the infection cycle for some period of time. Some examples include human immunodeficiency virus (HIV)/acquired immune deficiency syndrome (AIDS), certain influenza A strains, Ebola virus and severe acute respiratory syndrome (SARS).[77]
One example is Ebola which is spread by direct transmission to humans from handling bushmeat (wild animals hunted for food) and contact with infected bats or close contact with infected animals, including chimpanzees, fruit bats, and forest antelope. Secondary transmission also occurs from human to human by direct contact with blood, bodily fluids, or skin of patients with or who died of Ebola virus disease.[78] Some examples of pathogens with this pattern of secondary transmission are human immunodeficiency virus/acquired immune deficiency syndrome, influenza A, Ebola virus and severe acute respiratory syndrome. Recent infections of these emerging and re-emerging zoonotic infections have occurred as a results of many ecological and sociological changes globally.[77]
During most of human prehistory groups of hunter-gatherers were probably very small. Such groups probably made contact with other such bands only rarely. Such isolation would have caused epidemic diseases to be restricted to any given local population, because propagation and expansion of epidemics depend on frequent contact with other individuals who have not yet developed an adequate immune response.[79] To persist in such a population, a pathogen either had to be a chronic infection, staying present and potentially infectious in the infected host for long periods, or it had to have other additional species as reservoir where it can maintain itself until further susceptible hosts are contacted and infected.[80][81] In fact, for many "human" diseases, the human is actually better viewed as an accidental or incidental victim and a dead-end host. Examples include rabies, anthrax, tularemia, and West Nile fever. Thus, much of human exposure to infectious disease has been zoonotic.[82]
Many diseases, even epidemic ones, have zoonotic origin and measles, smallpox, influenza, HIV, and diphtheria are particular examples.[83][84] Various forms of the common cold and tuberculosis also are adaptations of strains originating in other species.[85][86] Some experts have suggested that all human viral infections were originally zoonotic.[87]
Zoonoses are of interest because they are often previously unrecognized diseases or have increased virulence in populations lacking immunity. The West Nile virus first appeared in the United States in 1999, in the New York City area. Bubonic plague is a zoonotic disease,[88] as are salmonellosis, Rocky Mountain spotted fever, and Lyme disease.
A major factor contributing to the appearance of new zoonotic pathogens in human populations is increased contact between humans and wildlife.[89] This can be caused either by encroachment of human activity into wilderness areas or by movement of wild animals into areas of human activity. An example of this is the outbreak of Nipah virus in peninsular Malaysia, in 1999, when intensive pig farming began within the habitat of infected fruit bats.[90] The unidentified infection of these pigs amplified the force of infection, transmitting the virus to farmers, and eventually causing 105 human deaths.[91]
Similarly, in recent times avian influenza and West Nile virus have spilled over into human populations probably due to interactions between the carrier host and domestic animals.[92] Highly mobile animals, such as bats and birds, may present a greater risk of zoonotic transmission than other animals due to the ease with which they can move into areas of human habitation.
Because they depend on the human host[93] for part of their life-cycle, diseases such as African schistosomiasis, river blindness, and elephantiasis are not defined as zoonotic, even though they may depend on transmission by insects or other vectors.[citation needed]
The first vaccine against smallpox by Edward Jenner in 1800 was by infection of a zoonotic bovine virus which caused a disease called cowpox.[94] Jenner had noticed that milkmaids were resistant to smallpox. Milkmaids contracted a milder version of the disease from infected cows that conferred cross immunity to the human disease. Jenner abstracted an infectious preparation of 'cowpox' and subsequently used it to inoculate persons against smallpox. As a result of vaccination, smallpox has been eradicated globally, and mass inoculation against this disease ceased in 1981.[95] There are a variety of vaccine types, including traditional inactivated pathogen vaccines, subunit vaccines, live attenuated vaccines. There are also new vaccine technologies such as viral vector vaccines and DNA/RNA vaccines, which include many of the COVID-19 vaccines.[96]

Steven Mithen, FBA, FSA, FSA Scot (born 16 October 1960) is an archaeologist.[1] He is noted for his work on the evolution of language, music and intelligence, prehistoric hunter-gatherers, and the origins of farming. He is professor of early prehistory at the University of Reading.
Mithen was born on 16 October 1960. In 1983 he graduated with a BA in Prehistory and Archaeology from Sheffield University,  followed by a MSc degree in biological computation from York University in 1984 and undertook a PhD in archaeology at Cambridge University, graduating in 1987.[2]
Mithen began his academic career as a research fellow in archaeology at Trinity Hall, Cambridge from 1987 to 1990. He was additionally a Cambridge University lecturer in archaeology (1989–1991), and then a research associate at the McDonald Institute for Archaeological Research from 1991 to 1992. In 1992, he joined the University of Reading as a lecturer in archaeology. He was promoted to senior lecturer in 1996, made Reader in Early Prehistory in 1998, and has been Professor of Early Prehistory since 2000.[2]
In 2004, Mithen was elected a Fellow of the British Academy (FBA), the United Kingdom's national academy for the humanities and social sciences.[3] He was elected a Fellow of the Society of Antiquaries of Scotland (FSA Scot) in 1993 and a Fellow of the Society of Antiquaries of London (FSA) in 1998.[2]

The sapient paradox is a question that can be formulated as "why there was such a long gap between emergence of genetically and anatomically modern humans and the development of complex behaviors?" Homo sapiens emerged as a species somewhere between 60,000 and 100,000 (or even 200,000) years ago, but the behaviour that is associated with modern humans began to emerge and accelerate only 10,000 years ago. The question was first formulated by archaeologist Colin Renfrew in 1996.[1][2][3][4]

Prehistoric France is the period in the human occupation (including early hominins) of the geographical area covered by present-day France which extended through prehistory and ended in the Iron Age with the Roman conquest, when the territory enters the domain of written history.
The Pleistocene is characterized by long glacial periods accompanied by marine regressions, interspersed at more or less regular intervals by milder but shorter interglacial stages. Human populations during this period consisted of nomadic hunter-gatherers. Several human species succeeded each other in the current territory of France until the arrival of modern humans in the Upper Palaeolithic .
The earliest known fossil man is Tautavel Man, dating from 570,000 years ago. Neanderthal Man is attested in France from about 335,000 years before present. Homo sapiens, modern humans, are attested since around 54,000 years ago in the Mandrin Cave.[1][2]
In the Neolithic, which begins in the south of France in the middle of the 6th millennium BC, the first farmers appeared. The first megaliths were erected in the early 5th millennium BC.
The lower paleolithic period began with the first human occupation of the region. Stone tools discovered at Lézignan-la-Cèbe indicate that early humans were present in France from least 1.57 million years ago.[3]
5 prehistoric sites in France are dated from between 1 and 1.2 million years ago:[4]
None of these sites have thus far revealed any evidence of lithic industry which prevents identification of the human species responsible for them.[4]
France includes Olduwan (Abbevillian) and Acheulean sites from early or non-modern (transitional) Hominini species, most notably Homo erectus and Homo heidelbergensis. Tooth Arago 149 - 560,000 years. Tautavel Man (Homo erectus tautavelensis), is a proposed subspecies of the hominid Homo erectus, the 450,000-year-old fossil remains of whom were discovered in the Arago cave in Tautavel.
The Grotte du Vallonnet near Menton contained simple stone tools dating to 1 million to 1.05 million years BC.[5] Cave sites were exploited for habitation, but the hunter-gatherers of the Palaeolithic era also possibly built shelters such as those identified in connection with Acheulean tools at Grotte du Lazaret and Terra Amata near Nice in France.  Excavations at Terra Amata found traces of the earliest known domestication of fire in Europe, from 400,000 BC.[5]
The Neanderthals are thought to have arrived earlier than 300,000 BC,[a] but seem to have died out by about by 30,000 BC, presumably unable to compete with modern humans during a period of cold weather.  Numerous Neanderthal, or "Mousterian", artifacts (named after the type site of Le Moustier, a rock shelter in the Dordogne region of France) have been found from this period, some using the "Levallois technique", a distinctive type of flint knapping developed by hominids during the Lower Palaeolithic but most commonly associated with the Neanderthal industries of the Middle Palaeolithic. Importantly, recent findings suggest that Neanderthals and modern humans may have interbred.[7]
Important Mousterian sites are found at:
The first identified Neanderthal burials were discovered at La Chapelle-aux-Saints in 1908 (dating from 70 ka) then at La Ferrassie in 1909.[10] The identification of burial practices in Neanderthals at these sites led to new insights concerning the capacity of Neanderthals to develop spiritual or metaphysical beliefs,[11] extending understanding of the human species beyond what had been hitherto assumed.[12]
The earliest indication of Upper Palaeolithic early modern human (formerly referred to as Cro-Magnon) migration into France, and indeed in the whole of Europe, is a series of modern human teeth with Neronian industry stone tools found at Grotte Mandrin Cave, Malataverne in France, dated in 2022 to between 56,800 and 51,700 years ago. The Neronian is one of the many industries associated with modern humans classed as transitional between the Middle and Upper Palaeolithic.[13] When they arrived in Europe, they brought with them sculpture, engraving, painting, body ornamentation, music and the painstaking decoration of utilitarian objects.  Some of the oldest works of art in the world, such as the cave paintings at Lascaux in southern France, are datable to shortly after this migration.[14]
European Palaeolithic cultures are divided into several chronological subgroups (the names are all based on French type sites, principally in the Dordogne region):[15]
From the Paleolithic to the Mesolithic, the Magdalenian culture evolved. The Early Mesolithic, or Azilian, began about 14,000 years ago, in the Franco-Cantabrian region of northern Spain and Southern France. This was ahead of other parts of Western Europe, where the Mesolithic began by 11,500 years ago at the beginning of the Holocene. It ended with the introduction of farming.[17]
The Azilian culture of the Late Glacial Maximum co-existed with similar early Mesolithic European cultures such as the Tjongerian of North-Western Europe, the Ahrensburgian of Northern Europe and the Swiderian of North-Eastern Europe, all succeeding the Federmesser complex.  The Azilian culture was followed  by the  Sauveterrian in Southern France and Switzerland, the Tardenoisian in Northern France, the Maglemosian in Northern Europe.[18]
Archeologists are unsure whether Western Europe saw a Mesolithic immigration. Populations speaking non-Indo-European languages are obvious candidates for Mesolithic remnants. The Vascons (Basques) of the Pyrenees present the strongest case, since their language is related to none other in the world, and the Basque population has a distinct genetic profile.[19] The disappearance of  Doggerland affected the surrounding territories and the hunter gatherers living there are believed to have migrated to northern France and as far as eastern Ireland to escape from the floods.[20]
The Neolithic period lasted in northern Europe for approximately 3,000 years (c. 5000 BC–2000 BC). It is characterised by the so-called Neolithic Revolution, a transitional period that included the adoption of agriculture, the development of tools and pottery (Cardium pottery, LBK), and the growth of larger, more complex settlements. There was an expansion of peoples from southwest Asia into Europe; this diffusion across Europe, from the Aegean to Britain, took about 2,500 years (6500 BC–4000 BC).[21] According to the leading Kurgan hypothesis, Indo-European languages were introduced to Europe later, during the succeeding Bronze Age, and Neolithic peoples in Europe are called "Pre-Indo-Europeans" or "Old Europe". Nevertheless, some archaeologists believe that the Neolithic expansion, and the eclipse of Mesolithic culture, coincided with the introduction of Indo-European speakers.[22] In what is known as the Anatolian hypothesis, it is postulated that Indo-European languages arrived in the early Neolithic. Old European hydronymy is taken by Hans Krahe to be the oldest reflection of the early presence of Indo-European languages in Europe.
Many European Neolithic groups share basic characteristics, such as living in small-scale family-based communities, subsisting on domestic plants and animals supplemented with the collection of wild plant foods and with hunting, and producing hand-made pottery (that is made without the potter's wheel).[citation needed]  Archeological sites from the Neolithic in France include artifacts from the Linear Pottery culture (c. 5500 – c. 4500 BC),  the Rössen culture (c. 4500—4000 BC), and the Chasséen culture (4,500 - 3,500 BC; named after Chassey-le-Camp in Saône-et-Loire), the name given to the late Neolithic pre-Beaker culture that spread throughout the plains and plateaux of France, including the Seine basin and the upper Loire valleys.[citation needed]
The 'Armorican' (Castellic culture) and Northern French Neolithic (Cerny culture) is based on traditions of the Linear Pottery culture or "Limburg pottery" in association with the La Hoguette/Cardial culture. The Armorican culture may also have origins in the Mesolithic tradition of Téviec and Hoedic in Brittany.[23]
It is most likely from the Neolithic that date the megalithic (large stone) monuments, such as the dolmens, menhirs, stone circles and chamber tombs, found throughout France, the largest selection of which are in the Brittany and Auvergne regions.  The most famous of these are the Carnac stones (c. 3300 BC, but may date to as old as 4500 BC) and the stones at Saint-Sulpice-de-Faleyrens.[24]
During the Chalcolithic or Copper Age, a transitional age from the Neolithic to the Bronze Age, France shows evidence of the Seine-Oise-Marne culture and the Beaker culture.
The Seine-Oise-Marne culture or "SOM culture" (c. 3100 to 2400 BC) is the name given by archaeologists to the final culture of the Neolithic in Northern France around the Oise River and Marne River.  It is most famous for its gallery grave megalithic tombs which incorporate a port-hole slab separating the entrance from the main burial chamber. In the chalk valley of the Marne River rock-cut tombs were dug to a similar design. In the Southeast, several groups whose culture had evolved from Chasséen culture also built megaliths.[27]
Beginning about 2600 BC, the Artenacian culture, a part of the larger European Megalithic Culture, developed in Dordogne, possibly as a reaction to the advance of Danubian peoples (such as SOM) over Western France. Armed with typical arrows, they took over all Atlantic France and Belgium by 2400 BC, establishing a stable border with the Indo-Europeans (Corded Ware) near the Rhine that would remain stable for more than a millennium.[citation needed]
The Bell Beaker culture (c. 2800–1900 BC) was a widespread phenomenon that expanded over most of France, excluding the Massif Central, in the third and early second millennia BC.[citation needed]
In the Kurgan Hypothesis, Indo-European languages spread to Europe in the Bronze Age. The culture of the Kurgans is also known as Yamnaya Culture and recent results from acheaogenetics have linked this culture with genetic ancestry components of the Western Steppe Herders, and it has been possible to reconstruct migrations of these people across Europe co-extensive with the arrival of the Yamnaya and Corded Ware cultures.[citation needed]
In France, the first studies on the Bronze Age date from the 19th century. The "Manuel d'archéologie préhistorique, celtique et gallo-romaine," (Manual of Prehistoric, Celtic and Gallo-Roman Archaeology), by Joseph Déchelette, published in 1910, was for a long time the reference for the study of this period.[30] In the 1950s, Jean-Jacques Hatt proposed  a subdivision of the French Bronze Age, and in 1958 he published a tripartate division.[31]  This model divided the Bronze Age into three parts, Early Bronze, Middle Bronze and Late Bronze Age and serves as a reference for the majority of subsequent studies on the Bronze Age in France.[32]
The Bronze Age archeological cultures in France include the transitional Beaker culture (c. 2800–1900 BC), the Early Bronze Age Rhône culture (c. 2300-1600 BC) and Armorican Tumulus culture (c. 2200 – c. 1400 BC), the Middle Bronze Age Tumulus culture (c. 1600-1200 BC), and the Late Bronze Age Atlantic Bronze Age (c. 1300 – c. 700 BC) and Urnfield culture  (c. 1300-800 BC). Early Bronze Age sites in Brittany (Armorican Tumulus culture) are believed to have grown out of Beaker roots, with some Wessex culture and Unetice culture influence.  Some scholars think that the Urnfield culture represents an origin for the Celts as a distinct cultural branch of the Indo-European family (see Proto-Celtic). This culture was preeminent in central Europe during the late Bronze Age; the Urnfield period saw a dramatic increase in population in the region, probably due to innovations in technology and agricultural practices.[citation needed]
Some archeologists date the arrival of several non-Indo-European peoples to this period, including the Iberians in southern France and Spain, the Ligures on the Mediterranean coast, and the Vascons (Basques) in southwest France and Spain.[citation needed]
The spread of iron-working led to the development of the Hallstatt culture (c. 700 to 500 BC) directly from the Urnfield. Proto-Celtic, the latest common ancestor of all known Celtic languages, is generally considered to have been spoken at the time of the late Urnfield or early Hallstatt cultures, in the early 1st millennium BC.[36]
The Hallstatt culture was succeeded by the La Tène culture, which developed out of the Hallstatt culture without any definite cultural break, under the impetus of considerable Mediterranean influence from Greek, and later Etruscan civilizations.  The La Tène culture developed and flourished during the late Iron Age (from 450 BC to the Roman conquest in the 1st century BC) in eastern France, Switzerland, Austria, southwest Germany, the Czech Republic, and Hungary. Farther to the north extended the contemporary Pre-Roman Iron Age culture of Northern Germany and Scandinavia.[36][37]
In addition, Greeks and Phoenicians settled outposts like Marseille in this period (c. 600 BC).[38]
By the 2nd century BC, Celtic France was called Gaul by the Romans, and its people were called Gauls. The people to the north (in what is present-day Belgium) were called Belgae (scholars believe this may represent a mixture of Celtic and Germanic elements) and the peoples of the south-west of France were called the Aquitani by the Romans, and may have been Celtiberians or Vascons.[citation needed]
Prehistoric and Iron Age France - all dates are BC

Sahul (/səˈhuːl/), also called Sahul-land, Meganesia, Papualand and Greater Australia,[1] was a paleocontinent that encompassed the modern-day landmasses of mainland Australia, Tasmania, New Guinea, and the Aru Islands.[2][3][4][5][6]
Sahul was in the south-western Pacific Ocean, located approximately north to south between the Equator and the 44th parallel south and west to east between the 112th and the 152nd meridians east.[2] Sahul was separated from Sunda to its west by the Wallacean Archipelago.[2][7] At its largest, when ocean levels were at their lowest, it was approximately 10,600,000 square kilometres (4,100,000 sq mi) in size.[note 1][2]
After the last Ice Age global temperatures increased and sea levels gradually rose, flooding the land bridge and separating mainland Australia from New Guinea and Tasmania.[8] New Guinea was separated from the Australian mainland approximately 8,000 years ago, and Tasmania approximately 6,000 years ago.[8]
Sahul hosted a large variety of unique fauna that changed independently from the rest of the world.[9] Most notably nearly all mammals on Sahul were marsupials including a range of browsers, burrowers, scavengers and predators; bats and rodents represented the only placental mammals.[9]
It is estimated humans first migrated to Sahul around 45,000 years ago, making the ocean crossing from Sunda through Wallacea.[10] From Sahul humans spread throughout Oceania.[3]
The name Sahul is used by archeologists, while the name Meganesia tends to be used by zoogeographers.[4] The name Greater Australia has been used, but it has been criticised as "cartographic imperialism" because it places greater emphasis upon what is now Australia at the expense of New Guinea.[6]
Africa
Antarctica
Asia
Australia
Europe
North America
South America
Afro-Eurasia
Americas
Eurasia
Oceania
This palaeogeography article is a stub. You can help Wikipedia by expanding it.

Human habitation in the North African region began over one million years ago[citation needed]. Remains of Homo erectus during the Middle Pleistocene period, has been found in North Africa. The Berbers, who generally antedate by many millennia the Phoenicians and the establishment of Carthage, are understood to have arisen out of social events shaped by the confluence of several earlier peoples, i.e., the Capsian culture, events which eventually constituted their ethnogenesis. Thereafter Berbers lived as an independent people in North Africa, including the Tunisian region.
On the most distant prehistoric epochs, the scattered evidence sheds a rather dim light. Also obscure is the subsequent "pre-Berber" situation, which later evolved into the incidents of Berber origins and early development. Yet Berber languages indicates a singular, ancient perspective. This field of study yields a suggested reconstruction of remote millennia of Berber prehistory, and insight into the ancient cultural and lineage relations of Tunisian Berbers—not only with their neighboring Berber brothers, but with other more distant peoples.
The prehistoric, of course, seamlessly passes into the earliest historic. The first meeting of Phoenician and Berber occurred well to the east of Tunisia, well before the rise of Carthage: a tenth-century invasion of Phoenicia was led by a pharaoh of the Berbero-Libyan dynasty (the XXII) of Ancient Egypt.
In the Maghreb, the first written records describing the Berbers begin with the Tunisian region, proximate to the founding there of Carthage. Unfortunately, surviving Punic writings are very scarce aside from funerary and votive inscriptions; remains of the ancient Berber script is also limited. The earliest written reports come from later Greek and Roman authors. From discovery of archaic material culture and such writings, early Berber culture and society, and religion, can be somewhat surmised.
Tunisia remained the leading region of the Berber peoples throughout the Punic era (and Roman, and into the Islamic). Here modern commentary and reconstructions are presented concerning their ancient livelihood, domestic culture, and social organization, including tribal confederacies. Evidence comes from various artifacts, settlement and burial sites, inscriptions, and historical writings; supplementary views are derived by disciplines studying genetics and linguistics.
Evidence of habitation in the North African region by human ancestors has been found stretching back one or two million years, yet not to rival those most-ancient finds in south and east Africa.[1] Remains of Homo erectus during the Middle Pleistocene, circa 750 kya (thousands of years ago), has been found in North Africa. These were associated with the change in early hominid tool use form pebble-choppers to hand-axes.[2]
Migrations out of south and east Africa since 100 Kya are thought to have established current human populations worldwide.[3] Cavalli-Sforza includes the Berber populations in a much larger genetic group, one which also includes S.W. Asians, Iranians, Europeans, Sardinians, Indians, S.E. Indians, and Lapps.[4]
"[B]y definition prehistoric archaeology is dealing with pre-written sources only, so that all prehistory is anonymous." Hence, "it is inevitably mainly concerned with the material culture" such as "stone tools, bronze weapons, hut foundations, tombs, field walks, and the like." ... "We have no way of learning the moral and religious ideas of the protohistoric city dwellers... ."[5]
Regarding the evidence of prehistory, very remote epochs often give clues only about physical anthropology, i.e., per biological remains re human evolution. Usually the later millennia progressively disclose more and more cultural information yet, absent writings, it is mostly limited to "material culture". Generally cultural data is considered a far more telling indication of prehistoric human behavior and society, as compared to only evidence of physical human remains.[6]
The cultural data available about human prehistory derived from material artifacts, however, too often directly concerns "non-essentials". It is limited as a useful source about the finer details of archaic human societies—the ethical norms, the individual dilemmas—when compared to the data from written sources. "When prehistorians speak of the ideas and ideals of men before writing, they are making guesses--intelligent guesses by people best qualified to make them, but nevertheless guesses."[7][8]
Perhaps the most significant prehistoric finding worldwide concerns events surrounding the Neolithic Revolution. Then humans developed significant jumps in cognitive ability. The evidence of the art and expressive artifacts dating back about 10 to 12 kya show a new sophistication in handling experience, perhaps being the fruits of prior advances in the articulation of symbols and language. Herding and farming develop. A new phase of human evolution had begun.[9][10] "The rich heritage of rock painting in North Africa... seem to date after the Pleistocene period... around twelve thousand years ago." Thus a period concurrent with the "neolithic" revolution.[11]
Dating to the much more recent Mesolithic era, stone blades and tools, as well as small stone human figurines, of the Capsian culture (named after Gafsa in Tunisia) are associated with the prehistoric presence of Berbers in North Africa. The Capsian is that archaic culture native to the Maghrib region, circa twelve to eight kya. During this period the Pleistocene came to an end with the last ice age, causing changes in the Mediterranean climate. The African shore slowly became drier as the "rain belts moved north".[12] Also related to the Berbers are some of the prehistoric monuments built using very large rocks (dolmens). Located both in Europe and Africa, these dolmens are found at locations throughout the western Mediterranean.[13] The Capsian culture was preceded by the Ibero-Maurusian in North Africa.[14]
Saharan rock art, the inscriptions and the paintings that show various design patterns as well as figures of animals and of humans, are attributed to the Berbers and also to black Africans from the south. Dating these art works has proven difficult and unsatisfactory.[15][16] Egyptian influence is considered very unlikely.[17] Some images infer a terrain much better watered. Among the animals depicted, alone or in staged scenes, are large-horned buffalo (the extinct bubalus antiquus), elephants, donkeys, colts, rams, herds of cattle, a lion and lioness with three cubs, leopards or cheetahs, hogs, jackles, rhinoceroses, giraffes, hippopotamus, a hunting dog, and various antelope. Human hunters may wear animal masks and carry their weapons. Herders are shown with elaborate head ornamentation; a few dance. Other human figures drive chariots, or ride camels.[18][19]
A commonly held view of Berber origins is that Paleo-Mediterranean peoples long occupying the region combined with several other largely Mediterranean groups, two from the east near S.W.Asia and bringing the Berber languages about eight to ten kya (one traveling west along the coast and the other by way of the Sahel and the Sahara), with a third intermingling earlier from Iberia.[20][21][22] "At all events, the historic peopling of the Maghrib is certainly the result of a merger, in proportions not yet determined, of three elements: Ibero-Maurusian, Capsian and Neolithic," the last being "true proto-Berbers".[23]
Cavalli-Sforza also makes two related observations. First, the Berbers and those S.W. Asians who speak Semitic idioms together belong to a large and ancient language family (the Afroasiatic), which dates back perhaps ten kya. Second, this large language family incorporates in its ranks members from two different genetic groups, i.e., (a) some elements of the one listed by Cavalli-Sforza immediately above, and (b) one called by him the Ethiopian group. This Ethiopian group inhabits lands from the Horn to the Sahel region of Africa.[24] In agreement with Cavalli-Sforza's work, recent demographic study indicates a common Neolithic origin for both the Berber and Semitic populations.[25] A widespread opinion is that the Berbers are a mixed ethnic group sharing the related and ancient Berber languages.[26][27]
Perhaps eight millennia ago, already there were prior peoples established here, among whom the proto-Berbers (coming from the east) mingled and mixed, and from whom the Berber people would spring, during an era of their ethnogenesis.[28][29] Today half or more of modern Tunisians appear to be the descendants, however mixed or not, of ancient Berber ancestors.[30]
In the study of languages, sophisticated techniques were developed enabling the understanding of how an idiom evolves over time. Hence, the speech of past ages may be successively reconstructed in theory by using only modern speech and the rules of phonetic and morphological change, and other learning, which may be augmented by and checked against the literature of the past, if available. Methods of Comparative linguistics also assisted in associating related 'sister' languages, which together stem from an ancient parent language. Further, such groups of related languages may form branches of even larger language families, e.g., Afroasiatic.[32][33][34][35][36]
Taken together the twenty odd Berber languages constitute one of the five branches[37][38][39] of Afroasiatic,[40][41][42][43][44][45] a pivotal world language family, which stretches from Mesopotamia and Arabia to the Nile river and to the Horn of Africa, across northern Africa to Lake Chad and to the Atlas Mountains by the Atlantic Ocean. The other four branches of Afroasiatic are: Ancient Egyptian, Semitic (which includes Akkadian, Aramaic, Hebrew, Arabic, and Amharic), Cushitic (around the Horn and the lower Red Sea),[46] and Chadic (e.g., Hausa). The Afroasiatic language family has great diversity among its member idioms and a corresponding antiquity in time depth,[47][48] both as to the results of analyses in historical linguistics and as regards the seniority of its written records, composed using the oldest of writing systems.[49][50][51] The combination of linguistic studies with other information about prehistory taken from archaeology and the biological sciences has been adumbrated.[52][53] Earlier academic speculation as to the prehistoric homeland of Afroasiatic and its geographic spread centered on a source in southwest Asia,[54][55][56] but more recent work in the various related disciplines has focused on Africa.[57][58][59][60]
The conjecture proposed by linguist and historian Igor M. Diakonoff may be summarized. From a prehistoric homeland near Darfur, which was better watered,[61][62] the "Egyptians" were the first to break from the proto Afroasiatic communities, before ten kya (thousand years ago). These proto Egyptian language speakers headed north. During the next millennium or so, proto-Semitic and -Berber speakers went their divergent ways. The Semites passed by the then marshlands of the lower Nile and crossed into Asia (evidently the Semitic speakers anciently present in Ethiopia remained in Africa or later crossed back to Africa from Arabia). Meanwhile, the peoples who spoke proto-Berbero-Libyan spread out westward across North Africa, along the Mediterranean coast and into a Sahara region then better watered, traveling in a centuries-long migration until reaching the Atlantic and its offshore islands.[63][64][65][66] Later, Diakonoff revised his proposed prehistory, moving the Afroasiatic homeland north toward the lower Nile, then a land of lakes and marshes. This change reflects several linguistic analyses showing that common Semitic then shared very little "cultural" lexicon with the common Afroasiatic.[67] Hence the proto Semitic speakers probably left the common Afroasiatic community earlier, by ten kya (thousand years ago), starting from an area nearby a more fruitful Sinai. Accordingly, he situates the related Berbero-Libyan speakers of that era by the coast, to the west of the lower Nile.[68][69][70]
By perhaps seven kya (thousand years ago) a Neolithic culture was evolving among a coalescence of people, the Berbers, in northwest Africa. Earlier, at the long-occupied cave of Haua Fteah in Cyrenaica, "food gatherers with a Caspian flint industry were succeeded by stock breeders with pottery."[71] Material culture progressed, resulting in the Neolithic Revolution with its animal domestication and agriculture advances; craft techniques included imprinted pottery, and finely chipped stone implements (evolved from earlier arrowheads).[72]
"The food gatherers who built up the shell middens round the salt lakes of Tunisia were succeeded by simple food producers, with little change in their flint industry... described as a 'Neolithic of Capsian tradition'. [H]ere at least native food gatherers were not displaced by immigrant farmers [from the east] but themselves adopted a food-producing economy. [In the] Maghreb simple farming culture survived very little alterred for millennia. A site with impressed pottery might date to the sixth millennium b.c. or the second."[73]
Wheat and barley were sown, beans and chickpeas cultivated. Ceramic bowls and basins, goblets, large plates, as well as dishes raised by a central column, were domestic items in daily use, sometimes hung up on the wall. For clothing findings indicate hooded cloaks, and cloth woven into stripes of different colors. Sheep, goats, and cattle measured wealth.[74] From physical evidence unearthed in Tunisia archaeologists present the Berbers as already "farmers with a strong pastoral element in their economy and fairly elaborate cemeteries", well over a thousand years before the Phoenicians arrived to found Carthage.[75]
Apparently, prior to written records about them, sedentary rural Berbers lived in semi-independent farming villages, composed of small, composite, tribal units under a local leader who worked to harmonize its clans.[76] Management of the affairs in such early Berber villages was probably shared with a council of elders.[77] By particularly fertile regions, the larger villages arose. Yet seasonally the villagers might have left to find the better pasture for their herds and flocks. On the marginal lands, the more pastoral tribes of Berbers roamed widely to find grazing for their animals. Modern conjecture is that feuding between neighborhood clans at first impeded organized political life among these ancient Berber farmers, so that social coordination did not develop beyond the village level, whose internal harmony could vary.[78] Tribal authority was strongest among the wandering pastoralists, much weaker among the agricultural villagers, and would later attenuate further with the advent of cities connected to strong commercial networks and foreign polities.[79]
Throughout the Maghrib (particularly in what is now Tunisia), the Berbers reacted to the growing military threat from colonies started by Phoenician traders. Eventually Carthage and its sister city-states would inspire Berber villages to join in order to marshall large-scale armies, which naturally called forth strong, centralizing leadership. Punic social techniques from the nearby prosperous cities were adopted by the Berbers, to be modified for their own use.[80][81][82] To the east, the Berbero-Libyans had already interacted with the Egyptians during the millennial rise of the ancient Nile civilization.
The people commonly known today as the Berbers were anciently more often known as Libyans. Yet many "Berbers" have for long self-identified as Imazighen or "free people" (etymology uncertain).[83] Mommsen, a widely admired historian of the 19th century, stated:
"They call themselves in the Riff near Tangier Amâzigh, in the Sahara Imôshagh, and the same name meets us, referred to particular tribes, on several occasions among the Greeks and Romans, thus as Maxyes at the founding of Carthage, as Mazices in the Roman period at different places in the Mauretanian north coast; the similar designation that has remained with the scattered remnants proves that this great people has once had a consciousness, and has permanently retained the impression, of the relationship of its members."
Other names, according to Mommsen, were used by their ancient neighbors: Libyans (by Egyptians and later by Greeks), Nomades (by Greeks), Numidians (by Romans), and later Berbers (by the Arabs); also the self-descriptive Mauri in the west; and Gaetulians in the south.[84][85]
Several ancient names of Berber polities may be related to their self-designated identity as Imazighen. The Egyptians knew as pharaohs the leaders of a powerful Berber tribe called Meshwesh of the XXII dynasty.[86] Located near Carthage was the Berber kingdom of Massyli, later called Numidia, ruled by Masinissa and his descendants.[87]
Berbers together, with their relations and descendants, have been the major population group to inhabit the Maghrib (North African apart from the Nile) since about eight kya (thousand years ago).[88][89][90][91] This region includes terrain from the Nile to the Atlantic, encompassing the vast Sahara at whose center rise the mountain heights of Ahaggar and Tibesti. In the west the Mediterranean coastlands are suitable for agriculture, having for hinterland the Atlas Mountains. It includes the land now known as Tunisia.
Yet the most ancient written records concerning the Berber peoples are those reported by neighboring peoples of the Mediterranean region. When the Berbers enter history during the first millennium BCE, their own points of view on situations and events are, unfortunately, not available to us. Due to the impact of Carthage, it is the people of Tunisia who dominate the early historical writings on the Berbers.[92]
Here described are Berber peoples in the first light of history, drawn from written records left by Egyptians in northeast Africa, and mainly by Greek and Roman authors in northwest Africa. To the east of Tunisia, a Libyan dynasty ruled in Egypt; their armies marched into Phoenicia a century before the founding of Carthage. Next is described Berber life and society in Tunisia and to its west, both before and during the hegemony of Carthage.
Egyptian hieroglyphs from early dynasties testify to Libyans, who were the Berbers of Egypt's "western desert"; they are first mentioned directly as the "Tehenou" during the pre-dynastic reigns of Scorpion (c. 3050) and of Narmer (on an ivory cylinder). The Berbero-Libyans later are shown in a bas relief at the temple of Sahure, from the Fifth Dynasty (2487-2348). The Palermo Stone,[93] also called the Libyan Stone, lists the earliest sovereigns of Egypt from the 31st century to the 24th century, i.e., the list includes: about fifty pre-dynastic rulers of Egypt, followed by the earliest pharaohs, those of the first five dynasties. Some conjecture that the fifty earlier rulers listed may be Libyan Berbers, from whom the pharaohs derived.[94]
Much later, Ramses II (r.1279-1213) was known to employ Libyan contingents in his army.[95] Tombs of the 13th century contain paintings of Libu leaders wearing fine robes, with ostrich feathers in their "dreadlocks", short pointed beards, and tattoos on their shoulders and arms.[96]
Osorkon the Elder (Akheperre setepenamun), a Berber leader of the Meshwesh tribe, appears to be the first Libyan pharaoh. Several decades later his nephew Shoshenq I (r.945-924) became Pharaoh of Egypt, and founder of its Twenty-second Dynasty (945-715).[97][98] In 926 Shoshenq (Shishak of the Bible) successfully campaigned to Jerusalem then under Solomon's heir.[99][100][101] The Phoenicians (particularly the people of the city-state of Tyre, who would later found Carthage during Egypt's Meshwesh dynasty), first came to know of the Berber people through these Libyan pharaohs.
For several centuries Egypt was governed by a decentralized political system loosely based on the Libyan tribal organization of the Meshwesh. Becoming acculturated, much of the evidence shows these Berbero-Libyans through an Egyptian lens. Eventually the Libyans served as high priests at centers of Egyptian religion.[102] Hence during the classical era of the Mediterranean, all of the Berber peoples of North Africa were often collectively called Libyans, due to the fame first won by the Meshwesh dynasty of Egypt.[103][104][105]
West of the Meshwesh dynasty of Egypt, later reports of foreigners mention more rustic Berber people by the Mediterranean, living in fertile and accessible coastal regions. Those located in or near Tunisia were known as Numidians; farther to the west, the Berbers were called the Mauri or Maurisi (later the Moors); and, in more remote mountains and deserts to the southwest were Berbers called Gaetulians.[106][107][108] Another group of Berbers in the steppe and desert to the southeast of Carthage were known as the Garamantes.[109]
During the 5th century BC, the Greek writer Herodotus (c.490-425) mentions Berbers as mercenaries of Carthage with regard to specific military events in Sicily, circa 480.[110] Thereafter the Berbers more frequently enter into the early light of history provided by various Greek and Roman historians. Yet unfortunately, apart from the Punic inscriptions, little Carthaginian literature has survived.[111][112] We do know that Mago of Carthage began to employ Berbers as mercenaries in the sixth century.[113]
During these centuries, Berbers of the western regions actively traded and intermingled with Phoenicians, who founded Carthage and its many trading stations. The name 'Libyphoenician' was then coined for the cultural and ethnic mix surrounding Punic settlements, particularly Carthage. Political skills and civic arrangements encountered in Carthage, as well as material culture, such as farming techniques, were adopted by the Berbers for their own use.[114][115] In the 4th century, Berber kingdoms are referenced, e.g., the ancient historian Diodorus Siculus evidently mentions the Libyo-Berber king Aelymas, a neighbor to the south of Carthage, who dealt with the invader Agathocles (361-289), a Greek ruler from Sicily.[116][117] Berbers here operated independently of Carthage.
Circa 220 BC, three large kingdoms had arisen among the Berbers. These Berbers, independent yet markedly influenced by Punic civilization, had nonetheless endured the long ascendancy of Carthage. West to east the kingdoms were: (1) Mauretania (in modern Morocco) under the Mauri king Baga; (2) the Masaesyli (in north Algeria) under their king Syphax who then ruled from two capitals, in the west Siga (near modern Oran) and in the east Cirta (modern Constantine); and (3) the Massyli (south of Cirta, directly west and south of nearby Carthage) ruled by king Gala [Gaia], father of Masinissa. Following the Second Punic War, Massyli and eastern Masaesyli were joined to form Numidia, located in historic Tunisia. Both Roman and Hellenic states gave its famous ruler, Masinissa, honors befitting esteemed royalty.[118]
Evidence of ancient Berber religion and sacred practices provide some views, however incomplete, of the interior life of the people, otherwise largely opaque, and thus also clues as to the character of the Berbers, who witnessed the founding of Carthage.
The religion of the ancient Berbers, of course, is difficult to uncover sufficiently to satisfy the imagination. Burial sites provide early indication of religious beliefs; more than sixty thousand tombs are located in the Fezzan alone.[119] The construction of many tombs indicates their continuing use for ceremonies and sacrifices.[120] A grand tomb for a Berber king, traditionally assigned to Masinissa (238-149) but perhaps rather to his father Gala, still stands: the Medracen in eastern Algeria. Architecture for the elegant tower tomb of his contemporary Syphax shows some Greek or Punic influence.[121] Much information about Berber beliefs comes from classical literature. Herodotus (c.  484-c. 425) mentions that Libyans of the Nasamone tribe, after prayers, slept on the graves of their ancestors in order to induce dreams for divination. The ancestor chosen being regarded the best in life for uprightness and valor, hence a tomb imbued with spiritual power. Oaths also were taken on the graves of the just.[122][123] In this regard, the Numidian king Masinissa was widely worshipped after his death.[124]
Early Berbers beliefs and practices are often characterized as a religion of nature. Procreative power was symbolized by the bull, the lion, the ram. Fish carvings represented the phallus, a sea shell the female sex, which objects could become charms.[125][126] The supernatural could reside in the waters, in trees, or come to rest in unusual stones (to which the Berbers would apply oils); such power might inhabit the winds (the Sirocco being formidable across North Africa).[127]  Herodotus writes that the Libyans sacrificed to the sun and moon.[128] The moon (Ayyur) was conceived as being masculine.[129][130]
Later many other supernatural entities became identified and personalized as gods, perhaps influenced by Egyptian or Punic practice; yet the Berbers seemed to be "drawn more to the sacred than to the gods."[131] Early worship sites might be in grottoes, on mountains, in clefts and cavities, along roadways, with the "altars casually made of turf, the vessels used still of clay with the deity himself nowhere", according to the Berber author Apuleius (born c. 125 CE), commenting on the local worship of earlier times.[132] Often only a little more than the names of the Berber deities are known, e.g., Bonchar, a leading god.[133] Julian Baldick, culling literature covering many eras and regions, provides the names and rôles of many Berber deities and spirits.[134][135]
The Berbero-Libyans came to adopt elements from ancient Egyptian religion. Herodotus writes of the divine oracle, sourced in the Egyptian god Ammon, located among the Libyans at the oasis of Siwa. This Libyan oasis of Ammon functioned a sister oracle to one at Dodona in Greece, according to Herodotus (c.484-c.425).[136][137] However, the god of the Siwa oracle, to the contrary, may be a Libyan deity.[138] The visit of Alexander in 331 brought to the Siwa oracle wide notice in the ancient world.[139]
Later, Berber beliefs would influence the Punic religion from Carthage, the city-state founded by Phoenicians.[140] George Aaron Barton suggested that the prominent goddess of Carthage Tanit originally was a Berbero-Libyan deity whom the newly arriving Phoenicians sought to propitiate by their worship.[141][142] Later archeological finds show a Tanit from Phoenicia.[143][144][145] From linguistic evidence Barton concluded that before developing into an agricultural deity, Tanit probably began as a goddess of fertility, symbolized by a tree bearing fruit.[146] The Phoenician goddess Ashtart was supplanted by Tanit at Carthage.[147][148]

Hominid dispersals in Europe refers to the colonisation of the European continent by various species of hominid, including hominins and archaic and modern humans.
Short and repetitive migrations of archaic humans before 1 million years ago suggest that their residence in Europe was not permanent at the time.[1]  Colonisation of Europe in prehistory was not achieved in one immigrating wave, but instead through multiple dispersal events.[2]  Most of these instances in Eurasia were limited to 40th parallel north.[2]  Besides the findings from East Anglia, the first constant presence of humans in Europe begins 500,000–600,000 years ago.[3] However, this presence was limited to western Europe, not reaching places like the Russian plains, until 200,000–300,000 years ago.[3]  The exception to this was discovered in East Anglia, England, where hominids briefly inhabited 700,000 years ago.[4] Prior to arriving in Europe, the source of hominids appeared to be East Africa, where stone tools and hominid fossils are the most abundant and recorded.[3]  Arising in Europe at least 400,000 years ago, the Neanderthal hominids (a descendant of Homo heidelbergensis) would become more stable residents of the continent, until H. sapiens would arrive about 50,000 years ago, leading to the extinction of the Neanderthals about 37,000 years ago.
In the early Miocene, Europe had a subtropical climate and was intermittently connected to Africa by land bridges. At the same time, Africa was becoming more arid, prompting the dispersal of its tropical fauna—including primates—north into Europe.[6] Apes first appear in the European fossil record 17 million years ago with Griphopithecus.[7] The closely related Kenyapithecus is also known from fossils in Germany, Slovakia and Turkey.[6] Both Griphopithecus and Kenyapithecus are considered likely to be ancestral to the great apes.[8] From 13 million to 9 million years ago, hominids flourished in Europe and underwent an adaptive radiation as they diversified in response to a gradually cooling climate.[6][9] Middle Miocene European hominids include Pierolapithecus, Anoiapithecus, Dryopithecus, Hispanopithecus, and Rudapithecus.[5] The diversity and early appearance of great apes in Europe has led some scientists to theorise that hominids in fact evolved there, before dispersing "back to Africa" in the Middle Miocene.[6][8]
Around 9 million years ago most of Europe's hominid species fell victim to the Vallesian crisis, an extinction event caused by the disappearance of the continent's forests.[6][9] Some hominid species survived the event: Oreopithecus, which became isolated in forest refugia; and Ouranopithecus, which adapted to the open environments of the late Miocene.[6] However, both were extinct by 7 million years ago.[5]
In 2017, a reanalysis of Graecopithecus fossils from Greece and Bulgaria, previously associated with Ouranopithecus, concluded that the species was in fact a hominin dating to just after the last common ancestor of humans and chimpanzees (about 7.2 million years ago).[10] The authors suggested that the origins of the human lineage were therefore in the Mediterranean, not Africa.[11][12][13] Others are sceptical of their claims.[13][14][15]
Although subtropical conditions returned to Europe in the Pliocene (5.33–2.58 million years ago), there are no known fossil hominids from this period.[16]
Homo erectus populations lived in southeastern Europe by 1.8 million years ago.[17]
The most archaic human fossils from the Middle Pleistocene (780,000–125,000 years ago)[18] have been found in Europe.  Remains of Homo heidelbergensis have been found as far north as the Atapuerca Mountains in Gran Dolina, Spain, and the oldest specimens can be dated from 850,000 to 200,000 years ago.[19][20]
Neanderthals evolved from a branch of Homo heidelbergensis that migrated to Europe during the Middle Pleistocene.[21]  Neanderthal populations date back at least as far as 400,000 years ago in the Atapuerca Mountains, Spain.[22]  While lacking the robustness attributed to west European Neanderthal morphology, other populations did inhabit parts of eastern Europe and western Asia.[22]  Between 45,000–35,000 years ago, modern humans (Homo sapiens) replaced all Neanderthal populations in Europe anatomically and genetically.[23]  This is evident in the transfer and combination of technology and culture.
The recent expansion of anatomically modern humans reached Europe around 40,000 years ago, from Central Asia and the Middle East, as a result of cultural adaption to big game hunting of sub-glacial steppe fauna.[24][25]
Neanderthals were present both in the Middle East and in Europe, and the arriving populations of anatomically modern humans (also known as "Cro-Magnon" or European early modern humans) have interbred with Neanderthal populations to a limited degree.
Modern human remains dating to 43–45,000 years ago have been discovered in Italy[26] and Britain,[27] with the remains found of those that reached the European Russian Arctic 40,000 years ago.[28][29]
The composition of European populations was later altered by further migrations, notably the Neolithic expansion from the Middle East, and still later the Chalcolithic population movements associated with Indo-European expansion.
The modern indigenous population of Europe is composed of three major foundational populations,
dubbed "Western Hunter-Gatherers" (WHG), "Early European Farmers" (EEF) and "Ancient North Eurasian" (ANE).
WHG represents the remnant of the original Cro-Magnon population after they re-peopled Europe after the Last Glacial Maximum.
EEF represents the introgression of Near Eastern populations during the Neolithic Revolution, and ANE is associated both with the Mesolithic Uralic expansion to Northern Europe and the Indo-European expansion to Europe in the Chalcolithic.[30]
Homo ergaster specimens indicate a change toward a diet more reliant on animal products, evident by greater encephalization with higher energy requirements.[31]  This transition to becoming more carnivorous affected the way of life unlike primates before.[32][33]  Archaeological evidence of cut bones from large mammals and broken stone tools increasing in frequency support this increasing trend.[3]  To meet increasing demand of calories, the range of hominids would have expanded, making the necessary hunting versus prior scavenging possible.[3]  It is believed that the adjustments required to meet these new demands would expand the home range size eight to ten times.[34]  Range could also increase or decrease in size due to environmental changes.[3]  A more recent example is absence of humans in Britain during the last glacial maximum which ended in the Late Pleistocene, 10,000 years ago.[35][36]  At this time, Russia had an influx of people following the major prey species shifting to this region.[37]  It has been argued that Neanderthals', and previous hominids', expansion northward were limited by lacking proper thermoregulation.[3]  Behavioural adaptations such as clothes-making to overcome the cold is evident in archaeological finds.[3]  The potential to expand also grew with the Neanderthal reaching the status of top carnivores.[3]  These humans could fear less during expansion, without the worry of other predators. The desire to push into these northern areas was influenced by this requirement to eat a lot of meat to satisfy the human brain which uses 20% of the body's energy.[38]   Larger game for hunting is available the closer you are to the poles.[3]

The Natufian culture (/nəˈtuːfiən/[1] nə-TOO-fee-ən) is a Late Epipaleolithic archaeological culture of the prehistoric Levant in Western Asia, dating to around 15,000 to 11,500 years ago.[2] The culture was unusual in that it supported a sedentary or semi-sedentary population even before the introduction of agriculture. Natufian communities may be the ancestors of the builders of the first Neolithic settlements of the region, which may have been the earliest in the world. Some evidence suggests deliberate cultivation of cereals, specifically rye, by the Natufian culture at Tell Abu Hureyra, the site of the earliest evidence of agriculture in the world.[3] The world's oldest known evidence of the production of bread-like foodstuff has been found at Shubayqa 1, a 14,400-year-old site in Jordan's northeastern desert, 4,000 years before the emergence of agriculture in Southwest Asia.[4] In addition, the oldest known evidence of possible beer-brewing, dating to approximately 13,000 BP, was found in Raqefet Cave on Mount Carmel, although the beer-related residues may simply be a result of a spontaneous fermentation.[5][6]
Generally, though, Natufians exploited wild cereals and hunted animals, notably gazelles.[7] Archaeogenetic analysis has revealed derivation of later (Neolithic to Bronze Age) Levantines primarily from Natufians, along with substantial later gene flow from Anatolia.[8]
Dorothy Garrod coined the term Natufian based on her excavations at the Shuqba cave (Wadi an-Natuf) near the town of Shuqba.
The Natufian culture was discovered by British archaeologist Dorothy Garrod during her excavations of Shuqba cave in the Judaean Hills, on the West Bank of the Jordan River.[9][10] Prior to the 1930s, the majority of archaeological work taking place in British Palestine was biblical archaeology focused on historic periods, and little was known about the region's prehistory.
In 1928, Garrod was invited by the British School of Archaeology in Jerusalem (BSAJ) to excavate Shuqba cave, where prehistoric stone tools had been discovered by Père Mallon four years earlier. She discovered a layer sandwiched between the Upper Palaeolithic and Bronze Age deposits characterised by the presence of microliths. She identified this with the Mesolithic, a transitional period between the Palaeolithic and the Neolithic which was well-represented in Europe but had not yet been found in the Near East. A year later, when she discovered similar material at el-Wad Terrace, Garrod suggested the name "the Natufian culture", after Wadi an-Natuf that ran close to Shuqba.
Over the next two decades Garrod found Natufian material at several of her pioneering excavations in the Mount Carmel region, including el-Wad, Kebara and Tabun, as did the French archaeologist René Neuville, firmly establishing the Natufian culture in the regional prehistoric chronology. As early as 1931, both Garrod and Neuville drew attention to the presence of stone sickles in Natufian assemblages and the possibility that this represented a very early agriculture.[10]
Epipalaeolithic Near East
Caucasus
Zagros
Fertile Crescent:
Europe:
Africa:
Siberia:
Radiocarbon dating places the Natufian culture at an epoch from the terminal Pleistocene to the very beginning of the Holocene, a time period between 12,500 and 9,500 BC.[12]
The period is commonly split into two subperiods: Early Natufian (12,000–10,800 BC) and Late Natufian (10,800–9,500 BC). The Late Natufian most likely occurred in tandem with the Younger Dryas (10,800 to 9,500 BC). The Levant hosts more than a hundred kinds of cereals, fruits, nuts, and other edible parts of plants, and the flora of the Levant during the Natufian period was not the dry, barren, and thorny landscape of today, but rather woodland.[9]
The Natufian developed in the same region as the earlier Kebaran culture. It is generally seen as a successor, which evolved out of elements within that preceding culture. There were also other industries in the region, such as the Mushabian culture of the Negev and the Sinai Peninsula, which are sometimes distinguished from the Kebaran culture or believed to have been involved in the evolution of the Natufian culture.
More generally there has been discussion of the similarities of these cultures with those found in coastal North Africa. Graeme Barker notes there are: "similarities in the respective archaeological records of the Natufian culture of the Levant and of contemporary foragers in coastal North Africa across the late Pleistocene and early Holocene boundary".[13] According to Isabelle De Groote and Louise Humphrey, Natufians practiced the Iberomaurusian and Capsian custom of sometimes extracting their maxillary central incisors (upper front teeth).[14]
Ofer Bar-Yosef has argued that there are signs of influences coming from North Africa to the Levant, citing the microburin technique and "microlithic forms such as arched backed bladelets and La Mouillah points."[15] But recent research has shown that the presence of arched backed bladelets, La Mouillah points, and the use of the microburin technique was already apparent in the Nebekian industry of the Eastern Levant.[16] And Maher et al. state that, "Many technological nuances that have often been always highlighted as significant during the Natufian were already present during the Early and Middle EP [Epipalaeolithic] and do not, in most cases, represent a radical departure in knowledge, tradition, or behavior."[17]
Authors such as Christopher Ehret have built upon the little evidence available to develop scenarios of intensive usage of plants having built up first in North Africa, as a precursor to the development of true farming in the Fertile Crescent, but such suggestions are considered highly speculative until more North African archaeological evidence can be gathered.[18][19] In fact, Weiss et al. have shown that the earliest known intensive usage of plants was in the Levant 23,000 years ago at the Ohalo II site.[20][21][22]
Anthropologist C. Loring Brace (1993) cross-analysed the craniometric traits of Natufian specimens with those of various ancient and modern groups from the Near East, Africa and Europe. The Late Pleistocene Epipalaeolithic Natufian sample was described as problematic due to its small size (consisting of only three males and one female), as well as the lack of a comparative sample from the Natufians' putative descendants in the Neolithic Near East. Brace observed that the Natufian fossils lay between those of the Niger–Congo-speaking series included and the other samples (Near East, Europe), which he suggested may point to a Sub-Saharan influence in their constitution.[23] Subsequent ancient DNA analysis of Natufian skeletal remains by Lazaridis et al. (2016) found that the specimens instead were a mix of 50% Basal Eurasian ancestral component (see Archaeogenetics) and 50% West-Eurasian Unknown Hunter Gatherer (UHG) population related to European Western Hunter-Gatherers.[24]
Natufians have also been described by anthropologists as a Proto-Mediterranean population.[25][26]
According to Bar-Yosef and Belfer-Cohen, "It seems that certain preadaptive traits, developed already by the Kebaran and Geometric Kebaran populations within the Mediterranean park forest, played an important role in the emergence of the new socioeconomic system known as the Natufian culture."[27]
Settlements occur mostly in Israel and Palestine. This could be deemed the core zone of the Natufian culture, but Israel is a place that has been excavated more frequently than other places hence the greater number of sites.[28] During the years more sites have been found outside the core zone of Israel and Palestine stretching into what now is Syria, Lebanon, Jordan, the Sinai Peninsula and the Negev desert.[28] The settlements in the Natufian culture were larger and more permanent than in preceding ones. Some Natufian sites had stone built architecture; Ain Mallaha is an example of round stone structures.[29] Cave sites are also seen frequently during the Natufian culture. El Wad is a Natufian cave site with occupation in the front part of the cave also called the terrace.[30] Some Natufian sites were located in forest/steppe areas and others near inland mountains. The Natufian settlements appear to be the first to exhibit evidence of food storage; not all Natufian sites have storage facilities, but they have been identified at certain sites.[31] Natufians are also suggested to have visited Cyprus, requiring travel over significant distances of water.[32]
The Natufian had a microlithic industry centered on short blades and bladelets. The microburin technique was used. Geometric microliths include lunates, trapezes, and triangles. There are backed blades as well. A special type of retouch (Helwan retouch) is characteristic for the early Natufian. In the late Natufian, the Harif-point, a typical arrowhead made from a regular blade, became common in the Negev. Some scholars[who?] use it to define a separate culture, the Harifian.
Sickle blades also appear for the first time in the Natufian lithic industry. The characteristic sickle-gloss shows that they were used to cut the silica-rich stems of cereals, indirectly suggesting the existence of incipient agriculture. Shaft straighteners made of ground stone indicate the practice of archery. There are heavy ground-stone bowl mortars as well.
The Ain Sakhri lovers, a carved stone object held at the British Museum, is the oldest known depiction of a couple having sex. It was found in the Ain Sakhri cave in the Judean desert.[33]
Natufian grave goods are typically made of shell, teeth (of red deer), bones, and stone. There are pendants, bracelets, necklaces, earrings, and belt-ornaments as well.
In 2008, the 12,400–12,000 cal BC grave of an apparently significant Natufian female was discovered in a ceremonial pit in the Hilazon Tachtit cave in northern Israel.[34]  Media reports referred to this person as a "shaman".[35] The burial contained the remains of at least three aurochs and 86 tortoises, all of which are thought to have been brought to the site during a funeral feast.  The body was surrounded by tortoise shells, the pelvis of a leopard, forearm of a boar, a wingtip of a golden eagle, and skull of a beech marten.[36][37]
At Ain Mallaha (in Northern Israel), Anatolian obsidian and shellfish from the Nile valley have been found. The source of malachite beads is still unknown. Epipaleolithic Natufians carried parthenocarpic figs from Africa to the southeastern corner of the Fertile Crescent, c. 10,000 BC.[38]
There was a rich bone industry, including harpoons and fish hooks. Stone and bone were worked into pendants and other ornaments. There are a few human figurines made of limestone (El-Wad, Ain Mallaha, Ain Sakhri), but the favorite subject of representative art seems to have been animals. Ostrich-shell containers have been found in the Negev.
In 2018, the world's oldest brewery was found, with the residue of 13,000-year-old beer, in a prehistoric cave near Haifa in Israel when researchers were looking for clues into what plant foods the Natufian people were eating. This is 8,000 years earlier than experts previously thought beer was invented.[39]
A study published in 2019 shows an advanced knowledge of lime plaster production at a Natufian cemetery in Nahal Ein Gev II site in the Upper Jordan Valley dated to 12 thousand (calibrated) years before present [k cal BP]. Production of plaster of this quality was previously thought to have been achieved some 2,000 years later.[40]
The Natufian people lived by hunting and gathering. The preservation of plant remains is poor because of the soil conditions, but at some sites such as Tell Abu Hureyra substantial  amounts of plant remains discovered through flotation have been excavated.[41] However wild cereals like legumes, almonds, acorns and pistachios have been collected throughout most of the Levant. Animal bones show that mountain and goitered gazelles (Gazella gazella and Gazella subgutturosa) were the main prey.
Additionally, deer, aurochs and wild boar were hunted in the steppe, as well as onagers and caprids (ibex). Waterfowl and freshwater fish formed part of the diet in the Jordan river valley. Animal bones from Salibiya I (12,300 – 10,800 cal BP) have been interpreted as evidence for communal hunts with nets, however, the radiocarbon dates are far too old compared to the cultural remains of this settlement, indicating contamination of the samples.[42]
A pita-like bread has been found from 12,500 BC attributed to Natufians. This bread is made of wild cereal seeds and papyrus cousin tubers, ground into flour.[43]
According to one theory,[35] it was a sudden change in climate, the Younger Dryas event (c. 10,800 to 9500 BC), which inspired the development of agriculture. The Younger Dryas was a 1,000-year-long interruption in the higher temperatures prevailing since the Last Glacial Maximum, which produced a sudden drought in the Levant.  This would have endangered the wild cereals, which could no longer compete with dryland scrub, but upon which the population had become dependent to sustain a relatively large sedentary population.  By artificially clearing scrub and planting seeds obtained from elsewhere, they began to practice agriculture. However, this theory of the origin of agriculture is controversial in the scientific community.[44]
At the Natufian site of Ain Mallaha in Israel, dated to 12,000 BC, the remains of an elderly human and a four-to-five-month-old puppy were found buried together.[45] At another Natufian site at the cave of Hayonim, humans were found buried with two canids.[45]
Ancient DNA analysis of Natufian skeletal remains found that the Natufian ancestry could be modelled as a mix of about 50% Basal Eurasian ancestry, and 50% from a West-Eurasian Unknown Hunter Gatherer (UHG) population, which was related to European Western Hunter-Gatherers.[24] Vallini et al. (2024) modeled the amount of Basal Eurasian ancestry among Natufians at roughly 15%, with the remainder being associated with West Eurasian sources.[47]
The Natufian population also displays ancestral ties to Paleolithic Taforalt samples, the makers of the Epipaleolithic Iberomaurusian culture of the Maghreb, the Pre-Pottery Neolithic culture of the Levant, the Early Neolithic Ifri N'Amr Ou Moussa and the Late Neolithic Kelif el Boroud culture of North Africa, with samples associated with these early cultures all sharing a common genomic component dubbed the "Natufian component", which diverged from other West Eurasian lineages ~26,000 years ago, and is most closely linked to the Arabian lineage. Possible bidirectional geneflow events between these groups has also been suggested, with particular evidence for affinity between the Natufians and Iberomaurusians.[48][49]
Contact between Natufians and other Neolithic Levantines, Caucasus hunter-gatherers (CHG), Anatolian and Iranian farmers is believed[who?] to have decreased genetic variability among later populations in the Middle East.[citation needed] Migrations from the Near-East also occurred towards Africa, and the West Eurasian-like ancestry among populations in the Horn of Africa being best represented by the Levant Neolithic, and may be associated with the spread of Afroasiatic languages.[citation needed]
Lazaridis et al.  (2016) did not find a greater genetic affinity between Natufians and sub-Saharan Africans than that existing between sub-Saharan Africans and other ancient populations of Western Eurasia, and also stated that the ancestry of a primitive population from North Africa could not be tested because modern North Africans are largely descended from late migrant populations from Eurasia.[50] However, Daniel Shriner (2018), using modern populations as a reference, found 28% autosomal African ancestry in Natufian samples, with 21.2% related to North Africa and 6.8% related to Omotic-speaking populations in southern Ethiopia, which reveals a plausible source for haplogroup E in Natufians; still according to Shriner, the Natufian samples had 61.2% ancestry related to Arabs and 10.8% ancestry related to West Asians.[51]
As summarized by Rosa Fregel, a later preprint from Lazaridis et al. (2018) has contested Loosdrecht's conclusion and argues for a minor sub-Saharan African component in Natufians, stating "that [the Iberomaurusians of] Taforalt can be better modeled as a mixture of a Dzudzuana component and a sub-Saharan African component" (or an ancient and now-extinct North African component that diverged prior to the Out-of-Africa migration) and "also argue that (...) the Taforalt people (...) contributed to the genetic composition of Natufians and not the other way around", which, according to Lazaridis et al., would be consistent with morphological and archaeological studies that indicate a dissemination of morphological characteristics and artifacts from North Africa to the Near East, as well as explaining the presence of Y-chromosome haplogroup E in Natufians and Levantine farmers. Fregel summarizes that "More evidence will be needed to determine the specific origin of the North African Upper Paleolithic populations".[52][53]
In their 2017 paper, Ranajit Das, Paul Wexler, Mehdi Pirooznia and Eran Elhaik analyzed the Lazaridis et al. (2016) study concluding that the Natufians, together with one Neolithic Levantine sample, clustered in the proximity to modern Palestinians and Bedouins, and also "marginally overlapped" with Yemenite Jews.[54] Ferreira et al. (2021) and Almarri et al. (2021) found that ancient Natufians cluster with modern Arabian groups, such as Saudi Arabians and Yemenis, which derive most of their ancestry from local Natufian-like hunter-gatherer peoples and have less Neolithic Anatolian ancestry than Levantines.[55][56] Sirak et al. (2024) found that medieval Socotra (the Soqotri people), similar to modern Saudis, Yemenis and Bedouins, have a majority component that is "maximized in Late Pleistocene (Epipaleolithic) Natufian hunter–gatherers from the Levant".[57]
Alexander Militarev, Vitaly Shevoroshkin and others have linked the Natufian culture to the proto-Afroasiatic language,[58][59] which they in turn believe has a Levantine origin. Some scholars, for example Christopher Ehret, Roger Blench and others, contend that the Afroasiatic Urheimat is to be found in North Africa or Northeast Africa, probably in the area of Egypt, the Sahara, Horn of Africa or Sudan.[60][61][62][63] Within this group, Ehret, who like Militarev believes Afroasiatic may already have been in existence in the Natufian period, would associate Natufians only with the Near Eastern Proto-Semitic branch of Afroasiatic.[64]
John Bengston documented that archeological and physical anthropological evidence showed Natufians are closely related to modern Semitic-speaking people from the Levant. Under his hypothesis, Afro-Asiatic branches originated in North Africa proper (Egypt), and the age of these languages can be dated to the periods of the Natufian culture around ~12,000 years ago. He postulated this based on the biological discontinuity between Pleistocene and Holocene North Africa, where there was population replacement and admixture in this region involving external migrants from northern areas, who were the ancestral Afro-Asiatic speakers.[65]
The Natufian culture has been documented at dozens of sites. Around 90 have been excavated, including:[66]

A state is a political entity that regulates society and the population within a definite territory.[1] Government is considered to form the fundamental apparatus of contemporary states.[2][3]
A country often has a single state, with various administrative divisions. A state may be a unitary state or some type of federal union; in the latter type, the term "state" is sometimes used to refer to the federated polities that make up the federation, and they may have some of the attributes of a sovereign state, except being under their federation and without the same capacity to act internationally. (Other terms that are used in such federal systems may include "province", "region" or other terms.)
For most of prehistory, people lived in stateless societies. The earliest forms of states arose about 5,500 years ago.[4] Over time societies became more stratified and developed institutions leading to centralised governments. These gained state capacity in conjunction with the growth of cities, which was often dependent on climate and economic development, with centralisation often spurred on by insecurity and territorial competition.
Over time, varied forms of states developed, that used many different justifications for their existence (such as divine right, the theory of the social contract, etc.). Today, the modern nation state is the predominant form of state to which people are subject.[5] Sovereign states have sovereignty; any ingroup's claim to have a state faces some practical limits via the degree to which other states recognize them as such. Satellite states are states that have de facto sovereignty but are often indirectly controlled by another state.
Definitions of a state are disputed.[6][7] According to sociologist Max Weber, a "state" is a polity that maintains a monopoly on the legitimate use of violence, although other definitions are common.[8][9] Absence of a state does not preclude the existence of a society, such as stateless societies like the Haudenosaunee Confederacy that "do not have either purely or even primarily political institutions or roles".[10]  The degree and extent of governance of a state is used to determine whether it has failed.[11]
The word state and its cognates in some other European languages (stato in Italian, estado in Spanish and Portuguese, état in French, Staat in German and Dutch) ultimately derive from the Latin word status, meaning "condition, circumstances". Latin status derives from stare, "to stand", or remain or be permanent, thus providing the sacred or magical connotation of the political entity.
The English noun state in the generic sense "condition, circumstances" predates the political sense. It was introduced to Middle English c. 1200 both from Old French and directly from Latin.
With the revival of the Roman law in 14th-century Europe, the term came to refer to the legal standing of persons (such as the various "estates of the realm" – noble, common, and clerical), and in particular the special status of the king. The highest estates, generally those with the most wealth and social rank, were those that held power. The word also had associations with Roman ideas (dating back to Cicero) about the "status rei publicae", the "condition of public matters". In time, the word lost its reference to particular social groups and became associated with the legal order of the entire society and the apparatus of its enforcement.[12]
The early 16th-century works of Machiavelli (especially The Prince) played a central role in popularizing the use of the word "state" in something similar to its modern sense.[13] The contrasting of church and state still dates to the 16th century. The North American colonies were called "states" as early as the 1630s.[citation needed] The expression "L'État, c'est moi" ("I am the State") attributed to Louis XIV, although probably apocryphal, is recorded in the late 18th century.[14]
There is no academic consensus on the definition of the state.[6] The term "state" refers to a set of different, but interrelated and often overlapping, theories about a certain range of political phenomena.[7] According to Walter Scheidel, mainstream definitions of the state have the following in common: "centralized institutions that impose rules, and back them up by force, over a territorially circumscribed population; a distinction between the rulers and the ruled; and an element of autonomy, stability, and differentiation. These distinguish the state from less stable forms of organization, such as the exercise of chiefly power."[15]
The most commonly used definition is by Max Weber[16][17][18][19][20] who describes the state as a compulsory political organization with a centralized government that maintains a monopoly of the legitimate use of force within a certain territory.[8][9] Weber writes that the state "is a human community that (successfully) claims the monopoly of the legitimate use of physical force within a given territory."[21]
While defining a state, it is important not to confuse it with a nation; an error that occurs frequently in common discussion. A state refers to a political unit with sovereignty over a given territory.[22] While a state is more of a "political-legal abstraction," the definition of a nation is more concerned with political identity and cultural or historical factors.[22] Importantly, nations do not possess the organizational characteristics like geographic boundaries or authority figures and officials that states do.[22] Additionally, a nation does not have a claim to a monopoly on the legitimate use of force over their populace,[22] while a state does, as Weber indicated. An example of the instability that arises when a state does not have a monopoly on the use of force can be seen in African states which remain weak due to the lack of war which European states relied on.[23] A state should not be confused with a government; a government is an organization that has been granted the authority to act on the behalf of a state.[22] Nor should a state be confused with a society; a society refers to all organized groups, movements, and individuals who are independent of the state and seek to remain out of its influence.[22]
Neuberger offers a slightly different definition of the state with respect to the nation: the state is "a primordial, essential, and permanent expression of the genius of a specific [nation]."[24]
The definition of a state is also dependent on how and why they form. The contractarian view of the state suggests that states form because people can all benefit from cooperation with others[25] and that without a state there would be chaos.[26] The contractarian view focuses more on the alignment and conflict of interests between individuals in a state. On the other hand, the predatory view of the state focuses on the potential mismatch between the interests of the people and the interests of the state. Charles Tilly goes so far to say that states "resemble a form of organized crime and should be viewed as extortion rackets."[27] He argued that the state sells protection from itself and raises the question about why people should trust a state when they cannot trust one another.[22]
Tilly defines states as "coercion-wielding organisations that are distinct from households and kinship groups and exercise a clear priority in some respects over all other organizations within substantial territories."[28] Tilly includes city-states, theocracies and empires in his definition along with nation-states, but excludes tribes, lineages, firms and churches.[29] According to Tilly, states can be seen in the archaeological record as of 6000 BC; in Europe they appeared around 990, but became particularly prominent after 1490.[29] Tilly defines a state's "essential minimal activities" as:
Importantly, Tilly makes the case that war is an essential part of state-making; that wars create states and vice versa.[32]
Modern academic definitions of the state frequently include the criterion that a state has to be recognized as such by the international community.[33]
Liberal thought provides another possible teleology of the state. According to John Locke, the goal of the state or commonwealth is "the preservation of property" (Second Treatise on Government), with 'property' in Locke's work referring not only to personal possessions but also to one's life and liberty. On this account, the state provides the basis for social cohesion and productivity, creating incentives for wealth-creation by providing guarantees of protection for one's life, liberty and personal property. Provision of public goods is considered by some such as Adam Smith[34] as a central function of the state, since these goods would otherwise be underprovided. Tilly has challenged narratives of the state as being the result of a societal contract or provision of services in a free market – he characterizes the state more akin as a protection racket in the vein of organized crime.[31]
While economic and political philosophers have contested the monopolistic tendency of states,[35] Robert Nozick argues that the use of force naturally tends towards monopoly.[36]
Another commonly accepted definition of the state is the one given at the Montevideo Convention on the Rights and Duties of States in 1933. It provides that "[t]he state as a person of international law should possess the following qualifications: (a) a permanent population; (b) a defined territory; (c) government; and (d) capacity to enter into relations with the other states."[37] And that "[t]he federal state shall constitute a sole person in the eyes of international law."[38]
Confounding the definition problem is that "state" and "government" are often used as synonyms in common conversation and even some academic discourse. According to this definition schema, the states are nonphysical persons of international law, and governments are organizations of people.[39] The relationship between a government and its state is one of representation and authorized agency.[40]
Charles Tilly distinguished between empires, theocracies, city-states and nation-states.[29] According to Michael Mann, the four persistent types of state activities are:
Josep Colomer distinguished between empires and states in the following way:
According to Michael Hechter and William Brustein, the modern state was differentiated from "leagues of independent cities, empires, federations held together by loose central control, and theocratic federations" by four characteristics:
States may be classified by political philosophers as sovereign if they are not dependent on, or subject to any other power or state. Other states are subject to external sovereignty or hegemony where ultimate sovereignty lies in another state.[44] Many states are federated states which participate in a federal union. A federated state is a territorial and constitutional community forming part of a federation.[45] (Compare confederacies or confederations such as Switzerland.) Such states differ from sovereign states in that they have transferred a portion of their sovereign powers to a federal government.[46]
One can commonly and sometimes readily (but not necessarily usefully) classify states according to their apparent make-up or focus. The concept of the nation-state, theoretically or ideally co-terminous with a "nation", became very popular by the 20th century in Europe, but occurred rarely elsewhere or at other times. In contrast, some states have sought to make a virtue of their multi-ethnic or multinational character (Habsburg Austria-Hungary, for example, or the Soviet Union), and have emphasised unifying characteristics such as autocracy, monarchical legitimacy, or ideology. Other states, often fascist or authoritarian ones, promoted state-sanctioned notions of racial superiority.[47] Other states may bring ideas of commonality and inclusiveness to the fore: note the res publica of ancient Rome and the Rzeczpospolita of Poland-Lithuania which finds echoes in the modern-day republic. The concept of temple states centred on religious shrines occurs in some discussions of the ancient world.[48] Relatively small city-states, once a relatively common and often successful form of polity,[49] have become rarer and comparatively less prominent in modern times. Modern-day independent city-states include Vatican City, Monaco, and Singapore. Other city-states survive as federated states, like the present day German city-states, or as otherwise autonomous entities with limited sovereignty, like Hong Kong, Gibraltar and Ceuta. To some extent, urban secession, the creation of a new city-state (sovereign or federated), continues to be discussed in the early 21st century in cities such as London.
A state can be distinguished from a government. The state is the organization while the government is the particular group of people, the administrative bureaucracy that controls the state apparatus at a given time.[50][51][52] That is, governments are the means through which state power is employed. States are served by a continuous succession of different governments.[52] States are immaterial and nonphysical social objects, whereas governments are groups of people with certain coercive powers.[53]
Each successive government is composed of a specialized and privileged body of individuals, who monopolize political decision-making and are separated by status and organization from the population as a whole.
States can also be distinguished from the concept of a "nation", where "nation" refers to a cultural-political community of people. A nation-state refers to a situation where a single ethnicity is associated with a specific state.
In the classical thought, the state was identified with both political society and civil society as a form of political community, while the modern thought distinguished the nation state as a political society from civil society as a form of economic society.[54]
Thus in the modern thought the state is contrasted with civil society.[55][56][57]
Antonio Gramsci believed that civil society is the primary locus of political activity because it is where all forms of "identity formation, ideological struggle, the activities of intellectuals, and the construction of hegemony take place." and that civil society was the nexus connecting the economic and political sphere. Arising out of the collective actions of civil society is what Gramsci calls "political society", which Gramsci differentiates from the notion of the state as a polity. He stated that politics was not a "one-way process of political management" but, rather, that the activities of civil organizations conditioned the activities of political parties and state institutions, and were conditioned by them in turn.[58][59] Louis Althusser argued that civil organizations such as church, schools, and the family are part of an "ideological state apparatus" which complements the "repressive state apparatus" (such as police and military) in reproducing social relations.[60][61][62]
Jürgen Habermas spoke of a public sphere that was distinct from both the economic and political sphere.[63]
Given the role that many social groups have in the development of public policy and the extensive connections between state bureaucracies and other institutions, it has become increasingly difficult to identify the boundaries of the state. Privatization, nationalization, and the creation of new regulatory bodies also change the boundaries of the state in relation to society. Often the nature of quasi-autonomous organizations is unclear, generating debate among political scientists on whether they are part of the state or civil society. Some political scientists thus prefer to speak of policy networks and decentralized governance in modern societies rather than of state bureaucracies and direct state control over policy.[64]
The earliest forms of the state emerged whenever it became possible to centralize power in a durable way. Agriculture and a settled population have been attributed as necessary conditions to form states.[65][66][67][68] Certain types of agriculture are more conducive to state formation, such as grain (wheat, barley, millet), because they are suited to concentrated production, taxation, and storage.[65][69][70][71] Agriculture and writing are almost everywhere associated with this process: agriculture because it allowed for the emergence of a social class of people who did not have to spend most of their time providing for their own subsistence, and writing (or an equivalent of writing, like Inca quipus) because it made possible the centralization of vital information.[72] Bureaucratization made expansion over large territories possible.[73]
The first known states were created in Egypt, Mesopotamia, India, China, Mesoamerica, and the Andes. It is only in relatively modern times that states have almost completely displaced alternative "stateless" forms of political organization of societies all over the planet. Roving bands of hunter-gatherers and even fairly sizable and complex tribal societies based on herding or agriculture have existed without any full-time specialized state organization, and these "stateless" forms of political organization have in fact prevailed for all of the prehistory and much of human history and civilization.
The primary competing organizational forms to the state were religious organizations (such as the Church), and city republics.[74]
Since the late 19th century, virtually the entirety of the world's inhabitable land has been parcelled up into areas with more or less definite borders claimed by various states. Earlier, quite large land areas had been either unclaimed or uninhabited, or inhabited by nomadic peoples who were not organised as states. However, even within present-day states there are vast areas of wilderness, like the Amazon rainforest, which are uninhabited or inhabited solely or mostly by indigenous people (and some of them remain uncontacted). Also, there are so-called "failed states" which do not hold de facto control over all of their claimed territory or where this control is challenged. Currently, the international community comprises around 200 sovereign states, the vast majority of which are represented in the United Nations.[citation needed]
For most of human history, people have lived in stateless societies, characterized by a lack of concentrated authority, and the absence of large inequalities in economic and political power.
The anthropologist Tim Ingold writes:
It is not enough to observe, in a now rather dated anthropological idiom, that hunter gatherers live in 'stateless societies', as though their social lives were somehow lacking or unfinished, waiting to be completed by the evolutionary development of a state apparatus. Rather, the principal of their socialty, as Pierre Clastres has put it, is fundamentally against the state.[75]
During the Neolithic period, human societies underwent major cultural and economic changes, including the development of agriculture, the formation of sedentary societies and fixed settlements, increasing population densities, and the use of pottery and more complex tools.[76][77]
Sedentary agriculture led to the development of property rights, domestication of plants and animals, and larger family sizes. It also provided the basis for an external centralized state.[78] By producing a large surplus of food, more division of labor was realized, which enabled people to specialize in tasks other than food production.[79] Early states were characterized by highly stratified societies, with a privileged and wealthy ruling class that was subordinate to a monarch. The ruling classes began to differentiate themselves through forms of architecture and other cultural practices that were different from those of the subordinate laboring classes.[80]
In the past, it was suggested that the centralized state was developed to administer large public works systems (such as irrigation systems) and to regulate complex economies.[81] However, modern archaeological and anthropological evidence does not support this thesis, pointing to the existence of several non-stratified and politically decentralized complex societies.[82]
Mesopotamia is generally considered to be the location of the earliest civilization or complex society, meaning that it contained cities, full-time division of labor, social concentration of wealth into capital, unequal distribution of wealth, ruling classes, community ties based on residency rather than kinship, long distance trade, monumental architecture, standardized forms of art and culture, writing, and mathematics and science.[83][84] It was the world's first literate civilization, and formed the first sets of written laws.[85][86] Bronze metallurgy spread within Afro-Eurasia from c. 3000 BC, leading to a military revolution in the use of bronze weaponry, which facilitated the rise of states.[87]
Although state-forms existed before the rise of the Ancient Greek empire, the Greeks were the first people known to have explicitly formulated a political philosophy of the state, and to have rationally analyzed political institutions. Prior to this, states were described and justified in terms of religious myths.[88]
Several important political innovations of classical antiquity came from the Greek city-states and the Roman Republic. The Greek city-states before the 4th century granted citizenship rights to their free population, and in Athens these rights were combined with a directly democratic form of government that was to have a long afterlife in political thought and history.
During medieval times in Europe, the state was organized on the principle of feudalism, and the relationship between lord and vassal became central to social organization. Feudalism led to the development of greater social hierarchies.[89]
The formalization of the struggles over taxation between the monarch and other elements of society (especially the nobility and the cities) gave rise to what is now called the Standestaat, or the state of Estates, characterized by parliaments in which key social groups negotiated with the king about legal and economic matters. These estates of the realm sometimes evolved in the direction of fully-fledged parliaments, but sometimes lost out in their struggles with the monarch, leading to greater centralization of lawmaking and military power in his hands. Beginning in the 15th century, this centralizing process gave rise to the absolutist state.[90]
Cultural and national homogenization figured prominently in the rise of the modern state system. Since the absolutist period, states have largely been organized on a national basis. The concept of a national state, however, is not synonymous with nation state. Even in the most ethnically homogeneous societies there is not always a complete correspondence between state and nation, hence the active role often taken by the state to promote nationalism through an emphasis on shared symbols and national identity.[91]
Charles Tilly argues that the number of total states in Western Europe declined rapidly from the Late Middle Ages to Early Modern Era during a process of state formation.[92] Other research has disputed whether such a decline took place.[93]
For Edmund Burke (Dublin 1729 - Beaconsfield 1797), "a state without the means of some change is without the means of its conservation" (Reflections on the Revolution in France).[94]
According to Hendrik Spruyt, the modern state is different from its predecessor polities in two main aspects: (1) Modern states have a greater capacity to intervene in their societies, and (2) Modern states are buttressed by the principle of international legal sovereignty and the judicial equivalence of states.[95] The two features began to emerge in the Late Middle Ages but the modern state form took centuries to come firmly into fruition.[95] Other aspects of modern states is that they tend to be organized as unified national polities, and that they have rational-legal bureaucracies.[96]
Sovereign equality did not become fully global until after World War II amid decolonization.[95] Adom Getachew writes that it was not until the 1960 Declaration on the Granting of Independence to Colonial Countries and Peoples that the international legal context for popular sovereignty was instituted.[97] Historians Jane Burbank and Frederick Cooper argue that "Westphalian sovereignty" – the notion that bounded, unitary states interact with equivalent states – "has more to do with 1948 than 1648."[98]
Theories for the emergence of the earliest states emphasize grain agriculture and settled populations as necessary conditions.[84]
However, not all types of property are equally exposed to the risk of looting or equally subject to taxation. Goods differ in their shelf life. Certain agricultural products, fish, and dairy spoil quickly and cannot be stored without refrigeration or freezing technology, which was unavailable in ancient times. As a result, such perishable goods were of little interest to either looters or the king (In ancient times, especially before the invention of money, taxation was primarily collected from agricultural produce.) Both looters and rulers sought goods with long shelf lives, such as grains (wheat, barley, rice, corn, etc.), which, under proper storage conditions, could be preserved for extended periods. With the domestication of wheat and the establishment of agricultural communities, the need for protection from bandits arose, along with the emergence of strong governance to provide it. Mayshar et al. (2020) demonstrated that societies cultivating grains tended to develop hierarchical structures with a ruling elite that collected taxes, whereas societies that relied on root crops (which have short shelf lives) did not develop such hierarchies. The cultivation of grains became concentrated in regions with fertile soil, where grain production was more profitable than root crops, even after accounting for taxes imposed by rulers and raids by looters.[99]
However, protection was not the only public good necessitating a centralized government. The shift to agriculture based on irrigation systems, as seen in ancient Egypt, required cooperation among farmers. An individual farmer could not control the floods from the Nile River alone. Managing the vast amounts of water during the annual floods and utilizing them efficiently allowed for a significant increase in agricultural yield, but this required an elaborate network of irrigation canals to distribute water efficiently across fields while minimizing waste.[100][101]
Such a system exhibited characteristics of a natural monopoly, as its construction involved substantial fixed costs, making it a lucrative asset for the ruling elite. Bentzen, Kaarsen, and Wingender (2017) showed that in pre-modern societies, regions dependent on irrigation-intensive agriculture experienced higher levels of land inequality. The concentration of land and control over water resources strengthened elite power, enabling them to resist democratization in the modern era. Even today, countries that rely on irrigated agriculture tend to be less democratic than those relying on rain-fed farming.[102]
Some argue that climate change led to a greater concentration of human populations around dwindling waterways.[84]
Hendrik Spruyt distinguishes between three prominent categories of explanations for the emergence of the modern state as a dominant polity: (1) Security-based explanations that emphasize the role of warfare, (2) Economy-based explanations that emphasize trade, property rights and capitalism as drivers behind state formation, and (3) Institutionalist theories that sees the state as an organizational form that is better able to resolve conflict and cooperation problems than competing political organizations.[95]
According to Philip Gorski and Vivek Swaroop Sharma, the "neo-Darwinian" framework for the emergence of sovereign states is the dominant explanation in the scholarship.[103] The neo-Darwininian framework emphasizes how the modern state emerged as the dominant organizational form through natural selection and competition.[103]
Most political theories of the state can roughly be classified into two categories:
Anarchism as a political philosophy regards the state and hierarchies as unnecessary and harmful, and instead promotes a stateless society, or anarchy, a self-managed, self-governed society based on voluntary, cooperative institutions.
Anarchists believe that the state is inherently an instrument of domination and repression, no matter who is in control of it. Anarchists note that the state possesses the monopoly on the legal use of violence. Unlike Marxists, anarchists believe that revolutionary seizure of state power should not be a political goal. They believe instead that the state apparatus should be completely dismantled, and an alternative set of social relations created, which are not based on state power at all.[104][105]
Various Christian anarchists, such as Jacques Ellul, have identified the state and political power as the Beast in the Book of Revelation.[106][107]
Anarcho-capitalists such as Murray Rothbard come to some of the same conclusions about the state apparatus as anarchists, but for different reasons.[108] The two principles that anarcho-capitalists rely on most are consent and non-initiation.[109] Consent in anarcho-capitalist theory requires that individuals explicitly assent to the jurisdiction of the State excluding Lockean tacit consent. Consent may also create a right of secession which destroys any concept of government monopoly on force.[108][110] Coercive monopolies are excluded by the non-initiation of force principle because they must use force in order to prevent others from offering the same service that they do. Anarcho-capitalists start from the belief that replacing monopolistic states with competitive providers is necessary from a normative, justice-based scenario.[109]
Anarcho-capitalists believe that the market values of competition and privatization can better provide the services provided by the state. Murray Rothbard argues in Power and Market that any and all government functions could better be fulfilled by private actors including: defense, infrastructure, and legal adjudication.[108]
Marx and Engels were clear in that the goal of communism was a classless society in which the state would have "withered away", replaced only by "administration of things".[111] Their views are found throughout their Collected Works, and address past or then-extant state forms from an analytical and tactical viewpoint, but not future social forms, speculation about which is generally antithetical[112] to groups considering themselves Marxist but who – not having conquered the existing state power(s) – are not in the situation of supplying the institutional form of an actual society. To the extent that it makes sense, there is no single "Marxist theory of state", but rather several different purportedly "Marxist" theories have been developed by adherents of Marxism.[113][114][115]
Marx's early writings portrayed the bourgeois state as parasitic, built upon the superstructure of the economy, and working against the public interest. He also wrote that the state mirrors class relations in society in general, acting as a regulator and repressor of class struggle, and as a tool of political power and domination for the ruling class.[116] The Communist Manifesto claims the state to be nothing more than "a committee for managing the common affairs of the bourgeoisie."[113]
For Marxist theorists, the role of the modern bourgeois state is determined by its function in the global capitalist order. Ralph Miliband argued that the ruling class uses the state as its instrument to dominate society by virtue of the interpersonal ties between state officials and economic elites. For Miliband, the state is dominated by an elite that comes from the same background as the capitalist class. State officials therefore share the same interests as owners of capital and are linked to them through a wide array of social, economic, and political ties.[117]
Gramsci's theories of state emphasized that the state is only one of the institutions in society that helps maintain the hegemony of the ruling class, and that state power is bolstered by the ideological domination of the institutions of civil society, such as churches, schools, and mass media.[118]
Pluralists view society as a collection of individuals and groups, who are competing for political power. They then view the state as a neutral body that simply enacts the will of whichever groups dominate the electoral process.[119] Within the pluralist tradition, Robert Dahl developed the theory of the state as a neutral arena for contending interests or its agencies as simply another set of interest groups. With power competitively arranged in society, state policy is a product of recurrent bargaining. Although pluralism recognizes the existence of inequality, it asserts that all groups have an opportunity to pressure the state. The pluralist approach suggests that the modern democratic state's actions are the result of pressures applied by a variety of organized interests. Dahl called this kind of state a polyarchy.[120]
Pluralism has been challenged on the ground that it is not supported by empirical evidence. Citing surveys showing that the large majority of people in high leadership positions are members of the wealthy upper class, critics of pluralism claim that the state serves the interests of the upper class rather than equitably serving the interests of all social groups.[121][122]
Jürgen Habermas believed that the base-superstructure framework, used by many Marxist theorists to describe the relation between the state and the economy, was overly simplistic. He felt that the modern state plays a large role in structuring the economy, by regulating economic activity and being a large-scale economic consumer/producer, and through its redistributive welfare state activities. Because of the way these activities structure the economic framework, Habermas felt that the state cannot be looked at as passively responding to economic class interests.[123][124][125]
Michel Foucault believed that modern political theory was too state-centric, saying "Maybe, after all, the state is no more than a composite reality and a mythologized abstraction, whose importance is a lot more limited than many of us think." He thought that political theory was focusing too much on abstract institutions, and not enough on the actual practices of government. In Foucault's opinion, the state had no essence. He believed that instead of trying to understand the activities of governments by analyzing the properties of the state (a reified abstraction), political theorists should be examining changes in the practice of government to understand changes in the nature of the state.[126][127][128] Foucault developed the concept of governmentality while considering the genealogy of state, and considers the way in which an individual's understanding of governance can influence the function of the state.[129]
Foucault argues that it is technology that has created and made the state so elusive and successful and that instead of looking at the state as something to be toppled we should look at the state as a technological manifestation or system with many heads; Foucault argues instead of something to be overthrown as in the sense of the Marxist and anarchist understanding of the state. Every single scientific technological advance has come to the service of the state Foucault argues and it is with the emergence of the Mathematical sciences and essentially the formation of mathematical statistics that one gets an understanding of the complex technology of producing how the modern state was so successfully created. Foucault insists that the nation state was not a historical accident but a deliberate production in which the modern state had to now manage coincidentally with the emerging practice of the police (cameral science) 'allowing' the population to now 'come in' into jus gentium and civitas (civil society) after deliberately being excluded for several millennia.[130] Democracy wasn't (the newly formed voting franchise) as is always painted by both political revolutionaries and political philosophers as a cry for political freedom or wanting to be accepted by the 'ruling elite', Foucault insists, but was a part of a skilled endeavour of switching over new technology such as; translatio imperii, plenitudo potestatis and extra Ecclesiam nulla salus readily available from the past medieval period, into mass persuasion for the future industrial 'political' population (deception over the population) in which the political population was now asked to insist upon itself "the president must be elected". Where these political symbol agents, represented by the pope and the president are now democratised. Foucault calls these new forms of technology biopower[131][132][130] and form part of our political inheritance which he calls biopolitics.
Heavily influenced by Gramsci, Nicos Poulantzas, a Greek neo-Marxist theorist argued that capitalist states do not always act on behalf of the ruling class, and when they do, it is not necessarily the case because state officials consciously strive to do so, but because the 'structural' position of the state is configured in such a way to ensure that the long-term interests of capital are always dominant. Poulantzas' main contribution to the Marxist literature on the state was the concept of 'relative autonomy' of the state. While Poulantzas' work on 'state autonomy' has served to sharpen and specify a great deal of Marxist literature on the state, his own framework came under criticism for its 'structural functionalism'.[citation needed]
It can be considered as a single structural universe: the historical reality that takes shape in societies characterized by a codified or crystallized right, with a power organized hierarchically and justified by the law that gives it authority, with a well-defined social and economic stratification, with an economic and social organization that gives the society precise organic characteristics, with one (or multiple) religious organizations, in justification of the power expressed by such a society and in support of the religious beliefs of individuals and accepted by society as a whole. Such a structural universe, evolves in a cyclical manner, presenting two different historical phases (a mercantile phase, or "open society", and a feudal phase or "closed society"), with characteristics so divergent that it can qualify as two different levels of civilization which, however, are never definitive, but that alternate cyclically, being able, each of the two different levels, to be considered progressive (in a partisan way, totally independent of the real value of well-being, degrees of freedom granted, equality realized and a concrete possibility to achieve further progress of the level of civilization), even by the most cultured fractions, educated and intellectually more equipped than the various societies, of both historical phases.[133]
State autonomy theorists believe that the state is an entity that is impervious to external social and economic influence and that it has interests of its own.[134]
"New institutionalist" writings on the state, such as the works of Theda Skocpol, suggest that state actors are to an important degree autonomous. In other words, state personnel have interests of their own, which they can and do pursue independently of (and at times in conflict with) actors in society. Since the state controls the means of coercion, and given the dependence of many groups in civil society on the state for achieving any goals they may espouse, state personnel can to some extent impose their own preferences on civil society.[135]
States generally rely on a claim to some form of political legitimacy in order to maintain domination over their subjects.[136][137][138]
Various social contract theories have been proffered to establish state legitimacy and to explain state formation. Common elements in these theories are a state of nature that incentivizes people to seek out the establishment of a state. Thomas Hobbes described the state of nature as "solitary, poor, nasty, brutish, and short" (Leviathan, Chapters XIII–XIV).[139] Locke takes a more benign view of the state of nature and is unwilling to take as hard a stance on the degeneracy of the state of nature. He does agree that it is equally incapable of providing a high quality of life. Locke argues for inalienable human rights. One of the most significant rights for Locke was the right to property. He viewed it as a keystone right that was inadequately protected in the state of nature.[140][141] Social contract theorists frequently argue for some level of natural rights. In order to protect their ability to exercise these rights, they are willing to give up some other rights to the state to allow it to establish governance.[citation needed] Social contract theory then bases government legitimacy on the consent of the governed, but such legitimacy only extends as far as the governed have consented. This line of reasoning figures prominently in The United States Declaration of Independence.
The rise of the modern-day state system was closely related to changes in political thought, especially concerning the changing understanding of legitimate state power and control. Early modern defenders of absolutism (Absolute monarchy), such as Thomas Hobbes and Jean Bodin undermined the doctrine of the divine right of kings by arguing that the power of kings should be justified by reference to the people. Hobbes in particular went further to argue that political power should be justified with reference to the individual (Hobbes wrote in the time of the English Civil War), not just to the people understood collectively. Both Hobbes and Bodin thought they were defending the power of kings, not advocating for democracy, but their arguments about the nature of sovereignty were fiercely resisted by more traditional defenders of the power of kings, such as Sir Robert Filmer in England, who thought that such defenses ultimately opened the way to more democratic claims.[citation needed]
Max Weber identified three main sources of political legitimacy in his works. The first, legitimacy based on traditional grounds is derived from a belief that things should be as they have been in the past, and that those who defend these traditions have a legitimate claim to power. The second, legitimacy based on charismatic leadership, is devotion to a leader or group that is viewed as exceptionally heroic or virtuous. Max Weber's concept of charisma is also explored by Fukuyama, who uses it to explain why individuals relinquish their personal freedoms and more egalitarian smaller communities in favor of larger, more authoritarian states. The Scholars goes further by saying that Charismatic leaders can leverage this mass mobilization as a military force, achieving victories and securing peace, which in turn further legitimizes their authority. Fukuyama cites the example of Muhammad, whose influence facilitated the rise of a powerful state in North Africa and the Middle East, despite limited economic foundations.[142]
The third is rational-legal authority, whereby legitimacy is derived from the belief that a certain group has been placed in power in a legal manner, and that their actions are justifiable according to a specific code of written laws. Weber believed that the modern state is characterized primarily by appeals to rational-legal authority.[143][144][145]
Some states are often labeled as "weak" or "failed". In David Samuels's words "...a failed state occurs when sovereignty over claimed territory has collapsed or was never effectively at all".[146] Authors like Samuels and Joel S. Migdal have explored the emergence of weak states, how they are different from Western "strong" states and its consequences to the economic development of developing countries.
Samuels introduces the idea of state capacity, which he uses to refer to the ability of the state to fulfill its basic functions, such as providing security, maintaining law and order, and delivering public services. When a state does not accomplish this, state failure happens (Samuels, 2012). Other authors like Jeffrey Herbst add to this idea by arguing that state failure is the result of weak or non-existent institutions, which means that there is no state legitimacy because states are not able to provide goods or services or maintain order and safety (Herbst, 1990). However, there are also ideas that challenge this notion of state failure. Stephen D. Krasner argues that state failure is not just the result of weak institutions, but rather a very complex phenomenon that varies according to context-specific circumstances, and should therefore not be analyzed through a simplistic understanding like the one normally presented (Krasner, 2004).
In "The Problem of Failed States", Susan Rice argues that state failure is an important threat to global stability and security, since failed states are vulnerable to terrorism and conflict (Rice, 1994).[full citation needed] Additionally, it is believed that state failure hinders democratic values, since these states often experience political violence, authoritarian rules, and a number of human rights abuses (Rotberg, 2004).[full citation needed] While there is great discussion regarding the direct effects of state failure, its indirect effects should also be highlighted: state failure could lead to refugee flows and cross-border conflicts, while also becoming safe havens for criminal or extremist groups (Corbridge, 2005).[full citation needed] In order to solve and prevent these issues in the future, it is necessary to focus on building strong institutions, promoting economic diversification and development, and addressing the causes of violence in each state (Mkandawire, 2001).[full citation needed]
To understand the formation of weak states, Samuels compares the formation of European states in the 1600s with the conditions under which more recent states were formed in the twentieth century. In this line of argument, the state allows a population to resolve a collective action problem, in which citizens recognize the authority of the state and exercise the power of coercion over them. This kind of social organization required a decline in the legitimacy of traditional forms of ruling (like religious authorities) and replaced them with an increase in the legitimacy of depersonalized rule; an increase in the central government's sovereignty; and an increase in the organizational complexity of the central government (bureaucracy).
The transition to this modern state was possible in Europe around 1600 thanks to the confluence of factors like the technological developments in warfare, which generated strong incentives to tax and consolidate central structures of governance to respond to external threats. This was complemented by the increase in the production of food (as a result of productivity improvements), which allowed to sustain a larger population and so increased the complexity and centralization of states. Finally, cultural changes challenged the authority of monarchies and paved the way for the emergence of modern states.[147]
The conditions that enabled the emergence of modern states in Europe were different for other countries that started this process later. As a result, many of these states lack effective capabilities to tax and extract revenue from their citizens, which derives in problems like corruption, tax evasion and low economic growth. Unlike the European case, late state formation occurred in a context of limited international conflict that diminished the incentives to tax and increase military spending. Also, many of these states emerged from colonization in a state of poverty and with institutions designed to extract natural resources, which have made more difficult to form states. European colonization also defined many arbitrary borders that mixed different cultural groups under the same national identities, which has made difficult to build states with legitimacy among all the population, since some states have to compete for it with other forms of political identity.[147]
As a complement to this argument, Migdal gives a historical account on how sudden social changes in the Third World during the Industrial Revolution contributed to the formation of weak states. The expansion of international trade that started around 1850, brought profound changes in Africa, Asia and Latin America that were introduced with the objective of assure the availability of raw materials for the European market. These changes consisted in: i) reforms to landownership laws with the objective of integrate more lands to the international economy, ii) increase in the taxation of peasants and little landowners, as well as collecting of these taxes in cash instead of in kind as was usual up to that moment and iii) the introduction of new and less costly modes of transportation, mainly railroads. As a result, the traditional forms of social control became obsolete, deteriorating the existing institutions and opening the way to the creation of new ones, that not necessarily lead these countries to build strong states.[148] This fragmentation of the social order induced a political logic in which these states were captured to some extent by "strongmen", who were capable to take advantage of the above-mentioned changes and that challenge the sovereignty of the state. As a result, these decentralization of social control impedes to consolidate strong states.[149]

The control of fire by early humans was a critical technology enabling the evolution of humans. Fire provided a source of warmth and lighting, protection from predators (especially at night), a way to create more advanced hunting tools, and a method for cooking food. These cultural advances allowed human geographic dispersal, cultural innovations, and changes to diet and behavior. Additionally, creating fire allowed human activity to continue into the dark and colder hours of the evening.
Claims for the earliest definitive evidence of control of fire by a member of Homo range from 1.7 to 2.0 million years ago (Mya).[1] Evidence for the "microscopic traces of wood ash" as controlled use of fire by Homo erectus, beginning roughly 1 million years ago, has wide scholarly support.[2][3] Some of the earliest known traces of controlled fire were found at the Daughters of Jacob Bridge, Israel, and dated to ~790,000 years ago.[4][5] At the site, archaeologists also found the oldest likely evidence of controlled use of fire to cook food ~780,000 years ago.[6][7] However, some studies suggest cooking started ~1.8 million years ago.[8][9][clarification needed]
Flint blades burned in fires roughly 300,000 years ago were found near fossils of early but not entirely modern Homo sapiens in Morocco.[10] Fire was used regularly and systematically by early modern humans to heat treat silcrete stone to increase its flake-ability for the purpose of toolmaking approximately 164,000 years ago at the South African site of Pinnacle Point.[11] Evidence of widespread control of fire by anatomically modern humans dates to approximately 125,000 years ago.[12]
The use and control of fire was a gradual process proceeding through more than one stage. One was a change in habitat, from dense forest, where wildfires were rare but difficult to escape, to savanna (mixed grass/woodland) where wildfires were common but easier to survive. Such a change may have occurred about 3 million years ago, when the savanna expanded in East Africa due to cooler and drier climate.[13][14]
The next stage involved interaction with burned landscapes and foraging in the wake of wildfires, as observed in various wild animals.[13][14] In the African savanna, animals that preferentially forage in recently burned areas include savanna chimpanzees (a variety of Pan troglodytes verus),[13][15] vervet monkeys (Cercopithecus aethiops)[16] and a variety of birds, some of which also hunt insects and small vertebrates in the wake of grass fires.[15][17]
The next step would be to make some use of residual hot spots that occur in the wake of wildfires. For example, foods found in the wake of wildfires tend to be either burned or undercooked. This might have provided incentives to place undercooked foods on a hotspot or to pull food out of the fire if it was in danger of getting burned. This would require familiarity with fire and its behavior.[18][14]
An early step in the control of fire would have been transporting it from burned to unburned areas and lighting them on fire, providing advantages in food acquisition.[14] Maintaining a fire over an extended period of time, as for a season (such as the dry season), may have led to the development of base campsites. Building a hearth or other fire enclosure such as a circle of stones would have been a later development.[19] The ability to make fire, generally with a friction device with hardwood rubbing against softwood (as in a bow drill), was a later development.[13]
Each of these stages could occur at different intensities, ranging from occasional or "opportunistic" to "habitual" to "obligate" (unable to survive without it).[14][19]
Most of the evidence of controlled use of fire during the Lower Paleolithic is uncertain and has limited scholarly support.[3] Some of the evidence is inconclusive because other plausible explanations, such as natural processes, exist for the findings.[1] Findings support that the earliest known controlled use of fire took place in Wonderwerk Cave, South Africa, 1.0 Mya.[3][20]
Findings from Wonderwerk provide the earliest evidence for controlled use of fire. Intact sediments were analyzed using micromorphological analysis. Fourier transform infrared microspectroscopy (mFTIR) yielded evidence, in the form of burned bones and ashed plant remains, that burning took place at the site 1.0 Mya.[3]
East African sites, such as Chesowanja near Lake Baringo, Koobi Fora, and Olorgesailie in Kenya, show possible evidence that fire was controlled by early humans.[1] In Chesowanja, archaeologists found red clay clasts dated to 1.4 Mya. These clasts must have been heated to 400 °C (750 °F) to harden. However, tree stumps burned in bush fires in East Africa produce clasts, which, when broken by erosion, are like those described at Chesownja. Controlled use of fire at Chesowanja is unproven.[1]
In Koobi Fora, sites show evidence of control of fire by Homo erectus at 1.5 Mya with findings of reddened sediment that could come from heating at 200–400 °C (400–750 °F).[1] Evidence of possible human control of fire, found at Swartkrans, South Africa,[21] includes burned bones, including ones with hominin-inflicted cut marks, along with Acheulean and bone tools.[1] This site shows some of the earliest evidence of carnivorous behavior in H. erectus. A "hearth-like depression" that could have been used to burn bones was found in Olorgesailie, Kenya. However, it did not contain any charcoal, and no signs of fire have been observed. Some microscopic charcoal was found, but it could have resulted from a natural brush fire.[1]
In Gadeb, Ethiopia, fragments of welded tuff that appeared to have been burned were found in Locality 8E but refiring of the rocks might have occurred due to local volcanic activity.[1]
In the Middle Awash River Valley, cone-shaped depressions of reddish clay were found that could have been formed by temperatures of 200 °C (400 °F). These features, thought to have been created by burning tree stumps, were hypothesized to have been produced by early hominids lighting tree stumps so they could have fire away from their habitation site. This view is not widely accepted, though.[1] Burned stones were found in Awash Valley, but volcanic welded tuff is found in the area, which could explain the burned stones.[1]
Burned flints discovered near Jebel Irhoud, Morocco, dated by thermoluminescence to around 300,000 years old, were discovered in the same sedimentary layer as skulls of early Homo sapiens. Paleoanthropologist Jean-Jacques Hublin believes the flints were used as spear tips and left in fires used by the early humans for cooking food.[10]
In Xihoudu in Shanxi Province, China, the black, blue, and grayish-green discoloration of mammalian bones found at the site illustrates evidence of burning by early hominids. In 1985, at a parallel site in China, Yuanmou in Yunnan Province, archaeologists found blackened mammal bones that date back to 1.7 Mya.[1]
A site at Bnot Ya'akov Bridge, Israel, has been claimed to show that H. erectus or H. ergaster controlled fires between 790,000 and 690,000 BP.[22] An AI-powered spectroscopy helped researchers unearth evidence of the use of fire dating 800,000 and 1 million years ago.[23] In an article published in June 2022,[24] researchers from Weizmann Institute of Science, along with researchers at the University of Toronto and Hebrew University of Jerusalem described the use of deep learning models to analyze heat exposure of 26 flint tools that were found in 1970s at the Evron Quarry in the northwest of Israel. The results showed the tools were heated up to 600°C.[23]
At Trinil, Java, burned wood has been found in layers that carried H. erectus (Java Man) fossils dating from 830,000 to 500,000 BP.[1] The burned wood has been claimed to indicate the use of fire by early hominids.
The Cave of Hearths in South Africa has burn deposits, which date from 700,000 to 200,000 BP, as do various other sites such as Montagu Cave (200,000 to 58,000 BP) and the Klasies River Mouth (130,000 to 120,000 BP).[1]
Strong evidence comes from Kalambo Falls in Zambia, where several artifacts related to the use of fire by humans have been recovered, including charred logs, charcoal, carbonized grass stems and plants, and wooden implements, which may have been hardened by fire. The site has been dated through radiocarbon dating to 180,000 BP, through amino-acid racemization.[1]
Fire was used for heat treatment of silcrete stones to increase their workability before they were knapped into tools by Stillbay culture in South Africa.[25][26][27] These Stillbay sites date back from 164,000 to 72,000 years ago, with the heat treatment of stone beginning by about 164,000 years ago.[25]
Evidence at Zhoukoudian cave in China suggests control of fire as early as 460,000 to 230,000 BP.[12] Fire in Zhoukoudian is suggested by the presence of burned bones, burned chipped-stone artifacts, charcoal, ash, and hearths alongside H. erectus fossils in Layer 10, the earliest archaeological horizon at the site.[1][28] This evidence comes from Locality 1, also known as the Peking Man site, where several bones were found to be uniformly black to grey. The bone extracts were determined to be characteristic of burned bone rather than manganese staining. These residues also showed IR spectra for oxides, and a turquoise bone was reproduced in the laboratory by heating some of the other bones found in Layer 10. The same effect might have been at the site due to natural heating, as the effect was produced on white, yellow, and black bones.[28]
Layer 10 is ash with biologically produced silicon, aluminum, iron, and potassium, but wood ash remnants such as siliceous aggregates are missing. Among these are possible hearths "represented by finely laminated silt and clay interbedded with reddish-brown and yellow-brown fragments of organic matter, locally mixed with limestone fragments and dark brown finely laminated silt, clay, and organic matter."[28] The site itself does not show that fires were made in Zhoukoudian, but the association of blackened bones with quartzite artifacts at least shows that humans did control fire at the time of the habitation of the Zhoukoudian cave.[citation needed]
At the Amudian site of Qesem Cave, near the city of Kfar Qasim, Israel, evidence exists of the regular use of fire from before 382,000 BP to around 200,000 BP, at the end of Lower Pleistocene. Large quantities of burned bone and moderately heated soil lumps were found, and the cut marks found on the bones suggest that butchering and prey-defleshing took place near fireplaces.[29] In addition, hominins living in Qesem cave managed to heat their flint to varying temperatures before knapping it into different tools.[30]
The earliest evidence for controlled fire use by humans on the Indian subcontinent, dating to between 50,000 and 55,000 years ago, comes from the Main Belan archaeological site, located in the Belan River valley in Uttar Pradesh, India.[31]
Multiple sites in Europe, such as Torralba and Ambrona, Spain, and St. Esteve-Janson, France, have also shown evidence of the use of fire by later versions of H. erectus. The oldest has been found in England at the site of Beeches Pit, Suffolk; uranium series dating and thermoluminescence dating place the use of fire at 415,000 BP.[32] At Vértesszőlős, Hungary, while no charcoal has been found, burned bones have been discovered dating from c. 350,000 years ago. At Torralba and Ambrona, Spain, objects such as Acheulean stone tools, remains of large mammals such as extinct elephants, charcoal, and wood were discovered.[1] At Terra Amata in France, there is a fireplace with ashes (dated between 380,000 BP and 230,000 BP). At Saint-Estève-Janson in France, there is evidence of five hearths and reddened earth in the Escale Cave; these hearths have been dated to 200,000 BP.[1] Evidence for fire making dates to at least the Middle Paleolithic, with dozens of Neanderthal hand axes from France exhibiting use-wear traces suggesting these tools were struck with the mineral pyrite to produce sparks around 50,000 years ago.[33]
The discovery of fire provided various uses for early hominids. Its warmth kept them alive during low nighttime temperatures in colder environments, allowing geographic expansion from tropical and subtropical climates to temperate areas. Its blaze warded off predatory animals, especially in the dark.[34]
Fire also played a major role in changing food habits. Cooking allowed a significant increase in meat consumption and calorie intake.[34] It was soon discovered that meat could be dried and smoked by fire, preserving it for lean seasons.[35] Fire was even used in manufacturing tools for hunting and butchering.[36] Hominids also learned that starting bushfires to burn large areas could increase land fertility and clear terrain to make hunting easier.[35][37] Evidence shows that early hominids were able to corral and trap prey animals using fire.[citation needed] Fire was used to clear out caves before living in them, helping to begin the use of shelter.[38] The many uses of fire may have led to specialized social roles, such as the separation of cooking from hunting.[39]
The control of fire enabled important changes in human behavior, health, energy expenditure, and geographic expansion. After the loss of body hair, hominids could move into much colder regions that would have previously been uninhabitable. Evidence of more complex management to change biomes can be found as far back as 200,000 to 100,000 years ago, at minimum.
Fire allowed major innovations in tool and weapon manufacture. Evidence dating to roughly 164,000 years ago indicates that early humans in South Africa during the Middle Stone Age used fire to alter the mechanical properties of tool materials applying heat treatment to a fine-grained rock called silcrete.[40] The heated rocks were then tempered into crescent-shaped blades or arrowheads for hunting and butchering prey. This may have been the first time that bow and arrow were used for hunting, with far-ranging impact.[40][41]
Fire was used in the creation of art. Archaeologists have discovered several 1- to 10-inch Venus figurine statues in Europe dating to the Paleolithic, several carved from stone and ivory, others shaped from clay and then fired. These are some of the earliest examples of ceramics.[42] Fire was also commonly used to create pottery. Although pottery was formerly thought to have begun with the Neolithic around 10,000 years ago, scientists in China discovered pottery fragments in the Xianrendong Cave that were about 20,000 years old.[43] During the Neolithic Age and agricultural revolution about 10,000 years ago, pottery became far more common and widespread, often carved and painted with simple linear designs and geometric shapes.[44]
Fire was an important factor in expanding and developing societies of early hominids. One impact fire might have had was social stratification. The power to make and wield fire may have conferred prestige and social position.[35] Fire also led to a lengthening of daytime activities and allowed more nighttime activities.[45] Evidence of large hearths indicate that the majority of nighttime was spent around the fire.[46] The increased social interaction from gathering around the fire may have fostered the development of language.[45]
Another effect of fire use on hominid societies was that it required larger groups to work together to maintain the fire, finding fuel, portioning it onto the fire, and re-igniting it when necessary. These larger groups might have included older individuals, such as grandparents, who helped to care for children. Ultimately, fire significantly influenced the size and social interactions of early hominid communities.[45][46]
Exposure to artificial light during later hours of the day changed humans' circadian rhythms, contributing to a longer waking day.[47] The modern human's waking day is 16 hours, while many mammals are only awake for half as many hours.[46] Additionally, humans are most awake during the early evening hours, while other primates' days begin at dawn and end at sundown. Many of these behavioral changes can be attributed to the control of fire and its impact on daylight extension.[46]
The cooking hypothesis proposes that the ability to cook allowed for the brain size of hominids to increase over time. This idea was first presented by Friedrich Engels in the article "The Part Played by Labour in the Transition from Ape to Man" and later recapitulated in the book Catching Fire: How Cooking Made Us Human by Richard Wrangham and then in a book by Suzana Herculano-Houzel.[48] Critics of the hypothesis argue that cooking with controlled fire was insufficient to start the increasing brain size trend.
The cooking hypothesis gains support by comparing the nutrients in raw food to the much more easily digested nutrients in cooked food, as in an examination of protein ingestion from raw vs. cooked egg.[49] Scientists have found that among several primates, the restriction of feeding to raw foods during daylight hours limits the metabolic energy available.[50] Genus Homo was able to break through the limit by cooking food to shorten their feeding times and be able to absorb more nutrients to accommodate the increasing need for energy.[51] In addition, scientists argue that the Homo species was also able to obtain nutrients like docosahexaenoic acid from algae that were especially beneficial and critical for brain evolution. The detoxification of food by the cooking process enabled early humans to access these resources.[52]
Besides the brain, other human organs also demand a high metabolism.[51] During human evolution, the body-mass proportion of different organs changed to allow brain expansion.
Before the advent of fire, the hominid diet was limited to mostly plant parts composed of simple sugars and carbohydrates such as seeds, flowers, and fleshy fruits. Parts of the plant, such as stems, mature leaves, enlarged roots, and tubers, would have been inaccessible as a food source due to the indigestibility of raw cellulose and starch. Cooking, however, made starchy and fibrous foods edible and greatly increased the diversity of other foods available to early humans. Toxin-containing foods, including seeds and similar carbohydrate sources, such as cyanogenic glycosides found in linseed and cassava, were incorporated into their diets as cooking rendered them nontoxic.[53]
Cooking could also kill parasites, reduce the amount of energy required for chewing and digestion, and release more nutrients from plants and meat. Due to the difficulty of chewing raw meat and digesting tough proteins (e.g. collagen) and carbohydrates, the development of cooking served as an effective mechanism to process meat efficiently and allow for its consumption in larger quantities. With its high caloric density and content of important nutrients, meat thus became a staple in the diet of early humans.[54] By increasing digestibility, cooking allowed hominids to maximize the energy gained from consuming foods. Studies show that caloric intake from cooking starches improves 12-35% and 45-78% for protein. As a result of the increases in net energy gain from food consumption, survival and reproductive rates in hominids increased.[55] Through lowering food toxicity and increasing nutritive yield, cooking allowed for an earlier weaning age, permitting females to have more children.[56] In this way, too, it facilitated population growth.
It has been proposed that the use of fire for cooking caused environmental toxins to accumulate in the placenta, which led to a species-wide taboo on human placentophagy around the time of the mastery of fire. Placentophagy is common in other primates.[57]
Before their use of fire, the hominid species had large premolars, which were used to chew harder foods, such as large seeds. In addition, due to the shape of the molar cusps, the diet is inferred to have been more leaf- or fruit-based. Probably in response to consuming cooked foods, the molar teeth of H. erectus gradually shrank, suggesting that their diet had changed from more challenging foods such as crisp root vegetables to softer cooked foods such as meat.[58][59] Cooked foods further selected for the differentiation of their teeth and eventually led to a decreased jaw volume with a variety of smaller teeth in hominids. Today, a smaller jaw volume and teeth size of humans is seen in comparison to other primates.[60]
Due to the increased digestibility of many cooked foods, less digestion was needed to procure the necessary nutrients. As a result, the gastrointestinal tract and organs in the digestive system decreased in size. This is in contrast to other primates, where a larger digestive tract is needed for the fermentation of long carbohydrate chains. Thus, humans evolved from the large colons and tracts that are seen in other primates to smaller ones.[61]
According to Wrangham, fire control allowed hominids to sleep on the ground and in caves instead of trees and led to more time spent on the ground. This may have contributed to the evolution of bipedalism, as such an ability became increasingly necessary for human activity.[62]
Critics of the hypothesis argue that while a linear increase in brain volume of the genus Homo is seen over time, adding fire control and cooking does not add anything meaningful to the data. Species such as H. ergaster existed with large brain volumes during periods with little to no evidence of fire for cooking. Little variation exists in the brain sizes of H. erectus dated from periods of weak and strong evidence for cooking.[46] An experiment involving mice fed raw versus cooked meat found that cooking meat did not increase the amount of calories taken up by mice, leading to the study's conclusion that the energetic gain is the same, if not greater, in raw meat diets than cooked meats.[63] Studies such as this and others have led to criticisms of the hypothesis that state that the increases in human brain size occurred well before the advent of cooking due to a shift away from the consumption of nuts and berries to the consumption of meat.[64][65] Other anthropologists argue that the evidence suggests that cooking fires began in earnest only 250,000 BP, when ancient hearths, earth ovens, burned animal bones, and flint appear across Europe and the Middle East.[66]

The International Union of Prehistoric and Protohistoric Sciences (Union internationale des sciences préhistoriques et protohistoriques – UISPP) is a learned society, linked through the International Council for Philosophy and Human Sciences to UNESCO, and concerned with the study of prehistory and protohistory. In the words of its constitution:
The UISPP, as an international association of scholars, is founded on the principle of the universality of science. It firmly upholds academic freedom, recognizing that the study of humanity is relevant to all contemporary societies. In this spirit, the UISPP staunchly opposes any form of discrimination, whether based on race, creed, philosophical or ideological beliefs, ethnic or geographical background, nationality, gender, language, or any other criteria. Such discrimination, rooted in intolerance, inherently contradicts the scientific approach.
Furthermore, the UISPP rejects attempts to fictionally rewrite history or engage in negationism. As a non-governmental organization, it welcomes all bona fide scholars to participate in its scientific activities, regardless of their background or affiliations. This inclusive approach reflects the UISPP's commitment to fostering a diverse and open academic community dedicated to the advancement of knowledge about human prehistory and protohistory.[1]
The origins of the UISPP lie in an 1865 meeting of the Società italiana di scienze naturali (Italian Society of Natural Science) that led to the creation of the Congrès paléoethnologique international (CPI – International Palaeoethnological Congress). The first meeting of the CPI was held in Neuchâtel in 1866. The following year, in Paris, the name was changed to Congrès international d'anthropologie et d'archéologie préhistoriques (CIAAP – International Congress of Anthropology and Prehistoric Archaeology).[2][3]
A permanent council of the CIAAP was founded in 1880, and regular congresses continued to be held until the outbreak of World War I disrupted the regular scholarly meetings, causing a hiatus in international academic collaboration and exchange. In the aftermath of the war, efforts to revive the tradition of regular congresses were spearheaded by the Institut international d'anthropologie (IIA – International Institute of Anthropology). However, this organization's structure and focus proved inadequate for the evolving needs of prehistoric and protohistoric research. The Institut's limited international perspective and the secondary role it assigned to the fields of prehistory and protohistory ultimately led to calls for the establishment of a more specialized body that would more closely continue the tradition of the CIAAP.[4] In response to these calls, the Congrès international des Sciences préhistoriques et protohistoriques (CISPP – International Congress of Prehistoric and Protohistoric Sciences) was formed in 1931, providing a dedicated platform for scholars active in prehistoric and protohistoric studies.[5] This new organization aimed to foster international collaboration and advance research in these specific fields, addressing the gaps left by its predecessor.[6] The creation of CISPP marked a significant step towards a more focused and globally-oriented approach to prehistoric and protohistoric sciences in the interwar period. Congresses were held in London in 1932 and in Oslo in 1936.[4]
However, the rise of fascism and the outbreak of World War II significantly impacted the activities of the CISPP, and the third congress originally planned for 1940 in Budapest had to be cancelled. No major congresses were held between 1936 and 1950, when a concerted effort by scholars from across Europe finally reestablished the tradition of regular international congresses for the prehistoric and protohistoric sciences, with the first post-war congress held in Zurich in 1950.[7] In 1955, the permanent council decided to affiliate the CISPP with a member organisation of UNESCO, the International Council for Philosophy and Human Sciences. This required a change of name, and in 1956 the CISPP became the Union internationale des sciences préhistoriques et protohistoriques (UISPP – International Union for Prehistoric and Protohistoric Sciences). UISPP congresses subsequently emerged as a vital forum for scientific exchange, bridging the divide between scholars from both sides of the Iron Curtain, despite the tensions of the Cold War. This role as a conduit for east–west dialogue was made possible by adhering to the principle of intellectual neutrality, which explicitly does not regard participants as representatives of any state or government.[8] This approach proved key to fostering an environment of open academic discourse, transcending political boundaries and ideological differences. In 2019, UISPP joined the International Union of Academies (UAI – Union académique internationale).[4]
The structure of the UISPP has changed several times since its foundation, most recently in 2011, when the permanent council was abolished and individual and institutional memberships were introduced, making the organisation more democratic and transforming it into a fully representative body. Membership in the UISPP is open to all bona fide scholars. However, participation in its scientific commissions typically requires researchers to hold a PhD or be pursuing doctoral studies.[1]
The general assembly comprises all registered UISPP members and convenes during each world congress. It approves motions proposed by the executive eommittee and elects the president, general secretary, and treasurer. Additionally, the general assembly decides on the establishment and dissolution of scientific commissions and makes other significant decisions within the UISPP framework, following consultation with the executive committee.[1]
The executive committee comprises the members of the board (president, general secretary, treasurer, vice-president(s)) and the presidents of all UISPP scientific commissions. Working in tandem with the board, its role is to uphold the principles of the UISPP and to oversee the activities of scientific commissions and the broader organization. The executive committee also monitors preparations for the triennial world congress, intervening in its organization only when faced with unexpected events or significant delays in the program proposed by the national organizing committee.[1]
The primary objective of the UISPP's scientific commissions is to promote and coordinate international research in specific or specialized domains of prehistoric and protohistoric sciences between each world congress. The commissions are organized into six broader categories, although this list is not exhaustive.
The principal activities of a scientific commission are:
The frequency of UISPP congresses has evolved over time. Initially held every four years, they shifted to a five-year cycle in 1966 following UISPP's affiliation with CIPSH, aligning with UNESCO guidelines. Since 2011, world congresses have been organized triennially, aiming to alternate between Europe and other continents.
The XIth UISPP World Congress was originally to be held in Southampton in 1986. However, the decision of the British organising committee, led by Peter Ucko, to exclude South African and Namibian archaeologists for political reasons, despite the declared opposition of many of them to apartheid and despite that exclusion constituting a violation of UISPP statutes, led the UISPP to withdraw its endorsement of the congress and postpone the XIth World Congress until 1987. The British organising committee went ahead with the congress planned for Southampton under the new name of the World Archaeological Congress (WAC).[9][10] Whereas the majority of UISPP members rejected the split as a division of scholars and an opportunistic move, writing in 1987, Peter Ucko still described the UISPP as:
Despite these initial divides, since 2005 relations between UISPP and WAC have resumed. The UISPP remains focused on prehistoric and protohistoric research, while WAC evolved to focus primarily on politically committed approaches and advocacy. The two organisations therefore have different scopes.[12]
The XVIIIth World Congress of UISPP was held in Paris in June 2018. Its overarching theme was "Adaptation and sustainability of prehistoric and protohistoric societies confronted with climate change"1. This major congress formed part of the ongoing refoundation of Humanities led by the International Council for Philosophy and Human Sciences (which includes UISPP) and UNESCO. While all UISPP congresses have a general theme, they remain open to sessions on any other topic, which can be proposed during the general call for sessions.
The XIXth UISPP World Congress, originally planned for 2020 in Meknes, was postponed to 2021 due to the COVID-19 pandemic and was ultimately held as a fully virtual event.[4]
UISPP has also adopted a new format alongside its triennial world congresses: continental congresses focusing on the prehistory and protohistory of specific regions. The first congress in this new format will be a multi-venue event in Java, Indonesia in 2025, with the main theme "Asian Prehistory Today: Bridging Science, Heritage and Development"
The UISPP awards several prizes to recognize excellence in archaeology and related sciences. These distinctions aim to acknowledge significant contributions to archaeological research in the fields of prehistory and protohistory, knowledge dissemination, and the promotion of prehistoric and protohistoric heritage. The awards are presented at each UISPP World Congress:
In addition to the publication of the proceedings from its congresses, UISPP since the 1950s has been overseeing the publication af a number of international monograph series aimed chiefly at the systematic publication of primary archaeological source material (Inventaria Archaeologica, Prähistorische Bronzefunde, Fiches typlogiques de l'industrie osseuse préhistorique) but also at specific themes of global interest (Human Societies facing Climate Change). Since 2018, UISPP has been publishing the peer-reviewed and open access Journal of the International Union of Prehistoric and Protohistoric Sciences / Revue de l'Union Internationale des Sciences Préhistoriques et Protohistoriques (short: UISPP Journal).[13]

The Neolithic Revolution, also known as the First Agricultural Revolution, was the wide-scale transition of many human cultures during the Neolithic period in Afro-Eurasia from a lifestyle of hunting and gathering to one of agriculture and settlement, making an increasingly large population possible.[1] These settled communities permitted humans to observe and experiment with plants, learning how they grew and developed.[2] This new knowledge led to the domestication of plants into crops.[2][3]
Archaeological data indicate that the domestication of various types of plants and animals happened in separate locations worldwide, starting in the geological epoch of the Holocene 11,700 years ago, after the end of the last Ice Age.[4] It was humankind's first historically verifiable transition to agriculture. The Neolithic Revolution greatly narrowed the diversity of foods available, resulting in a decrease in the quality of human nutrition compared with that obtained previously from foraging,[5][6][7] but because food production became more efficient, it released humans to invest their efforts in other activities and was thus "ultimately necessary to the rise of modern civilization by creating the foundation for the later process of industrialization and sustained economic growth".[8]
The Neolithic Revolution involved much more than the adoption of a limited set of food-producing techniques. During the next millennia, it transformed the small and mobile groups of hunter-gatherers that had hitherto dominated human prehistory into sedentary (non-nomadic) societies based in built-up villages and towns. These societies radically modified their natural environment by means of specialized food-crop cultivation, with activities such as irrigation and deforestation which allowed the production of surplus food. Other developments that are found very widely during this era are the domestication of animals, pottery, polished stone tools, and rectangular houses. In many regions, the adoption of agriculture by prehistoric societies caused episodes of rapid population growth, a phenomenon known as the Neolithic demographic transition.
These developments, sometimes called the Neolithic package,[9] provided the basis for centralized administrations and political structures, hierarchical ideologies,[10] depersonalized systems of knowledge (e.g. writing), densely populated settlements, specialization and division of labour, more trade, the development of non-portable art and architecture, and greater property ownership.[11] The earliest known civilization developed in Sumer in southern Mesopotamia (c. 6,500 BP); its emergence also heralded the beginning of the Bronze Age.[12]
The relationship of the aforementioned Neolithic characteristics to the onset of agriculture, their sequence of emergence, and their empirical relation to each other at various Neolithic sites remains the subject of academic debate. It is usually understood to vary from place to place, rather than being the outcome of universal laws of social evolution.[13][14]
Prehistoric hunter-gatherers had different subsistence requirements and lifestyles from agriculturalists. Hunter-gatherers were often highly mobile and migratory, living in temporary shelters and in small tribal groups, and having limited contact with outsiders. Their diet was well-balanced[citation needed] though heavily dependent on what the environment could provide each season. In contrast, because the surplus and plannable supply of food provided by agriculture made it possible to support larger population groups, agriculturalists lived in more permanent dwellings in more densely populated settlements than what could be supported by a hunter-gatherer lifestyle. The agricultural communities' seasonal need to plan and coordinate resource and manpower encouraged division of labour, which gradually led to specialization of labourers and complex societies. The subsequent development of trading networks to exchange surplus commodities and services brought agriculturalists into contact with outside groups, which promoted cultural exchanges that led to the rise of civilizations and technological evolutions.[15][full citation needed]
However, higher population and food abundance did not necessarily correlate with improved health. Reliance on a very limited variety of staple crops can adversely affect health even while making it possible to feed more people. Maize is deficient in certain essential amino acids (lysine and tryptophan) and is a poor source of iron. The phytic acid it contains may inhibit nutrient absorption. Other factors that likely affected the health of early agriculturalists and their domesticated livestock would have been increased numbers of parasites and disease-bearing pests associated with human waste and contaminated food and water supplies. Fertilizers and irrigation may have increased crop yields but also would have promoted proliferation of insects and bacteria in the local environment while grain storage attracted additional insects and rodents.[15]
The term 'neolithic revolution' was invented by V. Gordon Childe in his book Man Makes Himself (1936).[18][19] Childe introduced it as the first in a series of agricultural revolutions in Middle Eastern history,[20] calling it a "revolution" to denote its significance, the degree of change to communities adopting and refining agricultural practices.[21]
The beginning of this process in different regions has been dated from 10,000 to 8,000 BCE in the Fertile Crescent,[22][23] and perhaps 8000 BCE in the Kuk Early Agricultural Site of Papua New Guinea in Melanesia.[24][25] Everywhere, this transition is associated with a change from a largely nomadic hunter-gatherer way of life to a more settled, agrarian one, with the domestication of various plant and animal species – depending on the species locally available, and influenced by local culture. Archaeological research in 2003 suggests that in some regions, such as the Southeast Asian peninsula, the transition from hunter-gatherer to agriculturalist was not linear, but region-specific.[26]
Once agriculture started gaining momentum, around 9000 BP, human activity resulted in the selective breeding of cereal grasses (beginning with emmer, einkorn and barley), and not simply of those that favoured greater caloric returns through larger seeds. Plants with traits such as small seeds or bitter taste were seen as undesirable. Plants that rapidly shed their seeds on maturity tended not to be gathered at harvest, therefore not stored and not seeded the following season; successive years of harvesting spontaneously selected for strains that retained their edible seeds longer.
Daniel Zohary identified several plant species as "pioneer crops" or Neolithic founder crops. He highlighted the importance of wheat, barley and rye, and suggested that domestication of flax, peas, chickpeas, bitter vetch and lentils came a little later. Based on analysis of the genes of domesticated plants, he preferred theories of a single, or at most a very small number of domestication events for each taxon that spread in an arc from the Levantine corridor around the Fertile Crescent and later into Europe.[27][28] Gordon Hillman and Stuart Davies carried out experiments with varieties of wild wheat to show that the process of domestication would have occurred over a relatively short period of between 20 and 200 years.[29]
Some of the pioneering attempts failed at first and crops were abandoned, sometimes to be taken up again and successfully domesticated thousands of years later: rye, tried and abandoned in Neolithic Anatolia, made its way to Europe as weed seeds and was successfully domesticated in Europe, thousands of years after the earliest agriculture.[30] Wild lentils presented a different problem: most of the wild seeds do not germinate in the first year; the first evidence of lentil domestication, breaking dormancy in their first year, appears in the early Neolithic at Jerf el Ahmar (in modern Syria), and lentils quickly spread south to the Netiv HaGdud site in the Jordan Valley.[30] The process of domestication allowed the founder crops to adapt and eventually become larger, more easily harvested, more dependable[clarification needed] in storage and more useful to the human population.
Selectively propagated figs, wild barley and wild oats were cultivated at the early Neolithic site of Gilgal I, where in 2006[31] archaeologists found caches of seeds of each in quantities too large to be accounted for even by intensive gathering, at strata datable to c. 11,000 years ago. Some of the plants tried and then abandoned during the Neolithic period in the Ancient Near East, at sites like Gilgal, were later successfully domesticated in other parts of the world.
Once early farmers perfected their agricultural techniques like irrigation (traced as far back as the 6th millennium BCE in Khuzistan[32][33]), their crops yielded surpluses that needed storage. Most hunter-gatherers could not easily store food for long due to their migratory lifestyle, whereas those with a sedentary dwelling could store their surplus grain. Eventually granaries were developed that allowed villages to store their seeds longer. So with more food, the population expanded and communities developed specialized workers and more advanced tools.
The process was not as linear as was once thought, but a more complicated effort, which was undertaken by different human populations in different regions in many different ways.
One of the world's most important crops, barley, was domesticated in the Near East around 11,000 years ago (c. 9,000 BCE).[34] Barley is a highly resilient crop, able to grow in varied and marginal environments, such as in regions of high altitude and latitude.[34] Archaeobotanical evidence shows that barley had spread throughout Eurasia by 2,000 BCE.[34] To further elucidate the routes by which barley cultivation was spread through Eurasia, genetic analysis was used to determine genetic diversity and population structure in extant barley taxa.[34] Genetic analysis shows that cultivated barley spread through Eurasia via several different routes, which were most likely separated in both time and space.[34]
When hunter-gathering began to be replaced by sedentary food production it became more efficient to keep animals close at hand. Therefore, it became necessary to bring animals permanently to their settlements, although in many cases there was a distinction between relatively sedentary farmers and nomadic herders.[35][original research?] The animals' size, temperament, diet, mating patterns, and life span were factors in the desire and success in domesticating animals. Animals that provided milk, such as cows and goats, offered a source of protein that was renewable and therefore quite valuable. The animal's ability as a worker (for example ploughing or towing), as well as a food source, also had to be taken into account. Besides being a direct source of food, certain animals could provide leather, wool, hides, and fertilizer. Some of the earliest domesticated animals included dogs (East Asia, about 15,000 years ago),[36] sheep, goats, cows, and pigs.
West Asia was the source for many animals that could be domesticated, such as sheep, goats and pigs. This area was also the first region to domesticate the dromedary. Henri Fleisch discovered and termed the Shepherd Neolithic flint industry from the Bekaa Valley in Lebanon and suggested that it could have been used by the earliest nomadic shepherds. He dated this industry to the Epipaleolithic or Pre-Pottery Neolithic as it is evidently not Paleolithic, Mesolithic or even Pottery Neolithic.[37][38]
The presence of these animals gave the region a large advantage in cultural and economic development. As the climate in the Middle East changed and became drier, many of the farmers were forced to leave, taking their domesticated animals with them. It was this massive emigration from the Middle East that later helped distribute these animals to the rest of Afroeurasia. This emigration was mainly on an east–west axis of similar climates, as crops usually have a narrow optimal climatic range outside of which they cannot grow for reasons of light or rain changes. For instance, wheat does not normally grow in tropical climates, just like tropical crops such as bananas do not grow in colder climates. Some authors, like Jared Diamond, have postulated that this east–west axis is the main reason why plant and animal domestication spread so quickly from the Fertile Crescent to the rest of Eurasia and North Africa, while it did not reach through the north–south axis of Africa to reach the Mediterranean climates of South Africa, where temperate crops were successfully imported by ships in the last 500 years.[39] Similarly, the African Zebu of central Africa and the domesticated bovines of the fertile-crescent – separated by the dry sahara desert – were not introduced into each other's region.
Use-wear analysis of five glossed flint blades found at Ohalo II, a 23,000-years-old fisher-hunter-gatherers' camp on the shore of the Sea of Galilee, Northern Israel, provides the earliest evidence for the use of composite cereal harvesting tools.[40] The Ohalo site is at the junction of the Upper Paleolithic and the Early Epipaleolithic, and has been attributed to both periods.[41]
The wear traces indicate that tools were used for harvesting near-ripe semi-green wild cereals, shortly before grains are ripe and disperse naturally.[40] The studied tools were not used intensively, and they reflect two harvesting modes: flint knives held by hand and inserts hafted in a handle.[40] The finds shed new light on cereal harvesting techniques some 8,000 years before the Natufian and 12,000 years before the establishment of sedentary farming communities in the Near East.[40] Furthermore, the new finds accord well with evidence for the earliest ever cereal cultivation at the site and the use of stone-made grinding implements.[40]
Agriculture appeared first in West Asia about 2,000 years later,[clarification needed] around 10,000–9,000 years ago. The region was the centre of domestication for three cereals (einkorn wheat, emmer wheat and barley), four legumes (lentil, pea, bitter vetch and chickpea), and flax. Domestication was a slow process that unfolded across multiple regions, and was preceded by centuries if not millennia of pre-domestication cultivation.[42]
Finds of large quantities of seeds and a grinding stone at the Epipalaeolithic site of Ohalo II, dating to around 19,400 BP, has shown some of the earliest evidence for advanced planning of plants for food consumption and suggests that humans at Ohalo II processed the grain before consumption.[43][44] Tell Aswad is the oldest site of agriculture, with domesticated emmer wheat dated to 10,800 BP.[45][46] Soon after came hulled, two-row barley – found domesticated earliest at Jericho in the Jordan valley and at Iraq ed-Dubb in Jordan.[47]
Other sites in the Levantine corridor that show early evidence of agriculture include Wadi Faynan 16 and Netiv Hagdud.[22] Jacques Cauvin noted that the settlers of Aswad did not domesticate on site, but "arrived, perhaps from the neighbouring Anti-Lebanon, already equipped with the seed for planting".[48] In the Eastern Fertile Crescent, evidence of cultivation of wild plants has been found in Choga Gholan in Iran dated to 12,000 BP, with domesticated emmer wheat appearing in 9,800 BP, suggesting there may have been multiple regions in the Fertile Crescent where cereal domestication evolved roughly contemporaneously.[49] The Heavy Neolithic Qaraoun culture has been identified at around fifty sites in Lebanon around the source springs of the River Jordan, but never reliably dated.[50][37]
In his 1997 book Guns, Germs, and Steel, Jared Diamond argues that the vast continuous east–west stretch of temperate climatic zones of Eurasia and North Africa gave peoples living there a highly advantageous geographical location that afforded them a head start in the Neolithic Revolution. Both shared the temperate climate ideal for the first agricultural settings, and both were near a number of easily domesticable plant and animal species.  In areas where continents aligned north–south such as the Americas and Africa, crops—and later domesticated animals—could not spread across tropical zones.[51]
Agriculture in Neolithic China can be separated into two broad regions, Northern China and Southern China.[52][53]
The agricultural centre in northern China is believed to be the homelands of the early Sino-Tibetan-speakers, associated with the Houli, Peiligang, Cishan, and Xinglongwa cultures, clustered around the Yellow River basin.[52][53] It was the domestication centre for foxtail millet (Setaria italica) and broomcorn millet (Panicum miliaceum), with early evidence of domestication approximately 8,000 years ago,[54] and widespread cultivation 7,500 years ago.[54] (Soybean was also domesticated in northern China 4,500 years ago.[55] Orange and peach also originated in China, being cultivated c. 2500 BCE.[56][57])
The agricultural centres in southern China are clustered around the Yangtze River basin. Rice was domesticated in this region, together with the development of paddy field cultivation, between 13,500 and 8,200 years ago.[52][58][59]
There are two possible centres of domestication for rice. The first is in the lower Yangtze River, believed to be the homelands of pre-Austronesians and associated with the Kauhuqiao, Hemudu, Majiabang, and Songze cultures. It is characterized by typical pre-Austronesian features, including stilt houses, jade carving, and boat technologies. Their diet were also supplemented by acorns, water chestnuts, foxnuts, and pig domestication. The second is in the middle Yangtze River, believed to be the homelands of the early Hmong-Mien-speakers and associated with the Pengtoushan and Daxi cultures. Both of these regions were heavily populated and had regular trade contacts with each other, as well as with early Austroasiatic speakers to the west, and early Kra-Dai speakers to the south, facilitating the spread of rice cultivation throughout southern China.[59][52][53]
The millet and rice-farming cultures also first came into contact with each other at around 9,000 to 7,000 BP, resulting in a corridor between the millet and rice cultivation centres where both rice and millet were cultivated.[52] At around 5,500 to 4,000 BP, there was increasing migration into Taiwan from the early Austronesian Dapenkeng culture, bringing rice and millet cultivation technology with them. During this period, there is evidence of large settlements and intensive rice cultivation in Taiwan and the Penghu Islands, which may have resulted in overexploitation. Bellwood (2011) proposes that this may have been the impetus of the Austronesian expansion which started with the migration of the Austronesian-speakers from Taiwan to the Philippines at around 5,000 BP.[53]
Austronesians carried rice cultivation technology to Island Southeast Asia along with other domesticated species. The new tropical island environments also had new food plants that they exploited. They carried useful plants and animals during each colonization voyage, resulting in the rapid introduction of domesticated and semi-domesticated species throughout Oceania. They also came into contact with the early agricultural centres of Papuan-speaking populations of New Guinea as well as the Dravidian-speaking regions of South India and Sri Lanka by around 3,500 BP. They acquired further cultivated food plants like bananas and pepper from them, and in turn introduced Austronesian technologies like wetland cultivation and outrigger canoes.[53][60][61][62] During the 1st millennium CE, they also colonized Madagascar and the Comoros, bringing Southeast Asian food plants, including rice, to East Africa.[63][64]
On the African continent, three areas have been identified as independently developing agriculture: the Ethiopian highlands, the Sahel and West Africa.[39] By contrast, Agriculture in the Nile River Valley is thought to have developed from the original Neolithic Revolution in the Fertile Crescent.
Many grinding stones are found with the early Egyptian Sebilian and Mechian cultures and evidence has been found of a Neolithic domesticated crop-based economy dating around 7,000 BP.[65][66]
Unlike the Middle East, this evidence appears as a "false dawn" to agriculture, as the sites were later abandoned, and permanent farming then was delayed until 6,500 BP with the Tasian culture and Badarian culture and the arrival of crops and animals from the Near East.
Bananas and plantains, which were first domesticated in Southeast Asia, most likely Papua New Guinea, were re-domesticated in Africa possibly as early as 5,000 years ago. Asian yams and taro were also cultivated in Africa.[39]
The most famous crop domesticated in the Ethiopian highlands is coffee. In addition, khat, ensete, noog, teff and finger millet were also domesticated in the Ethiopian highlands. Crops domesticated in the Sahel region include sorghum and pearl millet. The kola nut was first domesticated in West Africa. Other crops domesticated in West Africa include African rice, yams and the oil palm.[39]
Agriculture spread to Central and Southern Africa in the Bantu expansion during the 1st millennium BCE to 1st millennium CE.
The term "Neolithic" is not customarily used in describing cultures in the Americas.  However, a broad similarity exists between Eastern Hemisphere cultures of the Neolithic and cultures in the Americas. Maize (corn), beans and squash were among the earliest crops domesticated in Mesoamerica: squash as early as 6000 BCE, beans no later than 4000 BCE, and maize beginning about 7000 BCE.[67] Potatoes and manioc were domesticated in South America. In what is now the eastern United States, Native Americans domesticated sunflower, sumpweed and goosefoot c. 2500 BCE.  In the highlands of central Mexico, sedentary village life based on farming did not develop until the "formative period" in the second millennium BCE.[68]
Evidence of drainage ditches at Kuk Swamp on the borders of the Western and Southern Highlands of Papua New Guinea indicates cultivation of taro and a variety of other crops, dating back to 11,000 BP. Two potentially significant economic species, taro (Colocasia esculenta) and yam (Dioscorea sp.), have been identified dating at least to 10,200 calibrated years before present (cal BP). Further evidence of bananas and sugarcane dates to 6,950 to 6,440 BCE. This was at the altitudinal limits of these crops, and it has been suggested that cultivation in more favourable ranges in the lowlands may have been even earlier. CSIRO has found evidence that taro was introduced into the Solomon Islands for human use, from 28,000 years ago, making taro the earliest cultivated crop in the world.[69][70]
It seems to have resulted in the spread of the Trans–New Guinea languages from New Guinea east into the Solomon Islands and west into Timor and adjacent areas of Indonesia. This seems to confirm the theories of Carl Sauer who, in "Agricultural Origins and Dispersals", suggested as early as 1952 that this region was a centre of early agriculture.
Archaeologists trace the emergence of food-producing societies in the Levantine region of southwest Asia at the close of the last glacial period around 12,000 BCE, and developed into a number of regionally distinctive cultures by the eighth millennium BCE. Remains of food-producing societies in the Aegean have been carbon-dated to c. 6500 BCE at Knossos, Franchthi Cave, and a number of mainland sites in Thessaly. Neolithic groups appear soon afterwards in the Balkans and south-central Europe. The Neolithic cultures of southeastern Europe (the Balkans and the Aegean) show some continuity with groups in southwest Asia and Anatolia (e.g., Çatalhöyük).
Current evidence suggests that Neolithic material culture was introduced to Europe via western Anatolia. All Neolithic sites in Europe contain ceramics, and contain the plants and animals domesticated in Southwest Asia: einkorn, emmer, barley, lentils, pigs, goats, sheep, and cattle. Genetic data suggest that no independent domestication of animals took place in Neolithic Europe, and that all domesticated animals were originally domesticated in Southwest Asia.[71] The only domesticate not from Southwest Asia was broomcorn millet, domesticated in East Asia.[72]The earliest evidence of cheese-making dates to 5500 BCE in Kujawy, Poland.[73]
The diffusion across Europe, from the Aegean to Britain, took about 2,500 years (8500–6000 BP). The Baltic region was penetrated a bit later, around 5500 BP, and there was also a delay in settling the Pannonian plain. In general, colonization shows a "saltatory" pattern, as the Neolithic advanced from one patch of fertile alluvial soil to another, bypassing mountainous areas. Analysis of radiocarbon dates show clearly that Mesolithic and Neolithic populations lived side by side for as much as a millennium in many parts of Europe, especially in the Iberian peninsula and along the Atlantic coast.[74]
The spread of the Neolithic from the Near East Neolithic to Europe was first studied quantitatively in the 1970s, when a sufficient number of Carbon 14 age determinations for early Neolithic sites had become available.[76] In 1973, Ammerman and Cavalli-Sforza discovered a linear relationship between the age of an Early Neolithic site and its distance from the conventional source in the Near East (Jericho), demonstrating that the Neolithic spread at an average speed of about 1 km/yr.[76] More recent studies (2005) confirm these results and yield the speed of 0.6–1.3 km/yr (at 95% confidence level).[76]
Since the original human expansions out of Africa 200,000 years ago, different prehistoric and historic migration events have taken place in Europe.[77] Considering that the movement of the people implies a consequent movement of their genes, it is possible to estimate the impact of these migrations through the genetic analysis of human populations.[77] Agricultural and husbandry practices originated 10,000 years ago in a region of the Near East known as the Fertile Crescent.[77] According to the archaeological record this phenomenon, known as "Neolithic", rapidly expanded from these territories into Europe.[77]
However, whether this diffusion was accompanied or not by human migrations is greatly debated.[77] Mitochondrial DNA – a type of maternally inherited DNA located in the cell cytoplasm – was recovered from the remains of Pre-Pottery Neolithic B (PPNB) farmers in the Near East and then compared to available data from other Neolithic populations in Europe and also to modern populations from South Eastern Europe and the Near East.[77] The obtained results show that substantial human migrations were involved in the Neolithic spread and suggest that the first Neolithic farmers entered Europe following a maritime route through Cyprus and the Aegean Islands.[77]
The earliest Neolithic sites in South Asia are Bhirrana in Haryana dated to 7570–6200 BCE,[78] and Mehrgarh, dated to between 6500 and 5500 BP, in the Kachi plain of Balochistan, Pakistan; the site has evidence of farming (wheat and barley) and herding (cattle, sheep and goats).
There is strong evidence for causal connections between the Near-Eastern Neolithic and that further east, up to the Indus Valley.[79] There are several lines of evidence that support the idea of connection between the Neolithic in the Near East and in the Indian subcontinent.[79] The prehistoric site of Mehrgarh in Baluchistan (modern Pakistan) is the earliest Neolithic site in the north-west Indian subcontinent, dated as early as 8500 BCE.[79]
Neolithic domesticated crops in Mehrgarh include more than 90% barley and a small amount of wheat. There is good evidence for the local domestication of barley and the zebu cattle at Mehrgarh, but the wheat varieties are suggested to be of Near-Eastern origin, as the modern distribution of wild varieties of wheat is limited to Northern Levant and Southern Turkey.[79]
A detailed satellite map study of a few archaeological sites in the Baluchistan and Khybar Pakhtunkhwa regions also suggests similarities in early phases of farming with sites in Western Asia.[79] Pottery prepared by sequential slab construction, circular fire pits filled with burnt pebbles, and large granaries are common to both Mehrgarh and many Mesopotamian sites.[79]
The postures of the skeletal remains in graves at Mehrgarh bear strong resemblance to those at Ali Kosh in the Zagros Mountains of southern Iran.[79] Despite their scarcity, the Carbon-14 and archaeological age determinations for early Neolithic sites in Southern Asia exhibit remarkable continuity across the vast region from the Near East to the Indian Subcontinent, consistent with a systematic eastward spread at a speed of about 0.65 km/yr.[79]
The most prominent of several theories (not mutually exclusive) as to factors that caused populations to develop agriculture include:
Despite the significant technological advance and advancements in knowledge, arts and trade, the Neolithic revolution did not lead immediately to a rapid growth of population. Its benefits appear to have been offset by various adverse effects, mostly diseases and warfare.[91][92]
The introduction of agriculture has not necessarily led to unequivocal progress. The nutritional standards of the growing Neolithic populations were inferior to that of hunter-gatherers. Several ethnological and archaeological studies conclude that the transition to cereal-based diets caused a reduction in life expectancy and stature, an increase in infant mortality and infectious diseases, the development of chronic, inflammatory or degenerative diseases (such as obesity, type 2 diabetes and cardiovascular diseases) and multiple nutritional deficiencies, including vitamin deficiencies, iron deficiency anemia and mineral disorders affecting bones (such as osteoporosis and rickets) and teeth.[93][94][95] Average height for Europeans went down from 178 cm (5'10") for men and  168 cm (5'6") for women to 165 cm (5'5") and 155 cm (5'1") respectively, and it took until the twentieth century for average height for Europeans to return to the pre-Neolithic Revolution levels.[96]
The traditional view is that agricultural food production supported a denser population, which in turn supported larger sedentary communities, the accumulation of goods and tools, and specialization in diverse forms of new labor. Food surpluses made possible the development of a social elite who were not otherwise engaged in agriculture, industry or commerce, but dominated their communities by other means and monopolized decision-making. Nonetheless, larger societies made it more feasible for people to adopt diverse decision making and governance models.[97] Jared Diamond (in The World Until Yesterday) identifies the availability of milk and cereal grains as permitting mothers to raise both an older (e.g. 3 or 4 year old) and a younger child concurrently. The result is that a population can increase more rapidly. Diamond, in agreement with feminist scholars such as V. Spike Peterson, points out that agriculture brought about deep social divisions and encouraged gender inequality.[98][99] This social reshuffle is traced by historical theorists, like Veronica Strang, through developments in theological depictions.[100] Strang supports her theory through a comparison of aquatic deities before and after the Neolithic Agricultural Revolution, most notably the Venus of Lespugue and the Greco-Roman deities such as Circe or Charybdis: the former venerated and respected, the latter dominated and conquered. The theory, supplemented by the widely accepted assumption from Parsons that "society is always the object of religious veneration",[101] argues that with the centralization of government and the dawn of the Anthropocene, roles within society became more restrictive and were rationalized through the conditioning effect of religion; a process that is crystallized in the progression from polytheism to monotheism.
Andrew Sherratt has argued that following upon the Neolithic Revolution was a second phase of discovery that he refers to as the secondary products revolution. Animals, it appears, were first domesticated purely as a source of meat.[102] The Secondary Products Revolution occurred when it was recognised that animals also provided a number of other useful products. These included:
Sherratt argued that this phase in agricultural development enabled humans to make use of the energy possibilities of their animals in new ways, and permitted permanent intensive subsistence farming and crop production, and the opening up of heavier soils for farming. It also made possible nomadic pastoralism in semi arid areas, along the margins of deserts, and eventually led to the domestication of both the dromedary and Bactrian camel.[102] Overgrazing of these areas, particularly by herds of goats, greatly extended the areal extent of deserts.
Compared to foragers, Neolithic farmers' diets were higher in carbohydrates but lower in fibre, micronutrients, and protein. This led to an increase in the frequency of carious teeth[7] and slower growth in childhood and increased body fat[clarification needed], and studies have consistently found that populations around the world became shorter after the transition to agriculture. This trend may have been exacerbated by the greater seasonality of farming diets and with it the increased risk of famine due to crop failure.[6]
Throughout the development of sedentary societies, disease spread more rapidly than it had during the time in which hunter-gatherer societies existed. Inadequate sanitary practices and the domestication of animals may explain the rise in deaths and sickness following the Neolithic Revolution, as diseases jumped from the animal to the human population. Some examples of infectious diseases spread from animals to humans are influenza, smallpox, and measles.[103] Ancient microbial genomics has shown that progenitors to human-adapted strains of Salmonella enterica infected up to 5,500 year old agro-pastoralists throughout Western Eurasia, providing molecular evidence for the hypothesis that the Neolithization process facilitated the emergence of Salmonella entericia.[104]
In concordance with a process of natural selection, the humans who first domesticated the big mammals quickly built up immunities to the diseases as within each generation the individuals with better immunities had better chances of survival. In their approximately 10,000 years of shared proximity with animals, such as cows, Eurasians and Africans became more resistant to those diseases compared with the indigenous populations encountered outside Eurasia and Africa.[39] For instance, the population of most Caribbean and several Pacific Islands have been completely wiped out by diseases. 90% or more of many populations of the Americas were wiped out by European and African diseases before recorded contact with European explorers or colonists. Some cultures like the Inca Empire did have a large domestic mammal, the llama, but llama milk was not drunk, nor did llamas live in a closed space with humans, so the risk of contagion was limited. According to bioarchaeological research, the effects of agriculture on dental health in Southeast Asian rice farming societies from 4000 to 1500 BP was not detrimental to the same extent as in other world regions.[105]
Jonathan C. K. Wells and Jay T. Stock have argued that the dietary changes and increased pathogen exposure associated with agriculture profoundly altered human biology and life history, creating conditions where natural selection favoured the allocation of resources towards reproduction over somatic effort.[6]

A cradle of civilization is a location and a culture where civilization was developed independent of other civilizations in other locations. A civilization is any complex society characterized by the development of the state, social stratification, urbanization, and symbolic systems of communication beyond signed or spoken languages (namely, writing systems and graphic arts).[1][2][3][4][5]
Scholars generally acknowledge six cradles of civilization: Mesopotamia, Ancient Egypt, Ancient India and Ancient China are believed to be the earliest in Afro-Eurasia (previously called the Old World),[6][7] while the Caral–Supe civilization of coastal Peru and the Olmec civilization of Mexico are believed to be the earliest in the Americas – previously known in  Western literature as the New World. All of the cradles of civilization depended upon agriculture for sustenance (except possibly Caral–Supe which may have depended initially on marine resources). All depended upon farmers producing an agricultural surplus to support the centralized government, political leaders, religious leaders, and public works of the urban centers of the early civilizations.
Less formally, the term "cradle of civilization" is often used to refer to other historic ancient civilizations, such as Greece or Rome, which have both been called the "cradle of Western civilization".
The earliest signs of a process leading to sedentary culture can be seen in the Levant to as early as 12,000 BC, when the Natufian culture became sedentary; it evolved into an agricultural society by 10,000 BC.[8] The importance of water to safeguard an abundant and stable food supply, due to favourable conditions for hunting, fishing and gathering resources including cereals, provided an initial wide spectrum economy that triggered the creation of permanent villages.[9]
The earliest proto-urban settlements with several thousand inhabitants emerged in the Neolithic which began in Western Asia in 10,000 BC. The first cities to house several tens of thousands were Uruk, Ur, Kish and Eridu in Mesopotamia, followed by Susa in Elam and Memphis in Egypt, all by the 31st century BC (see Historical urban community sizes).
Historic times are marked apart from prehistoric times when "records of the past begin to be kept for the benefit of future generations"[10]—in written or oral form. If the rise of civilization is taken to coincide with the development of writing out of proto-writing, then the Near Eastern Chalcolithic (the transitional period between the Neolithic and the Bronze Age during the 4th millennium BC) and the development of proto-writing in Harappa in the Indus Valley of South Asia around 3,300 BC are the earliest instances, followed by Chinese proto-writing evolving into the oracle bone script, and again by the emergence of Mesoamerican writing systems from about 900 BC.
In the absence of written documents, most aspects of the rise of early civilizations are contained in archaeological assessments that document the development of formal institutions and the material culture. A "civilized" way of life is ultimately linked to conditions coming almost exclusively from intensive agriculture. Gordon Childe defined the development of civilization as the result of two successive revolutions: the Neolithic Revolution of Western Asia, triggering the development of settled communities, and the urban revolution which also first emerged in Western Asia, which enhanced tendencies towards dense settlements, specialized occupational groups, social classes, exploitation of surpluses, monumental public buildings and writing. Few of those conditions, however, are unchallenged by the records: dense cities were not attested in Egypt's Old Kingdom (unlike Mesopotamia) and cities had a dispersed population in the Maya area;[11] the Incas lacked writing although they could keep records with Quipus which might also have had literary uses; and often monumental architecture preceded any indication of village settlement. For instance, in present-day Louisiana, researchers have determined that cultures that were primarily nomadic organized over generations to build earthwork mounds at seasonal settlements as early as 3400 BC. Rather than a succession of events and preconditions, the rise of civilization could equally be hypothesized as an accelerated process that started with incipient agriculture and culminated in the Oriental Bronze Age.[12]
Scholars once thought that civilization began in the Fertile Crescent and spread out from there by influence.[13] Scholars now believe that civilizations arose independently at several locations in both hemispheres. They have observed that sociocultural developments occurred along different timeframes. "Sedentary" and "nomadic" communities continued to interact considerably; they were not strictly divided among widely different cultural groups. The concept of a cradle of civilization has a focus where the inhabitants came to build cities, to create writing systems, to experiment in techniques for making pottery and using metals, to domesticate animals, and to develop complex social structures involving class systems.[14]
Today, scholarship generally identifies six areas where civilization emerged independently:[15][16] the Fertile Crescent, including Mesopotamia and the Levant; the Nile Valley; the Indo-Gangetic Plain; the North China Plain; the Andean Coast; and the Mesoamerican Gulf Coast.
The Fertile Crescent comprises a crescent-shaped region of elevated terrain in West Asia, encompassing regions of modern-day Egypt, Israel, Palestine, Lebanon, Syria, Jordan, Turkey, and Iraq, extending to the Zagros Mountains in Iran. It stands as one of the earliest regions globally where agricultural practices emerged, marking the advent of sedentary farming communities.[17]
By 10,200 BC, fully developed Neolithic cultures, characterized by the Pre-Pottery Neolithic A (PPNA) and Pre-Pottery Neolithic B (7600 to 6000 BC) phases, emerged within the Fertile Crescent. These cultures diffused eastward into South Asia and westward into Europe and North Africa.[18] Among the notable PPNA settlements is Jericho, located in the Jordan Valley, believed to be the world's earliest established city, with initial settlement dating back to around 9600 BC and fortification occurring around 6800 BC.[19][20]
Current theories and findings identify the Fertile Crescent as the first and oldest cradle of civilization. Examples of sites in this area are the early Neolithic site of Göbekli Tepe (9500–8000 BC) and Çatalhöyük (7500–5700 BC).
In Mesopotamia (a region encompassing modern Iraq and bordering regions of Southeast Turkey, Northeast Syria and Northwest Iran), the convergence of the Tigris and Euphrates rivers produced rich fertile soil and a supply of water for irrigation. Neolithic cultures emerged in the region from 8000 BC onwards. The civilizations that emerged around these rivers are the earliest known non-nomadic agrarian societies. It is because of this that the Fertile Crescent region, and Mesopotamia in particular, are often referred to as the cradle of civilization.[21] The period known as the Ubaid period (c. 6500 to 3800 BC) is the earliest known period on the alluvial plain, although it is likely earlier periods exist obscured under the alluvium.[22][23] It was during the Ubaid period that the movement toward urbanization began. Agriculture and animal husbandry were widely practiced in sedentary communities, particularly in Northern Mesopotamia (later Assyria), and intensive irrigated hydraulic agriculture began to be practiced in the south.[24]
Around 6000 BC, Neolithic settlements began to appear all over Egypt.[25] Studies based on morphological,[26] genetic,[27][28][29][30][31] and archaeological data[32][33][34][35] have attributed these settlements to migrants from the Fertile Crescent in the Near East arriving in Egypt and North Africa during the Egyptian and North African Neolithic Revolution and bringing agriculture to the region. Tell el-'Oueili is the oldest Sumerian site settled during this period, around 5400 BC, and the city of Ur also first dates to the end of this period.[36] In the south, the Ubaid period lasted from around 6500 to 3800 BC.[37]
Sumerian civilization coalesced in the subsequent Uruk period (4000 to 3100 BC).[38] Named after the Sumerian city of Uruk, this period saw the emergence of urban life in Mesopotamia and, during its later phase, the gradual emergence of the cuneiform script. Proto-writing in the region dates to around 3800 BC, with the earliest texts dating to 3300 BC; early cuneiform writing emerged in 3000 BC.[citation needed] It was also during this period that pottery painting declined as copper started to become popular, along with cylinder seals.[39] Sumerian cities during the Uruk period were probably theocratic and were most likely headed by a priest-king (ensi), assisted by a council of elders, including both men and women.[40] It is quite possible that the later Sumerian pantheon was modeled upon this political structure.
The Jemdet Nasr period, which is generally dated from 3100 to 2900 BC and succeeds the Uruk period, is known as one of the formative stages in the development of the cuneiform script. The oldest clay tablets come from Uruk and date to the late fourth millennium BC, slightly earlier than the Jemdet Nasr Period. By the time of the Jemdet Nasr Period, the script had already undergone a number of significant changes. It originally consisted of pictographs, but by the time of the Jemdet Nasr Period it was already adopting simpler and more abstract designs. It is also during this period that the script acquired its iconic wedge-shaped appearance.[41][42]
Uruk trade networks started to expand to other parts of Mesopotamia and as far as North Caucasus, and strong signs of governmental organization and social stratification began to emerge, leading to the Early Dynastic Period (c. 2900 BC).[43][44][45] After the Early Dynastic period began, there was a shift in control of the city-states from the temple establishment headed by council of elders led by a priestly "En" (a male figure when it was a temple for a goddess, or a female figure when headed by a male god)[46] towards a more secular Lugal (Lu = man, Gal = great). The Lugals included such legendary patriarchal figures as Enmerkar, Lugalbanda and Gilgamesh, who supposedly reigned shortly before the historic record opens around 2700 BC, when syllabic writing started to develop from the early pictograms. The center of Sumerian culture remained in southern Mesopotamia, even though rulers soon began expanding into neighboring areas. Neighboring Semitic groups, including the Akkadian speaking Semites (Assyrians, Babylonians) who lived alongside the Sumerians in Mesopotamia, adopted much of Sumerian culture for their own. The earliest ziggurats began near the end of the Early Dynastic Period, although architectural precursors in the form of raised platforms date back to the Ubaid period.[47] The Sumerian King List dates to the early second millennium BC. It consists of a succession of royal dynasties from different Sumerian cities, ranging back into the Early Dynastic Period. Each dynasty rises to prominence and dominates the region, only to be replaced by the next. The document was used by later Mesopotamian kings to legitimize their rule. While some of the information in the list can be checked against other texts such as economic documents, much of it is probably purely fictional, and its use as a historical document is limited.[45]
Eannatum, the Sumerian king of Lagash, established the first verifiable empire in history in 2500 BC.[48] The neighboring Elam, in modern Iran, was also part of the early urbanization during the Chalcolithic period.[49] Elamite states were among the leading political forces of the Ancient Near East.[50] The emergence of Elamite written records from around 3000 BC also parallels Sumerian history, where slightly earlier records have been found.[51][52] During the 3rd millennium BC, there developed a very intimate cultural symbiosis between the Sumerians and the Akkadians.[53] Akkadian gradually replaced Sumerian as a spoken language somewhere between the 3rd and the 2nd millennia BC.[54] The Semitic-speaking Akkadian empire emerged around 2350 BC under Sargon the Great.[43] The Akkadian Empire reached its political peak between the 24th and 22nd centuries BC. Under Sargon and his successors, the Akkadian language was briefly imposed on neighboring conquered states such as Elam and Gutium. After the fall of the Akkadian Empire and the overthrow of the Gutians, there was a brief reassertion of Sumerian dominance in Mesopotamia under the Third Dynasty of Ur.[55] After the final collapse of Sumerian hegemony in Mesopotamia around 2004 BC, the Semitic Akkadian people of Mesopotamia eventually coalesced into two major Akkadian-speaking nations: Assyria in the north (whose earliest kings date to the 25th century BC), and, a few centuries later, Babylonia in the south, both of which (Assyria in particular) would go on to form powerful empires between the 20th and 6th centuries BC. The Sumerians were eventually absorbed into the Semitic Assyrian-Babylonian population.[56][57]
The developed Neolithic cultures belonging to the phases Pre-Pottery Neolithic A (10,200 BC) and Pre-Pottery Neolithic B (7600 to 6000 BC) appeared in the fertile crescent and from there spread eastwards and westwards.[18] Contemporaneously, a grain-grinding culture using the earliest type of sickle blades had replaced the culture of hunters, fishers, and gathering people using stone tools along the Nile. Geological evidence and computer climate modeling studies also suggest that natural climate changes around 8000 BC began to desiccate the extensive pastoral lands of northern Africa, eventually forming the Sahara. Continued desiccation forced the early ancestors of the Egyptians to settle around the Nile more permanently and to adopt a more sedentary lifestyle.[58] The oldest fully developed neolithic culture in Egypt is Fayum A culture that began around 5500 B.C.
By about 5500 BC, small tribes living in the Nile valley had developed into a series of inter-related cultures as far south as Sudan, demonstrating firm control of agriculture and animal husbandry, and identifiable by their pottery and personal items, such as combs, bracelets, and beads. The largest of these early cultures in northern Upper Egypt was the Badari, which probably originated in the Western Desert; it was known for its high quality ceramics, stone tools, and use of copper.[59] The oldest known domesticated bovine in Africa are from Fayum dating to around 4400 BC.[60] The Badari cultures was followed by the Naqada culture, which brought a number of technological improvements.[61] As early as the first Naqada Period, Amratia, Egyptians imported obsidian from Ethiopia, used to shape blades and other objects from flakes.[62] By 3300 BC, just before the first Egyptian dynasty, Egypt was divided into two kingdoms, known as Upper Egypt to the south, and Lower Egypt to the north.[63]
Egyptian civilization begins during the second phase of the Naqada culture, known as the Gerzeh period, around 3500 BC and coalesces with the unification of Upper and Lower Egypt around 3150 BC.[64] Farming produced the vast majority of food; with increased food supplies, the populace adopted a much more sedentary lifestyle, and the larger settlements grew to cities of about 5,000 residents. It was in this time that the city dwellers started using mud brick to build their cities, and the use of the arch and recessed walls for decorative effect became popular.[65] Copper instead of stone was increasingly used to make tools[65] and weaponry.[66] Symbols on Gerzean pottery also resemble nascent Egyptian hieroglyphs.[67] Early evidence also exists of contact with the Near East, particularly Canaan and the Byblos coast, during this time.[68] Concurrent with these cultural advances, a process of unification of the societies and towns of the upper Nile River, or Upper Egypt, occurred. At the same time the societies of the Nile Delta, or Lower Egypt, also underwent a unification process. During his reign in Upper Egypt, King Narmer defeated his enemies on the Delta and merged both the Kingdom of Upper and Lower Egypt under his single rule.[69]
The Early Dynastic Period of Egypt immediately followed the unification of Upper and Lower Egypt. It is generally taken to include the First and Second Dynasties, lasting from the Naqada III archaeological period until about the beginning of the Old Kingdom, c. 2686 BC.[70] With the First Dynasty, the capital moved from Thinis to Memphis with a unified Egypt ruled by a god-king. The hallmarks of ancient Egyptian civilization, such as art, architecture and many aspects of religion, took shape during the Early Dynastic period. The strong institution of kingship developed by the pharaohs served to legitimize state control over the land, labor, and resources that were essential to the survival and growth of ancient Egyptian civilization.[71]
Major advances in architecture, art, and technology were made during the subsequent Old Kingdom, fueled by the increased agricultural productivity and resulting population, made possible by a well-developed central administration.[72] Some of ancient Egypt's crowning achievements, the Giza pyramids and Great Sphinx, were constructed during the Old Kingdom. Under the direction of the vizier, state officials collected taxes, coordinated irrigation projects to improve crop yield, drafted peasants to work on construction projects, and established a justice system to maintain peace and order. Along with the rising importance of a central administration there arose a new class of educated scribes and officials who were granted estates by the pharaoh in payment for their services. Pharaohs also made land grants to their mortuary cults and local temples, to ensure that these institutions had the resources to worship the pharaoh after his death. Scholars believe that five centuries of these practices slowly eroded the economic power of the pharaoh, and that the economy could no longer afford to support a large centralized administration.[70] As the power of the pharaoh diminished, regional governors called nomarchs began to challenge the supremacy of the pharaoh. This, coupled with severe droughts between 2200 and 2150 BC,[73] is assumed to have caused the country to enter the 140-year period of famine and strife known as the First Intermediate Period.[74]
One of the earliest Neolithic sites in the Indian subcontinent is Bhirrana along the ancient Ghaggar-Hakra riverine system in the present day state of Haryana in India, dating to around 7600 BC.[75] Other early sites include Lahuradewa in the Middle Ganges region and Jhusi near the confluence of Ganges and Yamuna rivers, both dating to around 7000 BC.[76][77]
The aceramic Neolithic at Mehrgarh in present-day Pakistan lasts from 7000 to 5500 BC, with the ceramic Neolithic at Mehrgarh lasting up to 3300 BC; blending into the Early Bronze Age. Mehrgarh is one of the earliest sites with evidence of farming and herding in the Indian subcontinent.[78]  It is likely that the culture centered around Mehrgarh migrated into the Indus Valley in present-day Pakistan and became the Indus Valley Civilisation.[79] The earliest fortified town in the region is found at Rehman Dheri, dated 4000 BC in Khyber Pakhtunkhwa close to River Zhob Valley in present-day Pakistan. Other fortified towns found to date are at Amri (3600–3300 BC), Kot Diji in Sindh, and at Kalibangan (3000 BC) at the Hakra River.[80][81][82][83]
The Indus Valley Civilization starts around 3300 BC with what is referred to as the Early Harappan Phase (3300 to 2600 BC), although at the start this was still a village-based culture, leaving mostly pottery for archaeologists. The earliest examples of the Indus script date to this period,[84][85] as well as the emergence of citadels representing centralised authority and an increasingly urban quality of life.[86] Trade networks linked this culture with related regional cultures and distant sources of raw materials, including lapis lazuli and other materials for bead-making. By around 2600 BC, villagers had domesticated numerous crops, including peas, sesame seeds, dates, and cotton, as well as animals, including the water buffalo.[87][88]
2600 to 1900 BC marks the Mature Harappan Phase during which Early Harappan communities turned into large urban centers including Harappa, Dholavira, Mohenjo-daro, Lothal, Rupar, and Rakhigarhi, and more than 1,000 towns and villages, often of relatively small size.[89] Mature Harappans evolved new techniques in metallurgy and produced copper, bronze, lead, and tin and displayed advanced levels of engineering.[90] As seen in Harappa, Mohenjo-daro and the recently partially excavated Rakhigarhi, this urban plan included the world's first known urban sanitation systems: see hydraulic engineering of the Indus Valley civilization. Within the city, individual homes or groups of homes obtained water from wells. From a room that appears to have been set aside for bathing, waste water was directed to covered drains, which lined the major streets. Houses opened only to inner courtyards and smaller lanes. The housebuilding in some villages in the region still resembles in some respects the housebuilding of the Harappans.[91] The advanced architecture of the Harappans is shown by their impressive dockyards, granaries, warehouses, brick platforms, and protective walls. The massive walls of Indus cities most likely protected the Harappans from floods and may have dissuaded military conflicts.[92]
The people of the Indus Civilization achieved great accuracy in measuring length, mass, and time. They were among the first to develop a system of uniform weights and measures. A comparison of available objects indicates large scale variation across the Indus territories. Their smallest division, which is marked on an ivory scale found in Lothal in Gujarat, was approximately 1.704 mm, the smallest division ever recorded on a scale of the Bronze Age. Harappan engineers followed the decimal division of measurement for all practical purposes, including the measurement of mass as revealed by their hexahedron weights.[93] These chert weights were in a ratio of 5:2:1 with weights of 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, and 500 units, with each unit weighing approximately 28 grams, similar to the English Imperial ounce or Greek uncia, and smaller objects were weighed in similar ratios with the units of 0.871. However, as in other cultures, actual weights were not uniform throughout the area. The weights and measures later used in Kautilya's Arthashastra (4th century BC) are the same as those used in Lothal.[94]
Around 1800 BC, signs of a gradual decline began to emerge, and by around 1700 BC most of the cities had been abandoned. Suggested contributory causes for the localisation of the IVC include changes in the course of the river,[95] and climate change that is also signalled for the neighbouring areas of the Middle East.[96][97] As of 2016[update] many scholars believe that drought led to a decline in trade with Egypt and Mesopotamia contributing to the collapse of the Indus Civilization.[98] The Ghaggar-Hakra system was rain-fed,[99][100][note 1][101][note 2] and water-supply depended on the monsoons. The Indus Valley climate grew significantly cooler and drier from about 1800 BC, linked to a general weakening of the monsoon at that time.[99] The Indian monsoon declined and aridity increased, with the Ghaggar-Hakra retracting its reach towards the foothills of the Himalaya,[99][102][103] leading to erratic and less extensive floods that made inundation agriculture less sustainable. Aridification reduced the water supply enough to cause the civilization's demise, and to scatter its population eastward.[104][105][106][note 3] As the monsoons kept shifting south, the floods grew too erratic for sustainable agricultural activities. The residents then migrated away into smaller communities. However trade with the old cities did not flourish. The small surplus produced in these small communities did not allow development of trade, and the cities died out.[107] The Indo-Aryan peoples migrated into the Indus River Valley during this period and began the Vedic age of India.[108] The Indus Valley Civilization did not disappear suddenly and many elements of the civilization continued in later Indian subcontinent and Vedic cultures.[109]
Drawing on archaeology, geology and anthropology, modern scholars do not see the origins of the Chinese civilization or history as a linear story but rather the history of the interactions of different and distinct cultures and ethnic groups that influenced each other's development.[110] The specific cultural regions that developed Chinese civilization were the Yellow River civilization, the Yangtze civilization, and Liao civilization. Early evidence for Chinese millet agriculture is dated to around 7000 BC,[111] with the earliest evidence of cultivated rice found at Chengtoushan near the Yangtze River, dated to 6500 BC. Chengtoushan may also be the site of the first walled city in China.[112] By the beginning of the Neolithic Revolution, the Yellow River valley began to establish itself as a center of the Peiligang culture, which flourished from 7000 to 5000 BC, with evidence of agriculture, constructed buildings, pottery, and burial of the dead.[113] With agriculture came increased population, the ability to store and redistribute crops, and the potential to support specialist craftsmen and administrators.[114] Its most prominent site is Jiahu.[114] Some scholars have suggested that the Jiahu symbols (6600 BC) are the earliest form of proto-writing in China.[115] However, it is likely that they should not be understood as writing itself, but as features of a lengthy period of sign-use, which led eventually to a fully-fledged system of writing.[116] Archaeologists believe that the Peiligang culture was egalitarian, with little political organization.
It eventually evolved into the Yangshao culture (5000 to 3000 BC), and their stone tools were polished and highly specialized. They may also have practiced an early form of silkworm cultivation.[117] The main food of the Yangshao people was millet, with some sites using foxtail millet and others broomcorn millet, though some evidence of rice has been found.  The exact nature of Yangshao agriculture, small-scale slash-and-burn cultivation versus intensive agriculture in permanent fields, is currently a matter of debate. Once the soil was exhausted, residents picked up their belongings, moved to new lands, and constructed new villages.[118] However, Middle Yangshao settlements such as Jiangzhi contain raised-floor buildings that may have been used for the storage of surplus grains. Grinding stones for making flour were also found.[119]
Later, Yangshao culture was superseded by the Longshan culture, which was also centered on the Yellow River from about 3000 to 1900 BC, its most prominent site being Taosi.[120] The population expanded dramatically during the 3rd millennium BC, with many settlements having rammed earth walls.  It decreased in most areas around 2000 BC until the central area evolved into the Bronze Age Erlitou culture. The earliest bronze artifacts have been found in the Majiayao culture site (3100 to 2700 BC).[121][122]
Chinese civilization begins during the second phase of the Erlitou period (1900 to 1500 BC), with Erlitou considered the first state level society of East Asia.[123] There is considerable debate whether Erlitou sites correlate to the semi-legendary Xia dynasty. The Xia dynasty (2070 to 1600 BC) is the first dynasty to be described in ancient Chinese historical records such as the Bamboo Annals, first published more than a millennium later during the Western Zhou period. Although Xia is an important element in Chinese historiography, there is to date no contemporary written evidence to corroborate the dynasty. Erlitou saw an increase in bronze metallurgy and urbanization and was a rapidly growing regional center with palatial complexes that provide evidence for social stratification.[124] The Erlitou civilization is divided into four phases, each of roughly 50 years. During Phase I, covering 100 hectares (250 acres), Erlitou was a rapidly growing regional center with estimated population of several thousand[125] but not yet an urban civilization or capital.[126] Urbanization began in Phase II, expanding to 300 ha (740 acres) with a population around 11,000.[125]  A palace area of 12 ha (30 acres) was demarcated by four roads. It contained the 150x50 m Palace 3, composed of three courtyards along a 150-meter axis, and Palace 5.[127]  A bronze foundry was established to the south of the palatial complex that was controlled by the elite who lived in palaces.[128] The city reached its peak in Phase III, and may have had a population of around 24,000.[126] The palatial complex was surrounded by a two-meter-thick rammed-earth wall, and Palaces 1, 7, 8, 9 were built.  The earthwork volume of rammed earth for the base of largest Palace 1 is 20,000 m³ at least.[129] Palaces 3 and 5 were abandoned and replaced by 4,200-square-meter (45,000 sq ft) Palace 2 and Palace 4.[130] In Phase IV, the population decreased to around 20,000, but building continued. Palace 6 was built as an extension of Palace 2, and Palaces 10 and 11 were built.  Phase IV overlaps with the Lower phase of the Erligang culture (1600–1450 BC). Around 1600 to 1560 BC, about 6 km northeast of Erlitou, a culturally Erligang walled city was built at Yanshi,[130] which coincides with an increase in production of arrowheads at Erlitou.[125] This situation might indicate that the Yanshi city was competing for power and dominance with Erlitou.[125] Production of bronzes and other elite goods ceased at the end of Phase IV, at the same time as the Erligang city of Zhengzhou was established 85 km (53 mi) to the east. There is no evidence of destruction by fire or war, but, during the Upper Erligang phase (1450–1300 BC), all the palaces were abandoned, and Erlitou was reduced to a village of 30 ha (74 acres).[130]
The earliest traditional Chinese dynasty for which there is both archeological and written evidence is the Shang dynasty (1600 to 1046 BC). Shang sites have yielded the earliest known body of Chinese writing, the oracle bone script, mostly divinations inscribed on bones. These inscriptions provide critical insight into many topics from the politics, economy, and religious practices to the art and medicine of this early stage of Chinese civilization.[131] Some historians argue that Erlitou should be considered an early phase of the Shang dynasty. The U.S. National Gallery of Art defines the Chinese Bronze Age as the period between about 2000 and 771 BC; a period that begins with the Erlitou culture and ends abruptly with the disintegration of Western Zhou rule.[132] The Sanxingdui culture is another Chinese Bronze Age society, contemporaneous to the Shang dynasty, however they developed a different method of bronze-making from the Shang.[133]
The earliest evidence of agriculture in the Andean region dates to around 9000 BC in Ecuador at sites of the Las Vegas culture. The bottle gourd may have been the first plant cultivated.[134] The oldest evidence of canal irrigation in South America dates to 4700 to 2500 BC in the Zaña Valley of northern Peru.[135] The earliest urban settlements of the Andes, as well as North and South America, are dated to 3500 BC at Huaricanga, in the Fortaleza area,[14] and Sechin Bajo near the Sechin River. Both sites are in Peru.[136][137]
The Caral–Supe or Norte Chico civilization is understood to have emerged around 3200 BC, as it is at that point that large-scale human settlement and communal construction across multiple sites becomes clearly apparent.[138] In the early 21st century, Peruvian archaeologist Ruth Shady established Caral–Supe as the oldest known civilization in the Americas. The civilization flourished near the Pacific coast in the valleys of three small rivers, the Fortaleza, the Pativilca, and the Supe. These river valleys each have large clusters of sites. Further south, there are several associated sites along the Huaura River.[139] Notable settlements include the cities of Caral, the largest and most complex Preceramic site, and Aspero.[140] Norte Chico is distinguished by its density of large sites with immense architecture.[141] Haas argues that the density of sites in such a small area is globally unique for a nascent civilization. During the third millennium BC, Norte Chico may have been the most densely populated area of the world (excepting, possibly, northern China).[142] The Supe, Pativilca, Fortaleza, and Huaura River valleys each have several related sites.
Norte Chico is unusual in that it completely lacked ceramics and apparently had almost no visual art. Nevertheless, the civilization exhibited impressive architectural feats, including large earthwork platform mounds and sunken circular plazas, and an advanced textile industry.[14][143] The platform mounds, as well as large stone warehouses, provide evidence for a stratified society and a centralized authority necessary to distribute resources such as cotton.[14] However, there is no evidence of warfare or defensive structures during this period.[142] Originally, it was theorized that, unlike other early civilizations, Norte Chico developed by relying on maritime food sources in place of a staple cereal. This hypothesis, the Maritime Foundation of Andean Civilization, is still hotly debated; however, most researches now agree that agriculture played a central role in the civilization's development while still acknowledging a strong supplemental reliance on maritime proteins.[144][145][146]
The Norte Chico chiefdoms were "...almost certainly theocratic, though not brutally so," according to Mann. Construction areas show possible evidence of feasting, which would have included music and likely alcohol, suggesting an elite able to both mobilize and reward the population.[14] The degree of centralized authority is difficult to ascertain, but architectural construction patterns are indicative of an elite that, at least in certain places at certain times, wielded considerable power: while some of the monumental architecture was constructed incrementally, other buildings, such as the two main platform mounds at Caral, appear to have been constructed in one or two intense construction phases.[142] As further evidence of centralized control, Haas points to remains of large stone warehouses found at Upaca, on the Pativilca, as emblematic of authorities able to control vital resources such as cotton.[14] Economic authority would have rested on the control of cotton and edible plants and associated trade relationships, with power centered on the inland sites. Haas tentatively suggests that the scope of this economic power base may have extended widely: there are only two confirmed shore sites in the Norte Chico (Aspero and Bandurria) and possibly two more, but cotton fishing nets and domesticated plants have been found up and down the Peruvian coast. It is possible that the major inland centers of Norte Chico were at the center of a broad regional trade network centered on these resources.[142]
Discover magazine, citing Shady, suggests a rich and varied trade life: "[Caral] exported its own products and those of Aspero to distant communities in exchange for exotic imports: Spondylus shells from the coast of Ecuador, rich dyes from the Andean highlands, hallucinogenic snuff from the Amazon."[147] (Given the still limited extent of Norte Chico research, such claims should be treated circumspectly.) Other reports on Shady's work indicate Caral traded with communities in the Andes and in the jungles of the Amazon basin on the opposite side of the Andes.[148]
Leaders' ideological power was based on apparent access to deities and the supernatural.[142] Evidence regarding Norte Chico religion is limited: an image of the Staff God, a leering figure with a hood and fangs, has been found on a gourd dated to 2250 BC. The Staff God is a major deity of later Andean cultures, and Winifred Creamer suggests the find points to worship of common symbols of gods.[149][150] As with much other research at Norte Chico, the nature and significance of the find has been disputed by other researchers.[note 4] The act of architectural construction and maintenance may also have been a spiritual or religious experience: a process of communal exaltation and ceremony.[140] Shady has called Caral "the sacred city" (la ciudad sagrada): socio-economic and political focus was on the temples, which were periodically remodeled, with major burnt offerings associated with the remodeling.[151]
Bundles of strings uncovered at Norte Chico sites have been identified as quipu, a type of pre-writing recording device.[152] Quipu are thought to encode numeric information, but some have conjectured that quipu have been used to encode other forms of data, possibly including literary or musical applications.[153] However, the exact use of quipu by the Norte Chico and later Andean cultures has been widely debated.[14] The presence of quipu and the commonality of religious symbols suggests a cultural link between Norte Chico and later Andean cultures.[149][150]
Circa 1800 BC, the Norte Chico civilization began to decline, with more powerful centers appearing to the south and north along the coast and to the east inside the belt of the Andes.[154] Pottery eventually developed in the Amazon Basin and spread to the Andean culture region around 2000 BC. The next major civilization to arise in the Andes would be the Chavín culture at Chavín de Huantar, located in the Andean highlands of the present-day Department of Ancash. It is believed to have been built around 900 BC and was the religious and political center of the Chavín people.[155]
Maize is believed to have been first domesticated in southern Mexico about 7000 BC.[156][157] The Coxcatlan Caves in the Valley of Tehuacán provide evidence for agriculture in components dated between 5000 and 3400 BC.[158] Similarly, sites such as Sipacate in Guatemala provide maize pollen samples dating to 3500 BC.[159] Around 1900 BC, the Mokaya domesticated one of the dozen species of cacao.[160][161] A Mokaya archaeological site provides evidence of cacao beverages dating to this time.[162] The Mokaya are also thought to have been among the first cultures in Mesoamerica to develop a hierarchical society. What would become the Olmec civilization had its roots in early farming cultures of Tabasco, which began around 5100 to 4600 BC.[163]
The emergence of the Olmec civilization has traditionally been dated to around 1600 to 1500 BC. Olmec features first emerged in the city of San Lorenzo Tenochtitlán, fully coalescing around 1400 BC. The rise of civilization was assisted by the local ecology of well-watered alluvial soil, as well as by the transportation network provided by the Coatzacoalcos River basin.[163] This environment encouraged a densely concentrated population, which in turn triggered the rise of an elite class and an associated demand for the production of the symbolic and sophisticated luxury artifacts that define Olmec culture.[164] Many of these luxury artifacts were made from materials such as jade, obsidian, and magnetite, which came from distant locations and suggest that early Olmec elites had access to an extensive trading network in Mesoamerica. The aspect of Olmec culture perhaps most familiar today is their artwork, particularly the Olmec colossal heads.[165] San Lorenzo was situated in the midst of a large agricultural area.[166] San Lorenzo seems to have been largely a ceremonial site, a town without city walls, centered in the midst of a widespread medium-to-large agricultural population. The ceremonial center and attendant buildings could have housed 5,500 while the entire area, including hinterlands, could have reached 13,000.[167] It is thought that while San Lorenzo controlled much or all of the Coatzacoalcos basin, areas to the east (such as the area where La Venta would rise to prominence) and north-northwest (such as the Tuxtla Mountains) were home to independent polities.[168] San Lorenzo was all but abandoned around 900 BC at about the same time that La Venta rose to prominence. A wholesale destruction of many San Lorenzo monuments also occurred circa 950 BC, which may indicate an internal uprising or, less likely, an invasion.[169] The latest thinking, however, is that environmental changes may have been responsible for this shift in Olmec centers, with certain important rivers changing course.[170]
La Venta became the cultural capital of the Olmec concentration in the region until its abandonment around 400 BC; constructing monumental architectural achievements such as the Great Pyramid of La Venta.[163][165] It contained a "concentration of power", as reflected by the sheer enormity of the architecture and the extreme value of the artifacts uncovered.[171] La Venta is perhaps the largest Olmec city and it was controlled and expanded by an extremely complex hierarchical system with a king, as the ruler and the elites below him. Priests had power and influence over life and death and likely great political sway as well. Unfortunately, not much is known about the political or social structure of the Olmec, though new dating techniques might, at some point, reveal more information about this elusive culture. It is possible that the signs of status exist in the artifacts recovered at the site such as depictions of feathered headdresses or of individuals wearing a mirror on their chest or forehead.[172] "High-status objects were a significant source of power in the La Venta polity political power, economic power, and ideological power. They were tools used by the elite to enhance and maintain rights to rulership".[173] It has been estimated that La Venta would need to be supported by a population of at least 18,000 people during its principal occupation.[174] To add to the mystique of La Venta, the alluvial soil did not preserve skeletal remains, so it is difficult to observe differences in burials. However, colossal heads provide proof that the elite had some control over the lower classes, as their construction would have been extremely labor-intensive. "Other features similarly indicate that many laborers were involved".[175] In addition, excavations over the years have discovered that different parts of the site were likely reserved for elites and other parts for non-elites. This segregation of the city indicates that there must have been social classes and therefore social inequality.[172]
The exact cause of the decline of the Olmec culture is uncertain. Between 400 and 350 BC, the population in the eastern half of the Olmec heartland dropped precipitously.[176] This depopulation was probably the result of serious environmental changes that rendered the region unsuited for large groups of farmers, in particular changes to the riverine environment that the Olmec depended upon for agriculture, hunting and gathering, and transportation. These changes may have been triggered by tectonic upheavals or subsidence, or the silting up of rivers due to agricultural practices.[163][165] Within a few hundred years of the abandonment of the last Olmec cities, successor cultures became firmly established. The Tres Zapotes site, on the western edge of the Olmec heartland, continued to be occupied well past 400 BC, but without the hallmarks of the Olmec culture. This post-Olmec culture, often labeled Epi-Olmec, has features similar to those found at Izapa, some 550 km (330 miles) to the southeast.[177]
The Olmecs are sometimes referred to as the mother culture of Mesoamerica, as they were the first Mesoamerican civilization and laid many of the foundations for the civilizations that followed.[178] However, the causes and degree of Olmec influences on Mesoamerican cultures has been a subject of debate over many decades.[179] Practices introduced by the Olmec include ritual bloodletting and the Mesoamerican ballgame; hallmarks of subsequent Mesoamerican societies such as the Maya and Aztec.[178] Although the Mesoamerican writing system would fully develop later, early Olmec ceramics show representations that may be interpreted as codices.[163]
The origins of Western civilization can be traced back to the ancient Mediterranean world. There is academic consensus that Classical Greece was a major culture that provided the foundation of modern Western culture, philosophy, democracy, art, science, aesthetics, theatre, as well as building designs and proportions and architecture.[180]
Along with Greece, Ancient Rome has sometimes been described as a birthplace or as the cradle of Western Civilization because of the role the city had in politics, republicanism, law, architecture, warfare and Western Christianity.[181]
Western Civilization is also closely associated with Christianity,[182] the predominant religion in the West, which has its origins in Judaism—the ethnic religion of the Jewish people—and Greco-Roman philosophy. Christianity emerged as a sect within Judaism and inherited many of its foundational beliefs, scriptures, and ethical principles from Jewish tradition. Christian ethics, influenced by its Jewish roots, has significantly influenced the foundational principles of Western societies.[183][184][185]
The blending of Greco-Roman and Judeo-Christian traditions in shaping Western civilization has led scholars to describe it as emerging from the legacies of Athens and Jerusalem,[186][187][188] or Athens, Jerusalem, and Rome.[189]
The phrase "cradle of civilization".... plays a certain role in national mysticism. It has been used in Eastern as well as Western cultures, for instance, in Indian nationalism (In Search of the Cradle of Civilization 1995) and Taiwanese nationalism (Taiwan;— The Cradle of Civilization[190] 2002). The terms also appear in esoteric pseudohistory, such as the Urantia Book, claiming the title for "the second Eden", or the pseudoarchaeology related to Megalithic Britain (Civilization One 2004,
Ancient Britain: The Cradle of Civilization 1921).
The following timeline shows a timeline of cultures, with the approximate dates of the emergence of civilization (as discussed in the article) in the featured areas, the primary cultures associated with these early civilizations. It is important to note that the timeline is not indicative of the beginning of human habitation, the start of a specific ethnic group, or the development of Neolithic cultures in the area – any of which often occurred significantly earlier than the emergence of civilization proper.
The dates given are only approximate as the development of civilization was incremental and the exact date when "civilization" began for a given culture is subject to interpretation.

Richard G. Klein (born April 11, 1941) is a professor of Biology and Anthropology at Stanford University. He is the Anne T. and Robert M. Bass Professor in the School of Humanities and Sciences. He earned his PhD at the University of Chicago in 1966, and was elected to the National Academy of Sciences in April 2003. His research interests include paleoanthropology, Africa and Europe. His primary thesis is that modern humans evolved in East Africa, perhaps 100,000 years ago and, starting 50,000 years ago, began spreading throughout the non-African world, replacing archaic human populations over time. He is a critic of the idea that behavioral modernity arose gradually over the course of tens of thousands, hundreds of thousands of years or millions of years, instead supporting the view that modern behavior arose suddenly in the transition from the Middle Stone Age to the Later Stone Age around 50–40,000 years ago.[1]
Klein was born in 1941 in Chicago, and went to college at the University of Michigan, Ann Arbor. In 1962, he enrolled as a graduate student at the University of Chicago to study with the Neanderthal expert, Francis Clark Howell. Of the two theories in vogue then, that Neanderthals had evolved into the Cro-Magnons of Europe or that they had been replaced by the Cro-Magnons, Klein favored the replacement theory. Klein completed a master's degree in 1964, and then studied at the University of Bordeaux with François Bordes, who specialized in prehistory. There he visited the La Quina and La Ferrassie caves in southwest France, containing Cro-Magnon artifacts layered on top of Neanderthal ones. These visits influenced him into believing the shift from Neanderthal to modern humans 40,000 to 35,000 years ago was sudden rather than gradual. Klein also visited Russia to examine artifacts.[2]
Klein briefly held positions at the University of Wisconsin–Milwaukee, Northwestern University in Evanston, Illinois, and the University of Washington, Seattle, before becoming a professor at the University of Chicago in 1973. Twenty years later, he moved to Stanford University.

Micronesia (UK: /ˌmaɪkrəˈniːziə/, US: /-ˈniːʒə/ ⓘ)[1] is a subregion of Oceania, consisting of approximately 2,000 small islands in the Northwestern Pacific Ocean. It has a close shared cultural history with three other island regions: Maritime Southeast Asia to the west, Polynesia to the east, and Melanesia to the south—as well as with the wider community of Austronesian peoples.
The region has a tropical marine climate and is part of the Oceanian realm. It includes four main archipelagos—the Caroline Islands, the Gilbert Islands, the Mariana Islands, and the Marshall Islands — as well as numerous islands that are not part of any archipelago.
Political control of areas within Micronesia varies depending on the island, and is distributed among six sovereign nations. Some of the Caroline Islands are part of the Republic of Palau and some are part of the Federated States of Micronesia (often shortened to "FSM" or "Micronesia"—not to be confused with the identical name for the overall region). The Gilbert Islands (along with the Phoenix Islands and the Line Islands in Polynesia) comprise the Republic of Kiribati. The Mariana Islands are affiliated with the United States; some of them belong to the U.S. Territory of Guam and the rest belong to the U.S. Commonwealth of the Northern Mariana Islands. The island of Nauru is its own sovereign nation. The Marshall Islands all belong to the Republic of the Marshall Islands. The sovereignty of Wake Island is contested: it is claimed both by the United States and by the Republic of the Marshall Islands. The United States has actual possession of Wake Island, which is under the immediate administration of the United States Air Force.
Notwithstanding the fact that the notion of "Micronesia" has been quite well established since 1832 and has been used ever since, by most popular works, this set does not correspond to any geomorphological, archaeological, linguistic, ethnic or cultural unity, but on the contrary represents a disparate ensemble, with no real deep unity. In fact, "Micronesian people" does not exist as a subset of the sea-migrating Austronesian people, who may also include the Polynesian people and the hypothetical Australo-Melanesian or "Melanesian people".[2]
Human settlement of Micronesia began several millennia ago.[3] Based on the current scientific consensus, the Austronesian peoples originated from a prehistoric seaborne migration, known as the Austronesian expansion, from pre-Han Formosa, at around 3000 to 1500 BCE. Austronesians reached the northernmost Philippines, specifically the Batanes Islands, by around 2200 BCE. Austronesians were the first people to invent oceangoing sailing technologies (notably catamarans, outrigger boats, lashed-lug boat building, and the crab claw sail), which enabled their rapid dispersal into the islands of the Indo-Pacific.[4][5][6] From 2000 BCE they assimilated (or were assimilated by) the earlier populations on the islands in their migration pathway.[7][8][9][10][11]
The earliest known contact of Europeans with Micronesia was in 1521, when Magellan expedition landed in the Marianas. Jules Dumont d'Urville is usually credited with coining the term "Micronesia" in 1832, but in fact, Louis Domeny de Rienzi [fr] used this term a year earlier.[12][13]
Micronesia is a region in Oceania that includes approximately 2100 islands, with a total land area of 2,700 km2 (1,000 sq mi), the largest of which is Guam, which covers 582 km2 (225 sq mi). The total ocean area within the perimeter of the islands is 7,400,000 km2 (2,900,000 sq mi).[14]
There are four main island groups in Micronesia:
This does not include the separate island nation of Nauru, along with other distinctly separate islands and smaller island groups.
The Caroline Islands are a widely scattered archipelago consisting of about 500 small coral islands, north of New Guinea and east of the Philippines. The Carolines consist of two nations: the Federated States of Micronesia, consisting of approximately 600 islands on the eastern side of the chain with Kosrae being the most eastern; and Palau consisting of 250 islands on the western side.
The Gilbert Islands are a chain of sixteen atolls and coral islands, arranged in an approximate north-to-south line. In a geographical sense, the equator serves as the dividing line between the northern Gilbert Islands and the southern Gilbert Islands. The Republic of Kiribati contains all of the Gilberts, including the island of Tarawa, the site of the country's capital.
The Mariana Islands are an arc-shaped archipelago made up by the summits of fifteen volcanic mountains. The island chain arises as a result of the western edge of the Pacific Plate moving westward and plunging downward below the Mariana plate, a region that is the most volcanically active convergent plate boundary on Earth. The Marianas were politically divided in 1898, when the United States acquired title to Guam under the Treaty of Paris, 1898, which ended the Spanish–American War. Spain then sold the remaining northerly islands to Germany in 1899. Germany lost all of her colonies at the end of World War I and the Northern Mariana Islands became a League of Nations Mandate, with Japan as the mandatory. After World War II, the islands were transferred into the United Nations Trust Territory System, with the United States as Trustee. In 1976, the Northern Mariana Islands and the United States entered into a covenant of political union under which commonwealth status was granted the Northern Mariana Islands and its residents received United States citizenship.
The Marshall Islands are located north of Nauru and Kiribati, east of the Federated States of Micronesia, and south of the U.S. territory of Wake Island. The islands consist of 29 low-lying atolls and 5 isolated islands,[15] comprising 1,156 individual islands and islets. The atolls and islands form two groups: the Ratak Chain and the Ralik Chain (meaning "sunrise" and "sunset" chains). All the islands in the chain are part of the Republic of the Marshall Islands, a presidential republic in free association with the United States. Having few natural resources, the islands' wealth is based on a service economy, as well as some fishing and agriculture. Of the 29 atolls, 24 of them are inhabited.
Bikini Atoll is an atoll in the Marshall Islands. There are 23 islands in the Bikini Atoll. The islands of Bokonijien, Aerokojlol and part of Nam were destroyed during nuclear tests that occurred there.[16][17] The islands are composed of low coral limestone and sand.[18] The average elevation is only about 2.1 metres (7 ft) above low tide level.[19]
Nauru is an oval-shaped island country in the southwestern Pacific Ocean, 42 km (26 mi) south of the Equator, listed as the world's smallest republic, covering just 21 km2 (8 sq mi).[21] With 12,511 residents, it is the third least-populated country, after Vatican City and Tuvalu. The island is surrounded by a coral reef, which is exposed at low tide and dotted with pinnacles.[22] The presence of the reef has prevented the establishment of a seaport, although channels in the reef allow small boats access to the island.[23] A fertile coastal strip 150 to 300 m (490 to 980 ft) wide lies inland from the beach.[22]
Wake Island is a coral atoll with a coastline of 19 km (12 mi) just north of the Marshall Islands. It is an unorganized, unincorporated territory of the United States. Access to the island is restricted and all activities on the island are managed by the United States Air Force. While geographically adjacent, it is not ethnoculturally part of Micronesia, due to its historical lack of human inhabitation.[citation needed] Micronesians may have possibly visited Wake Island in prehistoric times to harvest fish, but there is nothing to suggest any kind of settlement.[24]
The majority of the islands in the area are part of a coral atoll. Coral atolls begin as coral reefs that grow on the slopes of a central volcano. When the volcano sinks back down into the sea, the coral continues to grow, keeping the reef at or above water level. One exception is Pohnpei in the Federated States of Micronesia, which still has the central volcano and coral reefs around it.
The Yap Islands host a number of endemic bird species, including the Yap monarch and the Olive white-eye, in addition to four other restricted-range bird species.[25] The endangered Yap flying-fox, though often considered a subspecies of the Pelew flying fox or the Mariana fruit bat, is also endemic to Yap.[25]
The region has a tropical marine climate moderated by seasonal northeast trade winds. There is little seasonal temperature variation. The dry season runs from December or January to June and the rainy season from July to November or December. Because of the location of some islands, the rainy season can sometimes include typhoons.
The Northern Mariana Islands were the first islands in Oceania colonized by the Austronesian peoples. They were settled by the voyagers who sailed eastwards from the Philippines in approximately 1500 BCE. These populations gradually moved southwards until they reached the Bismarck Archipelago and the Solomon Islands by 1300 BCE and reconnected with the Lapita culture of the southeast migration branch of Austronesians moving through coastal New Guinea and Island Melanesia. By 1200 BCE, they again began crossing open seas beyond inter-island visibility, reaching Vanuatu, Fiji, and New Caledonia; before continuing eastwards to become the ancestors of the Polynesian people.[26][27][28]
Further migrations by other Austronesians also followed, likely from Sulawesi, settling Palau and Yap by around 1000 BCE. The details of this colonization, however, are not very well known.[26][27][29] In 200 BCE, a loosely connected group of Lapita colonists from Island Melanesia also migrated back northwards, settling the islands of eastern Micronesia almost simultaneously. This region became the center of another wave of migrations radiating outwards, reconnecting them with other settled islands in western Micronesia.[26][27]
Around 800 CE, a second wave of migrants from Southeast Asia arrived in the Marianas, beginning what is now known as the Latte period. These new settlers built large structures with distinctive capped stone pillars known as haligi. They also reintroduced rice (which did not survive earlier voyages), making the Northern Marianas the only islands in Oceania where rice was grown prior to European contact. However, it was considered a high-status crop and only used in rituals. It did not become a staple until after Spanish colonization.[28][30][31]
Construction of Nan Madol, a megalithic complex made from basalt lava logs in Pohnpei, began in around 1180 CE. This was followed by the construction of the Leluh complex in Kosrae in around 1200.[27][32][33]
The earliest known contact with Europeans occurred in 1521, when a Spanish expedition under Ferdinand Magellan reached the Marianas.[34] This contact is recorded in Antonio Pigafetta's chronicle of Magellan's voyage, in which he recounts that the Chamorro people had no apparent knowledge of people outside of their island group.[35] A Portuguese account of the same voyage suggests that the Chamorro people who greeted the travellers did so "without any shyness as if they were good acquaintances".[36]
Further contact was made during the sixteenth century, although often initial encounters were very brief. Documents relating to the 1525 voyage of Diogo da Rocha suggest that he made the first European contact with inhabitants of the Caroline Islands, possibly staying on the Ulithi atoll for four months and encountering Yap. Marshall Islanders were encountered by the expedition of Spanish navigator Álvaro de Saavedra Cerón in 1529.[37] Other contact with the Yap islands occurred in 1625.[38]
In the early 17th century Spain colonized Guam, the Northern Marianas and the Caroline Islands (what would later become the Federated States of Micronesia and the Republic of Palau), creating the Spanish East Indies, which was governed from the Spanish Philippines.
When Russian explorer Otto von Kotzebue visited the Marshall Islands in 1817, he noted that Marshallese families practiced infanticide after the birth of a third child as a form of population planning due to frequent famines.[39]
In 1819, the American Board of Commissioners for Foreign Missions—a Protestant group—brought their Puritan ways to Polynesia. Soon after, the Hawaiian Missionary Society was founded and sent missionaries into Micronesia. Conversion was not met with as much opposition, as the local religions were less developed (at least according to Western ethnographic accounts). In contrast, it took until the end of the 19th to the beginning of the 20th centuries for missionaries to fully convert the inhabitants of Melanesia; however, a comparison of the cultural contrast must take into account the fact that Melanesia has always had deadly strains of malaria present in various degrees and distributions throughout its history (see De Rays Expedition) and up to the present; conversely, Micronesia does not have—and never seems to have had—any malarial mosquitos nor pathogens on any of its islands in the past.[40]
In the Spanish–American War, Spain lost many of its remaining colonies. In the Pacific, the United States took possession of the Spanish Philippines and Guam. On 17 January 1899, the United States also took possession of unclaimed and uninhabited Wake Island. This left Spain with the remainder of the Spanish East Indies, about 6,000 tiny islands that were sparsely populated and not very productive. These islands were ungovernable after the loss of the administrative center of Manila and indefensible after the loss of two Spanish fleets in the war. The Spanish government therefore decided to sell the remaining islands to a new colonial power: the German Empire.
The treaty, which was signed by Spanish Prime Minister Francisco Silvela on 12 February 1899, transferred the Caroline Islands (Kosrae in the east to Palau in the west), the Mariana Islands, and other possessions to Germany. Under German control, the islands became a protectorate and were administered from German New Guinea. Nauru had already been annexed and claimed as a colony by Germany in 1888.
In the early 20th century, the islands of Micronesia were divided between three foreign powers:
During World War I, Germany's Pacific island territories were seized and became League of Nations mandates in 1923. Nauru became an Australian mandate, while Germany's other territories in Micronesia were given as a mandate to Japan and were named the South Seas Mandate. During World War II, Nauru and Ocean Island were occupied by Japanese troops, with also an occupation of some of the Gilbert Islands and were bypassed by the Allied advance across the Pacific. Following Japan's defeat in World War II its mandate became a United Nations Trusteeship administered by the United States as the Trust Territory of the Pacific Islands.[41] Nauru became independent in 1968.
Today, most of Micronesia are independent states, except for the U.S. Commonwealth of the Northern Mariana Islands, Guam and Wake Island, which are U.S. territories.
The Pacific Community (SPC) is a regional intergovernmental organization whose membership includes both nations and territories in the Pacific Ocean and their metropolitan powers.
Nationally, the primary income is the sale of fishing rights to foreign nations that harvest tuna using huge purse seiners. A few Japanese long liners still ply the waters. The crews aboard fishing fleets contribute little to the local economy since their ships typically set sail loaded with stores and provisions that are cheaper than local goods. Additional money comes in from government grants, mostly from the United States, and the $150 million the US paid into a trust fund for reparations of residents of Bikini Atoll who had to move after nuclear testing. Few mineral deposits worth exploiting exist, except for some high-grade phosphate, especially on Nauru.
Most residents of Micronesia can freely move to and work within, the United States. Relatives working in the US who send money home to relatives represent the primary source of individual income. Additional individual income comes mainly from government jobs and work within shops and restaurants.
The tourist industry consists mainly of scuba divers that come to see the coral reefs, do wall dives and visit sunken ships from WWII. Major stops for scuba divers in approximate order are Palau, Chuuk, Yap and Pohnpei. Some private yacht owners visit the area for months or years at a time. However, they tend to stay mainly at ports of entry and are too few in number to be counted as a major source of income.
Copra production used to be a more significant source of income, however, world prices have dropped in part to large palm plantations that are now planted in places like Borneo.
The people today form many ethnicities, but all are descended from and belong to the Micronesian culture.[48]
Because of this mixture of descent, many of the ethnicities of Micronesia feel closer to some groups in Melanesia, or the Philippines. A good example of this are the Yapese people who are related to Austronesian tribes in the northern Philippines.[49] Genetics also show a significant number of Micronesian have Japanese paternal ancestry: 9.5% of males from Micronesia as well as 0.2% in East Timor carry the Haplogroup D-M55.[50]
There are also substantial Asian communities found across the region, most notably in the Northern Mariana Islands where they form the majority and smaller communities of Europeans who have migrated from the United States or are descendants of settlers during European colonial rule in Micronesia.
Though they are all geographically part of the same region, they all have very different colonial histories. The US-administered areas of Micronesia have a unique experience that sets them apart from the rest of the Pacific. Micronesia has great economic dependency on its former or current motherlands, something only comparable to the French Pacific. Sometimes, the term American Micronesia is used to acknowledge the difference in cultural heritage.[51]
A 2011 survey found that 93.1% of Micronesian are Christians;[52] a survey in 2022 showed that 99% were Christian.[53]
It is thought that ancestors of the Carolinian people may have originally immigrated from the Asian mainland and Indonesia to Micronesia around 2,000 years ago. Their primary language is Carolinian, called Refaluwasch by native speakers, which has a total of about 5,700 speakers. The Carolinians have a matriarchal society in which respect is a very important factor in their daily lives, especially toward the matriarchs. Most Carolinians are of the Roman Catholic faith.
The immigration of Carolinians to Saipan began in the early 19th century, after the Spanish reduced the local population of Chamorro natives to just 3,700. They began to immigrate mostly sailing from small canoes from other islands, which a typhoon previously devastated. The Carolinians have a much darker complexion than the native Chamorros.
The Chamorro people are the indigenous peoples of the Mariana Islands, which are politically divided between the United States territory of Guam and the United States Commonwealth of the Northern Mariana Islands in Micronesia. The Chamorro are commonly believed to have come from Southeast Asia at around 2000 BC. They are most closely related to other Austronesian natives to the west in the Philippines and Taiwan, as well as the Carolines to the south.
The Chamorro language is included in the Malayo-Polynesian subgroup of the Austronesian family. Because Guam was colonized by Spain for over 300 years, many words derive from the Spanish language. The traditional Chamorro number system was replaced by Spanish numbers.[54]
The Chuukese people are an ethnic group in Oceania. They constitute 48% of the population of the Federated States of Micronesia. Their language is Chuukese. The home atoll of Chuuk is also known by the former name Truk.
The Nauruan people are an ethnicity inhabiting the Pacific island of Nauru. They are most likely a blend of other Pacific peoples.[55]
The origin of the Nauruan people has not yet been finally determined. It can possibly be explained by the last Malayo-Pacific human migration (c. 1200). It was probably seafaring or shipwrecked Polynesians or Melanesians that established themselves in Nauru because there was not already an indigenous people present, whereas the Micronesians were already crossed with the Melanesians in this area.
The roughly 3000 residents of the Federated States of Micronesia that reside in Kapingamarangi, nicknamed 'Kapings', live in one of the most remote locations in both Micronesia and the world at large. Their home atoll is almost 320 km (200 mi) from the nearest point of immigration.[56]  There are no regular flights; the only reliable way to legally visit is to travel on a high-speed sailboat to the atoll. Owing to this difficulty, few sailors travelling the Pacific attempt to visit. The local language is the Kapingamarangi language. From the 1970s, to attend high school the children needed to travel to Pohnpei, bringing their parents with them to create communities of Kapings on the island.[57]
There are large East, South and Southeast Asian communities found across certain Micronesian countries that are either immigrants, foreign workers or descendants of either one, most migrated to the islands during the 1800s and 1900s.[58] According to the 2010 census results Guam was 26.3% Filipino, 2.2% Korean, 1.6% Chinese and 2% other Asian.[59] The 2010 census showed the Northern Mariana Islands was 50% Asian of which 35.3% were Filipino, 6.8% Chinese, 4.2% Korean and 3.7% other Asian (mainly Japanese, Bangladeshi and Thai).[60] The 2010 census for the Federated States of Micronesia showed 1.4% were Asian while statistics for Nauru showed 8% of Nauruans were Chinese.[61][62] The 2005 census results for Palau showed 16.3% were Filipino, 1.6% Chinese, 1.6% Vietnamese and 3.4% other Asian (mostly Bangladeshi, Japanese and Korean).[63]
Japanese rule in Micronesia also led to Japanese people settling the islands and marrying native spouses. Kessai Note, the former president of the Marshall Islands has partial Japanese ancestry by way of his paternal grandfather, and Emanuel Mori, the former president of the Federated States of Micronesia, is descended from one of the first settlers from Japan, Koben Mori.
A significant number of Micronesians were shown to have paternal genetic relations with Japanese Haplogroup D-M55. Genetic testing found that 9.5% of males from Micronesia as well as 0.2% in East Timor[64] carry what is believed to reflect recent admixture from Japan. That is, D-M116.1 (D1b1) is generally believed to be a primary subclade of D-M64.1 (D1b), possibly as a result of the Japanese military occupation of Southeast Asia during World War II.[50]
The 2010 census results of Guam showed 7.1% were white while the 2005 census for Nauru showed 8% were European. Smaller numbers at 1.9% in Palau and 1.8% in the Northern Mariana Islands were recorded as "white". In conjunction to the European communities there are large amounts of mixed Micronesians, some of which have European ancestry.
The largest group of languages spoken in Micronesia are the Micronesian languages. They are in the family of Oceanic languages, part of the Austronesian language group. They descended from the Proto-Oceanic, which in turn descended via Proto-Malayo-Polynesian from Proto-Austronesian.
The languages in the Micronesian family are Marshallese, Gilbertese, Kosraean, Nauruan, as well as a large sub-family called the Chuukic–Pohnpeic languages containing 11 languages.
On the eastern edge of the Federated States of Micronesia, the languages Nukuoro and Kapingamarangi represent an extreme westward extension of the Polynesian branch of Oceanic.
Finally, there are two Malayo-Polynesian languages spoken in Micronesia that do not belong to the Oceanic languages: Chamorro in the Mariana Islands and Palauan in Palau.
By the time Western contact occurred, although Palau did not have dogs, they did have fowls and possibly pigs. Pigs are not native to Micronesia. Fruit bats are native to Palau, but other mammals are rare. Reptiles are numerous and both mollusks and fish are an important food source.[65] The people of Palau, the Marianas and Yap often chew betel nuts seasoned with lime and pepper leaf. Western Micronesia was unaware of the ceremonial drink, which was called saka on Kosrae and sakau on Pohnpei.[29]
The book Prehistoric Architecture in Micronesia argues that the most prolific pre-colonial Micronesian architecture is "Palau's monumental sculpted hills, megalithic stone carvings and elaborately decorated structure of wood placed on piers above elevated stone platforms".[66] The archeological traditions of the Yapese people remained relatively unchanged even after the first European contact with the region during Magellan's 1520s circumnavigation of the globe.[29]
Micronesia's artistic tradition has developed from the Lapita culture. Among the most prominent works of the region is the megalithic floating city of Nan Madol. The city began in 1200 CE and was still being built when European explorers begin to arrive around 1600. The city, however, had declined by around 1800 along with the Saudeleur dynasty and was completely abandoned by the 1820s. During the 19th century, the region was divided between the colonial powers, but art continued to thrive. Wood-carving, particularly by men, flourished in the region, resulted in richly decorated ceremonial houses in Belau, stylized bowls, canoe ornaments, ceremonial vessels and sometimes sculptured figures. Women created textiles and ornaments such as bracelets and headbands. Stylistically, traditional Micronesian art is streamlined and of a practical simplicity to its function, but is typically finished to a high standard of quality.
[67] This was mostly to make the best possible use of what few natural materials they had available to them.[68]
The first half of the 20th century saw a downturn in Micronesia's cultural integrity and a strong foreign influence from both western and Japanese Imperialist powers. A number of historical artistic traditions, especially sculpture, ceased to be practiced, although other art forms continued, including traditional architecture and weaving. Independence from colonial powers in the second half of the century resulted in a renewed interest in, and respect for, traditional arts. A notable movement of contemporary art also appeared in Micronesia towards the end of the 20th century.[69]
The cuisine of the Mariana Islands is tropical in nature, including such dishes as kelaguen as well as many others.
Marshallese cuisine comprises the fare and foodways of the Marshall Islands, and includes local foods such as breadfruit, taro root, pandanus and seafood, among others.
Palauan cuisine includes local foods such as cassava, taro, yam, potato, fish and pork. Western cuisine is favored among young Palauans.
The educational systems in the nations of Micronesia vary depending on the country and there are several higher-level educational institutions.
The CariPac consists of institutions of higher education in Guam, the Northern Mariana Islands, American Samoa, Puerto Rico, the U.S. Virgin Islands, the Federated States of Micronesia, the Marshall Islands and Palau. The Agricultural Development in the American Pacific is a partnership of the University of Hawaii, American Samoa Community College, College of Micronesia, Northern Marianas College and the University of Guam.
In the Federated States of Micronesia, education is required for citizens aged 6 to 13,[70] and is important to their economy.[71] The literacy rate for citizens aged 15 to 24 is 98.8%.[72] The College of Micronesia-FSM has a campus in each of the four states with its national campus in the capital city of Palikir, Pohnpei. The COM-FSM system also includes the Fisheries and Maritime Institute (FMI) on the Yap islands.[73][74]
The public education in Guam is organized by the Guam Department of Education. Guam also has several educational institutions, such as University of Guam, Pacific Islands University and Guam Community College, There is also the Guam Public Library System and the Umatac Outdoor Library.
Weriyeng[75] is one of the last two schools of traditional navigation found in the central Caroline Islands in Micronesia, the other being Fanur.[76]
The Northern Marianas College is a two-year community college located in the United States Commonwealth of the Northern Mariana Islands (CNMI).
The College of the Marshall Islands is a community college in the Marshall Islands.
Understanding Law in Micronesia notes that The Federated States of Micronesia's laws and legal institutions are "uninterestingly similar to [those of Western countries]". However, it explains that "law in Micronesia is an extraordinary flux and flow of contrasting thought and meaning, inside and outside the legal system". It says that a knee-jerk reaction would be that law is disarrayed in the region and that improvement is required, but argues that the failure is "one endemic to the nature of law or to the ideological views we hold about law".[77]
The Trust Territory of the Pacific Islands, a United Nations Trusteeship administered by the United States, borrowed heavily from United States law in establishing the Trust Territory Code during the Law and Development movement of the late 1950s and early 1960s. Many of those provisions were adopted by the new Congress of the Federated States of Micronesia when the Federated States of Micronesia became self-governing in 1979.[77]
In September 2007, journalists in the region founded the Micronesian Media Association.[78]
Micronesian music is influential to those living in the Micronesian islands.[79] Some of the music is based around mythology and ancient Micronesian rituals. It covers a range of styles from traditional songs, handed down through generations, to contemporary music.
Traditional beliefs suggest that the music can be presented to people in dreams and trances, rather than being written by composers themselves. Micronesian folk music is, like Polynesian music, primarily vocal-based.
In the Marshall Islands, the roro is a kind of traditional chant, usually about ancient legends and performed to give guidance during navigation and strength for mothers in labour. Modern bands have blended the unique songs of each island in the country with modern music. Though drums are not generally common in Micronesian music, one-sided hourglass-shaped drums are a major part of Marshallese music.[80] There is a traditional Marshallese dance called beet, which is influenced by Spanish folk dances; in it, men and women side-step in parallel lines. There is a kind of stick dance performed by the Jobwa, nowadays only for very special occasions.
Popular music, both from Micronesia and from other areas of the world, is played on radio stations in Micronesia.[79]
The region is home to the Micronesian Games.[81] This quadrennial international multi-sport event involves all of Micronesia's countries and territories except Wake Island.
Nauru has two national sports, weightlifting and Australian rules football.[82] According to 2007 Australian Football League International Census figures, there are around 180 players in the Nauru senior competition and 500 players in the junior competition,[83] representing a participation rate of over 30% overall for the country.
The predominant religion in Micronesia is Christianity (93%).[52]  According to 2023 government statistics, 55% of the population were Catholic and 42% were Protestant, while 2% belonged to other Christian denominations. Other religious groups exist including Baha’is, Buddhists, Hindus, Jews, and Muslims.[53]
Micronesian mythology comprises the traditional belief systems of the people of Micronesia. There is no single belief system in the islands of Micronesia, as each island region has its own mythological beings. It was noted that 2.7% of the population followed folk religions in 2014.[53]
There are several significant figures and myths in the traditions of the Federated States of Micronesia, Nauru, and Kiribati.
Shinto shrines dating from during or after World War II exist in some Micronesian countries.[84]

The History of Madagascar started from the ancient supercontinent of Pangaea, containing amongst others the African continent and the Indian subcontinent, and by the island's late colonization by human settlers from the Sunda Islands (Malay Archipelago) and from East Africa.[1] 
These two factors facilitated the evolution and survival of thousands of endemic plant and animal species, some of which have gone extinct or are currently threatened with extinction.
Trade in the Indian Ocean at the time of first colonization of Madagascar was dominated by Indonesian ships, probably of Borobudur ship and K'un-lun po types.[2][3]
Over two thousand years, the island has received waves of settlers of diverse origins, primarily Austronesian and Bantu.[4] Centuries of intermarriages between both groups created the Malagasy people, who are roughly an equal mixture of both groups. They speak the Malagasy, an Austronesian language with Bantu, French and Arabic influences.[5][6][7][8]
By the Middle Ages, over a dozen distinct ethnic identities had emerged on the island, typified by rule under a local chieftain. Some communities, such as the Sakalava, Merina and Betsimisaraka, were unified by leaders who established kingdoms, which gained wealth and power through commerce with Europeans and Arabs. Between the 16th and 18th centuries, pirate activity in the coastal areas of Madagascar was common. The Sakalava and Merina kingdoms in particular exploited European trade to strengthen the power of their kingdoms, trading Malagasy slaves for European firearms and other goods.  Beginning in the early 19th century, the British and French competed for influence in Madagascar.
By the turn of the 19th century, King Andrianampoinimerina had reunited the highly populous Kingdom of Imerina in the central highlands, with its capital at Antananarivo. His son Radama I the Great expanded its authority to the island's other polities and was the first Malagasy sovereign to be recognized by foreign states as the ruler of the greater Merina Kingdom. During the rule of Queen Ranavalona I (r. 1828–1861), the kingdom was further expanded to encompass most of the island. Madagascar's population is estimated to have declined by half from 5 million to 2.5 million between 1833 and 1839 from war, disease, slavery and other and violence. She also attempted to eradicate European and Christian influence in the country.[9]
Christianity was made the state religion under Queen Ranavalona II (r. 1868–1883). In the 1880s, Britain recognised France's authority on the island. This led in 1890 to the Malagasy Protectorate, which was however rejected by the Kingdom of Madagascar, which led to the two Franco-Hova Wars which ended with France capturing the capital in September 1895. Conflict continued in the Menalamba rebellion against French rule that was defeated in 1897. The monarchy was dissolved, and the queen was exiled. Following conquest, the French abolished slavery in 1896, freeing approximately 500,000 slaves.[10]
During French rule, Malagasy people were required to fulfill corvée labor on French-run plantations while access to education or skilled positions were limited, although basic services like schools and clinics were extended across the island. Several militant nationalist secret societies emerged in opposition to French rule, of which the most prominent was Vy Vato Sakelika formed in 1913. Many Malagasy were conscripted to fight for France during the First (1914–1918) and Second World Wars (1939–1945), and during the latter Madagascar came under Vichy French control before being captured by the British in the Battle of Madagascar and returned to Free French control in 1942. In 1944, Madagascar became an overseas territory with representatives in the French National Assembly. Militant nationalists launched a large uprising in 1947 that was brutally suppressed by 1949.
The country gained full independence from France in 1960. Madagascar's First Republic (1960–1972) was established as a democratic system modeled on that of France and led by President Philibert Tsiranana. Popular unrest led to the socialist Democratic Republic of Madagascar under Admiral Didier Ratsiraka (1975–1992) distinguished by economic isolationism and political alliances with pro-Soviet states. By 1992, free and fair multiparty elections were held, ushering in the democratic Third Republic (1992–2009). Under the new constitution, the Malagasy public elected successive presidents Albert Zafy, Didier Ratsiraka, and Marc Ravalomanana. This latter was ousted in the 2009 Malagasy political crisis by a popular movement under the leadership of Andry Rajoelina. Elections were held on December 20, 2013, to elect a new president and return the country to constitutional governance.
The earliest unambiguous evidence of continuous human presence in Madagascar was found at Andavakoera and dates to 490 CE,[11] and there is no archaeological evidence for human occupation in the highlands until around 1200. However, there is scattered evidence for much earlier human visits.
In 2009, archaeological excavations at Christmas River (south-central Madagascar) by Pat Wright and James Hansford located a purported elephant bird kill site, with bones showing human cut marks. These were dated to 8,500 BCE, but as yet there is no indication as to the identity of the hunters.[12][13] Archaeological finds such as cut marks on bones found in the northwest and stone tools in the northeast indicate that Madagascar was visited by foragers around 2000 BCE.[14][15]
There is potential evidence in the form of a cutmarked subfossil lemur bone from a palaeontological site, Taolambiby, in the southwest. One date was obtained, calibrated 530 to 300 BC (Godfrey & Jungers 2003). The cutmarking looks plausible, but there is a potential problem of old carbon from the limestone landscape compromising the date, and there are no associated artifacts or archaeological sites in the vicinity. Nearly contemporaneous potential evidence comes from cannabis or humulus pollen which occurs in a pollen column from the central highlands at an interpolated date of c. 2200 Before Present (BP).[16] There is a hypothesis that cannabis may have reached Africa 3000 years ago.
Necho II's Phoenician expedition c. 595 BCE circumnavigated Africa but did not see Madagascar when passing through the Mozambique Channel, as it stayed within sight of the African mainland. The island was likely uninhabited.[17]
Finally, a cutmarked pygmy hippo bone from Ambolisatra has been dated and calibrated to between 60 BC and 130 AD (2 SDs), but it is from a coastal swamp without indications of settlement in a heavily karstic region. Moreover, a similar bone from the same collection from a nearby site gave two widely divergent dates of 2020 and 3495 BC (MacPhee & Burney 1991). Transient visits to Madagascar that did not result in enduring settlement cannot be ruled out, and may have left some traces.[18]
Factual information about the peopling of Madagascar remains incomplete, but much recent multidisciplinary research and work in archaeology,[19] genetics,[20] linguistics,[21][22][23] and history[24][25][26][27] confirms that the Malagasy people were originally and overwhelmingly Austronesian peoples native to the Sunda Islands. They probably arrived on the west coast of Madagascar with outrigger canoes (waka) at the beginning of our era or as much as 300 years sooner according to archaeologists,[28] and perhaps even earlier under certain geneticists' assumptions.[29] On the basis of plant cultigens, Blench proposed the migrations occurred "at the earliest century BCE".[30]: 432  Archaeological work of Ardika and Bellwood suggests migration between 500 and 200 BCE.[31][32]
The Borobudur Ship Expedition in 2003–2004 affirmed scholars' ideas that ships from ancient Indonesia could have reached Madagascar and the west African coast for trade from the 8th century and after. A traditional Borobudur ship with outriggers was reconstructed and sailed in this expedition from Jakarta to Madagascar and Ghana.[33] As for the ancient route, one possibility is that Indonesian Austronesians came directly across the Indian Ocean from Java to Madagascar. It is likely that they went through the Maldives where evidence of old Indonesian boat design and fishing technology persists until the present.[34] The Malagasy language originated from the Southeast Barito language, and Ma'anyan language is its closest relative, with numerous Malay and Javanese loanwords.[35][36] It is known that Ma'anyan people were brought as laborers and slaves by Malay and Javanese people in their trading fleets, which reached Madagascar by ca. 50–500 AD.[37][38] These pioneers are known in the Malagasy oral tradition as the Ntaolo, from Proto-Malayo-Polynesian *tau-ulu, literally 'first men', from *tau, 'man', and *ulu, 'head; first; origin, beginning.[39] It is likely that those ancient people called themselves *va-waka, "the canoe people" from Proto-Malayo-Polynesian *va, 'people', and *waka 'canoe'. Today the term vahoaka means 'people' in Malagasy.
The Southeast Asian origin of the first Malagasy people explains certain features common among the Malagasy, for instance, the epicanthic fold common among all Malagasy whether coastal or highlands, whether pale, dark or copper skinned. This original population (vahoaka ntaolo) can be called the "Proto-Malagasy". They are the source of:
As for the cause of the coming of these Austronesians, the history of the Indian Ocean from the early first millennium CE is still poorly understood. Madagascar may have played an important role in the trade of spices (especially cinnamon) and timber between Southeast Asia and the Middle East, directly or through the African coast and Madagascar.[citation needed]
The first known concentrated population of human settlers emerged along the southeastern coast of the island, although the first landfall may have been made on the northern coast.[9] Upon arrival, early settlers practiced tavy (slash-and-burn agriculture) to clear the virgin coastal rainforests for the cultivation of their crops.[43] The first settlers encountered Madagascar's wealth of megafauna, including giant lemurs, elephant birds, giant fossa and the Malagasy hippopotamus, which have since become extinct due to hunting and habitat destruction.[44]
By 600, groups of these early settlers had moved inland and began clearing the forests of the central highlands (Imerina), where they particularly planted taro (saonjo) and probably rice (vary). These Vahoaka Ntaolo, hunters-gatherers and farmers, who decided to settle "in the forest", especially in the forests of the central highlands are known by the tradition[45] as the Vazimba (from *ba/va-yimba- 'those of the forest', from *yimba- 'forest' in Proto–Southeast Barito, today barimba or orang rimba in Malay[46]). Rafandrana, an ancestor of the Merina royal dynasty, for example, is known to have been a Vazimba. Rafohy and Rangita, the two founding queens of the Merina royalty, were also called Vazimbas.[45]
On the other side, the fishermen who, from the beginning, remained on the southwestern coast (probably the coasts of the first landing) were, according to the linguists, probably originally called the Vezo (from *ba/va/be/ve-jau – "those of the coast", borrowed from Proto-Malayo-Javanese, today veju in Bugis, bejau in Malay, and bajo in Javanese[23]), which today is still the name of a Southwestern tribe.
After the arrival of the newcomers (see below), as growing population density necessitated higher crop yields, irrigated rice paddies emerged in Betsileo country by 1600 and were complemented with terraced paddies throughout the central highlands a century later.[47] Zebu were introduced around 1000 by Bantu-speaking migrants from the African Great Lakes region (see below), who maintained large herds. The rising intensity of land cultivation and the ever-increasing demand for zebu pasturage in the central highlands had largely transformed the region from a forest ecosystem to barren grassland by the 17th century.[48]
By the mid-first millennium (ca 700) until about 1500, the inner Vazimbas as much as the coastal Vezos clans welcomed new visitors or immigrants. These goods and/or slave traders came from the Middle East (Shirazi Persians, Omani Arabs, Arab Jews), Africa (Swahilis), and from Asia (Gujaratis, Malays, Javanese, Bugis). They were sometimes integrated within the coastal Vezos and the inner Vazimbas clans.[49]
The written history of Madagascar begins in the 7th century when Omanis established trading posts along the northwest coast and introduced Islam, the Arabic script (used to transcribe the Malagasy language in a form of writing known as the sorabe alphabet), Arab astrology and other cultural elements.[50] During this early period, Madagascar served as an important transoceanic trading port for the East African coast that gave Africa a trade route to the Silk Road and served simultaneously as a port for incoming ships. There is evidence that Bantu or Swahili sailors or traders may have begun sailing to the western shores of Madagascar as early as around the 6th and 7th century.[51]
According to the traditions of some Malagasy peoples, the first Bantus and Arabs to settle in Madagascar came as refugees from the civil wars that followed the death of Muhammad in 632.[52]
Beginning in the 10th or 11th century, Arabic and Zanzibari slavers worked their way down the Swahili coast in their dhows and established settlements on the west coast of Madagascar. Notably they included the Zafiraminia, traditional ancestors of the Antemoro, Antanosy and other east coast ethnicities. The last wave of Arab immigrants, the Antalaotra, immigrated from Swahili colonies. They settled the northwest of the island (the Mahajanga area) and introduced, for the first time, Islam to Madagascar.[52]
Arab immigrants, though few in number compared to the native Austronesians and Bantus, nevertheless left a lasting impression. The Malagasy names for seasons, months, days, and coins in certain regions come from Arabic origins,[53][54] as do cultural features such as the practice of circumcision, the communal grain-pool, and different forms of salutation (such as salama).
According to oral tradition,[55] new Austronesian clans (Malays, Javanese, Bugis, and Orang Laut),[56] historically referred to in general, regardless of their native island, as the "Hova"[45] (from Old Bugis uwa, "commoner") landed in the north-west and east coast of the island. Adelaar's observations of Old Malay (Sanskritised), Old Javanese (Sanskritised) and Old Bugis borrowings in the initial Proto-Southeast-Barito language indicate that the first Hova waves came probably in the 7th century at the earliest.[57] Marre and Dahl pointed out that the number of Sanskrit words in Malagasy is very limited compared with the large number now found in Indonesian languages, which means that the Indonesian settlers must have come at an early stage of Hindu influence, that is ca. 400 AD.[58]
The Hova were probably derived from Indonesian thalassocracies. Their leaders were known as the diana in the Southeast and andriana or raondriana in the Center and the West[45][59][60] (from (ra)-hadi-an, "lord" or "master" in Old Javanese,[22] modern Javanese raden, also found in the Bugis noble title andi and the Tagalog word for "king" hari). They for the most part allied with Vazimba clans:[61][62]
With the arrival of Islam, Persian and Arab traders soon supplanted the Indonesians on the coast of Africa and eventually extended their control over the Comoro Islands and parts of the coast of Madagascar. Meanwhile, with competition in the new joint naval powers of Song China and Chola South India, the thalassocracies of Indonesia were in rapid decline, though the Portuguese still encountered Javanese sailors in Madagascar in the sixteenth century.
There is archaeological evidence that Bantu peoples, agro-pastoralists from East Africa, may have begun migrating to the island as early as the 6th and 7th century.[51] Other historical and archaeological records suggest that some of the Bantus were descendants of Swahili sailors and merchants who used dhows to traverse the seas to the western shores of Madagascar.[64] Finally some sources theorize that during the Middle Ages, Arab, Persian and Neo-Austronesian slave-traders[49] brought Bantu people to Madagascar transported by Swahili merchants to feed foreign demand for slaves.[65] Years of intermarriages created the Malagasy people, who primarily speak Malagasy, an Austronesian language with Bantu influences.[66] There are consequently many (Proto-)Swahili borrowings in the initial Proto-SEB Malagasy language.[67] This substratum is especially significantly present in the domestic and agricultural vocabulary (e.g. omby or aombe, "beef", from Swahili ng'ombe; tongolo "onion" from Swahili kitunguu; Malagasy nongo "pot" from nunggu in Swahili[22]).
Europe knew of Madagascar through Arab sources; thus The Travels of Marco Polo claimed that "the inhabitants are Saracens, or followers of the law of Mohammed", without mentioning other inhabitants. Other than its size and location, everything about the island in the book describes southeastern Africa, not Madagascar. European contact began on 10 August 1500, when the Portuguese sea captain Diogo Dias sighted the island after his ship separated from a fleet going to India.[68][17] The Portuguese traded with the islanders and named the island São Lourenço (Saint Lawrence). In 1666, François Caron, the director general of the newly formed French East India Company, sailed to Madagascar.[69] The company failed to establish a colony on Madagascar but established ports on the nearby islands of Bourbon (now Réunion) and Isle de France (now Mauritius). In the late 17th century, the French established trading posts along the east coast. On Île Sainte-Marie, a small island off the northeastern coast of Madagascar, Captain Misson and his pirate crew allegedly founded the famous pirate utopia of Libertatia in the late 17th century. From about 1774 to 1824, Madagascar was a favourite haunt for pirates.  Many European sailors were shipwrecked on the coasts of the island, among them Robert Drury, whose journal is one of the few written depictions of life in southern Madagascar during the 18th century.[70] Sailors sometimes called Madagascar "Island of the Moon".[71]
By the 15th century, Europeans had wrested control of the spice trade from the Muslims. They did this by bypassing the Middle East and sending their cargo-ships around the Cape of Good Hope to India. The Portuguese mariner Diogo Dias became the first European to set foot on Madagascar when his ship, bound for India, blew off course in 1500. In the ensuing 200 years, the English and French tried (and failed) to establish settlements on the island.
Fever, dysentery, hostile Malagasy, and the trying arid climate of southern Madagascar soon terminated the English settlement near Toliara in 1646. Another English settlement in the north in Île Sainte-Marie came to an end in 1649. The French colony at Tôlanaro (Fort Dauphin) fared a little better: it lasted thirty years. On Christmas night 1672, local Antanosy tribesmen, perhaps angry because fourteen French soldiers in the fort had recently divorced their Malagasy wives to marry fourteen French orphan-women sent out to the colony, massacred the fourteen grooms and thirteen of the fourteen brides. The Antanosy then besieged the stockade at Tôlanaro for eighteen months. A ship of the French East India Company rescued the surviving thirty men and one widow in 1674.
In 1665, François Caron, the Director General of the newly formed French East India Company, sailed to Madagascar. The Company failed to found a colony on Madagascar but established ports on the nearby islands of Bourbon and Île-de-France (today's Réunion and Mauritius respectively). In the late 17th century, the French established trading-posts along the east coast.[53]
Between 1680 and 1725, Madagascar became a pirate stronghold. Many unfortunate sailors became shipwrecked and stranded on the island. Those who survived settled down with the natives, or more often, found French or English colonies on the island or even pirate havens and thus became pirates themselves. One such case, that of Robert Drury,[72]
resulted in a journal giving one of the few written depictions of southern Madagascar in the 18th century.
Notable pirates including William Kidd, Henry Every, John Bowen, and Thomas Tew made Antongil Bay and Île Sainte-Marie (a small island 12 miles off the northeast coast of Madagascar) their bases of operations. The pirates plundered merchant ships in the Indian Ocean, the Red Sea, and the Persian Gulf. They deprived Europe-bound ships of their silks, cloth, spices, and jewels. Vessels captured going in the opposite direction (to India) lost their coin, gold, and silver. The pirates robbed the Indian cargo ships that traded between ports in the Indian Ocean as well as ships commissioned by the East India Companies of France, England, and the Netherlands. The pilgrim fleet sailing between Surat in India and Mocha on the tip of the Arabian Peninsula were a favorite target, because the wealthy Muslim pilgrims often carried jewels and other finery with them to Mecca. Merchants in India, various ports of Africa, and Réunion showed willingness to fence the pirates' stolen goods. The low-paid seamen who manned merchant ships in the Indian Ocean hardly put up a fight, seeing as they had little reason or motivation to risk their lives. The pirates often recruited crewmen from the ships they plundered.
With regard to piracy in Malagasy waters, note the (semi-)legendary accounts of the alleged pirate-state of Libertalia.
Prior to the arrival of the Europeans, certain Malagasy tribes occasionally waged wars to capture and enslave prisoners. They either sold the slaves to Arab traders or kept them on-hand as laborers. Following the arrival of European slavers, human slaves became more valuable, and the coastal tribes of Madagascar took to warring with each other to obtain prisoners for the lucrative slave-trade. Instead of spears and cutlasses, the tribesmen fought with muskets, musket-balls, and gunpowder that they obtained from the Europeans, conducting fierce and brutal wars. On account of their relationship to the pirates, the Betsimisaraka in eastern Madagascar had more firearms than anyone else. They overpowered their neighbors, the Antankarana and Tsimihety, and even raided the Comoro Islands. As the tribe on the west coast with the most connections to the slave trade, the Sakalava people also had access to guns and powder.
Today, the people of Madagascar can be considered as the product of mixing between the first occupants, the vahoaka ntaolo Austronesians (Vazimba and Vezo) and those arrived later (Hova neo-Austronesians, Persians, Arabs, Africans and Europeans).
Genotypically, the original Austronesian heritage is more or less evenly distributed throughout the island. Researchers have noticed the "Polynesian motif" everywhere:[73] an old marker of Austronesian populations from before the great immigration to the islands of Polynesia and Melanesia. This fact would require a starting common home among the Proto-Malagasy vahoaka ntaolo (gone west to Madagascar) and the ancestors of the current Polynesians (left for the Pacific Islands in the East) between 500 BCE – 1 CE.
Those new immigrants of the Middle Ages were a minority in numbers, yet their cultural contributions, political and technological to the neo-Vazimba and neo-Vezo world substantially altered their society and is the cause of the major upheavals of the sixteenth century that led to the Malagasy feudal era.
On the coasts, the integration of the East Asians, Middle Easterns, Bantus and Portuguese led to the establishment of the kingdoms of the Antakarana, Boina, Menabe and Vezo on the west coast, the Mahafaly and Antandroy in the south, and the Antesaka, Antambahoaka, Antemoro, Antanala and Betsimisaraka on the east coast.
In the interior, the struggle for hegemony between the different Neo-Vazimba clans of central highlands, called the Hova by the coastal Neo-Vezo clans, led to the creation of the Merina, Betsileo, Bezanozano, Sihanaka, Tsimihety and Bara kingdoms.
The birth of these kingdoms/tribes essentially altered the political structure of the ancient world of the Vahoaka Ntaolo, but for the most part the common language, customs, traditions, religion and economy was preserved.
Among the Central Kingdoms, the most important were the Betsileo kingdoms (Fandriana, Fisakana, Manandriana, Isandra) to the south, and the Merina kingdoms to the north. These were definitively unified in the early 19th century by Andrianampoinimerina. His son and successor Radama I (reigning 1810–1828) opened his country to foreign influence. With the support of the British, he extended its authority over much of the island. From 1817, the central Merina kingdoms, Betsileo, Bezanozano, and Sihanaka, unified by Radama I were known to the outside world as the Kingdom of Madagascar.
The island's West clan chiefs began to extend their power through trade with their Indian Ocean neighbors, first with Arab, Persian and Somali traders who connected Madagascar with East Africa, the Middle East and India, and later with European slave traders.[74] The wealth created in Madagascar through trade created a state system ruled by powerful regional monarchs known as the Maroserana. These monarchs adopted the cultural traditions of subjects in their territories and expanded their kingdoms. They took on divine status, and new nobility and artisan classes were created.[75] Madagascar functioned as a contact port for the other Swahili seaport city-states such as Sofala, Kilwa, Mombasa and Zanzibar. By the Middle Ages, large chiefdoms began to dominate considerable areas of the island. Among these were the Betsimisaraka alliance of the eastern coast and the Sakalava chiefdoms of the Menabe (centered in what is now the town of Morondava) and of Boina (centered in what is now the provincial capital of Mahajanga). The influence of the Sakalava extended across what are now the provinces of Antsiranana, Mahajanga and Toliara.
The island's chiefs began to extend their power through trade with their Indian Ocean neighbours, notably East Africa, the Middle East and India. Large chiefdoms began to dominate considerable areas of the island. Among these were the Sakalava chiefdoms of the Menabe, centred in what is now the town of Morondava, and of Boina, centered in what is now the provincial capital of Mahajanga (Majunga). The influence of the Sakalava extended across what are now the provinces of Antsiranana, Mahajanga and Toliara.
According to local tradition, the founders of the Sakalava kingdom were Maroseraña (or Maroseranana, "those who owned many ports") princes, from the Fiherenana (now Toliara). They quickly subdued the neighbouring princes, starting with the southern ones, in the Mahafaly area. The true founder of Sakalava dominance was Andriamisara; his son Andriandahifotsy (c. 1610–1658) then extended his authority northwards, past the Mangoky River. His two sons, Andriamanetiarivo and Andriamandisoarivo, extended gains further up to the Tsongay region (now Mahajanga). At about that time, the empire's unity starts to split, resulting in a southern kingdom (Menabe) and a northern kingdom (Boina). Further splits resulted, despite continued extension of the Boina princes' reach into the extreme north, in Antankarana country.
The Sakalava rulers of this period are known through the memoirs of Europeans such as Robert Drury, James Cook, Barnvelt (1719), Valentyn (1726).
King Andrianampoinimerina (1785–1810) and his son, Radama I (1810–1828) succeeded in uniting nearly all of Madagascar under Merina rule.  These kings and their successors descended from a line of ancient Merina royalty who ruled the lands of Imerina in the central Highlands of Madagascar since at least the 16th century. Even prior to their eventual domination and unification of the entire island, the political and cultural activities of Merina royalty were to leave an indelible mark on contemporary Malagasy identity.
With the establishment of dominion over the greater part of the Highlands, Andrianampoinimerina became the first Merina monarch to be considered a king of Madagascar.  The island continued to be ruled by a succession of Merina monarchs until the last of them, Ranavalona III, was deposed and exiled to Algeria by French forces who conquered and colonized the island in 1895.
Andrianampoinimerina, grandson of King Andriambelomasina and successor to his uncle King Andrianjafy, successfully reunited the fragmented Merina kingdom through a combination of diplomacy, strategic political marriages and successful military campaigns against rival princes.  Andrianampoinimerina distinguished himself from other kings by codifying laws and supervising the building of dikes and trenches to increase the amount of arable land around his capital at Antananarivo in a successful bid to end the famines that had wracked Imerina for decades. The king ambitiously proclaimed: Ny ranomasina no valapariako (“the sea is the boundary of my rice-field”), and by the time of his death in 1810 he had conquered the Bara and Betsileo highland tribes, laying the groundwork for expansion of his kingdom to the shores of the island.
Andrianampoinimerina's son Radama I (Radama the Great) assumed the throne during a turning-point in European history that had repercussions for Madagascar. With the defeat of Napoléon in 1814/1815, the balance of power in Europe and overseas shifted in Britain's favor. The British, eager to exert control over the trade routes of the Indian Ocean, had captured the islands of Réunion and Mauritius from the French in 1810. Although they returned Réunion to France, they kept Mauritius as a naval base which would maintain trade links throughout the British Empire. Mauritius's governor, in a bid to woo Madagascar from French control, recognized Radama I as King of Madagascar, a diplomatic maneuver meant to underscore the idea of the sovereignty of the island and thus to preclude claims by any European powers.
Radama I signed treaties with the United Kingdom outlawing the slave trade and admitting Protestant missionaries into Madagascar. As a result of these treaties Protestant missionaries from Britain would spread British influence in Madagascar; while outlawing the slave trade would weaken Réunion's economy by depriving the island of slave laborers for France's sugar plantations. In return for outlawing the slave trade, Madagascar received what the treaty called "The Equivalent": an annual sum of a thousand dollars in gold, another thousand in silver, stated amounts of gunpowder, flints, and muskets, plus 400 surplus British Army uniforms. The governor of Mauritius also sent military advisers who accompanied and sometimes led Merina soldiers in their battles against the Sakalava and Betsimisaraka. In 1824, having defeated the Betsimisaraka, Radama I declared, "Today, the whole island is mine! Madagascar has but one master." The king died in 1828 while leading his army on a punitive expedition against the Betsimisaraka.
The 33-year reign of Queen Ranavalona I, the widow of Radama I, was characterized by an increase in the size of the Kingdom of Madagascar as it conquered neighboring states as well as an effort to maintain the cultural and political sovereignty of Madagascar in the face of increasing foreign influence. The queen repudiated the treaties that Radama I had signed with Britain and, in 1835 after issuing a royal edict prohibiting the practice of Christianity in Madagascar, she expelled British missionaries from the island and began persecuting Christian converts who would not renounce their religion. Malagasy Christians would remember this period as ny tany maizina, or "the time when the land was dark".
During her reign, constant warfare, disease, slave labor, and harsh measures of justice resulted in a high mortality rate among the Malagasy population; the population of the island is estimated to have declined by half from 5 million to 2.5 million between 1833 and 1839.[77]
Unbeknownst to the queen, her son and heir, the crown-prince (the future Radama II), attended Roman Catholic masses in secret.[citation needed] The young man grew up under the influence of French nationals in Antananarivo. In 1854, he wrote a letter to Napoléon III inviting France to invade and uplift Madagascar.[citation needed] On 28 June 1855 he signed the Lambert Charter. This document gave Joseph-François Lambert, an enterprising French businessman who had arrived in Madagascar only three weeks before, the exclusive right to develop all minerals, forests, and unoccupied land in Madagascar in exchange for a 10-percent royalty payable to the Merina monarchy. In years to come, the French would show the Lambert Charter and the prince's letter to Napoléon III to explain the Franco-Hova Wars and the annexation of Madagascar as a colony. In 1857, the queen uncovered a plot by her son (the future Radama II) and French nationals in the capital to remove her from power. She immediately expelled all foreigners from Madagascar, sparing her son. Ranavalona died in 1861.
In his brief two years on the throne, King Radama II re-opened trade with Mauritius and Réunion, invited Christian missionaries[68] and foreigners to return to Madagascar, and re-instated most of Radama I's reforms. His liberal policies angered the aristocracy, however, and Rainivoninahitriniony, the prime minister, engineered a coup d'état in which Radama II was strangled to death.
A council of princes headed by Rainilaiarivony approached Rabodo, the widow of Radama II, the day after the death of her husband.  They gave her the conditions under which she could succeed to the throne.  These conditions included the suppression of trial by ordeal as well as the monarchy's defense of freedom of religion. Rabodo, crowned queen on 13 May 1863 under the throne name of Rasoherina, reigned until her death on 1 April 1868.[78]
The Malagasy people remember Queen Rasoherina for sending ambassadors to London and Paris and for prohibiting Sunday markets. On 30 June 1865, she signed a treaty with the United Kingdom giving British citizens the right to rent land and property on the island and to have a resident ambassador. With the United States of America she signed a trade agreement that also limited the importation of weapons and the export of cattle.  Finally, with France the queen signed a peace between her descendants and the descendants of the Emperor of France.[79] Rasoherina married her prime minister, Rainivoninahitriniony, but public outcry against his involvement in the murder of Radama II soon forced his resignation and exile to Betsileo country south of Imerina.  She then married his brother, Rainilaiarivony, head of the army at the time of Radama II's murder who was promoted to the post of Prime Minister upon the resignation and exile of his older brother.  Rainilaiarivony would rule Madagascar from behind the scenes for the remaining 32 years of the Merina monarchy, marrying each of the final three queens of Madagascar in succession.
In 1869, Queen Ranavalona II, previously educated by the London Missionary Society, underwent baptism into the Church of England and subsequently made the Anglican faith the official state religion of Madagascar.[80]
The queen had all the sampy (traditional royal idols) burned in a public display. Catholic and Protestant missionaries arrived in numbers to build churches and schools. The reign of Queen Ranavalona II proved the high water mark of British influence in Madagascar.  British goods and weapons arrived on the island by way of South Africa.
Her public coronation as queen took place on 22 November 1883 and she took the name Ranavalona III.  As her first order of business she confirmed the nomination of Rainilaiarivony and his entourage in their positions.  She also promised to do away with the French threat.[81]
Angry at the cancellation of the Lambert Charter and seeking to restore property seized from French citizens, France invaded Madagascar in 1883 in what became known as the first Franco-Hova War (Hova as a name referring to the Merina aristocrats). At the war's end, Madagascar ceded Antsiranana (Diégo Suarez) on the northern coast to France and paid 560,000 gold francs to the heirs of Joseph-François Lambert. In Europe, meanwhile, European diplomats had worked out an agreement whereby Britain, in order to establish control over the Sultanate of Zanzibar, ceded its rights over the island of Heligoland to Germany and renounced all claims of influence in Madagascar in favor of France. The agreement spelled the end of the Madagascan political independence. Rainilaiarivony had succeeded in playing the various European powers against one another, but now France could act without fear of British support towards the Madagascans.[citation needed]
In 1895, a French flying column landed in Mahajanga (Majunga) and marched by way of the Betsiboka River to the capital, Antananarivo, taking the city's defenders by surprise (they had expected an attack from the much closer east coast). Twenty French soldiers died fighting and 6,000 died of malaria and other diseases before the second Franco-Hova War ended. In 1896 the French Parliament voted to annex Madagascar. The 103-year-old Merina monarchy ended with the royal family sent into exile in Algeria.
The kingdom of Madagascar continued its transformation throughout the 19th century from a locally grown monarchy into a modern state.
Before Radama I the Malagasy language was written in a script known as sorabe. In 1820 under the direction of David Jones, a Welsh missionary of the London Missionary Society, Radama I codified the new Malagasy Latin alphabet of 21 letters which replaced the old sorabe alphabet.[82] By 1830 the Bible was the first book written in this new Malagasy Latin alphabet. It is the oldest complete translation of the bible into a sub-Saharan African language.
The United States and the Kingdom of Madagascar concluded a commercial convention in 1867 after which Queen Rasoherina and Prime Minister Rainilaiarivoy exchanged gifts with president Andrew Johnson.[83] A treaty of peace, friendship, and commerce was then signed in 1881.[84]
During the reign of Ranavalona I, early attempts at industrialization took place from 1835 under the direction of the French Jean Laborde (a survivor of a shipwreck off the east coast), producing soap, porcelain, metal tools and firearms (rifles, cannons, etc.)..
In 1864 Antananarivo opened the first hospital and a modern medical school. Two years later appeared the first newspaper. A scientific journal in English (Antananarivo Annual) was released from 1875. In 1894, on the eve of the establishment of colonial rule, the schools of the kingdom, mainly led by the Protestant missions, were attended by over 200,000 students.
In 1750, the ruler of the Kingdom of Betsimisaraka, Bety of Betsimisaraka, ceded the island Nosy Boraha (Île Sainte-Marie) to the Kingdom of France. However, in 1752 the French Colonists were massacred when the local population rebelled. France left the settlement abandoned for roughly half a century until returning in 1818.[85]
In 1840 Tsiomeko, the ruler of Nosy Be island, accepted French protection in 1840. The French took possession of the island in 1841, and in 1849 an unsuccessful attempt was made to expel them.[86]
In the Berlin Treaty, the British accepted the claims of France to exert its influence on Madagascar, and after the first Franco-Hova Wara treaty of alliance between France and Madagascar was signed on 17 December 1885 by Queen Ranavalona III, granting France a protectorate over the Diego-Suarez bay and surrounding territory, as well as the islands of Nosy-Be and Île Sainte-Marie.
Disagreements on the implementation of this treaty served as a pretext for the French invasion of 1895, which first met little resistance. The authority of the prime minister Rainilaiarivony, in power since 1864, had become very unpopular with the public.
The British accepted the imposition of a French protectorate over Madagascar in 1890 in return for recognition of British sovereignty over Zanzibar (subsequently part of Tanzania) and as part of an overall definition of spheres of influence in the area.[87] The intention of the French was initially to maintain the protectorate in order to control the economy and foreign relations of the island. But later, the outbreak of the Menalamba rebellion and the arrival of General Gallieni (responsible for "pacifying" the country) in 1896 led to the colonization of the island and the exile of the queen to Algeria.
In 1904–1905 Madagascar was the scene of a large-scale uprising by various tribes and tribal leaders, among whom Kotavy, a former French corporal who defected to the rebels, filled a preponderant role.[8]
Malagasy troops fought in France, Morocco, and Syria during World War II. Prior to the implementation of the Final Solution, Nazi Germany had considered the Madagascar Plan, which would have relocated European Jews to Madagascar. After France fell to the Germans in 1940, the Vichy government administered Madagascar until 1942, when British and Commonwealth troops occupied the strategic island in the Battle of Madagascar. The United Kingdom handed over control of the island to Free French Forces in 1943.
In 1948, with French prestige at a low ebb, the French government, headed by Prime Minister Paul Ramadier of the French Section of the Workers' International (SFIO) party, suppressed the Madagascar revolt, a nationalist uprising.[88][89][90][91][92][93]
The French subsequently established reformed institutions in 1956 under the Loi Cadre (Overseas Reform Act), and Madagascar moved peacefully toward independence. The Malagasy Republic, proclaimed on 14 October 1958, became an autonomous state within the French Community. On 26 March 1960 France agreed to Madagascar becoming fully independent.[94] On 26 June 1960 Madagascar became an independent country and Philibert Tsiranana became its first president.
Tsiranana's rule represented continuation, with French settlers (or colons) still in positions of power.  Unlike many of France's former colonies, the Malagasy Republic strongly resisted movements towards communism.[95]
In 1972, protests against these policies came to a head and Tsiranana had to step down. He handed power to General Gabriel Ramanantsoa of the army and his provisional government. This régime reversed previous policy in favour of closer ties with the Soviet Union.[96]
On 5 February 1975, Colonel Richard Ratsimandrava became the President of Madagascar. After six days as head of the country, he died in an assassination while driving from the presidential palace to his home. Political power passed to Gilles Andriamahazo.
On 15 June 1975, Lieutenant-Commander Didier Ratsiraka (who had previously served as foreign minister) came to power in a coup.  Elected president for a seven-year term, Ratsiraka moved further towards socialism, nationalising much of the economy and cutting all ties with France.[96] These policies hastened the decline in the Madagascan economy that had begun after independence as French immigrants left the country, leaving a shortage of skills and technology behind. Ratsiraka's original seven-year term as president continued after his party (Avant-garde de la Révolution Malgache or AREMA) became the only legal party in the 1977 elections.[95]
In the 1980s, Madagascar moved back towards France, abandoning many of its communist-inspired policies in favour of a market economy, though Ratsiraka still kept hold of power.[53]
Eventually, opposition, both within and without, forced Ratsiraka to consider his position and in 1992 the country adopted a new and democratic constitution.[96]
The first multi-party elections came in 1993, with Albert Zafy defeating Ratsiraka.[95] Despite being a strong proponent of a liberal, free-market economy, Zafy ran on a ticket critical of the International Monetary Fund (IMF) and the World Bank. During his presidency the country struggled to implement IMF and World Bank guidelines that were, on the short term, suicidal politically.[97]
As president Zafy was frustrated by the restraints placed upon the powers of his office by the new constitution. His quest for increased executive power put him on a collision course with the parliament led by then prime minister Francisque Ravony.[98] Zafy eventually won the power he sought after but suffered impeachment at the hands of the disenfranchised parliament in 1996 for violating the constitution by refusing to promulgate specific laws.[99]
The ensuing elections saw a turnout of less than 50% and unexpectedly resulted in the re-election of Didier Ratsiraka.[96]
He moved further towards capitalism. The influence of the IMF and the World Bank led to widespread privatisation.
Opposition to Ratsiraka began to grow again. Opposition parties boycotted provincial elections in 2000, and the 2001 presidential election produced more controversy. The opposition candidate Marc Ravalomanana claimed victory after the first round (in December) but the incumbent rejected this position. In early 2002 supporters of the two sides took to the streets and violent clashes took place. Ravalomanana claimed that fraud had occurred in the polls. After an April recount the High Constitutional Court declared Ravalomanana president. Ratsiraka continued to dispute the result but his opponent gained international recognition, and Ratsiraka had to go into exile in France, though forces loyal to him continued activities in Madagascar.[95]
Ravalomanana's Tiako I Madagasikara party achieved overwhelming electoral success in December 2001 and he survived an attempted coup in January 2003. He used his mandate to work closely with the IMF and the World Bank to reform the economy, to end corruption and to realise the country's potential.[95]
Ratsiraka went on trial (in absentia) for embezzlement (the authorities charged him with taking $8m of public money with him into exile) and the court sentenced him to ten years' hard labour.[100]
Ravalomanana is credited with improving the country's infrastructure, such as roads, along with making improvements in education and health, but has faced criticism for his lack of progress against poverty; purchasing power is said to have declined during his time in office.[101][102] On 18 November 2006, his plane was forced to divert from Madagascar's capital during a return trip from Europe following reports of a coup underway in Antananarivo and shooting near the airport;[103] however, this alleged coup attempt was unsuccessful.
Ravalomanana ran for a second term in the presidential election held on December 3, 2006.[104] According to official results, he won the election with 54.79% of the vote in the first round; his best results were in Antananarivo Province, where he received the support of 75.39% of voters.[105] He was sworn in for his second term on January 19, 2007.[106]
Ravalomanana dissolved the National Assembly in July 2007, prior to the end of its term, following a constitutional referendum earlier in the year. Ravalomanana said that a new election needed to be held so that the National Assembly would reflect the changes made in this referendum.[107]
He became involved in a political standoff after he closed the TV station belonging to Antananarivo mayor Andry Rajoelina. In January 2009, protests which then turned violent were organized and spearheaded by Andry Rajoelina, the mayor of the capital city of Antananarivo and a prominent opponent of President Ravalomanana.[108][109]
The situation fundamentally changed on 10 March 2009 when army leaders forced the recently appointed defense secretary to resign (the previous one had decided to resign after the killings by the presidential guard on 7 February 2009). They also announced that they gave the opponents 72 hours to dialogue and find a solution to the crisis before they would take further action. This move came after the leaders of the main military camp had announced a day earlier that they would not execute orders coming from the presidency any more since their duty was to protect the people, and not to oppress them, as groups of the military had done over the last few days.[110][111]
On 16 March 2009, the army seized the presidential palace in the centre of Antananarivo. Ravalomanana was not in the palace at the time.[112] He handed his resignation to the army, which then decided to hand over power to his fierce political rival, Andry Rajoelina. The second round of the postponed presidential elections was held in December 2013 and the results were announced in January 2014. The winner and the next president was Hery Rajaonarimampianina. He was backed by Rajoelina who led the 2009 coup and still was very influential political figure.[113][114]
In 2018 the first round of the presidential election was held on 7 November and the second round was held on 10 December. Three former presidents and the most recent president were the main candidates of the elections. Former president Andry Rajoelina won the second round of the elections. He was previously president from 2009 to 2014. Former president Marc Ravalomanana lost the second round and he did not accept the results because of allegations of fraud. Ravalomanana was president from 2002 to 2009. The most recent president Hery Rajaonarimampianina received very modest support in the first round. In January 2019 the High Constitutional Court declared Rajoelina as the winner of the elections and the new president.[115][116][117]
In 2019, an epidemic of measles killed 1,200 people.[118]
In 2021, Madagascar's worst drought in 40 years left more than a million people in southern Madagascar food insecure. This forced thousands of people to leave their homes to search for food.[119][120]
In November 2023, Andry Rajoelina was re-elected to another term with 58.95% of the vote in the first round of the election. Turnout was 46.36%, the lowest in a presidential election in the country's history.[121]
15th century
16th century
15th century
16th century
17th century
18th century
19th century
16th century
17th century
15th century
16th century
Portuguese India
17th century
Portuguese India
18th century
Portuguese India
16th century
17th century
19th century
Portuguese Macau
20th century
Portuguese Macau
15th century [Atlantic islands]
16th century [Canada]
16th century
17th century
18th century
19th century

The prehistory of the Levant includes the various cultural changes that occurred, as revealed by archaeological evidence, prior to recorded traditions in the area of the Levant. Archaeological evidence suggests that Homo sapiens and other hominid species originated in Africa (see hominid dispersal) and that one of the routes taken to colonize Eurasia was through the Sinai Peninsula desert and the Levant, which means that this is one of the most occupied locations in the history of the Earth. 
Not only have many cultures lived here, but also many species of the genus Homo. In addition, this region is one of the centers for the development of agriculture.[1]
Geographically, the area is divided between a coastal plain, the hill country to the east, and the Jordan Valley joining the Sea of Galilee to the Dead Sea. Rainfall decreases from the north to the south, with the result that the northern region of Palestine has generally been more economically developed than the southern one of Judea.[citation needed]
At the latest from the Neolithic period onwards, the area's location at the center of three trade routes linking three continents made it the meeting place for religious and cultural influences from Egypt, Syria, Mesopotamia, and Asia Minor:
The area seems to have suffered from acute periods of desiccation and reduced rainfall, influencing the relative importance of settled versus nomadic ways of living. The cycle seems to have been repeated, during which a reduced rainfall increases periods of fallow, with farmers spending increasing amounts of time with their flocks and away from cultivation. Eventually, they revert fully to nomadism. When rainfall increases, they settle around important water sources and increase cultivation. Increased prosperity leads to a revival of inter-regional and, eventually, international trade. The growth of villages rapidly proceeds to the increased prosperity of market towns and city-states, which attract the attention of neighbouring great powers, who may invade to capture control of regional trade networks and possibilities for tribute and taxation. Warfare leads to opening the region to pandemics, with resultant depopulation, overuse of fragile soils and a reversion to nomadic pastoralism.[citation needed]
The earliest traces of the human occupation in the Levant are documented in Ubeidiya in the Jordan Valley of the Southern Levant. The site was dated to c. 1.4 million years ago,[2] but further research has fixed its chronological context to 1.5–1.2 million years ago.[3] The site yielded stone tools typical of the Acheulean industry which appears in East Africa as early as c. 1.76 million years ago.[4] An earlier site is found in Dmanisi, Georgia, dated to 1.85–1.78 million years ago[5] suggest the existence of other sites in the Levant which are yet to be found. Stone tools of the Oldowan industry, preceding the Acheulian, were found in the Negev and Syrian deserts and support the presence of pre-Acheulian cultures in the Levantine corridor, but their chronological context cannot be determined.[6]
Ubeidiya prehistoric site is an open site that existed alongside the extinct Lake Ubeidiya whose shores were inhabited by over a hundred Asian and African animal species including mammals such as giraffes, Syrian elephants, Persian fallow deer, mountain and Dorcas gazelles, and the now-extinct pelorovis; birds; reptiles; amphibians; and insects. Some of these animals have been hunted by hominins who inhabited the site, as evidenced by the cut marks observed on the fossilized bones. The stone tools found in Ubeidiya include hand axes, picks, chopping tools, and spheroids. These tools have been attested to the Early Acheulian industry. The tools show a preference for specific rock types such as basalt, limestone and flint for particular tool types. This implies a sophisticated understanding of raw materials by the hominins who located and selected them for production. Other stone tool assemblages in the Levant have been attested to the early Acheulean but lack sufficient dating evidence to compare with Ubeidiya's finds. These sites include Abbassia near the Nile, Evron Quarry and Zihor in Israel and al-Lataminah in Syria.[6]
North of Ubeidiya is the crucial site of Daughters of Jacob Bridge (Gesher Benot Ya'akov, abbreviated as "GBY") dated to slightly after c. 790,000 years ago. The stone tool assemblage belongs to the "Large Flake" stage of the Acheulian, testifying to an advanced knapping technique. GBY provides information on many aspects of the life of its inhabitants: Many large mammal bones were found at the site, including those of the elephant Palaeoloxodon recki display evidence of butchery by the early humans. Nuts and tools used to crack them, as well as fish bones, were collected. The earliest wooden artifact - a plank with evidence of polishing - was found at the site and one of the earliest traces of fire use. In some layers, the organization of living space was observed, with certain activities limited to specific areas at the site.[6]
The late stage of the Acheulian industry is observed in thousands of sites and find spots in the Middle East, though only a few were excavated. Most of the sites did not yield enough datable evidence. The site at Lake Ram in the Golan Heights was dated based on the basalt flows below and above to an unknown timespan between c. 800,000–233,000 years ago. More accurate dates from Ma'ayan Baruch and the Revadim Quarry in Israel provide the timeframe of c. 500,000–400,000 years ago. Late Acheulian sites and finds are found spread all across the regions of the Levant, including desert regions in modern-day Saudi Arabia, Jordan and Saudi Arabia, primarily associated with oases, as well as the coastal plains and rift valleys of Israel, Lebanon and Syria. This distribution of sites in various regions of different conditions indicates either a more suitable climate in this period (the Chibanian stage of the Pleistocene) or alternatively better human adapting skills. The earliest cave sites also appear in this stage. Unlike the earlier Acheulian industries in the Levant, flint is the primary material used for tool making, with the handaxe being the primary tool. The toolmakers developed different variants of handaxes different in shape and function, which replaced other tools such as cleavers. Some of the most significant assemblages of stone tools are found in Nadaouiyeh (in central Syria), Tabun, Um Qatafa and Ma'ayan Baruch (in Israel). These sites yield an enormous amount of stone tools, reaching several thousands. An important discovery from Lake Ram is a stone pebble with evidence of artificial shaping and polishing, which resembles the body of a woman and thus serves as one of the earliest figurines known.[6]
The Middle Paleolithic (c. 250,000 – c. 48,000 BCE) is represented in the Levant by the Mousterian, known from numerous sites (both caves and open-air sites) through the region. The chronological subdivision of the Mousterian is based on the stratigraphic sequence of the Tabun Cave. Middle Paleolithic human remains include both the Neandertals (in Kebara Cave, Amud Cave and Tabun) and the anatomically modern humans (AMH), the Skhul and Qafzeh hominins.
The Upper Palaeolithic period is dated in the Levant to c. 48,000 – c. 20,000 BCE.
The Epipalaeolithic period (c. 20,000 – c. 9,500 cal. BCE; also known as Mesolithic period) is characterized by significant cultural variability and wide spread of the microlithic technologies. Beginning with the appearance of the Kebaran culture (18,000–12,500 BCE) a microlithic toolkit was associated with the appearance of the bow and arrow into the area.  Kebaran shows affinities with the earlier Helwan phase in the Egyptian Fayyum, and may be associated with a movement of people across the Sinai associated with the climatic warming after the Late Glacial Maxima of 20,000 BCE.  Kebaran affiliated cultures spread as far as Southern Turkey.  The latest part of the period (c. 12,500 – c. 9,500 cal. BCE) is the time of flourishing of the Natufian culture and development of sedentism among the hunter-gatherers.
This culture existed from about 13,000 to 9,800 BCE in the Levant. Numerous archaeological excavations have led to a relatively well defined understanding of these people. Two of the most significant aspects of this culture were their large community sizes and their sedentary lifestyles.[7] Although the Late Natufian experienced a slight reversal in this trend (possibly a result of the cold period known as the Younger Dryas) as their community size shrank and they became more nomadic, it is believed that this culture continued through and was the foundation for the Neolithic Revolution.[8]
The Neolithic is traditionally divided to the Pre-Pottery (A and B) and Pottery Late Neolithic phases. Pre-Pottery Neolithic A developed from the earlier Natufian cultures of the area. This is the time of the Neolithic Revolution and development of agricultural economies in the Near East, and the region's first known megaliths (and Earth's oldest known megalith, other than Göbekli Tepe, which is in the Northern Levant and from an unknown culture) with a burial chamber and tracking of the sun or other stars.[citation needed]
In addition, the Levant in the Neolithic (and later, in the Chalcolithic) was involved in large scale, far reaching trade.[9]
Trade on an impressive scale and covering large distances continued during the Chalcolithic (c. 4500–3300 BCE). Obsidian found in the Chalcolithic levels at what is now Gilat in Israel have had their origins traced via elemental analysis to three sources in southern Anatolia: Hotamış Dağ, Mount Göllü, and as far east as Mount Nemrut, 500 km (310 mi) east of the other two sources. This indicates a vast trade circle reaching as far as the northern Fertile Crescent at these three Anatolian sites.[9]
The Ghassulian created the basis of the Mediterranean economy, which has characterized the area ever since. A Chalcolithic culture, the Ghassulian economy was a mixed agricultural system consisting of extensive cultivation of wheat and barley, intensive horticulture of vegetable crops, commercial production of vines and olives, and a combination of transhumance and nomadic pastoralism. The Ghassulian culture, according to Juris Zarins, developed out of the earlier Munhata phase of what he calls the "circum-Arabian nomadic pastoral complex", probably associated with the first appearance of ancient Semitic-speaking peoples in this area.[10]
The urban development of Canaan lagged considerably behind that of Egypt, Mesopotamia, and even that of Syria, where from 3500 BCE, a sizable city developed at Hamoukar. This city, which was conquered, probably by people coming from the Lower Mesopotamian city of Uruk, saw the first connections between Syria and Lower Mesopotamia that some[11][12] have suggested lying behind the Biblical Patriarchs. Urban development again began culminating in Early Bronze Age sites like Ebla, which by 2300 BCE, was incorporated once again into the empires of Sargon and Naram-Sin of Akkad. The archives of Ebla show reference to several Biblical sites, including Hazor, Jerusalem, and several people have claimed, also to Sodom and Gomorrah. The collapse of the Akkadian Empire saw the arrival of peoples using Khirbet Kerak Syro-Palestinian pottery ware[13] which originated from the Zagros Mountains east of the Tigris.  It is suspected by some Ur seals that this event marks the arrival in Syria and Palestine of the Hurrians, people later known in the Biblical tradition as Horites.[citation needed]
The following Middle Bronze Age period was initiated by the arrival of Amorites from Syria in Lower Mesopotamia. This period saw the pinnacle of urban development in Syria and Palestine. Archaeologists show that the chief state at this time was the city of Hazor, the head of all the Canaanite kingdoms of the northern region of Palestine. This is also the period in which ancient Semitic-speaking peoples began to appear in more significant numbers in the Nile Delta.

Clive Stephen Gamble, FSA, FRAI, FBA (born 1951) is a British archaeologist and anthropologist. He has been described as the "UK’s foremost archaeologist investigating our
earliest ancestors."[1]
Gamble was born in 1951. He was educated at Brighton College, a private school in Brighton, England. He studied at Jesus College, Cambridge, graduating with a Bachelor of Arts (BA) degree in 1972: as per tradition, bi BA was promoted to a Master of Arts (MA Cantab) degree in 1975. Remaining at Jesus College, he studied for a Doctor of Philosophy (PhD) degree which he completed in 1978.[2] His doctoral thesis was titled "Animal Communities and their Relationship to Prehistoric Economies in Western Europe".[3]
From 1975 to 1979, Gamble was an experimental officer in archaeology at the University of Southampton.[1] In 1979, he became a lecturer in archaeology.[4] He was promoted to Professor in 1996. In 1999 he founded the Centre for the Archaeology of Human Origins at Southampton.[1]
In 2004 Gamble was appointed to a Research Professorship in the Centre for Quaternary Research at Royal Holloway College, in the University of London.[5]  He subsequently returned to Southampton as a professor in the Department of Archaeology in 2011.[5]  In 2015 he was a Trustee of the British Museum.[6] He retired from Southampton in 2017 and was appointed an emeritus professor.[7]
Gamble's main research interests are the archaeology of human origins, the social life of the earliest humans and the timing of their global colonisation.[8]
Gamble is a Trustee of the British Museum (August 2010-August 2014),[1] Fellow of the British Academy, Fellow and Vice President of the Society of Antiquaries and Fellow and, from 2011 to 2014, President of the Royal Anthropological Institute.[8] He received the Rivers Memorial Medal from the Royal Anthropological Institute in 2005.[9] In 2002 he presented Where Do We Come From?, a six-part documentary screened on Channel Five.[10]
In 2000 his book The Palaeolithic Societies of Europe won the Society for American Archaeology Book Award.[10]
Gamble is currently part of the NERC-sponsored team that is looking to date key evolutionary events in Europe over the last 60,000 years by dating deposits of volcanic ash. The events that the team is seeking to date includes the arrival of modern humans, the Neanderthal extinction, and the post-Ice Age re-colonisation of northern Europe approximately 16,000 years ago by the direct ancestors of most modern Europeans.[5]
Gamble was a co-director on the British Academy Centenary project (2003-2010) Lucy to language: The archaeology of the social brain [5]
Gamble led a fieldwork programme in Greece, which recorded and published all the evidence from field surveys for Palaeolithic and Mesolithic settlement undertaken there in the last 50 years.[5] This led to the publication of The Prehistoric Stones of Greece which provided the first overview of all stone tools discovered in Greece. There is no comparable overview elsewhere in Europe.[1]
On 26 November 1981, Gamble was elected a Fellow of the Society of Antiquaries of London (FSA).[11] In 2000, he was elected a Fellow of the British Academy (FBA).[4] He is also an elected Fellow of the Royal Anthropological Institute (FRAI).[12]

The Elamo-Dravidian language family is a hypothesised language family that links the Elamite language of ancient Elam (present-day southwestern Iran, and southeastern Iraq) to the Dravidian languages of South Asia. The latest version (2015) of the hypothesis entails a reclassification of Brahui as being more closely related to Elamite than to the remaining Dravidian languages. Linguist David McAlpin has been a chief proponent of the Elamo-Dravidian hypothesis, followed by Franklin Southworth as the other major supporter.[1] The hypothesis has gained attention in academic circles, but has been subject to serious criticism by linguists, and remains only one of several possible scenarios for the origins of the Dravidian languages.[note 1] Elamite is generally accepted by scholars to be a language isolate, unrelated to any other known language.[3]
The concept that Elamite and Dravidian are in some way related dates from the beginnings of both fields in the early nineteenth century. Edwin Norris was the first to publish an article in support of the hypothesis in 1853.[4] Further evidence was proposed by Robert Caldwell when he published a comparative linguistics book in 1856 about the Dravidian languages.[5] David McAlpin, assistant professor of Dravidian languages and linguistics at the University of Pennsylvania, published a series of papers providing evidence supporting the theory.[6][1] He also speculated that the Harappan language (the language of the Indus Valley civilization) might also have been part of this family.
According to David McAlpin, the Dravidian languages were brought to present day Pakistan by immigration from the Middle East via Elam, located in present-day southwestern Iran.[7][6] McAlpin (1975) in his study identified some similarities between Elamite and Dravidian. He proposed that 20% of Dravidian and Elamite vocabulary are cognates while 12% are probable cognates. He further claimed that Elamite and Dravidian possess similar second-person pronouns and parallel case endings. They have a number of similar derivatives, abstract nouns, and the same verb stem+tense marker+personal ending structure. Both have two positive tenses, a "past" and a "non-past".[8]
The hypothesis has gained attention in academic circles but is difficult to assess due to the limited resources on the Elamite language.[5] Supporters of the Elamo-Dravidian hypothesis include Igor M. Diakonoff[9] and Franklin Southworth.[1]
Bhadriraju Krishnamurti regarded McAlpin's proposed morphological correspondences between Elamite and Dravidian to be ad hoc, and found them to be lacking phonological motivation.[10] Similar criticisms have been made by Kamil Zvelebil and others.[10] Georgiy Starostin criticized them as no closer than correspondences with other nearby language families.[5] For the majority of historical linguists, the Elamo-Dravidian hypothesis remains unproven, and Elamite is generally accepted by scholars to be a language isolate, unrelated to any other known language.[11][12][13]
Apart from the linguistic similarities, the Elamo-Dravidian hypothesis rests on the claim that agriculture spread from the Near East to the Indus Valley region via Elam. This would suggest that agriculturalists brought a new language as well as farming from Elam. Supporting ethno-botanical data include the Near Eastern origin and name of wheat (D. Fuller). Later evidence of extensive trade between Elam and the Indus Valley Civilization suggests ongoing links between the two regions.
Renfrew and Cavalli-Sforza have also argued that Proto-Dravidian was brought to the Indus Valley by farmers from the Fertile Crescent,[14][15][16][note 2] but more recently Heggarty and Renfrew noted that "McAlpin's analysis of the language data, and thus his claims, remain far from orthodoxy", adding that Fuller finds no relation of Dravidian languages with other languages, and thus assumes it to be native to India.[2] Renfrew and Bahn conclude that several scenarios are compatible with the data, and that "the linguistic jury is still very much out".[2]
Narasimhan et al. (2019) conclude that the Iranian ancestral component in the IVC people was contributed by people related to but distinct from Iranian agriculturalists, lacking the Anatolian farmer-related ancestry which was common in Iranian farmers after 6000 BCE.[17][note 3] Those Iranian farmers-related people may have arrived in India before the advent of farming in northern India,[17] and mixed with people related to Indian hunter-gatherers c. 5400 to 3700 BCE, before the advent of the mature IVC.[20][note 4] Sylvester et al. (2019) noted that (referring to Renfrew (1996)) "the existence of Brahui speakers, solitary Dravidian language speakers in Balochistan in Pakistan, supports the Elamo-Dravidian hypothesis",[22][23] and concluded that bidirectional migration and admixture occurred during neolithic times.[24]

The killer ape theory or killer ape hypothesis is the theory that war and interpersonal aggression was the driving force behind human evolution. It was originated by Raymond Dart in his 1953 article "The predatory transition from ape to man"; it was developed further in African Genesis by Robert Ardrey in 1961.[1] The theory gained notoriety for suggesting that the urge to violence was a fundamental part of human psychology. It is associated with the hunting hypothesis, also developed by Ardrey.
According to the theory, the ancestors of humans were distinguished from other primate species by their greater aggressiveness, and this aggression is the source of humanity's murderous instincts.
However, subsequent research has shown that both chimpanzees and bonobos may exhibit aggressive behaviors over 100 times more often than humans.[2]
The theory has variations as to what kind of violence served as the evolutionary catalyst: one-on-one aggression or group-based aggression.
Several theories suggest the primary reason humans evolved bipedalism was to conserve energy while running, and to free the use of upper limbs.[citation needed] The killer ape theory posits that violence was a driving factor in evolving bipedalism, freeing the upper limbs to wield weapons.[citation needed]
In Creatures of Cain: The Hunt for Human Nature in Cold War America by Erika Lorraine Milam (2018), she states that, "in the 1970s, the theory unraveled altogether when primatologists discovered that chimpanzees also kill members of their own species." This still does not answer whether interpersonal violence derives from biological or social factors.
Ethologist Konrad Lorenz showed interest in similar ideas in his book On Aggression (1963).[3] In his introduction, he describes how rival butterfly fish defend their territories, leading him to raise the question of whether humans, too, tend to intraspecific conflict.
A 1984 article said Dart's evidence was slim, and was refuted by paleontologists in the early 1970's, in particular CK Brain and Elisabeth Vrba.[4]
A 2008 article in Nature by Dan Jones stated, "A growing number of psychologists, neuroscientists, and anthropologists have accumulated evidence that understanding many aspects of antisocial behaviour, including violence and murder, requires the study of brains, genes, and evolution, as well as the societies those factors have wrought." Evolutionary psychologists generally argue that violence is not done for its own sake, but is a by-product of goals such as higher status or reproductive success. Some evolutionary psychologists argue that humans have specific mechanisms for specific forms of violence such as against stepchildren (the Cinderella effect). Chimpanzees have violence between groups, which are similar to raids and violence between human groups in nonstate societies, and produce similar death rates.[5][6] On the other hand, intragroup violence is lower among humans living in small-group societies than among chimpanzees. Humans may have a strong tendency to differ between ingroup and outgroup, which affects altruistic and aggressive behavior. Also, evidence exists that both intragroup and intergroup violence were much more prevalent in the recent past and in tribal societies. This suggests that tendencies to use violence to achieve goals are affected by social mores. Reduced inequalities, more available resources, and reduced blood feuds due to better-functioning justice systems may have contributed to declining intragroup violence.[7]
The idea that man is naturally warlike has been challenged, for example in the book War, Peace, and Human Nature (2013), edited by Douglas P. Fry.[8] The Seville Statement on Violence, released under UNESCO auspices in 1986, specifically rejects any genetic basis to violence or warfare though is considered outdated in light of more contemporary studies. More modern research and criticism has focused on misinterpretations of fossil evidence, lack of research into other apes, and the political climate of the Cold War.[9][10]
The association of violence with a dramatic leap in human evolution can be seen in the opening sequence of 2001: A Space Odyssey.
The television show Sliders made extensive use of the killer ape theory in storyline arcs involving the Kromaggs.

The Narmada Human, originally the Narmada Man, is a species of extinct human that lived in central India during the Middle and Late Pleistocene.[1][2] From a skull cup discovered from the bank of the Narmada River in Madhya Pradesh in 1982, the discoverer, Arun Sonakia classified it was an archaic human and gave the name Narmada Man, with the scientific name H. erectus narmadensis.[3] Analysis of additional fossils from the same location in 1997 indicated that the individual could be a female, hence, a revised name, Narmada Human, was introduced. It remains the oldest human species in India.[4]
The Narmada Valley became a fossil attraction in the early 19th century following the discovery of a dinosaur, Titanosaurus. Discovery of stone tools prompted a search for early human fossils, but over a century of research was in vain. The discovery of the Narmada Human is remarked as the moment that "brought the Narmada Valley back into palaeoanthropological focus."[5] The fossil had been variously reclassified as archaic Homo sapiens, evolved Homo erectus, Homo heidelbergensis, and also dubiously[a] as a distinct species, Homo narmadensis.[6] Additional fossils described since 1997 have suggested more relatedness to archaic H. sapiens.
The Narmada Valley is one of the earliest and richest fossil sites in India. The first fossils were discovered by British Army Captain William Henry Sleeman in 1828.[7] Sleeman found two backbones (caudal vertebrae) from the Lameta Formation at Jabalpur that were later identified as those of a dinosaur, Titanosaurus.[8] Since then, many fossils of invertebrates and vertebrates have been discovered.[9][10] The search for prehistoric human remains in the region was inspired by the discovery of a Stone Age hand axe by C.A. Hacket that was reported in 1873.[11]
In October 1982, the Geological Survey of India (GSI) assigned Arun Sonakia to explore Hoshangabad district.[12] On 5 December, Sonakia found a skull cup (calvaria) lying on the surface of an alluvial soil on the northern bank of Narmada River, near Hathnora village.[13][14] The fossil was among several other fossils of horse, pig and Stegodon, and various stone tools. After careful analysis of the skull cup as part of a prehistoric human, the discovery was officially announced by the government on 21–22 July 1983, and through the newsletter of GSI. In 1984, N. G. K. Murthy, Director of GSI Southern Region, presented the technical report to the Birla Archaeological and Cultural Geological Research Institute in Hyderabad.[15]
Sonakia displayed the fossil cast at the first[16] "Ancestors exhibit" of the American Museum of Natural History in New York during 6 to 10 April 1984.[12] The exhibit was recorded the next year in the American Museum of Natural History's proceedings Ancestors, the Hard Evidence.[17][18]
The first scientific description by Sonakia appeared in the Records of the Geological Survey of India in 1984 which described the specimen as Homo erectus narmadensis.[19][20] In 1985, Sonakia sought the help of french palaeontologist Henry de Lumley, with whom he made further descriptions in two articles simultaneously published in January issue of L'Anthropologie.[21][22] The fossil was again identified as H. erectus.[23]
Kenneth A. R. Kennedy of Cornell University conveyed the report in the Records of the Geological Survey of India to the American Anthropological Association, which published it in its September 1985 issue of American Anthropologist. In it, Sonakia gave a more careful description:
The hominid fossil specimen, presently designated  as "Narmada  Man," is represented by a  complete right half of the skull cap, to which a part of the left parietal is attached... [Its] cranial capacity... would fall around 1,200 cc... [It] bears a number of similarities to skulls of Asian Homo erectus, hence an affinity of "Narmada Man" to Homo erectus is suggested.[12]
Kennedy added a cautionary note that the fossil was an undetermined species of human ("hominid calvarium of Homo sp. indet.).[12] In 1988, Sonakia invited Kennedy to further examine the fossil kept at Nagpur.[20] Reanalysis by the GSI and Cornell teams were jointly published in the American Journal of Physical Anthropology in 1991, which concluded the identity as Homo sapiens.[24]
The Anthropological Survey of India (ASI) organised an archaeological exploration of the central Narmada valley between 1983 and 1992, resulting in the discoveries of many animal remains, stone tools and new human fossil.[25] The collarbone (clavicle) among animal bones collected from Hathnora fossil site was not recognised that of humans until careful analysis was done. In 1997, Anek Ram Sankhyan, the ASI senior anthropologist reported the description of a right collarbone in the Journal of Human Evolution,[26] and further explained it in Current Science.[25]
In 1998, Sankhyan discovered another collarbone, a left one, along with one lower rib at Hathnora fossil site.[27] He reported the findings in Current Science in 2005.[28] Another ASI exploration between 2005 and 2010 led to the discovery of parts of a thigh bone (femur) and arm bone (humerus) at Netankheri village that was reported in Current Science in 2012.[29][30]
Sonakia established from the first skull cup that the individual was an adult male,[24] and originally gave the name Narmada Man[12] to match those of other asian H. erectus, like Peking Man and Java Man,[13] and was popularised as such.[22][31] When he reassessed the fossil with Lumley, it was identified a female in her 30s.[21] Kennedy also agreed that the individual was a female.[24][32] The more gender-accurate name Narmada Human was later adopted.[2] The individual could have lived any time between 50 ka and 160 ka, during the Middle and Late Pleistocene.[33] Sonakia had originally estimated the fossil age at around 500 to 600 ka based on the associated fossil.[13]
There is no consensus on the exact species identification of Narmada Human. It had been variously described as archaic Homo sapiens, evolved Homo erectus, or Homo heidelbergensis because it exhibits features shared with these human species, along with its own unique features.[33] Sonakia and Lumley firmly held the classification as an evolved H. erectus.[23][32] Kennedy was the first to be critical of this assignment,[12] and argued that it could be an archaic H. sapiens. Reporting his analysis (with Sonakia, John Chiment, and K.K. Verma) in 1991, he stated:
Results of the most recent study, which includes morphometric and comparative investigations, lead to the conclusion that "Narmada Man" is appropriately identified as Homo sapiens. While the calvaria shares some anatomical features with Asian Homo erectus specimens, it exhibits a broader suite of morphological and mensural characteristics suggesting affinities with early Homo sapiens fossils.[24]
However, Sonakia was not entire convinced and adhered to the H. erectus classification.[34] In 2007, Sheela Athreya of the Texas A&M University College Station revised the systematic identification and concluded that the Narmada Human could not be H. erectus, but instead could be loosely identified as H. heidelbergensis, as she noted:
Narmada [Human] shows affinities with anatomically modern Africans and Europeans as well as most Middle Pleistocene H. heidelbergensis specimens, and could be classified as such... If only the subjective criteria of brain size and "transitional" morphology are used, it could be classified as H. heidelbergensis... [or] it can simply be referred to as "Middle Pleistocene Homo" a term that is sufficiently descriptive without the historical baggage of nomenclature that comes from ascribing this specimen to Asian H. erectus.[35]
Sankhyan used to support Kennedy's assignment of the Narmada Human as archaic H. sapiens. However, his analysis in his doctoral thesis led him to realise that Athreya's classification is the most likely conclusion, as he remarked: "both the metric and nonmetric comparisons show that the Narmada calvarium has a generalized mosaic of primitive, shared, and unique morphological features, but cladistically it is closer to H. heidelbergensis."[32] David W. Cameron of the Australian National University, with Rajeev Patnaik and Ashok Sahni of the Punjab University, found that the Narmada Human fits well with the features of the Steinheim skull in Germany, which is either H. heidelbergensis or H. neanderthalensis.[5]
As originally identified, the Narmada Human shares most features with other Asian H. erectus.[13] The presence of a small mastoid process, narrow post-orbital constriction, large and thick cranial vault, and a distinct bone called torus angularis are the major and common features of Asian H. erectus.[20]
However, the Narmada Human has features that are more related to H. sapiens. The most important is its brain size, which falls between 1,155 and 1,421 cc,[20] with an approximate average of 1,200 cc.[12] The average brain size of Asian H. erectus is about 1,000 cc,[36] with mostly towards the lower range up to 800 cc;[37] while early H. sapiens have the brain size ranging from 1,200 to 1,400 cc.[38]
Some features of the Narmada Human are not shared with any other human species. A sagittal crest with a furrow on top of the skull, a large outer ear hole, and extended temporal bones are not known in other species.[20]
Sonakia held the view that the Narmada Human was a transitional group of H. erectus that links African and Asian populations. He wrote in 1998:
The discovery of Indian Homo erectus bridges the gap between African H. erectus in the west and Chinese [Peking Man] and Javan H. erectus [Java Man] in the east and south east respectively. There is a general consensus of opinion that Afro-Asian H. erectus ranges in age from Lower Pleistocene to Middle Pleistocene. Indian H. erectus falls within this range.[34]
According to Cameron, Patnaik and Sahni, the Narmada Human is most closely related to extinct humans such as H. heidelbergensis or H. neanderthalensis than H. erectus. It represents a different species that met evolutionary dead-end in India, fitting into the out of Africa theory of modern human origin.[5]

The peopling of India refers to the migration of Homo sapiens into the Indian subcontinent. Anatomically modern humans settled India in multiple waves of early migrations, over tens of millennia.[1] The first migrants came with the Coastal Migration/Southern Dispersal 65,000 years ago, whereafter complex migrations within South and Southeast Asia took place. West Asian (Iranian) hunter-gatherers migrated to South Asia after the Last Glacial Period but before the onset of farming. Together with ancient South Asian hunter-gatherers they formed the population of the Indus Valley Civilisation (IVC).
With the decline of the IVC, and the migration of Indo-Europeans, the IVC-people contributed to the formation of both the Ancestral North Indians ("ANI"), who were closer to contemporary West Eurasians, and the Ancestral South Indians ("ASI"), who were descended predominantly from the Southeastern Indian hunter gatherers (known as "AASI", who were distantly related to East Eurasians such as Aboriginal Australians, Andamanese, and also to East Asians), but also from West Eurasian hunter-gatherers from the Iranian Plateau. These two ancestral populations (ASI and ANI) mixed extensively between 1,900-4,200 years ago, after the fall of the IVC and their respective southward migration,[2] and affected both modern Indo-European populations as well as the Dravidian populations in the subcontinent, while the migrations of the Munda people and the Sino-Tibetan-speaking people from East Asia also added new elements.
The dating of the earliest successful migration of modern humans out of Africa is a matter of dispute.[3] It may have pre- or post-dated the Toba catastrophe, a volcanic super eruption that took place between 69,000 and 77,000 years ago at the site of present-day Lake Toba. According to Michael Petraglia, stone tools discovered below the layers of ash deposits in India at Jwalapuram, Andhra Pradesh point to a pre-Toba dispersal. The population who created these tools is not known with certainty as no human remains were found.[3] An indication for post-Toba is haplogroup L3, that originated before the dispersal of humans out of Africa, and can be dated to 60,000–70,000 years ago, "suggesting that humanity left Africa a few thousand years after Toba."[3]
It has been hypothesized that the Toba supereruption about 74,000 years ago destroyed much of India's central forests, covering it with a layer of volcanic ash, and may have brought humans worldwide to a state of near-extinction by suddenly plunging the planet into an ice-age that could have lasted for up to 1,800 years.[4] If true, this may "explain the apparent bottleneck in human populations that geneticists believe occurred between 50,000 and 100,000 years ago" and the relative "lack of genetic diversity among humans alive today".[4]
Since the Toba event is believed to have had such a harsh impact and "specifically blanketed the Indian subcontinent in a deep layer of ash", it was "difficult to see how India's first colonists could have survived this greatest of all disasters".[5] Therefore, it was believed that all humans previously present in India went extinct during, or shortly after, this event and these first Indians left "no trace of their DNA in present-day humans" – a theory seemingly backed by genetic studies.[6]
Research published in 2009 by a team led by Michael Petraglia of the University of Oxford suggested that some humans may have survived the hypothesized catastrophe on the Indian mainland. Undertaking "Pompeii-like excavations" under the layer of Toba ash, the team discovered tools and human habitations from both before and after the eruption.[7] However, human fossils have not been found from this period, and nothing is known of the ethnicity of these early humans in India.[7] Recent research also by Macauly et al. (2005)[who?][8] and Posth et al. (2016),[9] also argue for a post-Toba dispersal.[8]
Early Stone Age hominin fossils have been found in the Narmada valley of Madhya Pradesh. Some have been dated to 200- 700,000 BP. It is uncertain what species they represent.[10]
By some 70-50,000 years ago,[11][9][12][13] only a small group, possibly as few as 150 to 1,000 people, crossed the Red Sea.[14] The group that crossed the Red Sea travelled along the coastal route around the coast of Arabia and Persia until reaching India, which appears to be the first major settling point.[15] Geneticist Spencer Wells says that the early travellers followed the southern coastline of Asia, crossed about 250 kilometres (155 mi) of sea, and colonized Australia by around 50,000 years ago. The Aborigines of Australia, Wells says, are the descendants of the first wave of migrations.[16]
The oldest definitively identified Homo sapiens fossils yet found in South Asia are Balangoda Man. Named for the location in Sri Lanka where they were discovered, they are at least 28,000 years old.[17]
Theories around Indigenous Aryanism are popular among certain Hindutva circles, and do not have any support in peer-reviewed literature.[18]
Narasimhan et al. (2018) introduced the term AASI, "Ancient Ancestral South Indian"[note 1] (AASI) for these oldest human inhabitants, which were possibly distantly related to the common ancestors of East-Eurasians such as Andaman Islanders (such as the Onge), East Asians, and Australian Aboriginals.[20][21][22][23] According to Narasimhan et al. (2019), "essentially all the ancestry of present-day eastern and southern Asians (prior to West Eurasian-related admixture in southern Asians) derives from a hypothetical single eastward spread, which gave rise in a short span of time to the lineages leading to AASI, East Asians, Onge, and Australians.",[24] a lineage often referred to as "East-Eurasians".[25]
Several genetic studies have found evidence of a distant common ancestry between native Andaman Islanders and the AASI/ASI ancestral component found in South Asians.[20] Modern South Asians have not been found to carry the paternal lineages common in the Andamanese, which has been suggested to indicate that certain paternal lineages may have become extinct in India, or that they may be very rare and have not yet been sampled.[26] Chaubey and Endicott (2013) further noted that  "Overall, the Andamanese are more closely related to Southeast Asian Negritos than they are to present-day South Asians."[27][note 2]
Shinde et al. 2019 found either Andamanese or East Siberian hunter-gatherers fit as proxy for AASI "due to shared ancestry deeply in time."[28] According to Yelmen et al. (2019) the native South Asian genetic component (AASI) is distinct from the Andamanese and not closely related, and that the Andamanese are thus an imperfect and imprecise proxy for AASI. According to Yelmen et al, the Andamanese component (represented by the Andamanese Onge) was not detected in the northern Indian Gujarati, and thus it is suggested that the South Indian tribal Paniya people (who are believed to be of largely AASI ancestry) would serve as a better proxy than the Andamanese (Onge) for the "native South Asian" component in modern South Asians.[29]
According to Narasimhan et al. (2019), the "AASI" component in South Asians shares a common root with the Andamanese (as exemplified by the Onge) and is distantly related to the Onge (Andamanese), as well as to East Asians, and Aboriginal Australians (with those groups and the AASI sharing a deep ancestral split around the same time),[23] which would place them in the East-Eurasian lineage.
The present-day Andamese are considered to be part of the "Negritos", several diverse ethnic groups who inhabit isolated parts of southeast Asia.[30] Based on their physical similarities, Negritos were once considered a single population of related people, but the appropriateness of using the label 'Negrito' to bundle together peoples of different ethnicity based on similarities in stature and complexion has been challenged.[31] Recent research suggests that the Negritos include several separate groups, as well as demonstrating that they are not closely related to the Pygmies of Africa.[32]
According to Vishwanathan et al. (2004), the typical "negrito" features could also have been developed by convergent evolution.[33] According to Gyaneshwer Chaubey and Endicott (2013), "At the current level of genetic resolution, however, there is no evidence of a single ancestral population for the different groups traditionally defined as 'negritos'."[27] Basu et al. 2016 concluded that the Andamanese have a distinct ancestry and are not closely related to other South Asians, but are closer to Southeast Asian Negritos, indicating that South Asian peoples do not descend directly from "Negritos" as such.[34]
Groups ancestral to the modern Veddas were probably the earliest inhabitants of Sri Lanka. Their arrival is dated tentatively to about 40,000–35,000 years ago. They are genetically distinguishable from the other peoples of Sri Lanka, and they show a high degree of intra-group diversity. This is consistent with a long history of existing as small subgroups undergoing significant genetic drift.[35][36]
A 2013 study by Raghavan et al. showed that the Vedda are closely related to other groups in Sri Lanka and India, especially to Sinhalese and Tamils. They additionally found deep relations between the indigenous Vedda and other South Asian populations with the modern populations of Europe, the Middle East and Northern Africa.[37]
After the Last Glacial Period, human populations started to grow and migrate. With the invention of agriculture, the so-called Neolithic revolution, larger numbers of people could be sustained. The use of metals (copper, bronze, iron) further changed human ways of life, giving an initial advance to early users, and aiding further migrations, and admixture.
According to Silva et al. (2017), multiple waves of migration from western Eurasia took place after the last Ice Age, both before and after the advent of farming in South Asia.[38] According to Narasimhan et al. (2019), people related to Iranian hunter-gatherers were present in South Asia before the advent of farming. They mixed with Ancestral Ancient South Asians (AASI) to form the Indus Valley population. With the decline of the IVC after 1900 BCE and the arrival of the Indo-Aryans, IVC-people mixed with incoming Indo-Aryans, forming the Ancestral North Indians (ANI). Other IVC-people mixed with AASI forming the Ancestral South Indians (ASI).[19][39][40][41]
These two ancestral groups mixed in India between 4,200 and 1,900 years ago (2200 BCE – 100 CE), whereafter a shift to endogamy took place,[41] possibly by the enforcement of "social values and norms" during the Hindu Gupta rule.[22] Reich et al. stated that "ANI ancestry ranges from 39–71% in India, and is higher in traditionally upper caste, martial races and Indo-European speakers.
".[39]
Basu et al. (2016) note that mainland India harbors two additional distinct ancestral components which have contributed to the gene pools of the Indian subcontinent,[note 3] namely Ancestral Austro-Asiatic (AAA) and Ancestral Tibeto-Burman (ATB).[22]
Narasimhan et al. (2019) and Shinde et al. (2019) conclude that west Eurasian ancestry was already present before the advent of farming in South Asia.[38][19][note 4]
Metspalu et al. (2011) detected a genetic component in India, k5, which "distributed across the Indus Valley, Central Asia, and the Caucasus".[43] According to Metspalu et al. (2011), k5 "might represent the genetic vestige of the ANI", though they also note that the geographic cline of this component within India "is very weak, which is unexpected under the ASI-ANI model", explaining that the ASI-ANI model implies an ANI contribution which decreases toward southern India.[44] According to Metspalu et al. (2011), "regardless of where this component was from (the Caucasus, Near East, Indus Valley, or Central Asia), its spread to other regions must have occurred well before our detection limits at 12,500 years."[45]
Speaking to Fountain Ink, Metspalu said, "the West Eurasian component in Indians appears to come from a population that diverged genetically from people actually living in Eurasia, and this separation happened at least 12,500 years ago."[web 1][note 5] Moorjani et al. (2013) refer to Metspalu (2011)[note 6] as "fail[ing] to find any evidence for shared ancestry between the ANI and groups in West Eurasia within the past 12,500 years".[50] CCMB researcher Thangaraj believes that "it was much longer ago", and that "the ANI came to India in a second wave of migration[note 7] that happened perhaps 40,000 years ago."[web 1]
According to Gallego Romero et al. (2011), their research on lactose tolerance in India suggests that "the west Eurasian genetic contribution identified by Reich et al. (2009) principally reflects gene flow from Iran and the Middle East."[51] Gallego Romero notes that Indians who are lactose-tolerant show a genetic pattern regarding this tolerance which is "characteristic of the common European mutation."[52] According to Romero, this suggests that "the most common lactose tolerance mutation made a two-way migration out of the Middle East less than 10,000 years ago. While the mutation spread across Europe, another explorer must have brought the mutation eastward to India – likely traveling along the coast of the Persian Gulf where other pockets of the same mutation have been found."[52]
According to Broushaki et al. (2016), evidence indicates that the Neolithic farmer component forms the main ancestry of many modern South Asians. These Neolithic farmers migrated from the fertile crescent, most likely from a region near the Zagros Mountains in modern day Iran, to South Asia some 10,000 years ago.[53][54]
Mehrgarh (7000 BCE to c. 2500 BCE), to the west of the Indus River valley,[55]  is a precursor of the Indus Valley Civilisation, whose inhabitants migrated into the Indus Valley and became the Indus Valley Civilisation.[56] It is one of the earliest sites with evidence of farming and herding in South Asia.[57][58] According to Lukacs and Hemphill, while there is a strong continuity between the Neolithic and Chalcolithic (Copper Age) cultures of Mehrgarh, dental evidence shows that the Chalcolithic population did not descend from the neolithic population of Mehrgarh,[59] which "suggests moderate levels of gene flow."[59] They further noted that "the direct lineal descendents of the Neolithic inhabitants of Mehrgarh are to be found to the south and the east of Mehrgarh, in northwestern India and the western edge of the Deccan Plateau", with Neolithic Mehrgarh showing greater affinity with Chalcolithic Inamgaon, south of Mehrgarh, than with Chalcolithic Mehrgarh.[59]
Shinde et al. (2019) and Narasimhan et al. (2019), analysing remains from the Indus Valley civilisation (of parts of Bronze Age Northwest India and East Pakistan) and "outliers" from surrounding cultures, conclude that the IVC-population was a mixture people related to Iranian herders and AASI:[19]
The only fitting two-way models were mixtures of a group related to herders from the western Zagros mountains of Iran and also to either Andamanese hunter-gatherers or East Siberian hunter-gatherers (the fact that the latter two populations both fit reflects that they have the same phylogenetic relationship to the non-West Eurasian-related component likely due to shared ancestry deeply in time)[28]
According to Shinde et al. (2019) about 50–98% of the IVC-genome came from people related to early Iranian farmers, and from 2–50% of the IVC-genome came from native South Asian hunter-gatherers sharing a common ancestry with the Andamanese.[28] Narasimhan et al. (2019) found the IVC-genome to consist of 45–82% Iranian farmer-related ancestry and 11–50% AASI (Andamanese-related hunter-gatherer) ancestry.[19] Narasimhan et al. (2019) conclude that the Iranian farmer-related ancestry is related to but distinct from Iranian agri-culturalists, lacking the Anatolian farmer-related ancestry which was common in Iranian farmers after 6000 BCE.[42][note 8] Those Iranian farmers-related people may have arrived in India before the advent of farming in northern India,[42] and mixed with people related to Indian hunter-gatherers c. 5400 to 3700 BCE, before the advent of the mature IVC.[62]
The analysed samples of both studies have little to none of the "Steppe ancestry" component associated with later Indo-European migrations into India. The authors found that the respective amounts of those ancestries varied significantly between individuals, and concluded that more samples are needed to get the full picture of Indian population history.[28][19]
While the IVC has been linked to the early Dravidian peoples, some scholars have suggested that their Neolithic farmer predecessors may have migrated from the Zagros Mountains to northern South Asia some 10,000 years ago.[63] According to David McAlpin, the Dravidian languages were brought to India by immigration into India from Elam.[64][65][66][67] Franklin Southworth also states that the Dravidian Languages originated in western Iran and that publications and research are "further evidence of [the relationship between Dravidian languages and Elamite] viability".[68] According to Renfrew and Cavalli-Sforza, proto-Dravidian was brought to India by farmers from the Iranian part of the Fertile Crescent,[69][70][71][note 9] but more recently Heggerty and Renfrew (2014) noted that "McAlpin's analysis of the language data, and thus his claims, remain far from orthodoxy", adding that Fuller finds no relation of Dravidian language with other languages, and thus assumes it to be native to India.[72] Renfrew and Bahn conclude that several scenarios are compatible with the data, and that "the linguistic jury is still very much out."[72][note 10]
In the second millennium BCE people from the Sintashta culture[79][80] migrated through Bactria-Margiana culture and into the northern Indian subcontinent (modern day India, Pakistan, Bangladesh and Nepal). The Indo-Aryan migrations started in approximately 1,800 BCE, after the invention of the war chariot, and also brought Indo-Aryan languages into the Levant and possibly Inner Asia. [81][82][note 11]
The Proto-Indo-Iranians, from which the Indo-Aryans developed, are identified with the Sintashta culture (2100–1800 BCE),[citation needed] and the Andronovo culture,[citation needed] which flourished c. 1800–1400 BCE in the steppes around the Aral sea, present-day Kazakhstan, Uzbekistan and Turkmenistan. The proto-Indo-Iranians were influenced by the Bactria-Margiana culture, south of the Andronovo culture, from which they borrowed their distinctive religious beliefs and practices. The Indo-Aryans split off around 1800–1600 BCE from the Iranians,[84] whereafter the Indo-Aryans migrated into the Levant and north-western India and possibly Inner Asia.
Lazaridis et al. (2016) notes that the demographic impact of steppe related populations on South Asia was substantial and forms a major component in northern India.[85] Lazaridis et al.'s 2016 study estimates 6.5–50.2% steppe related admixture in all modern South Asians with higher caste and Indo-Aryan speaking groups having more steppe admixture than others.[note 12]
A series of studies from 2009 to 2019 have shown that the Indian subcontinent harbours two major ancestral components,[39][40][41] formed in the 2nd millennium BCE,[19] namely the Ancestral North Indians (ANI), which is closely related to contemporary West-Eurasians, and the Ancestral South Indians (ASI) which is distinct from any outside population.[39][19][note 13] ANI formed out of a mixture of IVC-people and migrants from the steppe, while ASI was formed out of IVC-people who moved south and mixed further with local hunter-gatherers.[19]
These IVC-people did not carry steppe admixture and were instead a mixture of mostly Neolithic Iran-related ancestry and minor AASI (native South Asian hunter-gatherer) ancestry. According to Narasimhan et al. 2019, the genetic makeup of the ASI population consisted of about 73% AASI and about 27% from Iranian-related peoples.[19] This estimate is similar to that of Reich et al., who in 2018 note that the ASI have a West-Eurasian ancestry component (derived from Iranian-related farmers) which Reich estimates at about 25% of their ancestry (not detected in his initial 2009 analysis), with the remaining 75% of the ancestry of the ASI deriving from native South Asian hunter-gatherers.[86]
ANI formed out of a mixture of IVC-people and migrants from Bronze age steppe.[19] Lazaridis et al. (2016)[note 14] notes that the demographic impact of steppe related populations on South Asia was substantial. According to the results, the Mala, a south Indian Dalit population with minimal Ancestral North Indian (ANI) along the 'Indian Cline' have nevertheless c. 18% steppe-related ancestry, showing the strong influence of ANI ancestry in all populations of India. The Kalash of Pakistan are inferred to have c. 50% steppe-related ancestry, with the rest being of Iranian farmers ancestry.[87][note 15] Reich et al. stated that "ANI ancestry ranges from 39–71% in India, and is higher in traditionally upper caste and Indo-Aryan speakers".[39]
According to Ness, there are three broad theories on the origins of the Austroasiatic speakers, namely northeastern India, central or southern China, or southeast Asia.[88] Multiple researches indicate that the Austroasiatic populations in Central India are derived from (mostly male dominated) migrations from southeast Asia during the Holocene.[89][90][91][92][93][note 16] According to Van Driem (2007),
the mitochondrial picture indicates that the Munda maternal lineage derives from the earliest human settlers on the Subcontinent, whilst the predominant Y chromosome haplogroup argues for a Southeast Asian paternal homeland for Austroasiatic language communities in India.[94]
According to Chaubey et al. (2011), "AA speakers in India today are derived from dispersal from Southeast Asia, followed by extensive sex-specific admixture with local Indian populations."[90][note 17] According to Zhang et al. (2015), Austroasiatic (male) migrations from southeast Asia into India took place after the latest Glacial maximum, circa 4000 years ago.[92] According to Arunkumar et al. (2015), Y-chromosomal haplogroup O2a1-M95, which is typical for Austroasiatic speaking peoples, clearly decreases from Laos to east India, with "a serial decrease in expansion time from east to west", namely "5.7 ± 0.3 Kya in Laos, 5.2 ± 0.6 in Northeast India, and 4.3 ± 0.2 in East India". This suggests "a late Neolithic east to west spread of the lineage O2a1-M95 from Laos".[93][95]
According to Riccio et al. (2011), the Munda people are likely descended from Austroasiatic migrants from southeast Asia.[91][96] According to Ness, the Khasi probably migrated into India in the first millennium BCE.[88]
According to a genetic research (2015) including linguistic analyses, suggests an East Asian origin for proto-Austroasiatic groups, which first migrated to Southeast Asia and later into India.[92]
According to Cordaux et al. (2004), the Sino-Tibetan possibly came from the Himalayan and north-eastern borders of the subcontinent within the past 4,200 years.[97]
The ancient people, who lived in the upper-middle Yellow River basin about 10,000 years ago and developed one of the earliest Neolithic cultures in East Asia, were the ancestors of modern Sino-Tibetan populations.[99] Haplogroup O2-M122 is primarily found among males of Sino-Tibetan ancestry in the Himalayas and Northeast India and which is generally absent among other linguistic families other than Northeast India. O-M134, a subclade of O-M122, has a high percentage, 86.6%, among Tamangs of Nepal, with similar frequencies, ~85%, among the northeastern Indian Tibeto-Burman groups, including Adi, Naga, Apatani, and Nyishi.[100] In Assam, Tibeto-Burman expansion throughout Brahmaputra Valley associated with the patrilinial lineage of O-M134[102] which occurs at a high frequency of 85% in Kachari (Boro Kachari) peoples and 76.5% in Rabha peoples.[103][105] It has a significant presence among the Khasis (29%), despite being generally absent in other Austroasiatics of India, and it shows up at 55% among the neighbouring Garos, a Tibeto-Burmun group.[106]
A wide variety of Sino-Tibetan languages are spoken on the southern slopes of the Himalayas. Sizable groups that have been identified are the West Himalayish languages of Himachal Pradesh and western Nepal, the Tamangic languages of western Nepal, including Tamang with one million speakers, and the Kiranti languages of eastern Nepal. The remaining groups are small, with several isolates.
The Newar language (Nepal Bhasa) of central Nepal has a million speakers and a literature dating from the 12th century, and nearly a million people speak Magaric languages, but the rest have small speech communities. Other isolates and small groups in Nepal are Dura, Raji–Raute, Chepangic and Dhimalish. Lepcha is spoken in an area from eastern Nepal to western Bhutan.[107] Most of the languages of Bhutan are Bodish, but it also has three small isolates, 'Ole ("Black Mountain Monpa"), Lhokpu and Gongduk and a larger community of speakers of Tshangla.[108]
One complication in studying various population groups is that genetic and linguistic affiliations in India only are partially correlated, especially in cases where Austric-related peoples have adopted languages from their non-Austric neighbors. For example, while the Oraons have Austric-related ancestry, their language, called Kurukh, is Dravidian.[109] The Bhils and Gonds are frequently classified as "Austric" groups,[110] yet Bhil languages are Indo-European and the Gondi language is Dravidian.[109] On the other hand, the Khasis and the Nicobarese are considered to be a Mongoloid group,[111][112] and the Mundas and Santals are "Austric" groups,[113][114] but all four speak Austro-Asiatic languages.[111][112][113]

The Prehistory of Transylvania describes what can be learned about the region known as Transylvania through archaeology, anthropology, comparative linguistics and other allied sciences.
Transylvania proper is a plateau or tableland in northwest central Romania. It is bounded and defined by the Carpathian Mountains to the east and south, and the Apuseni Mountains to the west. As a diverse and relatively protected region, the area has always been rich in wildlife, and remains one of the more ecologically diverse areas in Europe.  The mountains contain a large number of caves, which attracted both human and animal residents. The Peștera Urșilor, the "Bears Cave", was home to a large number of cave bears (Ursus spelæus) whose remains were discovered when the cave was discovered in 1975. Other caves in the area sheltered early humans.
Prehistory is the longest period in the history of mankind, throughout of which writing was still unknown. In Transylvania specifically this applies to the Paleolithic, Neolithic, Bronze Age, and Iron Age.[citation needed][dubious – discuss]
The Paleolithic epoch, the oldest and longest period in the history of mankind, is divided by specialists into three stages of development: Lower Paleolithic, Middle Paleolithic, and Upper Paleolithic. The chronological frame of the Paleolithic coincides with that of the Pleistocene (the first period of the Quaternary), and is marked by four great glaciations, as established in the Alps (Günz, Mindel, Riss, and Würm).
While an ever-increasing amount of data has become available on the evolution of the climate, fauna and vegetation of present-day Romania, there is very little in the fossil record to give researchers an idea of what Paleolithic man in Romania looked like. To date, no human skeletal remains dating from the Low Paleolithic have been found, while the only Middle Paleolithic remains that have been discovered were a number of phalanges unearthed by Márton Roska [hu] in the Bordu Mare Cave at Ohaba Ponor, Hunedoara County. A skull capsule discovered by Roska in the Cioclovina Cave displays features attributed to Homo sapiens sapiens, and dates back to the Upper Paleolithic as indicated by three flint objects peculiar to the Aurignacian discovered next to them. Likewise, in the Ciurul Mare Cave in the Pădurea Craiului Mountains (Transylvania) speleologists have discovered some distinctively male, female and child footprints. An anthropological analysis has identified Cro-Magnon and even Neanderthal characteristics in these footprints.
The economy of the Paleolithic communities consisted mainly of exploiting natural resources: gathering, fishing and especially hunting were the main pursuits of the diverse human groups. As early as the Lower Paleolithic, human groups either hunted or trapped game. We can assume that in Transylvania, alongside mammoths or deer, horses were a fairly important food source, if our dating of the painting on the ceiling of the cave at Cuciulat, Sălaj County, is correct.
The Lower Paleolithic in Transylvania, because data are scarce, is largely a mystery. If the discovery of an Acheulean lithic item at Căpușu Mic, Cluj County, and of several Pre-Mousterian lithic items at Tălmaciu,  Sibiu County, are a certain fact, their precise stratigraphic position remains to be established. The same cannot be said about the discoveries in the Ciuc Depression [ro] at Sândominic, Harghita County, where several tools, and a rich fauna, have been encountered in certified stratigraphic positions, belonging to the geo-chronological interval covering the late Mindel to the early Riss.
The Middle Paleolithic – Mousterian – covers a time period much shorter than that of the prior epoch (c. 100,000 – 33,000/30,000 BP). It is a period set largely in Early Upper Pleistocene, and corresponds within the alpine glacial chronology to the interval covering the late Riss-Würm interglacial, or rather the Lower Würm, through middle Würm, as indicated by the dating of the late Mousterian dwellings in the Gura Cheii Cave, Râșnov, Brașov County, and the Spurcată Cave, Nandru, Hunedoara County.
The Mousterian period is closest to the alpine Paleolithic. Both periods were characterized by the presence of numerous quartzite slivers and chips, with the bones of hunted game outnumbering the tools. Consequently, specialists consider this Mousterian to be an "Eastern Charentian".
Likewise, North-Western and Northern Transylvania with the settlements at Boinești, Satu Mare County, and Remetea, Maramureș County, have revealed several typically Mousterian tools (flake scrapers, blade scrapers, target points etc.), some of which have been associated with a later stage of the Mousterian, or even with a transition stage to the Upper Paleolithic, at the onset of the Aurignacian culture of the Upper Paleolithic.
The process of regional diversification among cultures was accelerated in the Upper Paleolithic through the middle to upper Würm. The beginnings of the Upper Paleolithic on the territory of Romania is dated somewhere between 32,000/30,000 – 13,000 BP, corresponding paleoclimatically to the onset of the Arcy oscillation, and is marked by the development of the two great civilizations: the Aurignacian and the Gravettian both featuring several stages of development as established by stratigraphy.
The onset of the Aurignacian culture seems to have paralleled the late Mousterian facies in the Carpathian caves, if we accept as valid the C14 dating of level IIb in the cave of Gura Cheii in Râșnov. Northwestern Transylvania is the site where layers of the Middle Aurignacian culture have been identified, as signaled by the presence of blade scrapers, refitted core, [clarification needed] burins. In Banat, the settlements of Tincova, Coșava, and Românești-Dumbrăvița, have produced flint tools demonstrating that the Aurignacian in this area evolved closely with that in Central Europe (the Krems-Dufour group). Aurignician items were also found in the caves in the Western Carpathians, the most famous of which is the Cioclovina cave in Hunedoara County – the site, around the start of the 20th century, of the first Paleolithic discoveries in Transylvania.
The Eastern Gravettian had a long evolution, featuring several stages of development as documented especially by the settlements in Moldova. The Gravettian has left traces in Țara Oașului and Maramureș, the sites of microlite fashioned mainly out of obsidian indicating the connection with the Gravettian in the neighboring regions (Moldavia, South-Carpathian Ukraine, Eastern Slovakia, and Northeastern Hungary).
The Late Gravettian covers Banat too, particularly the area of the Iron Gates of the Danube, where heads identical to the Laugerie-Basse type heads were discovered in grottoes and open air dwellings. Still in Banat, a culture with several stages of development was identified and subsequently named the Quartzite Upper Paleolithic by its discoverer, considered to be synchronous with the local Aurignacian, later the Gravettian, and regarded as a prolongation of the late stages of the Mousterian with quartz and quartzite tools (Eastern Charentian).
The populations evolving at the onset of the Bölling oscillation (approximately 12,000 BP) and which have continued to the end of the Preboreal have been generally attributed to the Epipaleolithic. Consequently, this historical period could be associated with the interval between 13,000 and about 9,500-9,000 BP. These communities continued the lifestyles of the Upper Paleolithic. Due to numerous factors, including changes in the climate, the small groups of hunters-fishermen-gatherers innovated tool and weapon types – producing, for instance, microlites (trapeze) — while also keeping the traditional tool types.
The Iron Gates mesolithic culture of the central trans-Danube region, named by archaeologists after the gorge of Porţile de Fier ("Iron Gates"), was settled by a population attributed to the Late Epigravettian or Mediterranean Tardigravettian. The first stage of this period was discovered by archaeologists in the Climente II cave (Mehedinți County), and the second stage, by discoveries in the shelter under the rocks at Cuina Turcului, Dubova, both of which are located in the same limestone massif — Ciucaru Mare. The two dwelling levels at Cuina Turcului have produced a large quantity of tools and weapons made of flint in particular, and less so of obsidian, bone and horn, as well as body ornaments (shells and drilled teeth, bone pendants, etc.) The ornaments are often decorated with incised geometrical patterns. The most remarkable is a drilled horse phalange, wholly ornamented and probably representing a female figure.
Besides the mammal (beaver, boar, mountain goat, etc.), bird and fish remnants, fragments of human skeletons were also found. The Climente II cave has produced a human skeleton, set in a crouching position, and covered by a thick layer of red ochre, which is attributed to the Tardigravettian dwelling and which predates Level I at Cuina Turcului.
The discoveries in the Clisura area display striking similarities to the industries of the Italian Peninsula — the expression of the migrant human bearers of the Late Epigravettian in the mentioned area.[clarification needed]
Specialist opinions fix the beginning of the Mesolithic era at the end of the Preboreal, its development throughout the Boreal, and its end as late as the beginnings of the Atlantic. Chronologically then, it can be set between 9,500-9,000 and 7,500 BP. Two cultures are documented on the territory of Romania in this time period: the Tardenoisian and the Schela Cladova types.
The Tardenoisian spread in several of the country's regions (Moldavia, Muntenia, Dobruja), including the mountainous area of Transylvania in the southeast (Cremenea-Sita Buzăului, Costanța-Lădăuți) and northwest (Ciumești-Pășune). In the settlement of Ciumești, Satu Mare County, besides the typically Central and East European Tardenoisian microlitice tools made of flint and obsidian, some artefacts were found in the form of circular segments and two triangular ones, in addition to trapezes. The fauna remnants indicate the presence of wild boar and deer.
Some specialists do not exclude the possibility of identifying the Late Tardenoisian communities of the north-western Pontic or central European types (of which the settlement at Ciumești is one) as being in the process of neolithization, albeit incomplete, that is, displaying an incipient productive economy, whose foundations were laid by animal domestication and plant cultures.
The Schela Cladovei culture is known through the nine open air settlements in the proximity of the Danube. The lithic utensils come in numerous atypical forms and are fashioned of quartzite and siliceous sandstone while an additional small number are made of flint. The horn tools (agriculture artefacts with one or two handle attachment holes) apparently indicate the debut of plant cultivation. Some of the larger river rocks flattened by water or some of the thicker slabs might have served for grinding. The examination of the fauna indicates an economy based mainly on hunting. The targeted game were deer, roebucks, European bisons, wild boars, hares, wild donkeys, foxes, etc. Furthermore, it would seem that the representatives of this culture domesticated the dog.
Anthropological data are quite consistent. The physical type was evaluated as Oriental Cro-Magnon. The skeletons of the deceased were laid in rectangular holes, some dug in the floor of the dwelling itself. Part of them was laid in a crouching position, part was laid on their back, together with some of their personal belongings. Child mortality was high, while the average life expectancy for adults was 36.2 years.[citation needed] The discovery of some skeletons with arrowhead marks speaks of violent death. Research so far has proved that this culture does not have its roots in the Mediterranean type Tardigravettian, but rather originated by some new migration into the Iron Gates region. In addition, it would seem that on the arrival of the first bearers of the Neolithic civilisation (Precriș culture), the Schela Cladovei culture had already come to an end.
The Neolithic began with the slow migration of communities from the south of the Balkan Peninsula (the Protosesklo culture from the Thesalo-Macedonean area), who brought with them momentous economic progress. Consequently, the process of neolithisation, which is essentially a shift to plant growing and animal breeding, was not an innovation of the local Mesolithic population but rather the result of the penetration of this territory by communities carrying the Neolithic civilization.
The normal divisions of the Neolithic are: Early Neolithic, Developed Neolithic, and Chalcolithic (Copper Age). The Neolithic epoch on the territory of Romania, as certified by calibrated 14C dates, began around 6600 BC, and ended around 3800–3700 BC, and no later than 3500 BC.
The Early Neolithic (c. 6600–5500 BC) consists of two cultural layers: genetically linked and with similar physiognomies. The first (layer Gura Baciului – Cârcea/Precriș) is the exclusive result of the migration of a Neolithic population from the South Balkan area, while the second (the Starčevo-Criș culture) reflects the process of adjusting to local conditions by a South Balkan community, possibly a synthesis with the local Tardenoisian groups.
The layer Gura Baciului – Cârcea, also called the Precriș culture, is a spin-off of a Protosesklo culture group that advanced north and reached the North Danubian region where it founded the first culture of painted pottery in Romania. The small number of sites attributable to this early cultural time has not allowed the route followed by the group, to penetrate the Inter-Carpathian area, to be firmly established, yet in all likelihood, it was the Olt Valley.
Based on the stratigraphy in the site of Gura Baciului, Cluj County, and Ocna Sibiului, Sibiu County, the development of the culture is divided into three major stages.[clarification needed]
The settlements are situated on high terraces strung along secondary valleys. The dwellings are most often underground, but there are also ground level houses, usually standing on river stone platforms. Pottery (bowls, cups) is refined, with white painted dots or geometrical patterns on red or brown-red background. Concomitant with pottery, plant cultures and animal breeding, the new culture introduces implements of polished stone and the first clay statuettes. The dead are buried on the grounds of the settlements sometimes directly under the dwellings. Gura Baciului is the first site on the territory of Romania attesting incineration as a funerary practice.
Anthropomorphic and zoomorphic plastic art reveals a bipolar system of beliefs: the Great Mother, representing the female principle, and the Bull, representing the male principle. The presence among the findings at Gura Baciului of some anthropomorphic stone heads, similar to the famous stone heads of Lepenski Vir, signify possible contact between the locals, the Mesolithic cultures, and the newcomers. Furthermore, the adoption of these alien deities, even if exclusively a plastic substantiation,[clarification needed] speaks of a remarkable process of assimilation, characteristic of the layer above mentioned.[which?]
At Ocna Sibiului, at Precriș, level II, a small conical stone statuette was found, with a shape representing a couple embracing, and a plinth of the same material associated with the figure. On the statue and the plinth several symbols can be distinguished interpreted by the discoverer as ideograms.
The Starčevo-Criș culture, representing the generalisation of the early Neolithic in the Intra-Carpathian territory, has been regarded by some as the prolongation of the Gura Baciului-Cârcea/Precriș culture, disregarding that it is probably the result of a new south Balkan migration (the Presesklo culture) arriving in Transylvania via Banat. The Starčevo–Criș culture has a long evolution in four stages.
Dwellings were set up on meadows, terraces, hills and even in caves, wherever the environment was friendly. The dwellings were embedded in the early phases and were huts at ground level, in the later phases. Asymmetrical receptacles, bowls, spherical cups, all of which were made of clay, furnish the interiors of this culture. The lithic utensil inventory includes flint and obsidian microlites, as well as large polished stone axes of the Walzenbiele type. It is now, too, that the first small copper items occur sporadically. The pintaderas decorated with geometrical patterns as well as the Spondylus and Tridacna shells testify to possible connections with Eastern Mediterranean regions. Burials were performed both inside and among the dwellings. Anthropological analyses have revealed a major Mediterranean component suggesting a southern origin of this population.
The Developed Neolithic (c. 5500–4000 BC) covers the interval between the last phase of the Starčevo–Criș culture and the beginnings of the Petrești culture, the period including what has long been known as middle and late Neolithic. The Developed Neolithic is marked by the migration of some new groups of populations, whose point of departure was the south of the Balkan Peninsula, as part of the group of cultures with polished black pottery. These same groups created the Vinča culture (more commonly divided into four main phases: A, B, C, and D), whose beginning is synchronous with the final phase of the Sesklo culture (Greece) occupying Banat and most of Transylvania. In about the same period, the north-east of Transylvania was penetrated by several groups, bearing the linear and musical note pottery culture.
The Vinča culture in Romania comes in many forms depending on the local background against which it developed (the Starčevo-Criș culture and the linear pottery) and the degree of southern influence. The synthesis of the above-mentioned elements gave birth to numerous related regional elements, so that when referring to the Transylvania territory, specialists do not speak of a Vinča culture per se, but rather that of the Banat culture, the Bucovăț group, the Pișcolt group, the Turdaș culture, the Cluj-Cheile–Turzii–Lumea Nouă–Iclod complex, and the Iclod group. A general characteristic of these groups is the black polished pottery (cups, bowls, lids, etc.). The decorations are variously incised and impressed (bands filled in with stripes, in particular) in addition to displaying fine grooves. The statuettes feature oblong heads (possibly indicating a mask), cross-like bodies, and are often decorated with spiral winding patterns.
In Banat, with the end of the Vinča A2 stage there emerges the Banat culture with several distinctive regional peculiarities (groups Bucovăț and Parța). The Parța settlement, thoroughly researched, demonstrates that the culture reached a high level of civilization, attested to by the one storey buildings and by a complex spiritual life, partly decoded by the components of the great sanctuary studied here. The cult edifice (with maximum dimensions of 12x6x7m), with two stages of construction, had two chambers, the one to the east, the other to the west, separated by an altar table and then a wall. The west chamber served as a depository for daily offerings. In the foundation of the south entrance to this chamber was laid a zoomorphic idol and a tiny vessel. The east chamber served for the initiation ceremonies. Religious centers of this type through their prestige and grandeur most certainly congregated the population of an extended area.
The charred seeds found in the Liubcova settlement indicate that several cereals were grown. Wheat prevailed, particularly the Triticum dicoccum species, as well as the Triticum monococcum and Triticum aestivus species in proportion of approx. 10%.[clarification needed] The first occurrence on the territory of Romania of the Hordeum vulgare barley is seen. Also present are such leguminous plants as lentil and vetch. Of paramount interest is that wheat was harvested, as discovered in a settlement south of the Carpathians (Teiu, the Gumelnița culture), and was possibly used in other areas, too. The wheat was harvested by pulling out, then was sheaved and tied with a switch, vine shoots or ivy. Once carried to the settlement, the grain was threshed.
The Vinča communities that advanced on the middle course of the Mureș River, under the influence of the Starčevo-Criș traditions and the elements of the linear pottery, created a new cultural synthesis called the Turdaș culture. The occurrence of signs incised on the bottom of several vessels, particularly on those at Turdaș (Hunedoara County), have often been regarded as the potter's mark. More recently they have been considered by some researchers as early attempts at recording dates graphically. That things might stand this way[clarification needed] is demonstrated, apparently, by the baked clay tablets covered with incised pictographic patterns at Tărtăria (Alba County), discovered, according to Nicolae Vlassa, in a ritual hole in the ground, next to clay and alabaster idols and a fragment of an anchor, all of which have triggered hot debate over the stratigraphy and chronology of the settlement.
The preservation by some Starčevo-Criș communities of painted pottery, in addition to the Vinča elements, engendered[clarification needed] in the area of the eastern arch of the Western Carpathians the Cluj–Cheile Turzii–Lumea Nouă–Iclod cultural complex. This complex represents the substratum[clarification needed] for the emergence of the Petrești culture. Long-term research at Iclod has demonstrated that this station possessed a complex fortification system built during the Iclod, Phase I, still in use for some time in the Iclod II phase, eventually abandoned when the settlement expanded. It is in the same spot that research has been carried in two inhumation necropoles,[clarification needed] where the dead were laid on their backs hands across their chests or abdomens or along their bodies; the bodies were oriented east–west, their heads facing east. The inventory consists of vessels (cylindrical, painted bowls, and S profile pots), ochre, stone utensils, ornaments and animal offerings.
The Chalcolithic, Eneolithic or Copper Age (c. 4600/4500 – 3800/3700 BC) is characterized by an ever-increasing number of copper items, as well as the presence of stone, bone, horn and baked clay utensils. It marks the first production of heavy copper tools and moulds, (axes – chisels and axes), in close conjunction with the exploitation of copper deposits in Transylvania. Gold is used for ornaments and the fashioning of such idols as those at Moigrad in the Bodrogkeresztúr-Gornești culture. The craft of pottery reaches a peak, exemplified by the great number of exquisitely decorated pots.
Cultures typical for this period are the Cucuteni-Ariușd, Petrești, Tiszapolgár-Românești and Bodrogkeresztúr-Gornești. The first two cultures are among the numerous Eneolithic cultures with pottery painted in bi- and tri-chromatic patterns.
At Ariușd, Covasna County, in the east of Transylvania, the first systematic excavations were undertaken in what is considered the neo-Eneolithic epoch in Romania. The material discovered has been integrated into the greater painted pottery complex of Cucuteni-Ariușd-Trypillia.
The Petrești culture diffused across almost all of Transylvania, is regarded as local in origin by some specialists, and as a migration originating from the southern areas of the Balkans, by others. It is primarily known for its painted decoration – patterns painted in red, brown-red, later brown, on a brick-red background, which testifies to the high standard of civilization of the bearers of this culture. The ornamental motifs consist in bands, rhombuses, squares, spirals, and windings. The typical forms are bowls, tureens, high stands. Plastic art is fairly scarce and so are brass items.
The end of this culture [clarification needed] has been associated with the entry into central Transylvania by the bearers of the Decea Mureșului culture/horizon and the Gornești culture.
The graves at Decea Mureșului, according to some, are a continuation of the rituals of Iclod, whereas according to others, they are hard proof of the penetration of central Transylvania by a north-Pontic population. The presence of red ochre scattered over the skeletons, or laid at their feet in the form of little balls, as well as other ritual elements find better analogies, however, in the necropolis at Mariupol in south Ukraine.
The Gornești culture, characterized by the occurrence of the so-called high-necked milk pots with two small protuberances pulled at the margin and drilled vertically, is a continuation of the [Românești] (featuring receptacles with bird bill protuberances and decorated with step[clarification needed] or nettle incisions), in turn descended from the Tisa culture in the developed Neolithic period.
The settlements of the neo-Eneolithic cultures were located on the low or high river terraces, on hilltops or hillspurs and consisted in several dwellings whose positions sometimes observed particular rules. Recent research has tended to focus on the defense systems (ditches and scarps) of these sites. The culture strata are thick and superposed forming at times regular tells.
The dwellings of this period were of several types. The earth houses displayed an oval shaped hole, with a maximum of 5–6 metres (16–20 ft) and a minimum of 3 m (9.8 ft) in diameter. On one of the edges a simple fireplace was built out of a smoothed layer of clay. The thatched roof was conical or elongated and was supported by a trestle. The one room rectangular surface dwellings are also documented as dating back to the beginning of the Neolithic. They had wattle walls pasted with clay mixed with straw. The roof was double sloping, and the floor was made of trodden clay. The Cucuteni dwellings in south-east Transylvania are spacious (40–100 square metres (430–1,080 sq ft) and more), often have a platform and are divided into two or more rooms.
Neo-Eneolithic sculpture is represented by cultic figures, idols, and talismans fashioned out of bone, stone or clay. These are human or animal representations conveyed by stylized or exaggerated body parts. Among the thousand anthropomorphous statues discovered, the female ones, symbols of fertility and fecundity, prevail by far.
Copper was first used for fashioning small implements or ornaments (needles, awls, fishing hooks, pendants, etc.), while gold was used solely for aesthetic and decorative purposes. For a long time the items were produced by the technique of hammering, for the technique of the casting mould as well as that of "cire perdue" (lost wax) emerged much later. Although there is no proof of the provenance of the first metal items, they are seemingly local rather than imported products. That does not necessarily suggest that metallurgy was the invention of the local population, for it might have been introduced as a result of contact with regions where metal processing had started earlier (in the East or the Caucasus).
The Eneolithic marked a notable advancement in the development of metallurgy. Throughout this period copper artifacts are present in the settlements, in grave inventories or even in deposits (assemblies of whole or fragmentary objects concentrated in one, usually isolated, place). This period also marks a high incidence of flat axes, pins, simple or multi-spiral bracelets or necklaces. The most complex of all Eneolithic achievements is the axe. These weapon-implements are bound to[clarification needed] the late phases of the Cucuteni, Decea Mureșului, and Bodrogkeresztúr-Gornești cultures. The gold Eneolithic items, outnumbered by the copper, actually constitute the beginning of goldsmithing in the Transylvanian lands. An outstanding artifact was the great gold pendant in the thesaurus of Moigrad, Sălaj County, which is 30 cm (12 in) in height and weighs 750 g (26 oz).
We know little about the racial types of the Transylvanian Neolithic population. The area of some of the cultures, for instance Cucuteni, lack funeral finds, for they are the expression of ritual practices that elude archeological methods. The little anthropological data available (Gura Baciului, Iclod) suggests Mediterranean-type physical features.
The role of the invasion of the pastoral tribes coming from the north-Pontic (supposedly Indo-European kinship) in bringing to an end the Eneolithic culture of sedentary farmers, represents one of the hotly debated issues among specialists in the prehistory of south-eastern Europe. What once might have seemed exclusively a migration of nomadic tribes, now may be understood as a socio-economic transformation of the local population—its adaptation to the new environment, to the evolution of society (the increasing role of the animal breeders and shepherds, the development of metallurgy, extended mobility, the increasingly military role of the elites, changes in the belief systems, etc.).
In conclusion, the Eneolithic was a period of stability, in which the sedentary populations created a spectacular civilization.
For a long time the Romanian Bronze Age had been divided into four periods, but the archeological facts have imposed in the last decades the use of a three-part system: Early, Middle, and Late Bronze.
As communities acquired the secrets of alloying brass and arsenic, tin, zinc, or lead, achieving the first items in bronze, the long period during which stone constituted the main raw material for fashioning implements and weapons was coming to an end. The emergence and development of bronze metallurgy is accompanied by numerous substantial changes in economic and social life, in the spiritual life, and in the arts. The ensemble of these modifications – archeologically identifiable especially midway in the Bronze Age, yet already prefigured early on in the transition period from the Eneolithic to the Bronze Age – indicates a civilization far more sophisticated than we had imagined.
The first stage of the Early Bronze Age is a genuine cultural mosaic, juxtaposing transitory civilizations with those typical of the Bronze Age. For the first, the most typical is the Baden–Coțofeni cultural bloc, which perpetuated in many aspects a transitory lifestyle, but evolved in parallel to the pre-Schneckenberg and Schneckenberg civilisations, which were more active in taking over[clarification needed] the products of the Aegean-Anatolian Early Bronze. One can no longer speak of Eneolithic or neo-Eneolithic cultures, as defined by this historical period, for the changes occurring in the social structure are radical. The rise in status of the chieftains, indicated by the erection of tumulus funeral monuments, the different type of metallurgy, the different type of economy based on greater mobility as evinced by the impressive number of settlements belonging to the Coțofeni culture.
During the second stage, in the center of Transylvania there develops a cultural group bearing the name of the locality of Copăceni, Cluj County, which favored the locations afforded by the elevated sites in the eastern, and probably western, arch of the Western Carpathians and the upper basin of the Someș rivers. Their main pursuits were agriculture, animal breeding and ore extraction. They had surface dwellings, medium-sized (3x4m) with a rectangular layout, and pottery displays mainly high-necked pots with a short bottom portion often decorated with barbotine. Frequently the pots' rims are thickened and decorated with rope impressions. The dead are buried in tumuli such as those at Cheile Aiudului, Cheile Turzii, and Cheile Turului. The Copăceni group evolved in parallel to the Șoimuș and Jigodin groups, the former in the south-west, and the latter in south-east Transylvania.
Finally, the third stage is the least known, and is characterized by the use of ceramics with brush decorations and textile impressions.
Non-ferrous metallurgy in Early Bronze Age, given the substantial fall in production as compared to the Eneolithic, should be regarded as undergoing some sort of realignment, or repositioning, rather than indicating an acute decline. The causes of this phenomenon are many and diverse (exhaustion of the usual mineral sources, major technological changes, disturbing ethnic reshuffling, etc.). Significantly, the first bronze items (brass alloyed with arsenic, and later tin) now emerged.
The archeological sites of this period have uncovered more varied jewelry (hair rings, bracelets, necklaces, pendants made in copper, bronze or gold), poniards, flat axes as well as 'raised margin' axes. Yet the most important achievement of the age is the single-edged axe. Apparently the majority of these products were manufactured in local workshops. The proof is the numerous moulds for casting axes discovered at Leliceni, Harghita County, part of the Jigodin group. Hard to ignore is the often evoked ritual hole at Fântânele, part of the Copăceni group, where were found fragments of moulds for casting metal items (little chisels, poniards, massive axes), testifying that the level of the Vâlcele, Cluj County, type of axe had certainly been attained.
This culture occupied the Middle and Late Bronze Ages. In the diffusion of the archeological cultures on the lower course of the Mureș River, the Periam-Pecica/Mureș culture emerges, bordered in the south by the Vatina culture and in the north (territories in Hungary and Slovakia included), by the Otomani culture; the Transylvanian Plateau was occupied by the Wietenberg culture, which gradually ceded part of its northern area to the Suciu de Sus culture. All of these cultures evolved together, the earliest being evidently the Mureș culture, the Suciu de Sus culture appearing later. Among the five regional cultural groups, the Wietenberg and Otomani cultures occupy a special position. The division into periods, according to the stratigraphy of the sites at Derșida, Sălaj County, and Otomani, Bihor County, represents in addition to that of Sărata Monteoru in Muntenia, the major demarcations of the Romanian Bronze chronology.
The late period of the Bronze Age brings to Transylvania a marked process of cultural uniformity, whose direct manifestation is the local variety of the Noua culture. It is now, too, that the Lăpuș groups spins off the Suciu de Sus culture while the western areas are covered by the Cehăluț and Igrița groups.
Ceramics are the prehistoric artifacts that have been available in the greatest quantity and variety, thus providing the foundation of all of the above-mentioned cultural classifications.
The pattern repertoire of these cultures is abstract and geometric. The Wietenberg, Otomani and Suciu de Sus cultures, regularly and predominantly, displayed dynamically designed solar symbols (continuous spirals, crosses with spirals etc.) in the early stages of their cultural development. The same symbols appeared, in static form, (crosses, spiked wheels, rays, etc.) for the other cultures (Vatina, Mureș). Natural elements occurred rarely, and mainly as figurative art.
Most remarkable in this context were the super-elevated handles, shaped into ram heads, of a large size receptacle found south of the Carpathians, at Sărata Monteoru, Buzău County. The motif is repeated in markedly stylized forms on numerous pot handles of the Wietenberg culture. They were abstract to the extent that an animal was represented by a single defining element, for example a ram's horns. The same culture exhibits two rare achievements: a fragment of a cult wagon, exquisitely decorated, with both extremities ending in protomes, shaped as sheep-goat heads, discovered at Lechința de Mureș, Mureș County, and a gold axe displaying a fine engraving of a human silhouette next to a bovine silhouette, whose provenance is the thesaurus of Țufalău, Covasna County.
Close scrutiny of the production technique of the more complex vessels—the perfect duct[clarification needed] of some complex decoration patterns—strengthens the probability that the ceramics were produced by specialists. This does not exclude the possibility that other social groups, mainly children and adolescents, performed a secondary role. The transport of receptacles over long distances, in the absence of good roads, must have been an equally difficult operation, requiring itinerant craftsmen or special workshops near the more important centers.
The partial representations, the schematic physiognomies, as well as the faithful thematic rendering, though rare, all speak of a new symbolic expression that dominated the art of statuettes too. The moulding of the zoomorphic and anthropomorphic statuettes no longer attain the rich realism of the prior epoch, which is explained by the changes occurring in the religious and cult structure of the society. The incised and engraved decorations focus particularly on the details of the costume and the jewelry worn (hair rings, diadems, pendants, necklaces, etc.)
The importance of the settlements, as a constructed and limited human space for the prehistoric population, is graphically suggested by Mircea Eliade,[citation needed] when he interprets them as symbolic of the "centre of the world". The analyzed archeological sites evolved from simple groupings of lodges to complex urban facilities, directed towards maintaining collective lifestyle quality, ensuring the protection of life and goods, and meeting specific social, economic, defense and cultic needs.
Thus, there are central sites, with long-term developments, epicenters of a larger territory (Derşida, Otomani, etc.), and secondary sites evolving at the level of hamlets or seasonal dwellings (Suatu, Cluj-Napoca, etc.). The Otomani civilization in particular features a marked settlement hierarchy manifested in the ordered positioning of the dwellings, suggesting a pre-urban tendency. For instance, at Otomani – Cetățuie a circular settlement has been investigated, located on a hilltop and enclosed by a ditch and rampart. The dwellings were distributed in two concentric circles around an empty space at the center. The same organizing system is evident at Sălacea, where a megaron-type sanctuary has been explored.
Prior to this[clarification needed] century, the Intra-Carpathian space has been predominantly a land of farmers, as well as of craftsmen and animal breeders. In settlements belonging to the classical period of the Bronze Age were found charred seeds, numerous farming implements, grinding mills of diverse types, all attesting the intensive cultivation of grains. The widespread use of a primitive type of plough drawn by oxen is indicated by a great number of plough shares made of deer horn. Wheat, millet, barley, and rye were found in several Bronze Age sites. A Wietenberg ritual complex researched recently at Cluj-Napoca uncovered charred buckwheat, chick-peas and sesame seeds, and the ritual complexes at Oarța de Jos (Maramureș County) revealed the use of notch weed and sorrel.
The animal economy of the Bronze Age, with the familiar local variations, was based on pig, sheep and goat breeding, with a decline in large horned cattle. Thus, the inhabitants of the Vatina and Otomani cultures seem to have focused on breeding swine, sheep, goats, and on intensive hunting; while among the Wietenberg and Noua communities cattle were most common, used both for food and for traction, followed by sheep, goats, swine and horses. Horses were constantly present and revolutionized transportation and communication. The wagon with big wheels, later with spikes, emerged and spread, either as a warring and hunting vehicle, or to symbolize social status.
The food provided by agriculture and animal breeding was supplemented by hunting and fishing. Their proportion within the economy varied among the communities of the Bronze Age. For instance, at Sărata Monteoru (the Monteoru culture) they represented 8.11% and at Pecica, of the Mureș culture, 17.95%, in contrast to the area of the Noua culture where the percentage of hunting was, as a rule, much below 3%. Deer remained the most prized game in the Bronze Age, followed by wild boar and roebuck. A larger and more constant flow of the rivers, determined by an increasingly wet climate, is evident from the large fish bones found in many Bronze Age settlements.
There is no clear indication whether agriculture or animal breeding predominated within Bronze Age communities, with research revealing that both were being practiced together within the same area. But as populations stabilized, they tended towards a pastoral East and a farm-dominated West.
Men became more economically productive, due to improved metallurgy and better animal husbandry, and the use of draught animals in agriculture. Men acquired a dominant position within the family and in society.
For the Bronze Age people, the mountains provided hunting, timber and fruit, and held the copper and precious metal ores. Copper, silver and gold have always constituted major assets of the Intra-Carpathian region. The Apuseni Mountains are especially rich, as are the ores in the Maramureș Mountains, or the copper in the Giurgeului Mountains and Baia de Aramă. Metal outcrops are claimed to have been searched for by specialists, who perhaps then kept them secret. By washing gravel, or by digging pits for nuggets, the ore seekers satisfied the demand of local, prehistoric Europe, and even for the Mycenaean elites.
The unique direct proof of prehistoric exploitation of non-ferrous metals in Transylvania is the stone axe found in a gallery in Căraci (Hunedoara County). An impressive anthropomorphous statue was discovered at Baia de Criș Hunedoara or Ciceu-Mihăiești, Bistrița-Năsăud County. It portrayed implements (pickaxe and basket), whose absolutely sensational analogs were found in the photos of miners, taken by B. Roman at the middle of the last[clarification needed] century, strongly suggesting that the mining of non-ferrous metals was also performed underground.
Furthermore, the Natural History Museum in Vienna preserves two hair rings with the caption 'Dealul Vulcoi (Roșia Montană), district Câmpeni, region Cluj'. The museum in Lupșa exhibits a miner's axe and a club, both having come from the Lupșa valley. These exhibits demonstrate the presence of prehistoric miners in the ore-rich Apuseni Mountains.
Increasingly, traces of people involved in bronze-related activities are found. There are finished or semi-finished items, moulds, deposits or isolated items. The tracks of quarries and work-sheds are rather frail, firstly, because of subsequent exploitation, secondly, because of far too few exhaustive archeological investigations. The little workshop for moulding bronze pieces in the Wietenberg settlement at Derșida is well known.
The most complete and spectacular data related to metal processing workshops gathered so far, although partial, come from Palatca, Cluj County, from the Late Bronze Age, where the workshop was in close proximity to the dwelling area. The research has brought to light numerous moulds for casting metal items, unfortunately extremely fragmented, the fragment of a bronze cake, rectangular in shape, with curved sides, a bronze anvil, slag, several fragments of hand-mills, burnt out fireplaces and diverse rocks. Space in the workshop was organized in a complex way, depending on the current activities (selecting and grinding rocks, cutting and melting cakes, casting and retouching items). The presence at Palatca of the plano-convex type bronze cakes and, for the first time ever on Romanian territory, of the Aegean type, as well as the probable absence of metal reduction kilns demonstrate that this operation was performed in the mining areas.
The conversion of minerals to metal by means of fire was a process accompanied by rituals, magic formulas, and chanting to bring about the "birth of the metal". At the foundation of a kiln at Palatca formed by a burnt out clay fireplace and several slabs of whetstone laid one on top of the other, probably round in shape, a clay vessel had been deposited. Close to the workshop, a large ritual area has been explored. Receptacles with offerings were placed in multiple hypostases next to ore-refining items (hand-mills, bronze items, ash, coal, etc.), underneath or on top of the whetstone slabs, head down or head up.[clarification needed]
The mass of the ethnographic data which associates the ground with the belly, the mine with the womb, and the ore with the embryo, speaks of the sexuality of the mineral realm, and of the blacksmith's belongings and implements. The production of items is the equivalent of a birth and takes on an obstetrical dimension.  The blacksmith's implements have a sexual connotation. The anvils, for instance, are identified with the female principle. In this context, the closeness between the shape of the orifice for setting in place the anvil at Palatca, and the female generating organ was not coincidental. Another unique discovery was the meteorite. Meteorites coming from the skies fell on Earth with a celestial sacred charge and were often associated with the blacksmiths' activity.
The scarcity of settlements with metallurgic activity also hints at the possible existence of itinerant artisans and/or the centralization of the activity. This new development in bronze processing denotes a specialization in production with the appearance of prospectors, blacksmiths and merchants, who exported the surplus produce. Through exchange, the Transylvanian and east-Hungarian type axes with spiked discs spread as far east as Bug, and to the north, to the Oder and Elbe region, Pomerania included, a phenomenon connected with The Great Amber Road and the exploitation of brass and tin in the Elbe region. The metal artisans are not in power, but rather work under the control of an elite, which had seen the contingencies between metal and wealth, technology, war, and even the social and cultic structure.
The first level with gift depositaries[clarification needed] consisted in two main themes: the sword and the axe, outlining the role of the two weapons in the Intra-Carpathian warrior. The lance must have been yet another important weapon, but is a lesser find. The characteristics of the period are the bronze deposits at Apa, Satu Mare County (two swords, three war axes and a defense bracer[clarification needed]), Ighiu, Alba County (two axes with spiked discs and four defense bracers) and at Săpânța, Maramureș County (a spiked disc axe of type A2, exquisitely decorated, older than all the other pieces, spiral bracers, arm bands, and cordiform pendants). In the following stage, undecorated bronze items (single-edged axe and spiked disc axe), were produced and stored in ever increasing quantities. Many continued in the earlier style, but were also new types. Among the best creations of Bronze Age metallurgy were the Mycenaean type swords, whose dating is still debated.
The thesaurus found in 1840 at Țufalău, Covasna County, in the area of the Wietenberg culture, speaks clearly of the wealth and refined tastes of a social elite.  Kept in a clay pot, the thesaurus contained several solid gold axes, ornamental phalerae with spiral motifs, hair rings, one bracelet and one large gold piece. A great number of gold and silver items (bracelets, loop rings, etc.) were found at Oarța de Sus, with accurate stratigraphy, in a ritual space belonging to the Wietenberg culture. Such thesauruses containing hundreds of pieces weighing several kilograms, such as those at Sarasău, Maramureș County) or Hinova, Mehedinți County, are few and likely to represent the community treasure. They are outnumbered by those displaying fewer items which seem to have been the private property of some leaders.
Metal, bone, stone or clay processing were most certainly operations performed by specialists, who worked in small workshops, or sometimes larger ones such as those at Derșida or Palatca.
There certainly existed many wooden tools or receptacles, but they have not been preserved. Animal skin processing for fashioning clothing items, shields, harnesses, etc. must have been widespread.
The Bronze Age necropoles reveal funeral practices peculiar to each community. The graves, with variations specific to the different cultural entities, by their design and their contents attest to an advanced spiritual culture. Incineration (Wietenberg culture) or inhumation (Noua culture), the placing of offering-items alongside the deceased, all imply abstract thinking and belief in the afterlife.
Archeological investigations alone are too few and disparate for a detailed reconstruction of the religions of the Bronze Age people. The solar symbols, dynamic or static in form, (continuing spirals, simple crosses or crosses with spirals, spiked wheels, rays, etc.) are so numerous that they could be illustrated in a separate volume, and speak clearly about the prevailing role of this cult.
Cultic practices were performed by the people of the Bronze Age in diverse locations: in mountains, trees, springs, rivers, clearings or even, as noted, in specially assigned places inside the settlements. At Sălacea, Bihor County, in the southern area of the settlement of the Otomani culture there was a cultic edifice, a megaron type sanctuary measuring 5.20x8.80m, with a porch with two in antis pillars, a pronaos with an elevated altar and a naos with two fixed altars. The solid crust on the altar surface testifies to the rituals involving fire, while the walls nearby were provided with circular orifices (a ventilating system and alternative lighting of the altars depending on sunrise and sunset). On one of them were found nine clay weights, three curved stone knives, and one cylindrical clay stand. The other had nine clay weights in miniature, three curved stone knives and one cylindrical stand.
The symbolic value of the items and their number speak for themselves. The walls were decorated with plaster work with geometrical motifs (spirals, continuing spirals) randomly painted in white. Close by the entrance an infant grave has been researched, possibly partially deposited as an offering. Another founding ritual is encountered in Early Bronze Age at Copăceni, where under the lodge's floorboard were found five human skeletons (one female adult and four fetuses). Judging by their position – the female in an obstetric position with the fetuses around her basin and one between her inferior members – it could well be a mother and her infants.
All of these practices, judging by the archeological data mentioned above, as well as being based on other analogies, were accompanied by offerings, libations, chanting and cultic dancing. Apart from some daily festivals (sewing, harvesting, reaping, sheep loss or recovery, etc.), there must have been annual or multi-annual festivals of the whole community, or of part of it. This has been made clear from the above-mentioned research at Oarța de Sus – Ghiile Botii. The divinities guarding this space were in harmony with the weapons, ornaments or gifts personal or social in nature (grains, plants, food), with the animal, even human, sacrifices, with ceramics and bone, as well as with gold, silver or bronze. This wide variety of offerings, deposited in the course of grand religious ceremonies, indicate either an all-encompassing deity, or else several deities all worshiped within the same space.
In the Wietenberg culture area at Cluj-Napoca the underground deposition of offerings in a ritual hole and their contents (numerous receptacles filled with charred seeds) speaks of an agricultural ritual, one which was chthonic, dedicated to a harvest-giving deity ruling fertility. In this case sacred agrarian rituals, whose tradition is evident in the historical epochs too, was intended to inaugurate and imprint a rhythm to the agrarian calendar, and to achieve union between sun and soil through the agrarian ceremonial. The repeated occurrence of the solar motifs covering the walls of the receptacles deposited, typically masculine, might be speaking of the joining of the two spheres: earth-sun, female-male, immobile-mobile, thus demonstrating the dualism of creeds in the Bronze Age.
The link between the Carpathian region and the Mediterranean civilizations has often been the subject of debates, offering quite divergent opinions concerning their dating, direction, and significance. One of the main arguments concerns the bronze swords discovered on the territory of Romania. These long thrusting swords (symbols of dignity and power as well as formidable weapons) are obviously local products. The decorating motifs based on spirals and fine windings on bronze or gold weapons, on bone or horn items, are near to perfection, especially in the areas of the Wietenberg and Otomani cultures. If created independently of the Aegean models, they cannot be too far apart in time.
The glass in the Noua graves at Cluj-Napoca, and the Dentalium beads discovered at Derșida in a Wietenberg milieu, are also of Mediterranean origins. At Oarța de Sus on the shoulder of one of the cult receptacles are symbols in a line that are most certainly epigraphic. Similar images, also indicating a connection with southern civilizations, are found in the Otomani culture at Barca, Slovakia. Similarly, one of the bronze ingots at Palatca copies the well-known Aegean model. The striking similarities between the Wietenberg ceramics and the Apennine culture in northern Italy are difficult to explain. The assumption made long ago of a common generating center still stands, until final clarification.
In the same era, the metals produced on the slopes of the eastern arch of the Western Carpathians arrived in different ways in distant places all over Europe; so did the salt Transylvania is so rich in. Just as the obsidian, most probably exploited in the Bükk Mountains (Hungary), is encountered in the Wietenberg cultic complex discovered at Cluj-Napoca. The amber items in the deposit discovered at the Cioclovina cave came from the Baltic Sea, while the Caucasian influences are indicated by the axe discovered at Larga, Maramureș County.
The marked expansion of pan-European trade in middle and late Bronze Age created growing dependence between the different cultural groups, and an acceleration of uniformity in cultural values and produce. All of which sped up the general development of society and the passage to a new phase in historical evolution.
The Late Bronze Age shows a marked increase in metallurgic production based on the discovery of new non-ferrous mineral sources and the adoption of upgraded technology. The eastern experience brought by the bearers of the Noua culture and the southern experience (through Central European connections) brought by some late derivatives of the Otomani culture, both grafted onto the undoubted local experience, made Transylvania the most prolific metallurgic center in prehistoric Europe.
The differences identified between the deposits of the period speak not of unitary series,[clarification needed] but of types of deposits with a more limited geographic spread. One deposit, characterized by the almost exclusive presence of several types of axes, socked[clarification needed] axes, bracelets and foot rings, delineates the area of diffusion of the Suciu de Sus culture. The deposits in the area of the post-Otomani groups (Igrița and Cehăluț) contain almost exclusively ornament items, mainly pendants and pins. Finally, in central and eastern Transylvania, in the area of the Noua culture, we encounter the third type of deposit with the prevailing Transylvanian type of socked axes and the sickle.
Only a small number of bronze items were found in settlements and cemeteries. Most of them have a fortuitous appearance in what we call deposits. Romanian archaeology has interpreted their storage as a proof of troubled times, yet today a new interpretation is gaining ground: they are cultic deposits functioning as offerings, or at times, as the result of prestigious inter-community auctions of the "potlatch" type. The arguments in favour are strong: long periods of peaceful development, the location of the deposits (confluence of rivers, lakes, springs, clearings, mild slopes looking east, etc.), the number of items, the arrangements, their manipulations (fired, bent, fragmentation through bending, etc.), etc. Moreover, there is no logic in the locals burying their arms in the face of a military threat.
The multiplication of the offensive, in contrast to the defensive, fighting equipment (swords type Boiu – Sauerbrunn, battle axes with spiked disc, daggers, spearheads, arm bracers, all made of bronze), the development of settlements with man-made defenses, the existence of distinct warrior graves, gives the impression that the Bronze Age was a warring world. But there are numerous arguments that it was really a matter of parading rather than using force.
The extraordinary non-ferrous mineral wealth of the Intra-Carpathian region has often been remarked upon in the literature. The overwhelming number of finds of copper, bronze, silver, and gold products is hard to equal in prehistoric Europe. For instance, no other limited prehistoric space is known to have contained two large deposits dating from the same short range of time (Hallstatt A1). Uioara de Sus, accidentally found in 1909, contained 5,827 items weighing approximately 1,100 kg, while Şpalnaca II 1,000 paces away, in the year 1887, totaling a weight of 1,000 – 1,200 kg, was composed similarly of thousands of items. In addition to Șpalnaca I, Șpalnaca II, a deposit dated Hallstatt B1, was discovered a short distance away in the year 1881 and consisted of 120 bronze items.
The local copper ores often occur together with gold and silver. The gold must have been obtained, both by the washing gravel method in the valleys rich with such ores, as well as through mining the gold ore on the surface, or in shallow veins in ravines or landslides. There is no doubt that the tools and procedures of washing gold-bearing gravel did not differ greatly from those used throughout the ages up to the beginning of the 20th century. A wooden shovel, a vat (a similar clay item was found in one of the tumuli at Lăpuș), a screen, a piece of woolen linen or even a sheep's fleece sufficed. The output was a few grams per day per worker.
The First Iron Age period, also called the Hallstatt (after the finds in the locality of Hallstatt, Austria) covers the 10th to 5th centuries BC (1000-400 BC) and is divided into three periods: early (1000-700 BC), middle (700-600 BC) and late (600-400 BC). The Second Iron Age, also called La Tène,
generally covers the period between 450 BC and the peak of Roman Empire.
The defining phenomenon of the epoch is the use of iron with a paramount impact on humanity's subsequent evolution.
In contrast to the heterogeneity of the preceding ages, the first Iron Age is remarkable for its homogeneity, which is the result of the emergence and generalization of a new culture displaying black ceramics ornamented with grooves. This cultural homogenization in the First Iron Age represents essentially the material proof of the constitution within the Carpathian Danubian space of the early Geto-Dacians, who are culturally distinguishable from the southern Thracians and the other neighboring peoples. We are told this by the father of history himself—Herodotus. Recounting the Persian king Darius' expedition to the mouth of the Danube in 514 BC he mentions the Getae, praising them for their valour.[dubious – discuss]
Over 600 sites are known so far across the territory of Transylvania from the First Iron Age. Most sites were occupied during all stages of this epoch. Twenty-six fortifications, some inhabited permanently, others used for refuge and defense in times of peril, are among the more remarkable.
The fortified settlements and the refuge fortifications were usually located on inaccessible elevations and close to water courses and fertile areas. Their sizes vary with the location and its possibilities. For instance, the fortified settlement at Sântana (Arad County) with an area of approximately 100 hectares or those at Ciceu-Corabia (Bistrița-Năsăud County) and Teleac (Alba County), each measuring 30 hectares, count among the largest in Europe. The first Iron Age fortifications are also known in the county of Cluj, in Dej, Huedin and Someşul Rece.
The defense systems surrounding these regular strongholds consisted of a ditch, rampart and palisade, the last of which was designed as a wooden wall erected on the ridge of the rampart representing the most important part of the system. So designed, the fortifications generally measured 7–8 m in height, but could reach 10-12 making them difficult to conquer.
As tribal centers, the fortified settlements had multiple functions, the foremost of which was to ensure the defense of the community. The discovery of metallurgical workshops for manufacturing tools indicates that the settlements housed skilled craft activities, which included permanent exchange relations.
Agriculture was still the main pursuit in the First Iron Age and supplied food for the communities. Finding charred seeds indicates the cultivation of wheat, barley, rye, millet, as well hemp for linen, while the large size pots and the storage pits indicate how the harvests were preserved.  The emergence of the first iron farming implements, scythes and grubbing hoes, indicate notable progress in the agricultural practice.
The large quantity of bones discovered in the settlements, most originating from domestic animals, cattle, sheep, swine—as well as game—indicate the importance of domestic animals to supplement hunting, as well as the importance of meat in the daily diet.
Finally, besides some such crafts as metallurgy which imply special skill, members of every family engaged in a series of activities such as weaving, spinning, and leather dressing, shown by the discovery in the dwellings of spindle, spools, sewing needles, and scrapers for cleaning hide.
The occurrence of decorations on a large number of vessels, the most perishable of categories, as well as on numerous body ornaments (hair pins, fibulae and others) shows that in the First Iron Age the artistic phenomenon was manifested especially in decorative art as geometric patterns.
Religion was demonstrably a daily presence in prehistoric communities. Thus, besides the magic practice and the fertility cult of ancient tradition, the depositing of offerings in appropriate ground holes, as well as the representations linked to the Sun cult, foreshadow the two components: chtonian and Urano-solar to become the characteristics of the Geto-Dacian religion in the classical period.
During the First Iron Age, the local culture was influenced by neighboring areas. Midway through the epoch, on the middle course of the Mureș there arrived from Banat elements of a culture called Basarabi. Displaying ceramics with specific decorations (incised and impressed), the culture was assimilated by the autochthonous background.
Subsequently, at the beginning of the late period of this epoch (6th century BC), a group of Scythian-Iranian extraction came into Transylvania from the direction of the North-Pontic. This group is marked by a series of inhumation graves with a typical inventory: arrow heads, lances called akinakai, and animal art representations. Research shows that in about the mid-5th century BC this group disappears through assimilation into the local culture. Actually, the end of the century also delimits the First Iron Age. During the following centuries the Geto-Dacians would attain a level of development that would lead them to form a state.

The greater Basque Country comprises the Autonomous Communities of the Basque Country and Navarre in Spain and the Northern Basque Country in France. The Prehistory of the region begins with the arrival of the first hominin settlers during the Paleolithic and lasts until the conquest and colonisation of Hispania by the Romans after the Second Punic War, who introduced comprehensive administration, writing and regular recordings.[1]
Basque people are the only Western Europeans that speak a non-Indo-European language - the Basque language - without having any known contemporary European ethnic or linguistic relatives.[2] A 2015 DNA study supports the idea that the Basques descend from Neolithic farmers who mixed with local hunters and were subsequently isolated for millennia.[3][4]
The earliest Homo erectus migration events into Western Europe and the Basque Country had very little impact on history, culture and settlement forms. Notable is nonetheless the Atapuerca Mountains complex in northern Spain, that was inhabited in sizable communities by Homo antecessor (or Homo erectus antecessor), Homo heidelbergensis and Neanderthals for many generations.[5] The first substantial settler groups have arrived during the Riss-Würm interglacial period, between 150,000 and 75,000 BP, who left traces of Acheulean culture and technology. These groups usually settled in the riverine lowlands, near the rivers Ebro and Adour in the regions of Araba, Navarre, Labourd and Lower Navarre.[6]
Homo neanderthalensis Mousterian culture is introduced during the Middle Paleolithic. Neanderthals left a rather rich and long cultural sequence in northern Spain, who in the Basque Country populated the high coastal lands of Biscay and Gipuzkoa. Neanderthal remains have also been found in the Lezetxiki and Axlor caves.[7]
Homo sapiens first arrived on the Iberian Peninsula during the Upper Paleolithic period that starts the process of replacement of Mousterian industries by the Aurignacian culture. A number of researchers suggest that the Ebro river functioned for extended periods of time as a major biological/cultural frontier that separated the anatomically modern humans in the Franco-Cantabrian region to the north from the rest of the Iberian peninsula which is occupied by Neandertals for several thousand years longer. As modern humans settled in the northern territories from around 40,000 years BCE, earliest evidence from the south dates to between 34,000 and 32,000 years BCE.[8][7]  The term Vasco-Cantabrian is now often used for the coastal area of the modern Basque Country and neighbouring Cantabria.
Whether Homo sapiens or Neanderthal are to be attributed to the Châtelperronian (also called Lower Périgordian) culture is debated among specialists. Nonetheless, there are Châtelperronian remains (deposited between 33,000 and 29,000 BCE) that were found in the Basque Country in caves such as Santimamiñe (Biscay), Labeko Koba, Ekain (Gipuzkoa), Isturitz (Lower Navarre) and Gatzarria (Soule) as well as the open-air site of Le Basté (Labourd).
Artefacts of the Aurignacian culture, in particular Proto-Aurignacian objects have been found in Gatzarria and Labeko Koba.
The Aurignacian II is found in a few sites in Labourd: Le Basté and Bidart.
The Evolved Aurignacian is found mainly in Biscay and Gipuzkoa, at the sites of Lezetxiki, Aiztbitaterte IV, Koskobilo, Benta-Laperra, Kurtzia and Lumentxa.
The Gravettian period (also known as Périgordian) according to the classical French sequence, assimilates Chatelperronian and Gravettian in one single cultural complex.
Most of the findings belong to the upper and final Périgordian (V and VI): Santimamiñe, Atxurra, Bolinkoba, Amalda, Aitzbitarte III, Lezia, Isturitz and Gatzarria. The final phase of Périgordian VII is only found in Amalda (Gipuzkoa).
The Solutrean culture spans between around 18,000 to 15,000 BCE and only exists in the European south west, being coincident with the Last glacial maximum, a specially dry and cold period. Studies suggest that the higher regions of the western Pyrenees and massifs of the eastern Cantabrian Cordillera were only sparsely populated as glaciers descended to very low elevations and maximum sea level regression exposed another 5 to 12 km (3.1 to 7.5 mi) of the surrounding continental shelf. In fact Human population seems to have increased in these areas of Cantabria/Asturias, the result of a gradual influx of people, forced to abandon territories
in north-western Europe that have become off limits due to maximum glacial conditions. ″The Solutrean represents technological developments in weaponry, presumably for increased efficiency and effectiveness in hunting in the face of worsening environmental conditions and increased regional human population density. In the Basque Country, Solutrean points were added onto an extant lithic technology usually characterizable as Gravettian with Noailles burins.″[9] The Basque Solutrean facies is intermediate between the Cantabrian and Pyrenean ones and is found specially in Aitzbitarte IV, Bolinkoba, Santimamiñe, Koskobilo, Isturitz, Hareguy, Ermittia and Amalda.[9][10]
The Magdalenian culture can be found between around 15,000 and 8500 BCE and is widespread in Western - and later, Central Europe. Modern humans recolonize territories lost during the Last Glacial Maximum (LGM) and push into northern Europe for the first time, coming from the comparatively warmer Franco-Cantabrian region, where the Magdalenian originated.[11]
Magdelenian culture and its characteristic fine art is widespread in the Basque Country. Some of the most representative sites are Santimamiñe, Lumentxa, Aitzbitarte IV, Urtiaga, Ermittia, Erralla, Ekain and Berroberria. Researchers have argued, that the original Basque people have their genetic and ethnic origins in the Magdalenian paleo-human population.[12][13]
The oldest expression of cave art in the Basque country could be in Venta La Perra (Biscay) showing animals like bear and bison, as well as abstract signs.
Nevertheless, most of the artistic expressions belong to the Magdalenian period. The most important sites are:
Additionally 13 sites have yielded portable art, being most notable that of Isturitz.[14][15]
In the Epipaleolithic period, as the Last Ice Age came to an end, Magdalenian culture experienced a regionalization all around Europe, producing new localized cultural complexes. In the case of the Basque Country and the Franco-Cantabrian region as a whole, this product was Azilian, that in alter period would incorporate the geometric microliths associated with Tardenoisian and related cultures.[16]
As the climate improved gradually, population increased and colonized areas that before were out of reach. The regions of Araba and most of Navarre were hence colonized in this period.[17]
The period shows two phases, related to climatic conditions:
As in other post-Magdalenian areas, the disappearance of realistic cave art is quite noticeable. Instead the typical Azilian decorated pebbles have been found, as well as some geometrically decorated bones and plates. Additionally personal ornaments, made up of teeth or shells, are common as well.[18]
The Neolithic is characterized by agriculture and animal husbandry. In the Basque Country it was a late arrival, leaving its inhabitants in a subneolithic situation almost until the beginning of metallurgy in most of the territory.
The earliest evidence of contact with Neolithic peoples is in Zatoia, northern Navarre, with pottery remains dated to c. 6000 BP. The first evidence of domestication appears in Marizulo (Gipuzkoa) c. 5300 BP. These innovations gradually expanded, though hunter-gathering activities remained being important.
Overall the vast majority of important Neolithic sites are placed in the southern part of the country (Ebro valley): Fuente Hoz, Peña Larga, Berniollo and La Renke in Araba; Zatoia, Abauntz, Peña, Padre Areso and Urbasa 11 in Navarre; Herriko Barra in Gipuzkoa.
In the early phases there is only evidence of domestication of dogs. In the advanced Neolithic, remains of ovis and capra are found in sites like Fuente Hoz (Araba) and Abauntz (Navarre). In the late phase, oxen and pig are found as well. Seafood gathering remained being an important source of food in the coast.
Lithic industry shows total continuity with the Epipaleolithic (geometric microliths) but some new elements, like sickles and hand mills, begin to appear as well. Stone polishing makes in this period its first appearance, becoming more frequent at later dates.
Pottery was initially scarce, yet it became more common and variegated at the end of Neolithic (c. 3000 BCE).
Burial customs became more defined in this period, using specific burial spots like dolmens, mounds or caves. A remarkable case is the massive burial site under rock of San Juan Ante Porta Latinam (Araba) that included 8,000 bone remains, belonging to at least 100 individuals.
The human type is sometimes defined as Western Pyrenean.[19]
The Chalcolithic (Copper and Stone) period, also known as Eneolithic or Copper Age, lasts in the Basque Country from c. 2500 to c. 1700 BCE.
While hunting was still of some importance, especially in mountain areas, food production became finally dominant.
Lithic industry persists but some tools were already made of copper (axes, knives, etc.). Gold is also used for ornaments.
An important phenomenon in the late Chalcolithic is the Bell Beaker phenomenon of pan-European extension. Also through all the period Megalithism, specially in the form of burials in dolmens, was widespread.
The Basque Country has multitude of megaliths, described as dolmens or mounds, sometimes confusingly. They are in any case burials of collective nature, placed in spots of great visibility, often on top of mountain ridges. The materials used are always of local origin.
Dolmens are the most typical, being formed by a chamber delimited by flat stones, often quite large, covered by another stone as roof. The monument was then covered by stones and earth, making up a mound.
The chambers are of two types: simple or with corridor. The first are more common, while the latter are limited to the Ebro valley area. Dolmens are also classified by their size, normally the largest ones being in lowland areas and the smaller ones in mountain zones. This was probably a function of the number of people available to build the monument.
The burial classified as mounds lack of chamber but were otherwise used like dolmens for collective burials. There are around 800 dolmens known in The Basque Country and c. 500 mounds, though some of these could be dolmens as well, in wait of excavation.
Only a few Basque dolmens have clear stratigraphies, due to the usage of removing older remains to make room for new burials. In spite of this difficulty, it's known that megalithic burial customs arrived to the Basque Country in the late Neolithic being very frequently used in the Chalcolithic and Early Bronze Age, and, in the case of some mounds, as late as the Iron Age.
Other megalithic structures, such as standing stones (menhirs) and stone circles (cromlechs) seem to belong to later periods, specifically the Iron Age.
The Bronze Age spanned from c. 1700 to c. 700 BCE. It is largely a continuity of the previous period. Gradually bronze tools replaced stone and copper ones and we can find the first fortifications, that would become very common in the last centuries of this period.
This age is divided in three subperiods:
Megalithism continued for most of the period, yet external influences became increasingly noticeable since the Middle Bronze Age. In Araba the influence of Cogotas I is quite remarkable, while in the copper mine of Urbiola (Navarre) brachicephallic types, surely original from continental Europe, make up 30% of all remains.[19]
During the Iron Age in the 1st millennium BCE, with the arrival of Urnfield culture (proto-Celts) to the southern edge of the Basque Country (Ebro valley), there are some findings of iron tools and weapons. In the rest of the country it seems, from the few remains found, that the people remained in the cultural context of the Atlantic Bronze Age for some time.
Urnfield influence is limited to the Ebro valley, penetrating the Basque Country specially in Araba, where a peculiar facies of this culture, influenced as well by pre-Indo-European cultures of Aquitaine and the Iberian plateau (Cogotas I), exists.[20]
Since c. 400 BCE, there is a noticeable Iberian influence in the Ebro valley and central Navarre. Iron then became widespread, along with other advances such as the potter's wheel and an increase of production of cereal agriculture, that would allow for a larger population. Urbanization became more elaborated as well, with reticular street design in sites like La Hoya (Biasteri, Araba).
The Atlantic basin remains less developed and purely rural but there are many coincidences with the south. There are many sites, especially in the Northern Basque Country, that are awaiting archaeological excavation.
The economy became more and more centered in agriculture, specially cereals, with less importance of cattle and a marginal role for hunt. There is evidence of growing importance of bovine cattle (oxen).
Burial customs also changed, with a clear dominance of cremation in the Iron Age. The treatment of the ashes varies instead with burials in stone circles (cromlechs), mounds, caves, cists or urn fields.
The individual burial in cromlech is the most abundant but limited to the Pyrenean region, where 851 of these funerary monuments are documented. These cromlechs have diameters of 3–7 meters, with the burial located in the middle. Corpses were not cremated inside the cromlech but in a nearby spot, with only a handful of ashes being carried to the monument in fact.
Cave, cist and urn field burials were rare, the latter are only found in two sites at the Ebro valley. Cist burials, surely related to Iberian customs, have been found at La Hoya. Additionally many young children have been found buried inside homes.
Art was mostly limited to decorative purposes, especially in pottery. Some cases of painted geometric decorations in homes, with an occasional human figure as well, have been found in the prolific sites of southern Araba (La Hoya, Alto de la Cruz). Some alleged idols and carved wooden boxes are also known of. Schematic mural painting, in caves or exposed rocky walls, dates, according to some authors, from this period as well.
On this substrate, an irregular Romanization would take place at the beginning of our age. Some towns like La Custodia (Biana, Navarre) would become clearly romanized, while others not far away, like La Hoya, would retain their original native character fully.

Biological anthropology, also known as physical anthropology, is a natural science discipline concerned with the biological and behavioral aspects of human beings, their extinct hominin ancestors, and related non-human primates, particularly from an evolutionary perspective.[1] This subfield of anthropology systematically studies human beings from a biological perspective.
As a subfield of anthropology, biological anthropology itself is further divided into several branches. All branches are united in their common orientation and/or application of evolutionary theory to understanding human biology and behavior.
Biological Anthropology looks different today from the way it did even twenty years ago. Even the name is relatively new, having been 'physical anthropology' for over a century, with some practitioners still applying that term.[2] Biological anthropologists look back to the work of Charles Darwin as a major foundation for what they do today. However, if one traces the intellectual genealogy back to physical anthropology's beginnings—before the discovery of much of what we now know as the hominin fossil record—then the focus shifts to human biological variation. Some editors, see below, have rooted the field even deeper than formal science.
Attempts to study and classify human beings as living organisms date back to ancient Greece. The Greek philosopher Plato (c. 428–c. 347 BC) placed humans on the scala naturae, which included all things, from inanimate objects at the bottom to deities at the top.[3] This became the main system through which scholars thought about nature for the next roughly 2,000 years.[3] Plato's student Aristotle (c. 384–322 BC) observed in his History of Animals that human beings are the only animals to walk upright[3] and argued, in line with his teleological view of nature, that humans have buttocks and no tails in order to give them a soft place to sit when they are tired of standing.[3] He explained regional variations in human features as the result of different climates.[3] He also wrote about physiognomy, an idea derived from writings in the Hippocratic Corpus.[3] Scientific physical anthropology began in the 17th to 18th centuries with the study of racial classification (Georgius Hornius, François Bernier, Carl Linnaeus, Johann Friedrich Blumenbach).[4]
The first prominent physical anthropologist, the German physician Johann Friedrich Blumenbach (1752–1840) of Göttingen, amassed a large collection of human skulls (Decas craniorum, published during 1790–1828), from which he argued for the division of humankind into five major races (termed Caucasian, Mongolian, Aethiopian, Malayan and American).[5] In the 19th century, French physical anthropologists, led by Paul Broca (1824–1880), focused on craniometry[6] while the German tradition, led by Rudolf Virchow (1821–1902), emphasized the influence of environment and disease upon the human body.[7]
In the 1830s and 40s, physical anthropology was prominent in the debate about slavery, with the scientific, monogenist works of the British abolitionist James Cowles Prichard (1786–1848) opposing[8] those of the American polygenist Samuel George Morton (1799–1851).[9]
In the late 19th century, German-American anthropologist Franz Boas (1858–1942) strongly impacted biological anthropology by emphasizing the influence of culture and experience on the human form. His research showed that head shape was malleable to environmental and nutritional factors rather than a stable "racial" trait.[10] However, scientific racism still persisted in biological anthropology, with prominent figures such as Earnest Hooton and Aleš Hrdlička promoting theories of racial superiority[11] and a European origin of modern humans.[12]
In 1951 Sherwood Washburn, a former student of Hooton, introduced a "new physical anthropology."[13] He changed the focus from racial typology to concentrate upon the study of human evolution, moving away from classification towards evolutionary process. Anthropology expanded to include paleoanthropology and primatology.[14] The 20th century also saw the modern synthesis in biology: the reconciling of Charles Darwin's theory of evolution and Gregor Mendel's research on heredity. Advances in the understanding of the molecular structure of DNA and the development of chronological dating methods opened doors to understanding human variation, both past and present, more accurately and in much greater detail.

Bipedalism is a form of terrestrial locomotion where an animal moves by means of its two rear (or lower) limbs or legs. An animal or machine that usually moves in a bipedal manner is known as a biped /ˈbaɪpɛd/, meaning 'two feet' (from Latin bis 'double' and pes 'foot'). Types of bipedal movement include walking or running (a bipedal gait) and hopping.
Several groups of modern species are habitual bipeds whose normal method of locomotion is two-legged.  In the Triassic period some groups of archosaurs (a group that includes crocodiles and dinosaurs) developed bipedalism; among the dinosaurs, all the early forms and many later groups were habitual or exclusive bipeds; the birds are members of a clade of exclusively bipedal dinosaurs, the theropods. Within mammals, habitual bipedalism has evolved multiple times, with the macropods, kangaroo rats and mice, springhare,[4] hopping mice, pangolins and hominin apes (australopithecines, including humans) as well as various other extinct groups evolving the trait independently.
A larger number of modern species intermittently or briefly use a bipedal gait. Several lizard species move bipedally when running, usually to escape from threats.[5] Many primate and bear species will adopt a bipedal gait in order to reach food or explore their environment, though there are a few cases where they walk on their hind limbs only. Several arboreal primate species, such as gibbons and indriids, exclusively walk on two legs during the brief periods they spend on the ground. Many animals rear up on their hind legs while fighting or copulating. Some animals commonly stand on their hind legs to reach food, keep watch, threaten a competitor or predator, or pose in courtship, but do not move bipedally.
The word is derived from the Latin words bi(s) 'two' and ped- 'foot', as contrasted with quadruped 'four feet'.
Limited and exclusive bipedalism can offer a species several advantages. Bipedalism raises the head; this allows a greater field of vision with improved detection of distant dangers or resources, access to deeper water for wading animals and allows the animals to reach higher food sources with their mouths. While upright, non-locomotory limbs become free for other uses, including manipulation (in primates and rodents), flight (in birds), digging (in the giant pangolin), combat (in bears, great apes and the large monitor lizard) or camouflage.
The maximum bipedal speed appears slower than the maximum speed of quadrupedal movement with a flexible backbone – both the ostrich and the red kangaroo can reach speeds of 70 km/h (43 mph), while the cheetah can exceed 100 km/h (62 mph).[6][7] Even though bipedalism is slower at first, over long distances, it has allowed humans to outrun most other animals according to the endurance running hypothesis.[8] Bipedality in kangaroo rats has been hypothesized to improve locomotor performance, [clarification needed] which could aid in escaping from predators.[9][10]
Zoologists often label behaviors, including bipedalism, as "facultative" (i.e. optional) or "obligate" (the animal has no reasonable alternative). Even this distinction is not completely clear-cut — for example, humans other than infants normally walk and run in biped fashion, but almost all can crawl on hands and knees when necessary. There are even reports of humans who normally walk on all fours with their feet but not their knees on the ground, but these cases are a result of conditions such as Uner Tan syndrome — very rare genetic neurological disorders rather than normal behavior.[11] Even if one ignores exceptions caused by some kind of injury or illness, there are many unclear cases, including the fact that "normal" humans can crawl on hands and knees. This article therefore avoids the terms "facultative" and "obligate", and focuses on the range of styles of locomotion normally used by various groups of animals. Normal humans may be considered "obligate" bipeds because the alternatives are very uncomfortable and usually only resorted to when walking is impossible.
There are a number of states of movement commonly associated with bipedalism.
The great majority of living terrestrial vertebrates are quadrupeds, with bipedalism exhibited by only a handful of living groups. Humans, gibbons and large birds walk by raising one foot at a time. On the other hand, most macropods, smaller birds, lemurs and bipedal rodents move by hopping on both legs simultaneously. Tree kangaroos are able to walk or hop, most commonly alternating feet when moving arboreally and hopping on both feet simultaneously when on the ground.
Many species of lizards become bipedal during high-speed, sprint locomotion,[5] including the world's fastest lizard, the spiny-tailed iguana (genus Ctenosaura).
The first known biped is the bolosaurid Eudibamus whose fossils date from 290 million years ago.[12][13] Its long hind-legs, short forelegs, and distinctive joints all suggest bipedalism. The species became extinct in the early Permian.
All birds are bipeds, as is the case for all theropod dinosaurs. However, hoatzin chicks have claws on their wings which they use for climbing.
Bipedalism evolved more than once in archosaurs, the group that includes both dinosaurs and crocodilians.[14] All dinosaurs are thought to be descended from a fully bipedal ancestor, perhaps similar to Eoraptor.
Dinosaurs diverged from their archosaur ancestors approximately 230 million years ago during the Middle to Late Triassic period, roughly 20 million years after the Permian-Triassic extinction event wiped out an estimated 95 percent of all life on Earth.[15][16] Radiometric dating of fossils from the early dinosaur genus Eoraptor establishes its presence in the fossil record at this time. Paleontologists suspect Eoraptor resembles the common ancestor of all dinosaurs;[17] if this is true, its traits suggest that the first dinosaurs were small, bipedal predators.[18] The discovery of primitive, dinosaur-like ornithodirans such as Marasuchus and Lagerpeton in Argentinian Middle Triassic strata supports this view; analysis of recovered fossils suggests that these animals were indeed small, bipedal predators.
Bipedal movement also re-evolved in a number of other dinosaur lineages such as the iguanodonts. Some extinct members of Pseudosuchia, a sister group to the avemetatarsalians (the group including dinosaurs and relatives), also evolved bipedal forms – a poposauroid from the Triassic, Effigia okeeffeae, is thought to have been bipedal.[19] Pterosaurs were previously thought to have been bipedal, but recent trackways have all shown quadrupedal locomotion.
A number of groups of extant mammals have independently evolved bipedalism as their main form of locomotion –  for example, humans, ground pangolins, the extinct giant ground sloths, numerous species of jumping rodents and macropods. Humans, as their bipedalism has been extensively studied, are documented in the next section. Macropods are believed to have evolved bipedal hopping only once in their evolution, at some time no later than 45 million years ago.[20]
Bipedal movement is less common among mammals, most of which are quadrupedal. All primates possess some bipedal ability, though most species primarily use quadrupedal locomotion on land. Primates aside, the macropods (kangaroos, wallabies and their relatives), kangaroo rats and mice, hopping mice and springhare move bipedally by hopping. Very few non-primate mammals commonly move bipedally with an alternating leg gait. Exceptions are the ground pangolin and in some circumstances the tree kangaroo.[21] One black bear, Pedals, became famous locally and on the internet for having a frequent bipedal gait, although this is attributed to injuries on the bear's front paws. A two-legged fox was filmed in a Derbyshire garden in 2023, most likely having been born that way.[22]
Most bipedal animals move with their backs close to horizontal, using a long tail to balance the weight of their bodies. The primate version of bipedalism is unusual because the back is close to upright (completely upright in humans), and the tail may be absent entirely. Many primates can stand upright on their hind legs without any support. 
Chimpanzees, bonobos, gorillas, gibbons[23] and baboons[24] exhibit forms of bipedalism. On the ground sifakas move like all indrids with bipedal sideways hopping movements of the hind legs, holding their forelimbs up for balance.[25] Geladas, although usually quadrupedal, will sometimes move between adjacent feeding patches with a squatting, shuffling bipedal form of locomotion.[26] However, they can only do so for brief amounts, as their bodies are not adapted for constant bipedal locomotion.
Humans are the only primates who are normally biped, due to an extra curve in the spine which stabilizes the upright position, as well as shorter arms relative to the legs than is the case for the nonhuman great apes. The evolution of human bipedalism began in primates about four million years ago,[27] or as early as seven million years ago with Sahelanthropus[28][29] or about 12 million years ago with Danuvius guggenmosi. One hypothesis for human bipedalism is that it evolved as a result of differentially successful survival from carrying food to share with group members,[30] although there are alternative hypotheses.
Injured chimpanzees and bonobos have been capable of sustained bipedalism.[31]
Three captive primates, one macaque Natasha[32] and two chimps, Oliver and Poko[33] (chimpanzee), were found to move bipedally. Natasha switched to exclusive bipedalism after an illness, while Poko was discovered in captivity in a tall, narrow cage.[34][35] Oliver reverted to knuckle-walking after developing arthritis. Non-human primates often use bipedal locomotion when carrying food, or while moving through shallow water.
Other mammals engage in limited, non-locomotory, bipedalism. A number of other animals, such as rats, raccoons, and beavers will squat on their hindlegs to manipulate some objects but revert to four limbs when moving (the beaver will move bipedally if transporting wood for their dams, as will the raccoon when holding food). Bears will fight in a bipedal stance to use their forelegs as weapons. A number of mammals will adopt a bipedal stance in specific situations such as for feeding or fighting. Ground squirrels and meerkats will stand on hind legs to survey their surroundings, but will not walk bipedally. Dogs (e.g. Faith) can stand or move on two legs if trained, or if birth defect or injury precludes quadrupedalism. The gerenuk antelope stands on its hind legs while eating from trees, as did the extinct giant ground sloth and chalicotheres. The spotted skunk will walk on its front legs when threatened, rearing up on its front legs while facing the attacker so that its anal glands, capable of spraying an offensive oil, face its attacker.
Bipedalism is unknown among the amphibians. Among the non-archosaur reptiles bipedalism is rare, but it is found in the "reared-up" running of lizards such as agamids and monitor lizards.[5] Many reptile species will also temporarily adopt bipedalism while fighting.[36] One genus of basilisk lizard can run bipedally across the surface of water for some distance. Among arthropods, cockroaches are known to move bipedally at high speeds.[37] Bipedalism is rarely found outside terrestrial animals, though at least two species of octopus walk bipedally on the sea floor using two of their arms, allowing the remaining arms to be used to camouflage the octopus as a mat of algae or a floating coconut.[38]
There are at least twelve distinct hypotheses as to how and why bipedalism evolved in humans, and also some debate as to when. Bipedalism evolved well before the large human brain or the development of stone tools.[39] Bipedal specializations are found in Australopithecus fossils from 4.2 to 3.9 million years ago and recent studies have suggested that obligate bipedal hominid species were present as early as 7 million years ago.[28][40] Nonetheless, the evolution of bipedalism was accompanied by significant evolutions in the spine including the forward movement in position of the foramen magnum, where the spinal cord leaves the cranium.[41] Recent evidence regarding modern human sexual dimorphism (physical differences between male and female) in the lumbar spine has been seen in pre-modern primates such as Australopithecus africanus. This dimorphism has been seen as an evolutionary adaptation of females to bear lumbar load better during pregnancy, an adaptation that non-bipedal primates would not need to make.[42][43] Adapting bipedalism would have required less shoulder stability, which allowed the shoulder and other limbs to become more independent of each other and adapt for specific suspensory behaviors. In addition to the change in shoulder stability, changing locomotion would have increased the demand for shoulder mobility, which would have propelled the evolution of bipedalism forward.[44] The different hypotheses are not necessarily mutually exclusive and a number of selective forces may have acted together to lead to human bipedalism. It is important to distinguish between adaptations for bipedalism and adaptations for running, which came later still.
The form and function of modern-day humans' upper bodies appear to have evolved from living in a more forested setting. Living in this kind of environment would have made it so that being able to travel arboreally would have been advantageous at the time. Although different to human walking, bipedal locomotion in trees was thought to be advantageous.[45] It has also been proposed that, like some modern-day apes, early hominins had undergone a knuckle-walking stage prior to adapting the back limbs for bipedality while retaining forearms capable of grasping.[46] Numerous causes for the evolution of human bipedalism involve freeing the hands for carrying and using tools, sexual dimorphism in provisioning, changes in climate and environment (from jungle to savanna) that favored a more elevated eye-position, and to reduce the amount of skin exposed to the tropical sun.[47] It is possible that bipedalism provided a variety of benefits to the hominin species, and scientists have suggested multiple reasons for evolution of human bipedalism.[48] There is also not only the question of why the earliest hominins were partially bipedal but also why hominins became more bipedal over time. For example, the postural feeding hypothesis describes how the earliest hominins became bipedal for the benefit of reaching food in trees while the savanna-based theory describes how the late hominins that started to settle on the ground became increasingly bipedal.[49]
Napier (1963) argued that it is unlikely that a single factor drove the evolution of bipedalism. He stated "It seems unlikely that any single factor was responsible for such a dramatic change in behaviour. In addition to the advantages of accruing from ability to carry objects – food or otherwise – the improvement of the visual range and the freeing of the hands for purposes of defence and offence may equally have played their part as catalysts."[50] Sigmon (1971) demonstrated that chimpanzees exhibit bipedalism in different contexts, and one single factor should be used to explain bipedalism: preadaptation for human bipedalism.[51] Day (1986) emphasized three major pressures that drove evolution of bipedalism: food acquisition, predator avoidance, and reproductive success.[52] Ko (2015) stated that there are two main questions regarding bipedalism 1. Why were the earliest hominins partially bipedal? and 2. Why did hominins become more bipedal over time? He argued that these questions can be answered with combination of prominent theories such as Savanna-based, Postural feeding, and Provisioning.[53]
According to the Savanna-based theory, hominines came down from the tree's branches and adapted to life on the savanna by walking erect on two feet. The theory suggests that early hominids were forced to adapt to bipedal locomotion on the open savanna after they left the trees. One of the proposed mechanisms was the knuckle-walking hypothesis, which states that human ancestors used quadrupedal locomotion on the savanna, as evidenced by morphological characteristics found in Australopithecus anamensis and Australopithecus afarensis forelimbs, and that it is less parsimonious to assume that knuckle walking developed twice in genera Pan and Gorilla instead of evolving it once as synapomorphy for Pan and Gorilla before losing it in Australopithecus.[54] The evolution of an orthograde posture would have been very helpful on a savanna as it would allow the ability to look over tall grasses in order to watch out for predators, or terrestrially hunt and sneak up on prey.[55] It was also suggested in P. E. Wheeler's "The evolution of bipedality and loss of functional body hair in hominids", that a possible advantage of bipedalism in the savanna was reducing the amount of surface area of the body exposed to the sun, helping regulate body temperature.[56] In fact, Elizabeth Vrba's turnover pulse hypothesis supports the savanna-based theory by explaining the shrinking of forested areas due to global warming and cooling, which forced animals out into the open grasslands and caused the need for hominids to acquire bipedality.[57]
Others state hominines had already achieved the bipedal adaptation that was used in the savanna. The fossil evidence reveals that early bipedal hominins were still adapted to climbing trees at the time they were also walking upright.[58] It is possible that bipedalism evolved in the trees, and was later applied to the savanna as a vestigial trait. Humans and orangutans are both unique to a bipedal reactive adaptation when climbing on thin branches, in which they have increased hip and knee extension in relation to the diameter of the branch, which can increase an arboreal feeding range and can be attributed to a convergent evolution of bipedalism evolving in arboreal environments.[59] Hominine fossils found in dry grassland environments led anthropologists to believe hominines lived, slept, walked upright, and died only in those environments because no hominine fossils were found in forested areas. However, fossilization is a rare occurrence—the conditions must be just right in order for an organism that dies to become fossilized for somebody to find later, which is also a rare occurrence. The fact that no hominine fossils were found in forests does not ultimately lead to the conclusion that no hominines ever died there. The convenience of the savanna-based theory caused this point to be overlooked for over a hundred years.[57]
Some of the fossils found actually showed that there was still an adaptation to arboreal life. For example, Lucy, the famous Australopithecus afarensis, found in Hadar in Ethiopia, which may have been forested at the time of Lucy's death, had curved fingers that would still give her the ability to grasp tree branches, but she walked bipedally. "Little Foot", a nearly-complete specimen of Australopithecus africanus, has a divergent big toe as well as the ankle strength to walk upright. "Little Foot" could grasp things using his feet like an ape, perhaps tree branches, and he was bipedal. Ancient pollen found in the soil in the locations in which these fossils were found suggest that the area used to be much more wet and covered in thick vegetation and has only recently become the arid desert it is now.[57]
An alternative explanation is that the mixture of savanna and scattered forests increased terrestrial travel by proto-humans between clusters of trees, and bipedalism offered greater efficiency for long-distance travel between these clusters than quadrupedalism.[60][61] In an experiment monitoring chimpanzee metabolic rate via oxygen consumption, it was found that the quadrupedal and bipedal energy costs were very similar, implying that this transition in early ape-like ancestors would not have been very difficult or energetically costing.[62] This increased travel efficiency is likely to have been selected for as it assisted foraging across widely dispersed resources.
The postural feeding hypothesis has been recently supported by Dr. Kevin Hunt, a professor at Indiana University.[63] This hypothesis asserts that chimpanzees were only bipedal when they eat. While on the ground, they would reach up for fruit hanging from small trees and while in trees, bipedalism was used to reach up to grab for an overhead branch. These bipedal movements may have evolved into regular habits because they were so convenient in obtaining food. Also, Hunt's hypotheses states that these movements coevolved with chimpanzee arm-hanging, as this movement was very effective and efficient in harvesting food. When analyzing fossil anatomy, Australopithecus afarensis has very similar features of the hand and shoulder to the chimpanzee, which indicates hanging arms. Also, the Australopithecus hip and hind limb very clearly indicate bipedalism, but these fossils also indicate very inefficient locomotive movement when compared to humans. For this reason, Hunt argues that bipedalism evolved more as a terrestrial feeding posture than as a walking posture.[63]
A related study conducted by University of Birmingham, Professor Susannah Thorpe examined the most arboreal great ape, the orangutan, holding onto supporting branches in order to navigate branches that were too flexible or unstable otherwise. In more than 75 percent of observations, the orangutans used their forelimbs to stabilize themselves while navigating thinner branches. Increased fragmentation of forests where A. afarensis as well as other ancestors of modern humans and other apes resided could have contributed to this increase of bipedalism in order to navigate the diminishing forests. Findings also could shed light on discrepancies observed in the anatomy of A. afarensis, such as the ankle joint, which allowed it to "wobble" and long, highly flexible forelimbs. If bipedalism started from upright navigation in trees, it could explain both increased flexibility in the ankle as well as long forelimbs which grab hold of branches.[64][65][66][67][68][69]
One theory on the origin of bipedalism is the behavioral model presented by C. Owen Lovejoy, known as "male provisioning".[70] Lovejoy theorizes that the evolution of bipedalism was linked to monogamy. In the face of long inter-birth intervals and low reproductive rates typical of the apes, early hominids engaged in pair-bonding that enabled greater parental effort directed towards rearing offspring. Lovejoy proposes that male provisioning of food would improve the offspring survivorship and increase the pair's reproductive rate. Thus the male would leave his mate and offspring to search for food and return carrying the food in his arms walking on his legs. This model is supported by the reduction ("feminization") of the male canine teeth in early hominids such as Sahelanthropus tchadensis[71] and Ardipithecus ramidus,[72] which along with low body size dimorphism in Ardipithecus[73] and Australopithecus,[74][75][76] suggests a reduction in inter-male antagonistic behavior in early hominids.[77] In addition, this model is supported by a number of modern human traits associated with concealed ovulation (permanently enlarged breasts, lack of sexual swelling) and low sperm competition (moderate sized testes, low sperm mid-piece volume) that argues against recent adaptation to a polygynous reproductive system.[77]
However, this model has been debated, as others have argued that early bipedal hominids were instead polygynous. Among most monogamous primates, males and females are about the same size. That is sexual dimorphism is minimal, and other studies have suggested that Australopithecus afarensis males were nearly twice the weight of females. However, Lovejoy's model posits that the larger range a provisioning male would have to cover (to avoid competing with the female for resources she could attain herself) would select for increased male body size to limit predation risk.[78] Furthermore, as the species became more bipedal, specialized feet would prevent the infant from conveniently clinging to the mother –  hampering the mother's freedom[79] and thus make her and her offspring more dependent on resources collected by others. Modern monogamous primates such as gibbons tend to be also territorial, but fossil evidence indicates that Australopithecus afarensis lived in large groups. However, while both gibbons and hominids have reduced canine sexual dimorphism, female gibbons enlarge ('masculinize') their canines so they can actively share in the defense of their home territory. Instead, the reduction of the male hominid canine is consistent with reduced inter-male aggression in a pair-bonded though group living primate.
Recent studies of 4.4 million years old Ardipithecus ramidus suggest bipedalism. It is thus possible that bipedalism evolved very early in homininae and was reduced in chimpanzee and gorilla when they became more specialized. Other recent studies of the foot structure of Ardipithecus ramidus suggest that the species was closely related to African-ape ancestors. This possibly provides a species close to the true connection between fully bipedal hominins and quadruped apes.[80] According to Richard Dawkins in his book "The Ancestor's Tale", chimps and bonobos are descended from Australopithecus gracile type species while gorillas are descended from Paranthropus. These apes may have once been bipedal, but then lost this ability when they were forced back into an arboreal habitat, presumably by those australopithecines from whom eventually evolved hominins. Early hominines such as Ardipithecus ramidus may have possessed an arboreal type of bipedalism that later independently evolved towards knuckle-walking in chimpanzees and gorillas[81] and towards efficient walking and running in modern humans (see figure). It is also proposed that one cause of Neanderthal extinction was a less efficient running.
Joseph Jordania from the University of Melbourne recently (2011) suggested that bipedalism was one of the central elements of the general defense strategy of early hominids, based on aposematism, or warning display and intimidation of potential predators and competitors with exaggerated visual and audio signals. According to this model, hominids were trying to stay as visible and as loud as possible all the time. Several morphological and behavioral developments were employed to achieve this goal: upright bipedal posture, longer legs, long tightly coiled hair on the top of the head, body painting, threatening synchronous body movements, loud voice and extremely loud rhythmic singing/stomping/drumming on external subjects.[82] Slow locomotion and strong body odor (both characteristic for hominids and humans) are other features often employed by aposematic species to advertise their non-profitability for potential predators.
There are a variety of ideas which promote a specific change in behaviour as the key driver for the evolution of hominid bipedalism. For example, Wescott (1967) and later Jablonski & Chaplin (1993) suggest that bipedal threat displays could have been the transitional behaviour which led to some groups of apes beginning to adopt bipedal postures more often. Others (e.g. Dart 1925) have offered the idea that the need for more vigilance against predators could have provided the initial motivation. Dawkins (e.g. 2004) has argued that it could have begun as a kind of fashion that just caught on and then escalated through sexual selection. And it has even been suggested (e.g. Tanner 1981:165) that male phallic display could have been the initial incentive, as well as increased sexual signaling in upright female posture.[55]
The thermoregulatory model explaining the origin of bipedalism is one of the simplest theories so far advanced, but it is a viable explanation. Dr. Peter Wheeler, a professor of evolutionary biology, proposes that bipedalism raises the amount of body surface area higher above the ground which results in a reduction in heat gain and helps heat dissipation.[83][84][85] When a hominid is higher above the ground, the organism accesses more favorable wind speeds and temperatures. During heat seasons, greater wind flow results in a higher heat loss, which makes the organism more comfortable. Also, Wheeler explains that a vertical posture minimizes the direct exposure to the sun whereas quadrupedalism exposes more of the body to direct exposure. Analysis and interpretations of Ardipithecus reveal that this hypothesis needs modification to consider that the forest and woodland environmental preadaptation of early-stage hominid bipedalism preceded further refinement of bipedalism by the pressure of natural selection. This then allowed for the more efficient exploitation of the hotter conditions ecological niche, rather than the hotter conditions being hypothetically bipedalism's initial stimulus. A feedback mechanism from the advantages of bipedality in hot and open habitats would then in turn make a forest preadaptation solidify as a permanent state.[86]
Charles Darwin wrote that "Man could not have attained his present dominant position in the world without the use of his hands, which are so admirably adapted to the act of obedience of his will". Darwin (1871:52) and many models on bipedal origins are based on this line of thought. Gordon Hewes (1961) suggested that the carrying of meat "over considerable distances" (Hewes 1961:689) was the key factor. Isaac (1978) and Sinclair et al. (1986) offered modifications of this idea, as indeed did Lovejoy (1981) with his "provisioning model" described above. Others, such as Nancy Tanner (1981), have suggested that infant carrying was key, while others again have suggested stone tools and weapons drove the change.[87] This stone-tools theory is very unlikely, as though ancient humans were known to hunt, the discovery of tools was not discovered for thousands of years after the origin of bipedalism, chronologically precluding it from being a driving force of evolution. (Wooden tools and spears fossilize poorly and therefore it is difficult to make a judgment about their potential usage.)
The observation that large primates, including especially the great apes, that predominantly move quadrupedally on dry land, tend to switch to bipedal locomotion in waist deep water, has led to the idea that the origin of human bipedalism may have been influenced by waterside environments. This idea, labelled "the wading hypothesis",[88] was originally suggested by the Oxford marine biologist Alister Hardy who said: "It seems to me likely that Man learnt to stand erect first in water and then, as his balance improved, he found he became better equipped for standing up on the shore when he came out, and indeed also for running."[89] It was then promoted by Elaine Morgan, as part of the aquatic ape hypothesis, who cited bipedalism among a cluster of other human traits unique among primates, including voluntary control of breathing, hairlessness and subcutaneous fat.[90] The "aquatic ape hypothesis", as originally formulated, has not been accepted or considered a serious theory within the anthropological scholarly community.[91] Others, however, have sought to promote wading as a factor in the origin of human bipedalism without referring to further ("aquatic ape" related) factors. Since 2000 Carsten Niemitz has published a series of papers and a book[92] on a variant of the wading hypothesis, which he calls the "amphibian generalist theory" (German: Amphibische Generalistentheorie).
Other theories have been proposed that suggest wading and the exploitation of aquatic food sources (providing essential nutrients for human brain evolution[93] or critical fallback foods[94]) may have exerted evolutionary pressures on human ancestors promoting adaptations which later assisted full-time bipedalism. It has also been thought that consistent water-based food sources had developed early hominid dependency and facilitated dispersal along seas and rivers.[95]
Prehistoric fossil records show that early hominins first developed bipedalism before being followed by an increase in brain size.[96] The consequences of these two changes in particular resulted in painful and difficult labor due to the increased favor of a narrow pelvis for bipedalism being countered by larger heads passing through the constricted birth canal. This phenomenon is commonly known as the obstetrical dilemma.
Non-human primates habitually deliver their young on their own, but the same cannot be said for modern-day humans. Isolated birth appears to be rare and actively avoided cross-culturally, even if birthing methods may differ between said cultures. This is due to the fact that the narrowing of the hips and the change in the pelvic angle caused a discrepancy in the ratio of the size of the head to the birth canal. The result of this is that there is greater difficulty in birthing for hominins in general, let alone to be doing it by oneself.[97]
Bipedal movement occurs in a number of ways and requires many mechanical and neurological adaptations. Some of these are described below.
Energy-efficient means of standing bipedally involve constant adjustment of balance, and of course these must avoid overcorrection. The difficulties associated with simple standing in upright humans are highlighted by the greatly increased risk of falling present in the elderly, even with minimal reductions in control system effectiveness.
Shoulder stability would decrease with the evolution of bipedalism. Shoulder mobility would increase because the need for a stable shoulder is only present in arboreal habitats. Shoulder mobility would support suspensory locomotion behaviors which are present in human bipedalism. The forelimbs are freed from weight-bearing requirements, which makes the shoulder a place of evidence for the evolution of bipedalism.[98]
Unlike non-human apes that are able to practice bipedality such as Pan and Gorilla, hominins have the ability to move bipedally without the utilization of a bent-hip-bent-knee (BHBK) gait, which requires the engagement of both the hip and the knee joints. This human ability to walk is made possible by the spinal curvature humans have that non-human apes do not.[99] Rather, walking is characterized by an "inverted pendulum" movement in which the center of gravity vaults over a stiff leg with each step.[100] Force plates can be used to quantify the whole-body kinetic & potential energy, with walking displaying an out-of-phase relationship indicating exchange between the two.[100] This model applies to all walking organisms regardless of the number of legs, and thus bipedal locomotion does not differ in terms of whole-body kinetics.[101]
In humans, walking is composed of several separate processes:[100]
Early hominins underwent post-cranial changes in order to better adapt to bipedality, especially running. One of these changes is having longer hindlimbs proportional to the forelimbs and their effects. As previously mentioned, longer hindlimbs assist in thermoregulation by reducing the total surface area exposed to direct sunlight while simultaneously allowing for more space for cooling winds. Additionally, having longer limbs is more energy-efficient, since longer limbs mean that overall muscle strain is lessened. Better energy efficiency, in turn, means higher endurance, particularly when running long distances.[102]
Running is characterized by a spring-mass movement.[100] Kinetic and potential energy are in phase, and the energy is stored & released from a spring-like limb during foot contact,[100] achieved by the plantar arch and the Achilles tendon in the foot and leg, respectively.[102] Again, the whole-body kinetics are similar to animals with more limbs.[101]
Bipedalism requires strong leg muscles, particularly in the thighs. Contrast in domesticated poultry the well muscled legs, against the small and bony wings. Likewise in humans, the quadriceps and hamstring muscles of the thigh are both so crucial to bipedal activities that each alone is much larger than the well-developed biceps of the arms. In addition to the leg muscles, the increased size of the gluteus maximus in humans is an important adaptation as it provides support and stability to the trunk and lessens the amount of stress on the joints when running.[102]
Quadrupeds, have more restrictive breathing respire while moving than do bipedal humans.[103] "Quadrupedal species normally synchronize the locomotor and respiratory cycles at a constant ratio of 1:1 (strides per breath) in both the trot and gallop. Human runners differ from quadrupeds in that while running they employ several phase-locked patterns (4:1, 3:1, 2:1, 1:1, 5:2, and 3:2), although a 2:1 coupling ratio appears to be favored. Even though the evolution of bipedal gait has reduced the mechanical constraints on respiration in man, thereby permitting greater flexibility in breathing pattern, it has seemingly not eliminated the need for the synchronization of respiration and body motion during sustained running."[104]
Respiration through bipedality means that there is better breath control in bipeds, which can be associated with brain growth. The modern brain utilizes approximately 20% of energy input gained through breathing and eating, as opposed to species like chimpanzees who use up twice as much energy as humans for the same amount of movement. This excess energy, leading to brain growth, also leads to the development of verbal communication. This is because breath control means that the muscles associated with breathing can be manipulated into creating sounds. This means that the onset of bipedality, leading to more efficient breathing, may be related to the origin of verbal language.[103]
For nearly the whole of the 20th century, bipedal robots were very difficult to construct and robot locomotion involved only wheels, treads, or multiple legs. Recent cheap and compact computing power has made two-legged robots more feasible. Some notable biped robots are ASIMO, HUBO, MABEL and QRIO. Recently, spurred by the success of creating a fully passive, un-powered bipedal walking robot,[105] those working on such machines have begun using principles gleaned from the study of human and animal locomotion, which often relies on passive mechanisms to minimize power consumption.

The prehistory of the Philippines covers the events prior to the written history of what is now the Philippines. The current demarcation between this period and the early history of the Philippines is April 21, 900, which is the equivalent on the Proleptic Gregorian calendar for the date indicated on the Laguna Copperplate Inscription—the earliest known surviving written record to come from the Philippines. This period saw the immense change that took hold of the archipelago from Stone Age cultures in 50000 BC to the emergence of developed thalassocratic civilizations in the fourth century, continuing on with the gradual widening of trade until 900 and the first surviving written records.
Events/Artifacts
(north to south)
Events/Artifacts
Artifacts
A 2018 study led by Thomas Ingicco,[2] which analyzed rhino remains unearthed in Rizal Archaeological Site in Kalinga using several dating techniques,[3] pushes back the arrival of the first Homo species to the early Chibanian (late Pleistocene), between 631,000 and 777,000 years ago.[4]
Unearthed in the site was a 'nearly complete, disarticulated' rhinoceros skeleton, of the extinct species Rhinoceros philippinensis. It showed ridges left by tools made while removing flesh, and special tools designed to remove bone marrow. The site yielded more than 400 bones, including several dozen knapped and chipped tools, of which 49 are knife-like flakes with two hammers.[3] Also, among the finds are other skeletal remains, which include brown deer,[5] monitor lizards, freshwater turtles and stegodonts.[3]
While the earliest confirmed evidence of a hominin came from a 67,000-year-old foot bone from Sierra Madre discovered in 2007,[6] those finds had no direct trace of the butchers of the animals. On the other hand, it is possible that the butchers had by then evolved into a distinct subspecies.[3]
The earliest known hominin remains in the Philippines is the fossil discovered in 2007 in the Callao Caves in Cagayan. The 67,000-year-old find predates the 47,000-year-old Tabon Man, which was until then the earliest known set of human remains in the archipelago. The find consisted of a single 61 millimeter metatarsal which was dated using uranium series ablation. It was initially thought to be possibly one of the oldest Homo sapiens remains in the Asia-Pacific.[7][8][9][10]
In the same stratigraphic layer where the third metatarsal was discovered, continued excavations revealed 12 fossil bones (7  postcanine maxillary teeth, 2 manual phalanges, 2 pedal phalanges, 1 femoral shaft) from three hominin individuals. These remains and the Callao Man were identified to belong to a new species of hominins, Homo luzonensis.[11][12][13]
The first evidence of the systematic use of Stone Age technology in the Philippines is estimated to 50,000 BC,[14] and this phase in the development of proto-Philippine societies is considered to end with the rise of metal tools in about 500 BC, albeit with stone tools still used past that date.[15] Filipino anthropologist F. Landa Jocano refers to the earliest noticeable stage in the development of proto-Philippine societies as the Formative Phase.[16] He also identified stone tools and ceramic manufacture as the two core industries that defined the period's economic activity, and which shaped the means by which early Filipinos adapted to their environment during this period.[14]
By about 30,000 BC, the Negritos, who became the ancestors of today's aboriginal Filipinos (such as the Aeta), probably lived in the archipelago. No evidence has survived which would indicate details of ancient Filipino life such as their crops, culture, and architecture. Historian William Henry Scott noted any theory which describes such details for the period must be pure hypothesis, and thus be honestly presented as such.[17]
Fossilized fragments of a skull and jawbone of three individuals had been discovered on May 28, 1962 by Dr. Robert B. Fox, an American anthropologist of the National Museum.[18] These fragments are collectively called "Tabon Man" after the place where they were found on the west coast of Palawan. Tabon Cave appears to be a kind of a Stone Age factory, with both finished stone flake tools and waste core flakes having been found at four separate levels in the main chamber. Charcoal left from three assemblages of cooking fires there has been Carbon-14 dated to roughly 7,000, 20,000, and 22,000 BC.[19] These remains are the oldest modern human remains found on the islands, and have been U/Th-dated to 47,000 ± 11–10,000 years ago.[20] (In Mindanao, the existence and importance of these prehistoric tools was noted by famed José Rizal himself, because of his acquaintance with Spanish and German scientific archaeologists in the 1880s, while in Europe.)[citation needed]
Tabon Cave is named after the "Tabon bird" (Tabon scrubfowl, Megapodius cumingii), which deposited thick hard layers of guano during the period when the cave was still uninhabited, resulting to a cement-like floor made of bird dung where three succeeding groups of tool-makers settled. It is indicated that about half of the 3,000 specimens recovered from the cave are discarded cores of a material which had to be transported from some distance.  The Tabon man fossils are considered to have come from the third group of inhabitants who inhabited the cave between 22,000 and 20,000 BC. An earlier cave level lies so far below the level containing cooking fire assemblages that it must represent Upper Pleistocene dates from 45 or 50 thousand years ago.[19]
Physical anthropologists who have examined the Tabon Man skullcap have agreed that it belonged to a modern man (Homo sapiens), as distinguished from the mid-Pleistocene Homo erectus species. This indicates that Tabon Man was Pre-Mongoloid (Mongoloid being the term anthropologists apply to the racial stock which entered Southeast Asia during the Holocene and absorbed earlier peoples to produce the modern Malay, Indonesian, Filipino, and "Pacific" peoples). Two experts have given the opinion that the mandible is "Australian" in physical type, and that the skullcap measurements are most nearly like the Ainus or Tasmanians. Nothing can be concluded about Tabon man's physical appearance from the recovered skull fragments except that he was not a Negrito.[21]
The custom of Jar Burial, which ranges from Sri Lanka, to the Plain of Jars, in Laos, to Japan, also was practiced in the Tabon caves. A spectacular example of a secondary burial jar is owned by the National Museum, a National Treasure, with a jar lid topped with two figures, one the deceased, arms crossed, hands touching the shoulders, the other a steersman, both seated in a proa, with only the mast missing from the piece. Secondary burial was practiced across all the islands of the Philippines during this period, with the bones reburied, some in the burial jars. Seventy-eight earthenware vessels were recovered from the Manunggul cave, Palawan, specifically for burial.
Human remains in the cave are from both large and small individuals. The latter fit well with Philippine negritos who were among the archipelago's earliest inhabitants,[22] descendants of the first human migrations out of Africa via the coastal route along southern Asia to the now sunken landmasses of Sundaland and Sahul.[23]
The site is one of the earliest human settlement zones in the region. The site itself is part of a huge karst system with layers of shells and other minerals made by early humans. More excavation led to discovery of ancient artifacts like flake tools, polished stones, earthenware shards, bone tools and some animal remains. These remains and artifacts were dated by C-14 to be around 8,810 to 5,190 years ago, making the site one of the most significant archaeological sites in the region. The site was declared an Important Cultural Property in 2017 by the National Government.[24][better source needed]
The current scientific consensus of the settlement of the Philippines is the Out-of-Taiwan (OOT) hypothesis (also called the Austronesian expansion). It was first proposed by Peter Bellwood and was originally based largely on linguistics, hewing very close to Robert Blust's model of the history of the Austronesian language family.[26] It has since been strengthened by genetic and archaeological studies that broadly agree with the timeline of the Austronesian expansion.[25][27][28][29]
The modern Austronesian expansion model indicates that between 4500 BC and 4000 BC, developments in agricultural technology in the Yunnan Plateau in China created pressures which drove certain peoples to migrate to Taiwan. These people either already had or began to develop a unique language of their own, now referred to as Proto-Austronesian. By around 3000 BC, these groups started differentiating into three or four distinct subcultures. By 2500 to 1500 BC, one of these groups (the ancestral Malayo-Polynesian-speakers) began migrating southwards by sea towards the Philippines, then further onwards to the Marianas Islands by 1500 BC, and the rest of Island Southeast Asia, Island Melanesia, and eventually as far as Polynesia and Madagascar.[29][30] Before the expansion out of Taiwan, recent archaeological, linguistic and genetic evidence has linked Austronesian speakers in Insular Southeast Asia to cultures such as the Hemudu, Liangzhu and Dapenkeng in Neolithic China.[31][32][33][34][35]
Historian William Henry Scott has observed that, based on lexicostatistical analysis involving seven million word pairs linguist Isidore Dyen offered in 1962, two alternative scenarios explaining the origin and spread of Austronesian languages: (a) that they originated in some Pacific island and spread westward to Asia, or (b) that they originated in Taiwan and spread southward.[36] Based on subsequent study of the second alternative, Scott concludes that the Philippine language tree could have been introduced by Austronesian speakers as long ago as 5000 BC, probably from the north, with their descendants expanding throughout the Philippine archipelago and beyond in succeeding millennia, absorbing or replacing sparse populations already present, and their language diversifying into dozens of mutually unintelligible languages which replaced earlier ones. During those millennia, other Austronesian speakers entered the Philippines in large enough numbers to leave a linguistic mark but not to replace established languages. Scott suggested that if this scenario is correct all present Philippine languages (except for Sama–Bajaw languages, which probably have more speakers outside the Philippines than within) were produced within the archipelago, none of them being introduced by separate migration, and all of them having more in common with each other than with languages outside of the Philippines.
During this neolithic period, a trade route initially created primarily by natives of the Philippines and Taiwan was established. The route, known as the Maritime Jade Road, was one of the most extensive sea-based trade networks of a single geological material in the prehistoric world from 2000 BCE-1000 CE, much older than the Silk Road.[37][38][39][40] Jade was mined in Taiwan and was processed primarily in the Philippines, where the trade route reached many places in Southeast Asia such as Vietnam, Thailand, Malaysia, and Indonesia. A "jade culture" thrived during this era, as evidenced by tens of thousands of exquisitely crafted jade artifacts found at a site in Batangas province.[41][42] Jade artifacts have been found dated to 2000 BC,[41][43] with the lingling-o jade items crafted in Luzon made using raw materials originating from Taiwan.[44] During this peaceful pre-colonial period, not a single burial site studied by scholars yielded any osteological proof for violent death. No instances of mass burials were recorded as well, signifying the peaceful situation of the islands. Burials with violent proof were only found from burials beginning in the 15th century, likely due to the newer cultures of expansionism imported from India and China. When the Spanish arrived in the 16th century, they recorded some war-like groups, whose cultures have already been influenced by the imported Indian and Chinese expansionist cultures of the 15th century.[45] By 1000 BC, the inhabitants of the archipelago had developed into four kinds of social groups: hunter-gatherer tribes, warrior societies, highland plutocracies, and port principalities.[46]
The Austronesians that settled in the Philippines admixed with the preexisting earlier groups like the Negritos that had reached the islands via the now sunken Sundaland landmass. Genetic studies have shown that modern native Filipinos have varying degrees of Negrito ancestry in addition to the majority Austronesian ancestry.[47][28]
A 2002 China Medical University study indicated that some Filipinos shared genetic chromosome that is found among Asian people, such as Taiwanese aborigines, Indonesians, Thais, and Chinese.[48]
A 2008 genetic study by Leeds University and published in Molecular Biology and Evolution, showed that mitochondrial DNA lineages have been evolving within Island Southeast Asia (ISEA) since modern humans arrived approximately 50,000 years ago. The authors concluded that it was proof that Austronesians evolved within Island Southeast Asia and did not come from Taiwan (the "Out-of-Sundaland" hypothesis). Population dispersals occurred at the same time as sea levels rose, which resulted in migrations from the Philippine Islands into Taiwan within the last 10,000 years.[49]
A 2013 study on the genetics and origin of Polynesian people supported the Out of Taiwan scenario of Austronesian expansion from Taiwan, at around 2200 BC, settling the Batanes Islands and northern Luzon from Taiwan. From there, they rapidly spread downwards to the rest of the islands of the Philippines and Southeast Asia.[25][30] This population assimilated with the existing Negritos resulting in the modern Filipino ethnic groups which display various ratios of genetic admixture between Austronesian and Negrito groups.[47]
However, a 2014 study published by Nature using whole genome sequencing instead of only mtDNA sequencing confirmed the north-to-south dispersal of the Austronesian peoples in the "Out-of-Taiwan" hypothesis. Researchers further pointed out that, while humans have been living in Sundaland for at least 40,000 years, Austronesian people were recent arrivals. The results of the 2008 study failed to take into account admixture with the more ancient but unrelated Negrito and Papuan populations.[50][47]
A 2021 genetic study, which examined representatives of 115 indigenous communities, found evidence of at least five independent waves of early human migration. Negrito groups, divided between those in Luzon and those in Mindanao, may come from a single wave and diverged subsequently, or through two separate waves. This likely occurred sometime after 46,000 years ago. Another Negrito migration entered Mindanao sometime after 25,000 years ago. Two early East Asian waves were detected, one most strongly evidenced among the Manobo people who live in inland Mindanao, and the other in the Sama-Bajau and related people of the Sulu archipelago, Zamboanga Peninsula, and Palawan. The admixture found in the Sama people indicates a relationship with the Lua and Mlabri people of mainland Southeast Asia, and reflects a similar genetic signal found in western Indonesia. These happened sometime after 15,000 years ago and 12,000 years ago respectively, around the time the last glacial period was coming to an end. Austronesians, either from Southern China or Taiwan, were found to have come in at least two distinct waves. The first, occurring perhaps between 10,000 and 7,000 years ago, brought the ancestors of indigenous groups that today live around the Cordillera Central mountain range. Later migrations brought other Austronesian groups, along with agriculture, and the languages of these recent Austronesian migrants effectively replaced those existing populations. In all cases, new immigrants appear to have mixed to some degree with existing populations. The integration of Southeast Asia into Indian Ocean trading networks around 2,000 years ago also shows some impact, with South Asian genetic signals present within some Sama-Bajau communities.[51]
Although there is some evidence early Austronesian migrants having bronze or brass tools,[52][53] the earliest metal tools in the Philippines are generally said to have first been used somewhere around 500 BC, and this new technology coincided with considerable changes in the lifestyle of early Filipinos.  The new tools brought about a more stable way of life, and created more opportunities for communities to grow, both in terms of size and cultural development.[54]
Where communities once consisted of small bands of kinsmen living in campsites, larger villages came about- usually based near water, which made traveling and trading easier. The resulting ease of contact between communities meant that they began to share similar cultural traits, something which had not previously been possible when the communities consisted only of small kinship groups.
Jocano refers to the period between 500 BC and 1 AD as the incipient phase, which for the first time in the artifact record, sees the presence of artifacts that are similar in design from site to site throughout the archipelago. Along with the use of metal tools, this era also saw significant improvement in pottery technology.[54]
The Sa Huynh culture in what is now central and southern Vietnam had extensive trade with the Philippine archipelago during its height between 1000 BC and 200 AD.[55][56]
Sa Huynh beads were made from glass, carnelian, agate, olivine, zircon, gold and garnet; most of these materials were not local to the region, and were most likely imported. Han Dynasty-style bronze mirrors were also found in Sa Huynh sites. Conversely, Sa Huynh produced ear ornaments have been found in archaeological sites in Central Thailand, Taiwan (Orchid Island), and in the Philippines, in the Palawan Tabon Caves.[57][56] in The Kalanay Cave is a small cave located on the island of Masbate in central Philippines. The cave is located specifically at the northwest coast of the island within the municipality of Aroroy. The artifacts recovered from the site were similar to those found in Southeast Asia and South Vietnam. The site is one of the "Sa Huynh-Kalanay" pottery complex which is shares similarities with Vietnam. The type of pottery found in the site were dated 400BC-1500 AD.[55][56]
Maritime Southeast Asia began to be integrated into wider trade networks in the early centuries of the first millennium, with trade between China and the region becoming regular by the 5th century.[58]
Fragmented ethnic groups established numerous city-states formed by the assimilation of several small political units known as barangay each headed by a Datu or headman (still in use among non-Hispanic Filipino ethnic groups) and answerable to a king, titled Rajah. Even scattered barangays, through the development of inter-island and international trade, became more culturally homogeneous by the 4th century. Hindu-Buddhist culture and religion flourished among the noblemen in this era. Many of the barangay were, to varying extents, under the de jure jurisprudence of one of several neighboring empires, among them the Malay Sri Vijaya, Javanese Majapahit, Brunei, Melaka empires, although de facto had established their own independent system of rule. Trading links with Sumatra, Borneo, Thailand, Java, China, India, Arabia, Japan and the Ryukyu Kingdom flourished during this era. A thalassocracy had thus emerged based on international trade.[59]
Each barangay consisted of about 100 families. Some barangays were big, such as Zubu (Cebu), Butuan, Maktan (Mactan), Mandani (Mandaue), Lalan (Liloan), Irong-Irong (Iloilo), Bigan (Vigan), and Maynila (Manila). Each of these big barangays had a population of more than 2,000.
In the earliest times, the items which were prized by the peoples included jars, which were a symbol of wealth throughout South Asia, and later metal, salt and tobacco. In exchange, the peoples would trade feathers, rhino horn, hornbill beaks, beeswax, birds nests, resin, rattan.2 Wrought iron were produced and processed in the Philippines and exported to Taiwan.[37]
In the period between the 7th century to the beginning of the 15th century, numerous prosperous centers of trade had emerged, including Namayan which flourished alongside Manila Bay,[60] Cebu, Iloilo,[61] Butuan, Pangasinan, Pampanga[62] and Aparri (which specialized in trade with Japan and the Kingdom of Ryukyu in Okinawa).
The introduction of metal into the Philippines and the resulting changes did not follow the typical pattern. Robert Fox notes, "There is, for example, no real evidence of a "Bronze Age" or "Copper-Bronze Age" in the archipelago, a development which occurred in many areas of the world.  The transition, as shown by recent excavation, was from stone tools to iron tools."[63]
The earliest use of metal in the Philippines was the use of copper for ornamentation, not tools.  Even when copper and bronze tools became common, they were often used side by side with stone tools.  Metal only became the dominant material for tools late in this era, leading to a new phase in cultural development.
Bronze tools from the Philippines' early metal age have been encountered in various sites, but they were not widespread.  This has been attributed to the lack of a local source of tin, which when combined with copper produces bronze. This lack has led most anthropologists to conclude that bronze items were imported and that those bronze smelting sites which have been found in the Philippines, in Palawan, were for re-smelting and remolding.
Iron Age finds in Philippines also point to the existence of trade between Tamil Nadu and the Philippine Islands during the ninth and tenth centuries B.C.[64] When iron was introduced to the Philippines, it became the preferred material for tools and largely ended the use of stone tools. Whether the iron was imported or mined locally is still debated by scholars. Beyer thought that it was mined locally, but others point to the lack of iron smelting artifacts and conclude that the iron tools were probably imported.[65]
Metalsmiths from this era had already developed a crude version of modern metallurgical processes, notably the hardening of soft iron through carburization.[66]

The prehistory of Mesopotamia is the period between the Paleolithic and the emergence of writing in the area of the Fertile Crescent around the Tigris and Euphrates rivers, as well as surrounding areas such as the Zagros foothills, southeastern Anatolia, and northwestern Syria.
In general, Paleolithic Mesopotamia is poorly documented, with the situation worsening in southern Mesopotamia for periods prior to the 4th millennium BC. Geological conditions meant that most of the remains were buried under a thick layer of alluvium or submerged beneath the waters of the Persian Gulf. The Middle Paleolithic witnessed the emergence of a population of hunter-gatherers who lived in the caves of the Zagros and, seasonally, in numerous open-air sites. They were producers of a lithic industry of the Mousterian type, and their funerary remains, found in the cave of Shanidar, indicate the existence of solidarity and the practice of healing between the members of a group. During the Upper Paleolithic, the Zagros was probably occupied by modern man. The Shanidar cave contains only tools made of bone or antler, typical of a local Aurignacian called "Baradostian" by specialists.
The late Epipaleolithic period, characterized by the Zarzian (c. 17,000–12,000 years BC), saw the appearance of the first temporary villages with circular permanent structures. The appearance of fixed objects such as sandstone or granite millstones and cylindrical basalt pestles indicated the beginning of sedentarization.
Between the 11th and 10th millennia BC, the first villages of sedentary hunter-gatherers are known in northern Iraq. Houses seem to have been built around a "hearth", a kind of family "property". The preservation of the skulls of the dead and artistic activity related to birds of prey have also been found. Around 10,000 to 7,000 BC, villages expanded in the Zagros and Upper Mesopotamia. The economy was mixed (hunting and the beginnings of agriculture). Houses became rectangular and the use of obsidian was recorded, which testifies to contacts with Anatolia where there were numerous deposits.
The 7th and 6th millennia BC saw the development of the so-called "ceramic" cultures known as "Hassuna", "Samarra", and "Halaf". They were characterized by the definitive introduction of agriculture and animal husbandry. Houses became more complex, with large communal dwellings built around a collective granary. The introduction of irrigation was another feature. While the Samarra culture shows signs of social inequality, the Halaf culture appears to be composed of small, disparate communities with little or no apparent hierarchy.
At the same time, the Ubaid culture developed in southern Mesopotamia at the end of the 7th millennium BC. Tell el-'Oueili is the oldest known site of this culture. Their architecture was elaborate and they practiced irrigation, essential in a region where agriculture was impossible without artificial water. In its greatest expansion, the Ubaid Culture spread peacefully, probably by acculturating the Halaf Culture, across northern Mesopotamia to southeastern Anatolia and northeastern Syria.
Villages, apparently not very hierarchical, expanded into cities, society became more complex, and an increasingly dominant fixed elite emerged toward the end of the 4th millennium BC. The most influential centers of Mesopotamia (Uruk and Tepe Gawra) saw the gradual emergence of writing and the state. Traditionally, this marks the end of prehistory.
Mesopotamia is located in the historical region of the Fertile Crescent in the ancient Near East, around the Tigris and Euphrates Rivers. Mesopotamia corresponds for the most part to present-day Iraq, with the addition of a northern fringe of present-day Syria and the southeastern part of Turkey. Two regions are distinguished within this area: Upper Mesopotamia, or Jezirah, and Lower Mesopotamia.[1] Furthermore, the strong cultural similarities observed throughout Mesopotamian prehistory between the regions immediately east of the Tigris and west of the Euphrates allow for the study of a larger Mesopotamia bordering the foothills of the Zagros Mountains.[2]
Upper Mesopotamia is a hilly, flat, steppe vegetation region with an average elevation of 200–300 meters, extending east of the Tigris and west of the Euphrates. Most of the region receives sufficient rainfall for dry farming. However, its southern limits are Ramadi on the Euphrates and Samarra on the Tigris. Practically undocumented for the Paleolithic and Epipaleolithic periods is the part of the Jezirah between the tributaries of the Tigris and Euphrates. This can be explained both from an archaeological point of view, given the scarcity of surveys in this area, and from a human point of view, as the region may have been virtually uninhabited during the periods in question.[1]
Today, Lower Mesopotamia consists of a gently sloping alluvial plain (37 meters above sea level in Baghdad).[3] Around 14,000 BC, the ancient delta of the Tigris and Euphrates rivers was at the level of the Gulf of Oman. Around 10,000 BC, only a third of the present surface of the Persian Gulf was covered by the sea. As a result, many Paleolithic sites are found beneath the waters of the Gulf. Thus, the flora, fauna, and prehistory of the once-dry delta remain unknown. The Tigris and Euphrates rivers, which carry and deposit alluvial deposits at the end of their courses, caused the sea level to rise again after the waters of the Gulf had risen. This last phenomenon buried under several meters of alluvium or a water table all the remains of sites that might have been established before the 4th millennium BC on a plain covered by steppes.[4] The plains between the left bank of the Tigris and the foothills of the Zagros are the only remaining evidence of the steppe of southern Mesopotamia, once an extension of the southern Jezirah steppe. The present-day alluvial plains are therefore virtually devoid of any surface Paleolithic or Neolithic remains dating from before the 7th millennium BC. This is not because the sites were unoccupied, but rather because they have disappeared under several meters of alluvium, where they remain unexplored if not destroyed by natural forces.[1][5][6][7]
While 600,000 years ago the Archantropians left flint tools all over the Mediterranean (Egypt, Syria, Palestine, Iran), departing from Turkey and the valleys of the Tigris and Euphrates, the first human traces in Mesopotamia appear to be rolled and bifacially carved pebbles discovered in 1984 near Mosul in Iraq, dating from the Late Acheulean, i.e. from the last quarter of the Lower Paleolithic (between 500,000 and 110,000 years BC).[8]
Additional evidence of human presence was discovered in 1949 at Barda Balka ("Raised Stone" in Kurdish), not far from the town of Chamchamal in Iraqi Kurdistan. A number of carved flint artifacts found on the surface around a Neolithic megalith prompted a deeper investigation in 1950. Beneath fluvial sediments at a depth of 2 meters, hunter-gatherers discovered a kind of workshop or camp that had been occupied repeatedly over a short period of time. Their tools consisted of biface flint, axes, scrapers, and pebbles in the form of cleavers, suggesting stone working and butchery. These activities were carried out near a spring on the banks of a stream. On the basis of typological and geological criteria, it is estimated that the site dates back to 80,000 BP (years before  present). This date seems to be confirmed by the discovery of animal bones, such as Asian elephant and rhinoceros, around the tools. Some time later, these animals disappeared from these regions.[9][10]
At the beginning of the Würm Ice Age, prehistorians believed that people lived in caves on a seasonal basis. In 1928, British archaeologist Dorothy Garrod discovered a Mousterian lithic industry in the lower layers of the Hazar Merd cave in Iraqi Kurdistan, which is also found in many open-air sites.[10]
Meanwhile, in Iraqi Kurdistan, the Shanidar cave on the southern flank of Jebel Baradost in the foothills of the northern Zagros, in the province of Erbil in north-eastern Iraq, represents the most important archaeological site in the discovery of Middle and Upper Palaeolithic Mesopotamia. Between 1951 and 1960, the team led by American archaeologist Ralph Stefan Solecki excavated the floor of the cave and reached virgin rock at a depth of 14 meters.[11] They discovered four different cultural levels. The deepest and thickest of these (8.5 meters), level D, attributed to the Middle Paleolithic, reveals layers of ash mixed with flint tools and bones, suggesting that the cave has been occupied intermittently since 60,000 BP, for about ten thousand years. Numerous tools such as points, scrapers, chisels, and drills characteristic of the Mousterian lithic industry were also found. Arrowheads found in the upper layers of level D, of the "Emirati" type and similar to those found in Palestine, suggest contacts with the Mediterranean Levant.[12] In addition, the pollens found and analyzed in the laboratory allowed researchers to deduce strong climatic changes during the period of the cave's occupation: first warmer than today, then very cold, and finally hot and dry up to 44,000 years BP.[13]
But the greatest discovery was the presence of ten human skeletons, including two infants. These skeletons, most of them badly damaged, have been there since 60,000 BP for the oldest and 45,000 BP for the most recent. The skulls are in relatively good condition, with Neanderthal characteristics.[14][15] These skeletons fall into two categories, which are separated by 10,000 years: the majority belong to men, women, and children who lived in the cave and died as a result of rocks falling from the ceiling; the other are bodies that were apparently placed there for burial between 50,000 and 45,000 years BC. One of the deposited dead appears to have been lying in a fetal position and covered with flowers; another, discovered in 2017 not far from the first, is resting.[16]
These discoveries, in addition to observing possible burial practices, provide a better understanding of some Neanderthal behaviors. One of the skeletons shows so many traces of severe disabilities (including deafness and probably blindness) and severe injuries that its dependence on others seems obvious until its death at about forty. This suggests solidarity and care among members of a group.[17][18][19] Conversely, the existence of interpersonal violence within the same group is suggested by another skeleton that shows evidence of rib injuries.[20]
Regarding the lower regions, briefly excavated sites indicate that the desert south of the Euphrates and the regions adjacent to that river, or at least an earlier (fossil) system whose course was then quite different from today's, were traversed by bands during this period. Nevertheless, if there are campsites of this period in the plain, they are probably buried under river sediments.[21][22]
Level C of the Shanidar Cave (between 34,000 and 26,500 BP) does not contain any skeletons indicating occupation during the Upper Paleolithic (between 45,000 and 12,000 BP). Only bone or antler objects have been found. Nonetheless, their Aurignacian-like style was so original that in 1963 the American archaeologist Ralph Stefan Solecki called them Baradostian (a reference to the Baradost Mountains, where the cave is located). During the Würm glacial maximum, which reached its coldest point around 20,000 BC, Shanidar Cave seems to have been deserted for about 10,000 years, perhaps because the cold forced the inhabitants down into the surrounding valleys, or because of the threat of falling rocks from the ceiling of the cave.[23]
Mousterian stone tools, already present in the upper layers of Shanidar level D, seem to have become more refined: light and often pointed armatures indicate a renewed use of mechanically propelled weapons, such as the bow. This was an important feature of the lithic industry in the Upper Paleolithic of the Zagros. Moreover, despite the paucity of test pits, the pendants were numerous and mostly made from the atrophied canines of red deer, drilled into the flat of the root, or from perforated red marine shells, or fashioned from hematite. One of these imitates the silhouette of a deer's canine tooth, polished, shiny, and embellished with a series of peripheral notches. These aesthetic activities tend to reinforce the idea that a spiritual "revolution" accompanied the development of cutting techniques.[24]
Other sites from this period have been discovered and excavated, stretching across the width of the Zagros to the foothill of the Himalayas. The Yafteh Cave in Lorestan is one of the most important.[25][26] All these sites feature Aurignacian tools for the corresponding and higher time strata. Thus, in 2006, the term "Baradostian" was revised to define an earlier stage of the Aurignacian, intermediate between the Mousterian and an Aurignacian specific to the Zagros.[27][28] In the eyes of the researcher Marcel Otte, all the Aurignacian material from the Zagros seems to assume a continuity, both technical and aesthetic, from east to west, to such an extent that a migratory tendency of the Zagros Aurignacian is confirmed from Central Asia to southeastern Europe is confirmed, with the caves of the eastern foothills of the Zagros as an intermediary.[29]
Moreover, even if we can assume that the Baradostian population consisted of people whose anatomy is that of resembles that of modern humans,[23] there are very few paleoanthropological documents to support this: one molar from the Eshkaft-e Gavi cave (southern Zagros) and one premolar from the Wezmeh cave. What's more, the chronology of these two witnesses has not been precisely determined.[30]
At the beginning of the Epipaleolithic period (around 17,000–12,000 BP), during the last Ice Age, the northern regions of the Zagros, at medium and high altitudes, were deserted. The return of human presence was not felt until the climatic warming followed the last glacial maximum,[31] in parallel with the Epipaleolithic facies known in the Levant and on the southern coast of Anatolia.[32]
The end of the Epipaleolithic was marked by the Zarzian culture, which covered Iraqi Kurdistan with the Zarzi, Palegawra, and Shanidar B1 caves (reoccupied) then B2 (again uninhabited, but containing several burials), as well as the Zawi Chemi site. The latter is a "village" located 4 kilometers downstream from Shadinar Cave on the Great Zab River, and has been dated to the cave's B2 level (10,870 BP).[33] The Zarzi cave, eponymous with the Zarzian culture, was discovered and excavated in the 1920s and 1930s by archaeologist Dorothy Garrod. Her discoveries were instrumental in defining the characteristics of the Zarzi culture, alongside those of the Natufian and Kebarian cultures of the Levant, also discovered in the 1930s.[34] At the time, while archaeologists all but abandoned the Zagros, the Levant and the Natoufian culture were the focus of more detailed excavations. Despite this, more Zarzian sites were discovered in the 1960s - notably Warwasi, Pa Sangar, Ghar-i Khar - as well as a set of thirty-five bodies deposited in the Shadinar cave around 10,700 BP,[35] which provided a better description of the Zarzian culture. Further research since the 1980s has shed light on elements such as climate, vegetation, and human activity. In 2006 and 2017, carbon-14 dating of animal bones found at several sites, such as Palegawra, allowed the refinement of various dates.[36][37]
Carbon-14 dating at Shanidar and Palegawra indicates that the Zarzian presence began around 12,000 BP[nb 1] and probably ended with the advent of the Neolithic around 10,000 years ago.[23][38] During this period, climatic conditions improved sufficiently to allow vegetation to spread to higher altitudes.[39] The Western Zagros, populated by deer and goats, seems to have been covered by a dense wooded steppe, while the Palegawra documentation indicates the presence of oaks, poplars and even junipers or tamarisk. The banks of the Middle Euphrates and probably the Tigris are bordered by riparian forests of poplars, willows, and tamarisk. However, there is a certain discrepancy between pollen data and paleozoological data, especially in the Zagros region, where forest animal species sometimes appear where pollen diagrams indicate the existence of steppes.[37][40]
The reduction in tool size that began at the beginning of the Baradztian reached its peak during the Zarzian, with a variety of microlithic tools, notably in geometric shapes, including many triangles, trapezoids, and curved pieces at the end of the Zarzian. Polished axes have also been found at Palegawra and Zawi Chemi, but do not seem to belong to the Zarzian culture. The only polished axe that seems to belong to this culture comes from Deir Hajla in the Jezirah. As for heavy tools, various Late Zarzian sites, including Zawi Chemi, yield sandstone or granite millstones with cylindrical basalt pestles. Although some of these heavy tools can be transported - such as the 30-centimeter sandstone slabs found at Zawi Chemi - some appear to be embedded in the ground. Personal ornaments made from bones are also known. Some are made from marine shells, indicating the existence of a long-distance trade or travel network. Beads and pendants have also been found at Palegawra and Pa Sangar.[41][42][43]
The main characteristic of the Zagros inhabitants during the first part of the Zarzian period was movement. They traveled in bands from one place of subsistence to another. Places of occupation include small sites, such as hunting lodges without permanent structures, where small bands hunted, prepared, and brought the prey back to fixed base camps. Other sites appear to have been less frequently occupied transitional camps where stone tools were made or repaired. Base camps, on the other hand, were established under rock shelters or in larger caves such as Shanidar, Palegawra, or Pa Shangar. What these shelters seem to have in common is that they were located at the crossroads of different environments and offered a wide vantage point over the entire plain from which to observe the movements of prey (gazelles and onagers).[37][42][44][45]
At sites such as Pa Sangar, Palegawra, Zarzi, and Shanidar, archaeologists note that hunting practices remained unchanged from earlier periods: goats and onagers were still the main sources of meat. However, the range of species hunted expanded considerably from earlier periods, with an increase in small prey such as partridge, duck, freshwater crab, clams, turtles, and, for the first time, fish. The Zarzi and Shanidar assemblages also contain shells of land snails. This diversity seems to be due to nutritional stress, forcing hunter-gatherers to collect smaller animals as larger animals became less available due to environmental changes. The Palegawra site also indicates the appearance of domestic dogs.[37][41]
During the Late Zarzian, the Zawi Chemi site seems to indicate a trend towards sedentarization. Located in an open valley not far from the Great Zab River, a few kilometers from the Shanidar Cave, the site consists of separate dwellings of approximately circular or semicircular shape, wholly or partially dug into the ground. With a stone foundation, they sometimes have a light stone or wooden superstructure topped by an organic roof.[46][47] In Zawi Chemi, there is also an enigmatic circular structure 2–3 meters high, near which there is a deposit of fifteen wild goat skulls and seventeen wings of birds of prey.[48] This installation leads to the hypothesis of the ritual use of raptor wings or feathers to adorn clothing or headdresses,[49] and reminds researcher Joan Oates of the avian frescoes at Çatalhöyük.[50] However, although the site shows a clear trend towards sedentarization, it seems to have been occupied only during the spring and summer periods, as the winter residence is still unknown.[47]
Shanidar B2 - the latest level (c. 10,750–10,000 BP) - is contemporary with the Zawi Chemi settlement. It shows no signs of human occupation, but is characterized by a semicircular stone construction and, above all, by numerous burials, many of which have been identified as those of children or adolescents.[51] All the burials in the cave have probably not yet been excavated, as the excavation area only covers an area of 6 x 6 meters at the bottom of the cave. Therefore, there may be other unexplored tombs.[52] Twenty-six graves containing the remains of at least 35 people have been identified. The bodies are unnaturally folded and sometimes stacked in both individual and collective graves.[53] Some of the graves are surrounded by stones, and the bodies show no signs of specific mortuary practices. However, 15 graves have been provided with grave goods. These offerings mostly consist of beaded decorations, often made of colored stones, and sometimes shells or crab claws. The most richly decorated tombs are the children's tombs: one of them contains over 1,500 stone beads.[54]
The motives for choosing the Shanidar Cave as a burial ground are still unknown. Perhaps the cave was recognized by the inhabitants of the Shadinar Valley as a place once inhabited by distant ancestors, or perhaps it was considered an effective shelter against animals or bad weather. Either way, the cemetery was not used for very long, and the bodies piled there - some skeletons damaged by the placement of new bodies - are very contemporary.[55] While the Zarzian society seems to be organized in small groups of about twenty individuals under a more or less informal authority, but with an egalitarian structure, the discovery of gifts deposited near certain Shanidar bodies to the detriment of others suggests the emergence of an unequal social organization between individuals at the end of the Zarzien.[47]
The seasonal villages of Zawi Chemi and Shanidar B1 seem to have given way to the first known sedentary villages in Northern Iraq: M'lefaat, Qermez Dere and Nemrik. These are located at the junction of the Zagros foothills and the Upper Mesopotamian steppe. Most of them were established between the 12th and 10th millennia BC In many ways, they are considered the heirs of the Zarzians. However, their development is not felt until the 9th millennium BC, at the expense of the other cultures associated with the PPNA and PPNB, whose influence they only begin to feel from the 8th millennium BC onwards.[56]
Still, population density was probably very low in northern Mesopotamia, and no sites have been recorded in the Jezirah Plain. This does not mean that there was no human presence: nomadic or more mobile communities may still have used part of the region on a seasonal basis. From an archaeological point of view, this is difficult to prove..[57]
Inhabitants of these villages still hunt and gather, and there is no trace of agriculture or animal husbandry during this sequence. Gazelles, hares, foxes, a few oxen, equines, and ovicaprinae are the hunters' preferred game, while gatherers harvest wheat, barley, lentils, peas, and vetches, all part of the local wild environment. These plants show no trace of domestication and no evidence of a predominance of cereals.[56]
Nemrik, being closer to the mountains, which are less well supplied with plant foods than the plains, seems to have an economic base more closely linked to hunting. Numerous bolases, accompanied by the remains of hunted animals, which may simply have been shot by their hunters, suggest that young animals were captured for breeding purposes in the 7th millennium BC and that falconry was also possible, as it was much earlier in Zawi Chemi.[58]
The inhabitants of the villages of M'lefaat, Qermez Dere, and Nemrik had been sedentary hunter-gatherers since the 11th millennium BC, and in terms of housing they had perfected the skills of their Natoufian predecessors from the Levant: at Nemrik, houses buried in a circular pit included, in addition to wooden posts, two to four thick internal pillars. These vertically erect stone pillars, sometimes fashioned after an anthropomorphic model, support a terraced earth roof, some with ladders for roof access. These pillars are wrapped in a layer of clay and lime. Lime seems to have been used here earlier than in the Levant. In Nemrik and Qermez Dere, the floors are also repeatedly covered with lime.[59] This continuous renovation and cleaning of individual houses suggests a significant social change: the transformation of the shelter into a notion of "home", a place of permanent residence for which a kind of family "property" could develop.[58]
Initially, houses were simply dug into the ground and were four to six meters wide. By the 9th millennium BC, they had expanded to a radius of between six and eight metres. In addition, at M'lefaat and Qermez Dere, villages were equipped with an open central area paved with pebbles. Designed for common household tasks, researchers have discovered millstones and fireplaces.[59][60] Later, around the houses, the first low, above-ground walls of stone and/or earth were built around the original pit, and towards the end of the millennium, the first bricks appeared in Nemrik and M'lefaat. They replaced the stacked, dried earth of the low walls. They are made from a mixture of clay, water and a vegetable degreaser such as grain husk or chopped straw. The bricks are shaped in various ways (into wafers or cigars) and sun-dried - this is considered a characteristic of early Neolithic sites in northern and central Mesopotamia.[60] They are cemented with a clay mortar. This prefabricated element technique avoids the drying time previously required between each layer of stacked earth.[61][62]
Even before the Natoufian of the Levant, the lithic industry, which was very homogeneous in the region and based on local raw materials,[63] was characterized by the laminar cutting of flint by pressure and by stone polishing. However, changes in this industry seem to have been more discreet during the first two millennia of its existence.[64] At Nemrik, excavators found heavy objects such as millstones, grinders, mortars and pestles, grinding and polishing stones. Lighter objects include stone balls (bolases), plates, mace heads, pointed objects, and polished axes (including miniature specimens). Rarer stone vessels have also been discovered, one of which is made of marble.[63]
Around 9500 BC, M'lefaat, although far removed from the Khiamian context, seems to be under Syrian influence, since El Khiam points are used.[58][59] However, in the last phase of this industry - which Jacques Cauvin calls "Nemrikian" - more original diamond-shaped arrowheads appear: the "Nemrik points".[59] From this period, archaeologists at M'leffat have found bone punches, beveled edges, and the oldest known collection of tokens.[58]
Symbolic representations of birds of prey are found in polished stone, especially at Nemrik,[65] carved at the end of stone rods (probably pestles).[66] These birds of prey are related to the Zawi Chemi bone deposit discovered in 1960 and studied by Rose Solecki. They bear witness to an original local cult and could support the hypothesis of the use of falconry.[58] The skeleton of an individual was found in a burnt-out house, his arms apparently extended towards one of these bird heads.[67] Other animals are modeled in clay, carved or engraved on stone vases: wild boars, snakes, lions, and foxes at Hallan Çemi in southeastern Anatolia. Human figures were also made. The wild ox, though occasionally hunted, is nowhere to be seen except[68] in Hallan Çemi: one house contains the skull and horns of a wild aurochs, which were once hung on the wall.[69] The subjects are similar to those of objects from Mureybet in Syria and Cafer Höyük in Turkey. Other objects found at Nemrik include stone pendants of various shapes, malachite beads, stone representations of sexual organs, and river shell ornaments.[70]
At Qermez Dere, archaeologists have noted one or two pairs of clay stelae whose function does not appear to be to support the roofs of houses. These stelae could have a symbolic meaning, suggesting that both ritual and domestic activities took place in the houses.[69]
Qermez Dere and Nemrik show some evidence of ritual: in particular, the deliberate preservation of human skulls carried into each house in the late period of both villages. This indicates ancestor worship and early recognition of the house as a "family home," confirming the sedentary nature of the region's inhabitants and underscoring the solidarity between members of the same family. A cemetery from the last phase of occupation at Nemrik contains burial objects, but also flint projectile points embedded in the bones of some of the bodies - clear evidence of violent deaths.[60]
Around 7500 BC, the Neolithic movement continued with the arrival of the first agricultural hunters, who organized themselves into villages in the high valleys of the Tigris and Zagros. These Neolithic pottery villages, whose economy was based on both hunting and agriculture, were larger, housed more people for longer periods of time, and their architecture was more substantial. However, agriculture appeared very gradually: the first traces of domesticated plants were discovered only towards the end of the PPNB (early 7th millennium BC). Before that, the inhabitants of these villages intensively harvested wild plants with sickles, equipped themselves to crush the seeds and put them in containers. In addition, certain animals, especially goats, were domesticated at most sites.[58][68][71][72]
Despite an established network of trade between the Tigris valleys, the Zagros, and even Anatolia (the use of obsidian is one of the main indicators of this), it seems that the Zagros populations followed a development independent of that of the first farmers in Turkey, to the north, or those to the west of the Fertile Crescent: their lithic industry seems to be the heir of that of the M'lefaat and Nemrik villages of the preceding period. There are, however, two points common to all sites of this period: the mixed economy and the fact that the houses became rectangular.[58][68][73]
In the Zagros, which seems to have been a Neolithic center in its own right, the appearance of agriculture is illustrated by the sites of Bestansur and Chogha Golan. The main witness of the occupation of the Tigris valley is the site of Tell Maghzaliyah, the remains of a village located not far from Qermez Dere and related to those of the Balikh valley.[73][74]
It was also at the sites of Tell Maghzaliyah and Seker al-Aheimar (of the same cultural type from the Taurus Mountains, located in the Khabur Valley) that the first decorated pottery was discovered, dating to the end of the 8th millennium BC, thus establishing a link between the last PPNB and the beginning of the so-called "Proto-Hassuna" period.[58][75]
During the 8th and 7th millennia BC, the landscape of southern Mesopotamia seems to have been a collection of constantly fluctuating marsh lands alongside more arid areas, the whole presenting a landscape generally comparable to that in which the Marsh Arabs live today. It is possible that many of the remains of this period, located near the rivers (which were major settlement areas), still exist today but are inaccessible because they are buried under the alluvial deposits of the Tigris, Euphrates, and their numerous tributaries and wadis, if they have not been simply eroded by them. Some of them also lie beneath the remains of historical eras that are currently being explored. Some sites, such as Hadji Muhammed and Ras al Amiya, were discovered by accident during modern construction.[76]
In the desert areas west of the vast wadi systems, which are now drained and dry, archaeologists have uncovered evidence of flint industries similar to those in the late Levant, with parallels to the flint industries of northern Arabia. These industries are perhaps characteristic of the Gulf region, which was still arid at the time - and undoubtedly influenced by a monsoon climate.[77] In addition, "Mesolithic" lithic tools (with microliths, burins, scrapers) dating roughly to pre-Neolithic times have been found during surface surveys in the Burgan Hills of Kuwait..[78] But when it comes to the first prehistoric populations to the west and south of what would become Sumer, researchers can only speculate.[79]
Excavated between 2012 and 2017, the Bestansur site is located in the foothills of the Zagros Mountains on the edge of the Shahrizor Plain, 30 kilometers southeast of the city of Sulaymaniyah. Geographically close, but older than Jarmo, the village survived between 8000 BC and 7100 BC. The inhabitants enjoy a wide range of ecological opportunities, including fertile arable land, water sources, hilly and mountainous terrain suitable for hunting a variety of animals and harvesting a variety of plants, not to mention the proximity of several stone deposits suitable for the manufacture of polished and carved stone tools.[73]
The architecture of Bestansur was characterized by rectilinear, multi-room adobe buildings. Not all of these buildings appear to have been used for habitation: one was devoted entirely to burying the dead. The bodies, often dismembered, were probably exhumed and certain parts, especially skulls, removed for reburial in the dwellings. This "house of the dead" provides a wealth of information on the difficulties faced by the inhabitants, probably linked to the Neolithic transition: numerous skeletons of children and adolescents highlight serious problems of malnutrition and nutritional stress in early life.[73]
Nevertheless, this dietary stress occurred despite a high level of biodiversity that should have reduced dependence on a number of Neolithic plants and thus limited the risks associated with early agriculture and the early exploitation of animals such as domestic goats, sheep, and pigs. Again, some form of hunting of small and large mammals persisted, as did the consumption of fish, poultry, and land snails, important components of the diet.[73]
Chogha Golan is one of the oldest known Neolithic sites in Iran. Excavated in 2009 and 2010, it covers an area of about 3 hectares and 11 levels of occupation from the Ceramic Neolithic (between 12,000 and 9,800 BP calibrated) have been attributed to the site. The occupation of the site has left abundant and diverse archaeological remains: traces of plastered walls, lithic industry, stone crockery, engraved bone objects and clay figurines. The animal remains are very varied: goats, wild boars, gazelles, wild donkeys, cattle, hares, rodents, birds and fish, etc.[74]
The abundant botanical remains document the beginnings of agriculture and show that it appeared over a very long period of time. Here, as in Bestansur, wild varieties of some of the basic Neolithic crops, barley, wheat, lentils and peas, were found in the vicinity of the site at the time of occupation. Again, various studies on the evolution of domesticated plants indicate that the Zagros was one of the hotbeds of plant domestication, along with the northern and southern Levant.[74]
The settlement of the Tigris valleys is mainly documented by the site of Tell Maghzaliyah, the remains of a village dating from the end of the 8th millennium BC, located not far from Qermez Dere, in a wadi on the edge of the Jebel Sinjar mountain range, whose cultural affiliation brings it closer to the cultures that characterize the late PPNB of the northern Levant and the Belikh Valley in northern Syria, where some twenty-four sites, including Tell Sabi Abyad, have been discovered. The inhabitants of these villages belonged to the typical PPNB cultural facies of Cafer Höyük or Çayönü and originated from the Taurus Mountains and other regions of Anatolia. Their economy is based on both hunting and agriculture, a well-established feature of the entire Belikh Valley, where wheat and barley are grown and sheep and goats are raised.[58][68]
Unlike the ancient round houses of Qermez Dere and Nemrik, which were built around a common area, the buildings at Tell Maghzaliyah, all of which are clearly residential, are rectangular, with mud walls built on a foundation of limestone slabs and filled with stones. There are both storage and kitchen-living areas, and one of the houses has a chimney with an external flue, similar to later houses at Umm Dabaghiyah and Çatal Höyük. As in Nemrik, the final phases of the village show a wall flanked by towers surrounding the houses, indicating possible conflicts with neighbors.
Like Nemrik, the early levels of Tell Maghzaliyah show a predominance of obsidian, despite the remoteness of the nearest deposits in Anatolia, from where the chipping technique was imported. The site also contains projectile points, both leaf-shaped and knives, as well as clockwise rotating drills. Tell Sabi Abyad II, Byblos, and Amuq points were found in the upper levels of a site in the Belikh Valley. Flint scrapers, later used during the Hassuna period, are already present. Bitumen-coated sickles have also been found at Tell Maghzaliyah and other villages in the Belikh Valley, the blades of which show traces of grain harvesting. The inhabitants also left behind marble bowls, bracelets, and polished punches. Although the village of Tell Maghzaliyah is still pottery based, plaster tableware and decorated pottery are also used. These can be found throughout Jezirah and the Syrian Desert, especially at Seker al-Aheimar.[58][68]
Seker al-Aheimar is a large Mesopotamian site of about 4 hectares and is the only one that provides information on the transition from PPNB to Proto-Hassuna pottery. The acetate levels of the PPNB village show small rectangular buildings on a paved platform of a type already encountered at Tell Maghzaliyah. The use of lime plaster on brick walls was common. While some aspects of the utilitarian stone industry at Seker al-Aheimar resemble that of the Zagros, the end scrapers, burins, polished axes, and arrowhead types common at the end of the PPNB resemble those found in the Belikh Valley. Decorative objects include gypsum vessels as well as stone and Anatolian obsidian beads.[80]
The most important discovery, however, is that of numerous stone vessels, white pottery and, above all, an early type of watertight vessel, which is the oldest form of pottery found in all of North and Central Mesopotamia. Fired on a low, temperate fire, which is characteristic of the first ceramic Neolithic, this pottery has a dense, granular consistency and is very different from the future "Proto-Hassuna" pottery.[80][81] For researchers Yoshihiro Nishiaki and Marie Le Mière, this discovery demonstrates the existence of an early ceramic phase - represented by a cultural entity provisionally referred to as "Pre-Proto-Hassuna" - which appears to have begun in the Middle PPNB and ended in the Proto-Hassuna, introducing the latter.[81]
During the Ceramic Neolithic (c. 7000 to 5000 BC), agricultural communities increasingly occupied Mesopotamia. Villages were established in the drier climate of Lower Mesopotamia, indicating the development of irrigation systems. This does not mean, however, that Lower Mesopotamia was unoccupied before the 7th millennium BC. Remains of earlier settlements may be difficult to access, buried in deep layers of silt or simply destroyed by intense river erosion on the plains.[82] The most important settlements in central Mesopotamia are Bouqras, near the Euphrates, not far from the Iraqi border in eastern Syria, Tell es-Sawwan, south of Samarra on the Tigris, the sites of Umm Dabaghiyah in the steppe west of Hatra and Choga Mami at the eastern end of the Mesopotamian plain, at the foot of the Zagros Mountains east of Baghdad.[82][83]
At the beginning of this period (up to 6500 BC), the first decorated ceramics appeared, notably at Bouqras, Tell es-Sawwan and Umm Dabaghiyah. These were perfected until the beginning of the Chalcolithic period. Historians distinguish several cultures in relation to these painted ceramics: those of Hassuna, Samarra, Halaf and Ubaid, the names of the archaeological sites where they were first discovered. In the early centuries, ceramics were first partially decorated with the line incisions, wavy patterns or raised lozenges characteristic of the Hassuna culture. Then they became finer and finer, completely covered with geometric paintings, and more complex with representations of stylized natural elements such as plants or birds for the Samarra and Halaf cultures. Although the production of painted pottery disappeared at the same time as the use of the first pictographic signs, these decorations cannot be considered as a means of transmitting language, as is the case with writing. However, they are probably a code or pictorial language whose meaning has been lost..[83][84]
In the Zagros, communities seem to have diverged somewhat from those in the Mesopotamian lowlands. Jarmo is a case in point. It's a village located on the western outskirts of the Zagros, on a hill in the Chemchemal valley in north-eastern Iraq. It was originally excavated in the 1940s and 1950s by archaeologist Robert John Braidwood to investigate the origins of agriculture. The village consists of mud-brick houses with several rooms connected by low doorways. A type of foundation known from Çayönü has been identified in the form of a grid. The large number of clay figurines found in Jarmo probably indicates a local specialty: over 5,500 figurines and about 5,000 clay tokens have been found there. However, the low-fired "tadpole" pottery proved to be quite different from that of the Proto-Hassuna culture found at sites in the Mesopotamian lowlands, while other examples of this pottery are found further east. This seems to indicate a cultural demarcation between the Zagros and the Mesopotamian lowlands from this period onwards.[85]
In Upper Mesopotamia, the treatment of the dead prior to the expansion of the Ubaid culture in the 5th millennium BC varied in manner and location. It ranged from simple burials accompanied by a few objects to disarticulated bodies whose skulls were given special treatment, such as burial in a ceramic urn. Moreover, although the evidence is weak, it seems that some communities practiced burial outside the village: burials have been found on the top of the mound of Yarim Tepe (a village abandoned when the dead were deposited). At Tell Sabi Abyad, a village that was deliberately burned down, archaeologists found two skeletons that appeared to be lying on the roof of a house; this was undoubtedly an elaborate ritual in which the dead played a central role. At Domuztepe in southeastern Anatolia, the Death Pit is a mortuary complex containing the intentionally fragmented remains of at least forty individuals. Researchers see it as a transformation of the dead into a reconstituted collective ancestor. Cannibalism and human sacrifice are also possible.[86]
Around 6500 BC, the use of copper, the appearance of seals, irrigation, wall paintings, the first sanctuaries,[82] increasingly obvious social divisions, and increasingly complex management of society were noted.[79] In addition, the first rectangular-shaped bricks and the first buttresses inside and outside walls, previously observed at Cafer Höyük, appear mainly at Samarra, Bouqras, and to a lesser extent at Tell-Hassuna.[87]
Umm Dabaghiyah, occupied for three or four centuries, located in the Iraqi Jezireh between the Tigris and Euphrates rivers, much better irrigated in the 7th millennium BC than today, provides the most complete documentation of the Proto-Hasna phase (c. 7000 BC – 6500 BC). The population of Umm Dabaghiyah, undoubtedly the heir to the Syrian PPNB, still operated a mixed economy, with the evening primrose being hunted primarily for its skin and meat. This may be a seasonal site. However, wheat, lentils, and barley were present, as well as typical marsh plants, suggesting the presence of nearby salt marshes and shallow lakes in the area. Wall paintings suggest that the fast-moving onagers were caught in nets.[83][84]
The buildings at Umm Dabaghiyah show indentations in the walls, suggesting that they were accessed through the roofs. These buildings, with their generally very small rooms, have internal plaster fireplaces associated with external ovens, suggesting cold winters (as in modern times in the region), unless this device was used to dry evening primrose skins. Some of the buildings, built in long rows, appear to have been used for storage. Similar but shorter storage structures were found further north, at Yarim Tepe.[88][89]
Compared to other sites, Umm Dabaghiyah does not have many examples of pottery. Moreover, it is very similar to the pottery of Bouqras, which seems to be related to it, in that it is simple: large jars or bowls painted in red ochre and sometimes decorated with rudimentary motifs such as bands, chevrons, or dots. However, Bouqras is a much larger village with clear origins in the Syrian PPNB. In addition to the typical Neolithic acetate material and ceramics previously described for Umm Dabaghiyah, finds include alabaster pots and paintings of ostriches or cranes painted in red ochre on some interior walls, all covered with plaster. These buildings, with three or four rows of rooms, are regular in plan and arranged along narrow streets in a coherent layout. Although there is no clear evidence yet, the size of some of the buildings and this layout suggest the establishment of a formal authority.[90]
Pottery made with a technique similar to that of Umm Dabaghiyah and Bouqras, but in a different style, is also found, although more rarely, in Levels I and II of the Tell es-Sawwan site.[nb 2] Sawwan is a 7th millennium BC village in Lower Mesopotamia on the Tigris River, about 80 km north of Baghdad. About 400 tombs have been discovered here, three-quarters of which contain infants. Many of the graves are missing bones. One, for example, contains only the bones of a child's hand, suggesting that some bodies may have been moved to other gravesites. Beneath the tombs is an enormous amount of alabaster objects: vases, figurines, and miscellaneous items. All of these objects are specific to the Tell es-Sawwan site: some figurines have inlays of other materials, such as shells or stones of different colors, often highlighting the eyes. Depending on how they are arranged in relation to the tombs, researchers have inferred significant social differentiation between individuals. A few clay and alabaster figurines found outside the tombs may have been associated with funerary rituals. Most of the tombs were found under large, three-part buildings (known as "tripartite"), a structure that persisted well into Mesopotamian antiquity. However, some burials are located in other sectors and are clearly separated from any architecture, making it difficult to make a clear interpretation between "burials under dwellings" and "cemeteries".[90][91]
The houses here are "tripartite", that is, they are composed of a central rectangular space, which is entered through one of its widths, giving access to a series of small rooms that surround it symmetrically along its length.[92] At Tell es-Sawwan, these buildings are found empty of household goods, as if the inhabitants had left and taken everything with them. In fact, the site was probably abandoned and reoccupied after an estimated period of between 123 and 300 years.[90][93]
The end of the Proto-Hassuna period also marked the first use of seals: at Hassuna, Bouqras and Sawwan, they introduced a practice that developed in Mesopotamia and still persists today as a "guarantee". The first seal impressions appear on small white lids, clearly identifying the "property" contained in the sealed container. By the end of the Samarra Period, these seals were widely used as "contractual" devices in central and northern Mesopotamia.[94]
The Hassuna culture (c. 6500 BC – 6000 BC) takes its name from an archaeological site, Tell Hassuna, located in central Jezirah, between the Tigris Valley and Jebel Sinjar, not far from modern-day Mosul. Yarim Tepe and Tulul eth-Thalathat, on the Jezirah plateau and the Kurdish foothills, are also part of the Hassuna profile. These villages survived throughout the 7th millennium BC in a rainy region where dry farming was possible. The inhabitants grow a wide variety of grains, lentils and peas. They raise cattle, goats, sheep and pigs, and hunt aurochs, gazelles and bears.[95]
Initially built of adobe, the walls of the multiroom houses were gradually built of molded brick. Houses were also built closer together and became more complex, with one house containing eleven rooms. The system of families grouped around collective granaries, which were considered to be the center of the settlement, was already in use in the North Mesopotamian Neolithic society of Tell Maghzaliyah and reached its full development during the Hassuna period. This system seems to have ensured community cohesion.[95]
The Hassuna culture is characterized by very distinctive pottery, with motifs painted in red ochre or incised in parallel line networks and/or chevrons. This type of pottery is largely found in northern and central Mesopotamia, as well as in the Khabur region of northeastern Syria.[84]
It is made by firing in large kilns. At Yarim Tepe, archaeologists have discovered the first well-preserved example of a pottery kiln consisting of two superimposed chambers separated by a wall with a smoke extraction system..[95]
The Samarra Culture (c. 6200 BC – 5700 BC) is characterized by finely painted ceramics. Documented at Tell es-Sawwan and Choga Mami in central Mesopotamia, it is considered an evolution of the Hassuna Period ceramics and those of the Levantine corridor, whose villages were gradually depopulated.[96] Archaeologists have also found evidence of Samarra pottery at a number of sites in northern Mesopotamia, as far away as the Syrian Jezirah and the Belikh Valley. Samarra pottery was easily recognizable and often elaborately decorated.[97] Better made than its predecessors, it consists of large bowls and vases with rounded shoulders. Its light beige, slightly rough material is more harmonious. The decoration consists of geometric designs painted in bright red, brown, or purple-brown. These motifs can also represent men, women, birds, fish, antelopes, or scorpions.[83][98] Some necklaces even have reliefs of human faces with stylized features. Pottery characteristic of this culture was also found at Tell el-Oueili, the oldest known village in southern Lower Mesopotamia, which documents the Ubaid period (Ubaid 0 phase, c. 6200 BC – 5900 BC).[83][94]
Samaritan statuettes were made of terracotta or alabaster. The figures are generally standing or crouching, and most are female.[99] None of the figurines discovered at Choga Mami is intact: the heads are all broken and the presence of single legs with thin, flat inner surfaces suggests that the breakage was deliberate. Researchers believe that there was some kind of "contract" in place at the time the statues were broken.[100] Some statuettes have elongated skulls and wide-open "coffee-bean" eyes. As in the Hassuna phase, the eyes and necklaces are often made of an external material, added or inlaid: the pupil is made of mother-of-pearl, and the thick, black eyebrows of bitumen. A technique reminiscent of future Sumerian production.[99] These figurines seem to indicate a practice of cranial deformation - already attested in the pre-Ceramic Levant[79] - which spread until the last phase of the Ubaid period. Such deformation can also be seen on some skeletons from the Ubaid phase at Eridu and Tell Arpachiah. This practice seems to have been carried out by a ruling class, no doubt seeking to demonstrate that they were no longer able to carry loads on their heads.[100]
Levels III and IV at Tell es-Sawwan confirm the use of the "tripartite" Mesopotamian house plan already discovered in the Hassuna period, but in the shape of a capital T.[nb 3] These levels were also characterized by Samarra-style pottery and a second type of building that appears to have served as a "granary". These levels were also characterized by Samarra-style pottery and by a second type of building that appears to have been used as a "granary", identified by its lime-covered floor and containing agricultural implements. All of these houses were surrounded by a mud-brick wall (also visible at Choga Mami)[93] and a ditch, are composed of remarkably small rooms. This suggests the use of a flat roof for daily[101] activities or implies the presence of a floor covering the entire surface,[93] accessed by a staircase. The latter configuration could imply a division of activities between two levels: on the first floor are the service areas, storerooms, and workshops, where animal life takes place; on the first floor are the living quarters, a series of rooms, and various bedrooms.[102] The use of molded mud bricks has been documented at Tell es-Sawwan, Bouqras, and Samarra. This method of construction (of which a few isolated earlier attempts are known) became widespread during the Samarra period. It allowed for the standardization and rationalization of construction according to a pre-established plan. It anticipates the architecture of the ziggurats and large buildings of historical Mesopotamia.[87][93][94]
In the Samarra phase, the first Mesopotamian "public" buildings appear, evidence of social and religious activities. They can be considered the ancestors of Mesopotamian sanctuaries.[103] The tombs below the last levels of Tell es-Sawwan contained funerary objects consisting mainly of pottery and clay figurines, in stark contrast to the tombs on the first levels.[101]
Choga Mami, about 2 kilometers from the Iranian border, northeast of Baghdad, was built on an axis of communication along the Zagros Mountains, where water is available. Here the houses were built very close together, with large, separate courtyards, perhaps used as working areas.[100]Samarra period inhabitants raised sheep and goats and hunted gazelle and aurochs. In arid southern Mesopotamia, farmers had to rely on irrigation to fertilize soils that were already prone to erosion and salinization. The first traces of irrigation canals were discovered at Choga Mami.[104]
By the end of the Samarra period, seals and other "contractual" devices were more widely used in central and northern Mesopotamia. Only a small number of such seals appear in southern Mesopotamia, but this absence may be due to difficulties in exploring the sedimentary strata of the region.[94][103]
In Upper Jezirah and the rest of Upper Mesopotamia, the Halaf culture (c. 6100 BC – 5200 BC) developed alongside the Samarra culture. It was named after the site where the first examples of richly decorated pottery were discovered in 1929: the Tell Halaf site on the Turkish-Syrian border.[106] Other sherds of this pottery were later discovered in Nineveh, Tell Arpachiyah (both on the outskirts of modern-day Mosul), and in many other places such as Chagar Bazar, Tepe Gawra, Yarim Tepe, also in northern Syria (Yunus Höyük), and in southeastern Anatolia at Domuztepe. Along with Anatolian obsidian, Halaf pottery is the most widely distributed of all types of ancient ceramics, found from the Zagros to the Mediterranean, a distance of about 1,200 kilometers.[79] But it was in the Belikh valley, at the Tell Sabi Abyad site, that the earliest remains of this culture were discovered.[107] It was here that late Samarra-type pottery was discovered, referred to here as "transitional Halaf", was discovered, which evolved into true early Halaf and thus determined the origin of the Halaf culture, originally thought to be located in northeastern Anatolia.[79]
The Halaf culture is thus characterized by pottery that follows in the footsteps of that of Samarra, but is technically more sophisticated. So much so, in fact, that it attests to a much greater degree of specialization by the potters, which was already evident during the Samarra period. These potteries, with their very fine paste, were subjected to a hotter, more oxidizing firing process, giving them an orange or buff color.[108] Shaped using a "tournette" (slow wheel turned by hand),[109] the first rotary method of shaping ceramics, they are made from carefully selected local clays. They were generally fired in kilns with a round or oval base and an ascending draft. These were equipped with single-chamber kilns (pottery and fuel in the same chamber), which were gradually replaced by double-chamber kilns (pottery and fuel separated), which allowed a better distribution of heat in the objects being fired.[110] Initially dominated by geometric motifs - triangles, squares, checkerboards, crosses, festoons, small circles, and cross-hatching - pottery decorations gradually took on more complex decorative schemes, including highly stylized plant and animal motifs such as resting birds, gazelles, and cheetahs. Other motifs are undoubtedly more religious, such as bucraniums (stylized bull heads), double axes, or "Maltese squares" (a square with a triangle at each corner). Excavations on the Euphrates have revealed black and red bichrome decorations and, in smaller numbers on the banks of the Khabur, polychrome decorations combining red, black, and white.[111][112]
Halaf-type pottery seems to have been produced at specialized centers such as Tell Arpachiyah, Tell Brak, Chagar Bazar and Tell Halaf.[112] At Tell Arpachiyah, beneath the upper Ubaid-dated levels, archaeologists have uncovered what appear to be pottery workshops or stores, as numerous pottery-making and cutting tools were present: stone and obsidian objects, a large number of flint and obsidian cores. This level was completely burnt, apparently deliberately, probably for ritual reasons. Tell Arpachiyah may well have been a special religious site: an area of round buildings (wrongly assimilated to the tholos of Mycenae),[nb 4] surrounded by a perimeter wall, contains numerous burials, but, unlike the round constructions of other places, contains no trace of habitation.[106]
Once the pottery was produced in the center of the village, it was transported via intermediate centers to more distant destinations, notably as far as the Persian Gulf, in exchange for other products.[112] Apart from these production centers, the villages of the Halaf culture have the common characteristic of often being very small. The expansion of this culture seems to have taken place from one village to the next: as soon as a village exceeded a hundred inhabitants, settlers move away to build a new village nearby.[114] For prehistorian Jean-Paul Demoule, this is the hallmark of a low-hierarchical society that develops in small human agglomerations.[115] For Georges Roux, "nothing indicates a brutal invasion, and everything we know about them points to the slow infiltration of a peaceful people".[112] Nevertheless, for researcher Joan Oates, there is ample evidence of deliberate cranial deformation in this culture, a "practice with considerable elitist potential" already mentioned in the section devoted to the Samarra period. At Tell Arpachiyah, the dentition found on the deformed skulls indicates that their bearers were genetically related, suggesting the constitution of an elite family group.[79]
Another notable aspect of the Halaf culture is the female figures of childbirth that are typical of this culture. They depict a seated woman supporting her breasts with her arms. The body is decorated with lines and strokes that appear to represent tattoos, jewelry, or clothing. The head is often represented by a flattened neck stump, sometimes with two large eyes. These may be talismans against sterility.[112]
The Halaf culture is characterized by circular structures called "tholoi" by archaeologists, most of which were actually dwellings (unlike the Mycenaean tholoi). Built of mud or brick on pebble foundations, they sometimes had a quadrangular extension and were covered by a dome. The seeds of domesticated and dry-farmed plants collected in the villages indicate that the inhabitants of the round houses of the Halaf culture, like their contemporaries in Samarra and Hassuna, were largely sedentary farmers and breeders. This does not exclude the existence of itinerant herders who moved between base villages and seasonal transhumance camps. The bones found are those of common animals such as sheep, pigs, goats and oxen..[112][114][117]
It is worth noting the marginal existence of a form of hunting that has evolved from a collective practice, characteristic of the PPNB, to a more individual hunt practiced by a smaller group of people. This type of hunting, based on the bones of wild animals (gazelles, onagers, deer, fallow deer, and roe deer), seems to be more important in communities located in the south, where the steppes are more arid. The product of this hunt may have been exchanged for plant products cultivated further north. Primarily for food, it is also used to protect herds, fields, and livestock. However, Halafian arrows, probably coated with poison, are rare and made from local flint, often of poor quality, and more rarely from obsidian. Most of the arrows found are still unused. Consequently, other perishable weapons such as slings or traps may have been used.[118]
At Tell Sabi Abyad, archaeologists have uncovered some 300 clay seal impressions whose original seals cannot be traced. These impressions seal portable containers, baskets, or vases. Far more numerous than those already discovered for the Samarra period, these impressions depict caprids, plants, and geometric motifs. As in Samarra, it is difficult to derive any meaning from them other than that they may be part of an administrative practice of the "contract" type or reflect a desire to control the circulation of goods or commodities. Other Late Halaf Period seal impressions have also been discovered at Tell Arpachiyah.[57][119]
The long Ubaid period (c. 6500 BC – 3900 BC) refers to a late Neolithic and an early Chalcolithic phase of prehistory and is named after the site of Tell el-Ubaid, not far from Ur. There, in 1924, Leonard Woolley, while directing the excavation of a Sumerian temple, discovered numerous monochrome pottery pieces that testified to another culture.[77] Frustrated and less meticulous than the ceramics of the Halaf culture, this pottery, often hastily decorated with geometric motifs and sometimes inspired by nature,[120] was also discovered in a nearby ancient cemetery.[77]
Later, the remains of the city of Eridu provided the first important information about the architecture of the Ubaid culture.[121] Subsequently, the discovery in the 1980s of the Tell el-'Oueili site, north of the modern city of Nassiriyah, revealed the much older nature of the Ubaid culture and pushed back its origins to around 6500 BC, thus attributing Tell el-Ubaid to a very late phase of its eponymous culture. The term "Ubaid culture" is therefore a rather conventional appellation that does not reflect any real continuity or uniformity of this culture.[77]
Researchers have identified six successive phases, numbered from 0 to 5, and named after representative sites, based on visible changes in the forms and decoration of ceramics related to this culture. Early Ubaid is mainly documented by the sites of Tell el-Oueili (Ubaid 0 and 1), Eridu and Haggi Muhammad (Ubaid 1 and 2), which are the oldest known sites in Lower Mesopotamia (c. 6500 BC – 5300 BC). The last phases 3, 4, and 5 are documented by Tell el-Ubaid itself,[114][123] but are above all marked by the spread of the Ubaid culture to northern Iraq and northeastern Syria in the middle of the 6th millennium BC, with Tepe Gawra, northeast of Nineveh, and Zeidan, near the confluence of the Khabour and Euphrates rivers, as significant sites.[109] The Ubaid culture also appears to have spread southwards: sites related to the Ubaid culture are also found in Kuwait, with evidence of boat use suggesting the development of fishing and perhaps even the presence of pearl-seeking divers. Seasonal sites are also attested along the northeast coast of Saudi Arabia and in Qatar. Shards linked to the Ubaid culture have also been found in the southern Emirates.[109]
Until the end of the 5th millennium BC, the egalitarian, non-stratified character of Ubaid culture communities is generally accepted: as in Late Neolithic communities, temporary and cyclical leadership - based on a communal corporate identity[124] - seems to have been in the hands of the Elders. The scarcity of marks of status and differentiation in the remains, and the virtual absence of luxury, exotic, or prestige objects, in villages that were generally small in size and with very few differences in individual architecture, are indicators of a type of organization with little hierarchical structure. Added to this, even on sites that were probably of regional importance, is the scarcity of seals linked to administrative control and an apparent lack of specialization in trades.[124] It was not until the last centuries of Tell Abada's occupation (end of 5th millennium BC) that archaeologists observed specialized activity in the manufacture of ceramics and the intensive use of marker tokens, a sign of more elaborate administrative management.[125]
Described as "islands surrounded in a marshy plain", still evoked in the impressions of cylindrical seals from the late 4th millennium BC,[126] the sites discovered so far in southern Mesopotamia are located on so-called "turtle shell" terraces, the remains of stepped alluvial terraces incised during the Pleistocene that evoke the patterns of a tortoise shell when seen from the air. These sites, like Hadji Muhammed and Ras al-Amiya, were discovered accidentally during modern construction work, as they lie deep beneath thick layers of alluvium carried by the streams of the ancient Mesopotamian delta. It is therefore likely that many other sites remain unexplored under a thick layer of sediment.[79][123][127]
The oldest excavated levels of the Ubaid culture are found at Tell el-Oueili. Dated to 6200 BC using carbon-14 measurements, they lie just above a water table. The water table prevents archaeologists from reaching the virgin soil, leaving one or more archaeological layers currently unexplored, and it doesn't appear that occupation of the site began until 6200 BC, such is the high level of neolithization of the population on the levels that can be explored. They were already cultivating cereals such as wheat and barley, while beef, pork, sheep, and goats were domesticated.[77] In addition, the Oueili levels of Ubaid 0 have yielded pottery closely related to that of Choga Mami - attesting to contacts with the Samarra culture - female figurines and tripartite buildings similar to the Proto-Hassuna of Tell el-Sawwan.[128]
However, the buildings are much larger than those at Tell es-Sawwan: wooden posts support the roof, while stairs lead to terraces and a large living room with a fireplace, a possible place for family gatherings that seem to have brought two generations together. Adjacent to this room are a series of small rooms, each with a hollowed-out fireplace, probably providing private spaces for a nuclear family. In addition, another large outdoor structure with huge cellars appears to be a large communal attic. All of these structures are pre-designed using life-size plans drawn on the ground. These plans are based on a unit of measurement of about 5′ 9" (1.75 m) - that's six feet by 0′ 11" (0.29 m). In addition, familiarity with certain geometric properties of triangles made it possible to draw right angles. The walls are made of mud bricks pressed between two planks. The tops of these bricks are convex, with fingerprints that help them adhere to the mortar. It wasn't until the middle of the 6th millennium BC (Ubaid 1) that bricks of uniform size were formed into a frame. They were also used in the construction of more numerous but narrower granaries.[77][128]
Generally speaking, despite the apparently wetter climate of the time, irrigated agriculture seems to have gradually developed, while leaving some room for the exploitation of marshes (fish, reeds), another major factor in the development of human communities in southern Mesopotamia.[123][129] Date palms, wheat, and barley were also cultivated. Cattle and pigs were raised alongside smaller numbers of sheep and goats.[126]
During the Ubaid 2 and 3 periods, certain buildings were enlarged to such an extent that the archaeological community was divided over their use.[130][131] For others, the religion of the time still seems to have been practiced in small sanctuaries, and the large buildings discovered under the Eridu ziggurat seem more likely to have been used for meetings,[77][132] or to welcome visitors in the manner of the present-day mudhif of the Marsh Arabs of southern Iraq.[133] Pascal Butterlin hypothesizes that community buildings rubbed shoulders with those reserved for worship, both built in the tripartite model and expanded over the centuries. Later, during the Uruk period, the size of the buildings at Uruk seemed to increase more rapidly than those north of Mesopotamia at Tepe Gawra or Tell Brak.[134]
The third Ubaid phase (c. 5200 BC – 4500 BC) was marked by expansion into northern Iraq and northeastern Syria: pottery and utilitarian objects such as clay grinders and figurines identical to the sites discovered in the south are found at more northerly sites like Tell Arpachiyah and Tepe Gawra near Mosul, Tell Zeidan on the Middle Euphrates and as far as Değirmentepe in eastern Anatolia, where the first traces of copper metallurgy appear.[135] While it is well established that the spread of the Ubaid culture took place over several centuries by a slow, gradual, and peaceful change, its mechanism still remains the subject of debate.[136] For Joan Oates, this propagation is the product of cultural movements through marriages or exchange systems.[109] For Jean-Daniel Forest, the Halaf culture, as it advanced into southern Mesopotamia, was forced to adopt irrigated agriculture, imitating the already existing Ubaid culture to such an extent that the latter rapidly replaced the Halaf culture to the north.[77] However, it was a new way for the people to express their identity through new objects in their daily lives, especially a style of pottery very similar to that of the south, but with different, more naturalistic decorative motifs and with a more reddish color than that of the Halaf pottery and that of the southern Ubaid culture.[136][137]
By the middle of the 6th millennium BC, the circular, tholoi-type houses of the Halaf culture were gradually replaced by the rectangular, multi-room houses typical of the northern Ubaid period. Most of these followed a tripartite plan, with the smaller rooms often arranged symmetrically around a large central hall. This type of building is found at Tell Abada in the Hamrin Basin, where it is characterized by regular buttresses, at Tepe Gawra in northern Iraq, and as far away as Değirmentepe in Anatolia. Tell Abada also has a hydraulic pipe system that brings water from a nearby spring and wadi to a cistern inside the village. The village also seems to have specialized in the manufacture of pottery: the oldest evidence of the use of the potter's wheel can be found here, dating back to the 6th millennium BC.[125][138]
The culture's treatment of the dead seems to have gradually become more uniform. Whereas the Halaf culture showed great diversity, the Ubaid culture was content with burials in simple tombs equipped with a few ceramic vessels, with no apparent social differentiation. While Tell Arpachiyah has a small Ubaid cemetery outside the domestic dwellings, similar to the larger contemporary cemetery at Eridu in southern Mesopotamia, burial within the village seem to have been limited to the burial of infants under the houses. The same is true of Abada, where fifty-seven infant urns were found in the basements of large buildings.[86][139]
Late Chalcolithic Mesopotamia - which includes the end of the Final Ubaid period (c. 4000–3900 BC) and the Uruk period (3900–3600 BC) - witnessed a society that gradually became hierarchical around influential families occupying large houses in which no agricultural tools were found. These were adjacent to what are now recognized as temples built in towns that grew considerably in size, dominating neighboring villages to the point of taking on an urban or "proto-urban" character.[140][141] What's more, these temples seem to have housed a sacred class that would legitimize the new secular "officials" with whom they were closely associated.[142]
The discovery at Uruk of a series of tablets written in symbolic signs - the first of their kind - reflects the activity of a complex, stratified administration. Among these texts is the List of Titles and Professions. It probably dates from after the 4th millennium BC, but its compilation seems to illustrate an earlier situation indicating a four-level hierarchical society, with various professions, as well as economic and political groups, becoming increasingly complex.[143][144]
All these elements go hand in hand with the emergence of cities that seem to evolve from two geographic poles: those in northern Mesopotamia, such as Tell Brak or Tepe Gawra, and those in the south, such as Uruk. At the beginning of the 4th millennium BC, the cities of these two poles developed independently.[145]
However, from the middle of the 4th millennium BC, the development of the northern cities tended to slow down: they stopped growing and even began to shrink. At this time, relations between the two regions of north and south seemed to become unbalanced, leading to a kind of "colonization" of northern cities by southern ones. This colonization, more often than not economically motivated, took place along the trade routes of northern Iraq, northern Syria and southeastern Turkey, and can be recognized by the dominance of pottery with forms characteristic of the southern Mesopotamian city of Uruk, and by certain types of construction, seal design, and sealing practices.[145]
Habuba Kabira is a case in point: the Syrian site in the Middle Euphrates region shows such close links with southern Mesopotamia that this city and its religious center near Jebel Aruda are considered a "colony" of Uruk.[146] Tripartite houses with outdoor spaces and sometimes large reception halls were built along streets organized around the gates of a massive wall. They were surrounded by a dense assembly of smaller houses. This latter urban layout may have been similar to that of Uruk, whose site has only been partially excavated. At Haçinebi Tepe, on the Turkish Euphrates near the modern city of Birecik, local pottery has been found alongside pottery from southern Mesopotamia.[147]
Towards the end of the 4th millennium BC, colonization of Uruk seems to have come to a halt: the use of southern-style pottery diminished and disappeared. But this is not always followed by a simple and immediate rebound of northern political structures: the lower town of Tell Brak, for example, was abandoned and the site declined significantly. By contrast, in Arslantepe, where Uruk's colonization seems to have been less overwhelming, a new type of elite emerges, apparently based more on personal power than on temple institutions.[147]
While it has now been shown that irrigated agriculture in the south - especially with the help of the spider's ards and wagons - was not the sole cause of the centralization of social organization, it did produce a food surplus that was used to maintain specialists, leaders, priests, and artisans who did not produce their own food. This food surplus - along with pearls and fish - was also traded between the southern elite and those in the north, where dry agriculture was less intensive, but where copper ore, precious stones, obsidian (southeastern Turkey), and wood were available, woolly sheep were raised for cloth, and the potter's wheel (already known by the middle of the 6th millennium BC) was introduced.[147][148][149][150]
Similarly, the increased use of seals and tokens reflects the existence of increasingly centralized and controlled economic activities.[142] Moreover, these seals became increasingly complex. They are used to seal a wide variety of portable goods shipped in sacks, baskets, and jars, marking the authority under which the contents are placed. They are also placed around doors to control access to storage areas. In addition, broken seals appear to have been kept as a record of transactions. While clay seals were initially associated with one or more dwelling units, by 4000 BC they began to be concentrated around newly emerging religious institutions.[151]
Although the lack of distinctive signs on the remains of a Late Ubaid cemetery at Eridu is indicative of societies that were not very hierarchical until the middle of the 5th millennium BC, researcher Joan Oates points out that the social status of the people buried in the Eridu cemetery is still unknown and that this cemetery alone does not indicate the absence of social inequalities.[140] At Tepe Gawra, in northeastern Iraq near Mosul, the evolution of burial practices, which had previously been on the way to standardization, became more diversified: the 6th millennium BC cemetery saw the appearance of very rich tombs containing gold, silver, and other rare materials never before seen, sometimes associated with infant burials.[124][152]
Tell Majnuna, a subsidiary mound of the Tell Brak site, provides evidence of a mass burial of partially articulated individuals. These human remains probably represent a feast or massacre, suggesting a violent assertion of power. This is in contrast to the burials of the Ubaid period and undoubtedly implies a loss of individual identity.[152]
The painted pottery of earlier cultures tends to disappear in favor of simpler materials, sometimes mass-produced and identical[124] - so-called "Coba" bowls with the "ox" symbol inlaid on some of them, also found at Tell Zeidan and, later, at Uruk - whose use is not yet attested, but which could be measures of rations, indicative of a more complex management of resources.[153][154] During the Middle and Late Uruk periods, beveled-edged bowls found in very large quantities could also have been used to distribute standardized rations or to make bread.[144]
So-called "lizard-head" figures, probably representing the ruling elite, are very common in Eridu and Ur. They provide further evidence for the practice of skull deformation, an indicator (discussed above) for the existence of a non-working class. One of the male figures found at Eridu holds a club, which could be interpreted as a symbol of authority.[124][128]
At Tell Brak, numerous seal impressions have been found dating from 4200 to 3600 BC. They often depict animals such as lions alone or in groups, snakes, or goats. These animals are often fighting each other, and the only humans depicted are fighting a lion or snake with weapons or tools. The lion could be seen as a symbol of authority at a time when leadership was becoming more powerful. This was before they appeared in art from southern Mesopotamia at the end of the Uruk period. There are also hybrid figures that combine humans and animals (except for goat heads), or combine goat heads with the legs or wings of other animals. Geometric or abstract motifs, flowers and a few complex scenes complete the Tell Brak collection.[155]
There are, however, a considerable number of seals depicting vultures. An elegant symbol, especially integrated in everyday life: the hypothesis that it could be related to the processing of waste resulting from extensive urbanization seems plausible. The carrion and garbage eaten and processed by the vultures could have led the inhabitants of the time to consider these birds of prey as a blessing.[156]
The 4th millennium BC also saw the appearance of the first cylinder seals. The earliest were discovered in the Middle Uruk levels and predate the Eanna buildings. Their complexity provides greater security than the seals used previously. An important figure, the "man in the net skirt" (or "priest-king") is often depicted, sometimes hunting lions, attributes or prerogatives known later in Mesopotamian history as those of kings, priests, and warriors.[143]
At Tepe Gawra, the architecture of the large tripartite houses was reused for ritual purposes and became temples, often associated with fortified buildings, most likely under the control of an elite who could thus exercise political activity and mobilize labor and its production by acting as priests, sponsors of shrines, or in the name of deities. In this way, the communal character once associated with the great tripartite houses of the Ubaid period is preserved, but used to legitimize the new, closely linked secular and sacred "officials". The tripartite houses that housed the gatherings so dear to Ubaid communities have been replaced by growing, irregular clusters of buildings whose dimensions seem to reflect individual preferences rather than community rules and conventions.[142]
In the 4th millennium BC, the northern zone of Tell Brak was also a major example of urban expansion: the entire 45-hectare of Tell was occupied. During this phase, an accumulation of dwellings formed a "ring" around the site, probably indicating a 100-hectare urban agglomeration. On the other hand, the smallest sites in the vicinity, measuring 5 hectares, seem to indicate a system in which Tell Brak exercised political and economic domination over the surrounding communities.[142][151] There is also evidence of monumental architecture associated with institutional religion: notably the early phases of the "Temple with eyes" dating from the beginning of the 4th millennium BC, with its hundreds of small "idols with eyes". This temple was contemporary with the equally impressive building at Hammam et-Turkman, on the Belikh River in northern Syria, with 2-metre-thick walls and a buttressed façade.[152]
In the south, the existence of temples has been confirmed, especially at Eridu, where the oldest of them, built around 5000 BC during the Ubaid period, was discovered under the Ziggurat.[157] Eridu, Tell Dlehim and Tell al-Hayyad cover between 40 and 50 hectares, while Uruk covers more than 70 hectares.[158] Around 3400 BC, Uruk saw the appearance of a major architectural monument in the E-anna district. These were undoubtedly a number of formal public buildings, whose specific economic or religious purpose remains a matter of conjecture since, unfortunately for historians, they were emptied of their contents when the upper level was built. The surrounding enclosure covers an area of 8 to 9 hectares, and some of the buildings are decorated with cone mosaics.[143]
Writing was the most significant advance of the 4th millennium BC. Initially highly symbolic, it is highly probable that this was an extension of the use of stone tokens or clay bowls on which a symbol was carved or printed. However, it has not yet been formally established that tokens preceded tablets, and they could just as well have been used to print clay tablets, in order to standardize the use of symbols.[159][160] Alongside writing, a numerical system was developed, most probably to count livestock, cereals and dairy products, or to establish a calendar. Counting was based on a sexagesimal system, and its use probably began with the use of geometric tokens that were later printed on clay tablets. Although there is as yet no proof, the tokens found at Tell Abada could be the precursors of this counting system.[160] Traditionally, it is the appearance of writing that marks the end of prehistory, although according to the current state of research it appears more as a symbolic marker or culmination of the process of the emergence of the state and cities, which characterizes the transition from prehistory to history in the truest sense of the word.[161][162]

The Evolution of Human Languages (EHL) project is a historical-comparative linguistics research project hosted by the Santa Fe Institute.[1][2] It aims to provide a detailed genealogical classification of the world's languages.[3]
The project was founded in 2001 by Nobel laureate Murray Gell-Mann when he partnered with Sergei Starostin and Merritt Ruhlen to map out the evolutionary tree of human languages. Initial funding was provided by the Santa Fe Institute and the MacArthur Foundation.[4] It is currently led by Russian linguist Georgiy Starostin, the son of Sergei Starostin.[5]
Many of the project's members belong to the Moscow School of Comparative Linguistics, including Georgiy Starostin and Ilia Peiros.[6] Other project members include Vaclav Blazek, John D. Bengtson, Edward Vajda, and other linguists.
The Evolution of Human Languages (EHL) is an international project – of which Georgiy Starostin inherited his father's membership – on "the linguistic prehistory of humanity" coordinated by the Santa Fe Institute. The project distinguishes about 6,000 languages currently spoken around the world, and aims to provide a detailed classification similar to the accepted classification of biological species.
Their idea is that "all representatives of the species Homo sapiens presumably share a common origin, [so] it would be natural to suppose – although this is a goal yet to be achieved – that all human languages also go back to some common source. Most existing classifications, however, do not go beyond some 300-400 language families that are relatively easy to discern. This restriction has natural reasons: languages must have been spoken and constantly evolving for at least 40,000 years (and quite probably more), while any two languages separated from a common source inevitably lose almost all superficially common features after some 6,000-7,000 years".[7]
The Tower of Babel [ru] is an international etymological database project that is part of the Evolution of Human Languages project. It is coordinated by the Center of Comparative Linguistics [ru] of the Russian State University for the Humanities.[8]
In 2011, the Global Lexicostatistical Database [ru] (GLD) was launched as part of the EHL project. The database uses the Unified Transcription System (UTS), designed specifically for the database.[9]
The Global Lexicostatistical Database includes basic word lists of 110 items each for many of the world's languages.[10] The 110-word list is a modified 100-item Swadesh list consisting of the original 100 Swadesh list items, in addition to the following 10 additional words from the Swadesh–Yakhontov list:
The 110-word expanded Swadesh list by Kassian et al. (2010) is as follows.[11]
A 50-word list of "ultra-stable" items for lexicostatiscal use with the database was also proposed in 2010. The 50-word list is an abridged version of the 110-word list.[12]
Videos

Chronological history
The Scandinavian Peninsula became ice-free around the end of the last ice age. The Nordic Stone Age begins at that time, with the Upper Paleolithic Ahrensburg culture, giving way to the Mesolithic hunter-gatherers by the 7th millennium BC (Maglemosian culture c. 7500 – 6000 BC, Kongemose culture c. 6000 – 5200 BC, Ertebølle culture c. 5300 – 3950 BC). The Neolithic stage is marked by the  Funnelbeaker culture (4000–2700 BC), followed by the Pitted Ware culture (3200–2300 BC).
Around 2800 BC, metal was introduced in Scandinavia in the Corded Ware culture. In much of Scandinavia, a Battle Axe culture became prominent, known from some 3,000 graves. The period 2500–500 BC also left many visible remains to modern times, most notably the many thousands rock carvings (petroglyphs) in western Sweden at Tanumshede and in Norway at Alta. A more advanced culture came with the Nordic Bronze Age (c. 2000/1750 – 500 BC). It was followed by the Iron Age in the 4th century BC.
The pre-history of Scandinavia begins at the end of the Pleistocene epoch, following the last glacial period's receding Fenno-Scandian ice sheet.
Parts of Denmark, Scania and the Norwegian coast line were free from ice around 13,000 BC, and around 10,000 BC the rim of ice was around Dalsland, Västergötland and Östergötland. It wasn't until 7000 BC that all of Svealand and the modern coastal regions of northeastern Sweden were free of ice, although the huge weight of the ice sheet had caused isostatic depression of Fennoscandia, placing large parts of eastern Sweden and western Finland underwater.
In Scandinavia, the time following the last ice age period begins at circa 9500 BC and is called at first the Yoldia Stage, after the Yoldia Sea, then the Ancylus Stage, after the Ancylus Lake in turn named after Ancylus fluviatilis, a small fresh-water gastropod from this time. At this time, Denmark and Sweden were joined and the "Baltic Sea" of the age was a fresh water lake called the Ancylus Lake. The Ancylus age is followed by formation of the Littorina Sea and the Litorina Stage (named after the Littorina littorea mollusc) at around 6200 BC.
With the first human colonization of this new land (the territory of modern Sweden was partly under water though, and with radically different coastlines) during the Ancylus and Litorina ages begins the Nordic Stone Age. In recent years there have been archaeological finds in caves which strongly suggest human inhabitation of Scandinavia before the Weichsel glaciation, at least 50,000 years ago, presumably by Neanderthals.
As the ice receded reindeer grazed on the plains of Denmark and southernmost Sweden. This was the land of the Ahrensburg culture, whose members hunted over territories 100,000 km2 vast and lived in teepees on the tundra. On this land there was little forest but arctic white birch and rowan, but the taiga slowly appeared.
The Scandinavian peninsula was the last part of Europe to be colonized after the Last Glacial Maximum. The migration routes, cultural networks, and the genetic makeup of the first Scandinavians remain elusive and several hypotheses exist based on archaeology, climate modeling, and genetics. Analysis of genomes of early Scandinavian Hunter-Gatherers (SHGs) from the cave Stora Förvar on Stora Karlsö, Stora Bjers on Gotland, Hummervikholmen in Norway showed that migrations followed two routes: one from the south and another from the northeast along the ice-free Norwegian Atlantic coast. These groups met and mixed in Scandinavia, creating a population more diverse than contemporaneous central and western European hunter-gatherers.[1]
In the 7th millennium BC, when the reindeer and their hunters had moved for northern Scandinavia, forests had been established in the land. A culture called the Maglemosian culture lived in Denmark and southern Sweden, and north of them, in Norway and most of southern Sweden, the Fosna-Hensbacka culture, who lived mostly along the shores of the thriving forests. Utilizing fire, boats and stone tools enabled these Stone Age inhabitants to survive life in northern Europe. The northern hunter/gatherers followed the herds and the salmon runs, moving south during the winters, and moving north again during the summers. These early peoples followed cultural traditions similar to those practiced throughout other regions in the far-north areas, including modern Finland, Russia, and across the Bering Strait into the northernmost strip of North America (containing portions of today's Alaska and Canada)
The Maglemosian people lived in forest and wetland environments using fishing and hunting tools made from wood, bone and flint microliths. A characteristic of the culture are the sharply edged microliths of flintstone which were used for spear heads and arrowheads. Microliths finds are more sparse from c. 6000 BC and the period is said to transit into the Kongemose culture (c. 6000–5200 BC). The finds from this period are characterised by long flintstone flakes which were used for making the characteristic rhombic arrowheads, scrapers, drills, awls and toothed blades.
During the 6th millennium BC, southern Scandinavia was clad in lush forests of temperate broadleaf and mixed forests. In these forests roamed animals such as aurochs, wisent, moose and red deer. Now, the Kongemose culture lived off these animals. Like their predecessors, they also hunted seals and fished in the rich waters. North of the Kongemose people, lived other hunter-gatherers in most of southern Norway and Sweden, called the Nøstvet and Lihult cultures, descendants of the Fosna and Hensbacka cultures. These cultures still hunted, in the end of the 6th millennium BC when the Kongemose culture was replaced by the Ertebølle culture in the south.
During the 5th millennium BC, the Ertebølle culture took up point-based pottery, from human groups in the eastern Baltic areas (Narva).[2] About 4000 BC south Scandinavia up to River Dalälven in Sweden became part of the Funnelbeaker culture (4000–2700 BC), a culture that originated in southern parts of Europe and slowly advanced up through today's Uppland, Sweden. In southern Scandinavia it replaced the Ertebølle culture, which had maintained a Mesolithic lifestyle for about 1500 years after farming arrived in Central Europe.[3] Tribes along the coasts of Svealand, Götaland, Åland, northeastern Denmark and southern Norway learnt new technologies that became the Pitted Ware culture (3200–2300 BC). The Pitted Ware culture then developed along Sweden's east coast as a return to a hunting economy in the mid-4th millennium BC (see the Alvastra pile-dwelling).
Genetically, the Funnelbeaker culture population was of Neolithic Anatolian origin with a proportion of Hunter-gatherer ancestries.[4]  This archaeological culture is well-known for its intensive building of enclsoures and megalithic tombs, which are very similar to those found in many regions of western Europe. Before medieval and modern church building requeired stones and before modern land use started, the number of megaliths in northern Germany and Southern Scandinavia was much higher than today. In Denmark, 2,800 monuments are registered and about 7,300 additional examples existed. In northern Germany, Johannes Müller reports 11,658 known monuments. He expects about 75.000 megaliths to have originally been constructed.[5] Additionally, in the distribution area of this culture, thousands of deposition of (partly extremely large 40–50 cm or more) flint axes appear and finely made battle axes (often double axes) of hard stone.
Towards the end of the 3rd millennium BC, they were overrun by new groups who many scholars think spoke Proto-Indo-European, the Battle-Axe culture. This new people with Steppe-derived ancestry advanced up to Uppland and the Oslofjord, and they probably provided the language that was the ancestor of the modern Scandinavian languages. This new culture had the battle axe as a status symbol, and were cattle herders.[citation needed]
During the Nordic Bronze Age from c. 1700–500 BC, an advanced civilization appears in Denmark, parts of Sweden and parts of Norway. They manufactured bronze tools and weapons as well as jewellery and artifacts of bronze and gold. All the bronze and gold was imported and it has been assumed that the civilization was founded in amber trade, through contacts with Central European and Mediterranean cultures.
The period between 2300 and 500 BC was the most intensive petroglyph-carving period in Scandinavia, with carvings depicting agricultural activities, animals, nature, hunts, ships, ceremonies, warfare, etc.. Petroglyphs with themes of a sexual nature have also been found in Bohuslän, dating from 800 to 500 BC.
Tacitus (about 98 AD) described a nation called "Suiones" living on an island in the Ocean. These Suiones had ships that were peculiar because they had a prow in both ends (the shape we recognise as Viking ships). This word Suiones is the same name as in Anglo-Saxon Sweon whose country in Angle-Saxon was called Sweoland (Svealand). In Beowulf, this tribe is also called Sweoðeod, from which the name Sweden is derived, and the country has the name Sweorice, which is an old form, in Old English (Anglo-Saxon), of the present Swedish name for Sweden, Sverige.
In the 6th century the Ostrogoth Jordanes mentioned a tribe named Suehans which is the same name as Tacitus' Suiones. He also unwittingly described the same tribe by a different name, the Suetidi which is the same as an old name for Sweden, Svíþjóð and the English Sweoðeod.
Several sources, such as Beowulf, Ynglingatal, Ynglinga saga, Saxo Grammaticus and Historia Norwegiae, mention a number of Swedish kings who lived in the 6th century, such as Eadgils, Ohthere and Onela, as well as a number of Geatish kings. Some of these kings were in all likelihood historic kings, although the sources sometimes give contradictory information, such as the death of Ottar. See Mythological kings of Sweden and Semi-legendary kings of Sweden.
In those days the kings were warlords rather than kings as we understand that title today, and what was to become Sweden, Norway and Denmark in a modern sense, were a number of petty kingdoms whose borders changed constantly as the kings killed each other, and had the local assemblies accept them as kings. The politics of these early kingdoms are retold in Beowulf (see e.g. the semi-legendary Swedish-Geatish wars) and the Norse sagas.
One of the most powerful kings was the Swedish king who according to early sources only ruled what is today eastern Svealand. It is unknown when it happened and it probably happened several times, but when sources become more reliable the territories of the Swedish kings include Västergötland and other parts of Götaland. This stage is by some considered to be the beginning of Sweden, as we know it today.

Old Tagalog (Tagalog: Lumang Tagalog; Baybayin: pre-virama: ᜎᜓᜋ ᜆᜄᜎᜓ, post-virama [krus kudlit]: ᜎᜓᜋᜅ᜔ ᜆᜄᜎᜓᜄ᜔; post-virama [pamudpod]: ᜎᜓᜋᜅ᜕ ᜆᜄᜎᜓᜄ᜕), also known as Old Filipino, is the earliest form of the Tagalog language during the Classical period. It is the primary language of pre-colonial Tondo, Namayan and Maynila. The language originated from the Proto-Philippine language and evolved to Classical Tagalog, which was the basis for Modern Tagalog. Old Tagalog uses the Tagalog script or Baybayin, one of the scripts indigenous to the Philippines.
The word Tagalog is derived from the endonym ᜆᜄ ᜁᜎᜓᜄ᜔ or ᜆᜄ ᜁᜎᜓ (taga-ilog, "river dweller"), composed of ᜆᜄ (tagá-, "native of" or "from") and ᜁᜎᜓᜄ᜔ or ᜁᜎᜓ (ilog, "river"). Very little is known about the ancient history of the language; linguists such as David Zorc and Robert Blust speculate that the Tagalogs and other Central Philippine ethno-linguistic groups had originated in Northeastern Mindanao or the Eastern Visayas.[2][3]
Old Tagalog is one of the Central Philippine languages, which evolved from the Proto-Philippine language, which comes from the Austronesian peoples who settled in the Philippines around 2200 BC.[4]
The early history of the Tagalog language remains relatively obscure, and a number of theories exist as to the exact origins of the Tagalog peoples and their language. Scholars such as Robert Blust suggest that the Tagalogs originated in northeastern Mindanao or the eastern Visayas.[5] Possible words of Old Tagalog origin are attested in the Laguna Copperplate Inscription from the 10th century, which is largely written in Old Malay.[6] The first known complete book to be written in Tagalog is the Doctrina Christiana (Christian Doctrine), printed in 1593. The book also used Baybayin script.[7]
The question has been raised about the origin of some words in the various languages of the Philippines and their possible connection to ancient Buddhist and Hindu culture in the region, as the language is influenced by Sanskrit, Malay, Tamil and Chinese.[8][9]
Old Tagalog was written in Baybayin, a writing system formerly used in the Philippines which belongs to the Brahmic family of scripts.

The prehistory of Anatolia stretches from the Paleolithic era[1] through to the appearance of classical civilization in the middle of the 1st millennium BC. It is generally regarded as being divided into three ages reflecting the dominant materials used for the making of domestic implements and weapons: Stone Age, Bronze Age and Iron Age. The term Copper Age (Chalcolithic) is used to denote the period straddling the stone and Bronze Ages.
Anatolia (Turkish: Anadolu), also known by the Latin name of Asia Minor, is considered to be the westernmost extent of Western Asia. Geographically it encompasses the central uplands of modern Turkey, from the coastal plain of the Aegean Sea east to the western edge of the Armenian Highlands and from the narrow coast of the Black Sea south to the Taurus mountains and Mediterranean Sea coast.
The earliest representations of culture in Anatolia can be found in several archaeological sites located in the central and eastern part of the region. Stone Age artifacts such as animal bones and food fossils were found at Burdur (north of Antalya). Although the origins of some of the earliest peoples are shrouded in mystery, the remnants of Bronze Age civilizations, such as Troy, the Hattians, the Akkadian Empire, Assyria, and the Hittites, provide us with many examples of the daily lives of its citizens and their trade. After the fall of the Hittites, the new states of Phrygia and Lydia stood strong on the western coast together with Lykia and Caria. Only the threat from a distant Persian kingdom prevented them from advancing past their peak of success.
The Stone Age is a prehistoric period in which stone was widely used in the manufacture of implements. This period occurred after the appearance of the genus Homo about 2.6 million years ago[citation needed] and roughly lasted 2.5 million years to the period between 4500 and 2000 BC with the appearance of metalworking.
In 2014, a stone tool was found in the Gediz River that was securely dated to 1.2 million years ago.[1] Evidence of Paleolithic (prehistory 500,000–10,000 BC) habitation include the Yarimburgaz Cave (Istanbul), Karain Cave (Antalya), and the Okuzini, Beldibi and Belbasi, Kumbucagi and Kadiini caves in adjacent areas. Examples of paleolithic humans can be found in the Museum of Anatolian Civilizations (Ankara), in the Archaeological Museum in Antalya, and in other Turkish institutions.
Evidence of fruit and of animal bones has been found at Yarimburgaz. The caves of the Mediterranean region contain murals.[2] Original claims (1975) of 250,000-year-old, Middle Pleistocene, Homo sapiens footprints at Kula[3] and Karain Caves are now considered erroneous and have been revised to the Late Pleistocene era.[4]
Remains of a Mesolithic culture in Anatolia can be found along the Mediterranean coast and also in Thrace and the western Black Sea area. Mesolithic remains have been located in the same caves as the paleolithic artefacts and drawings. Additional findings come from the Sarklimagara cave in Gaziantep, the Baradiz cave (Burdur), as well as the cemeteries and open air settlements at Sogut Tarlasi, Biris (Bozova) and Urfa.[5]
Because of its strategic location at the intersection of Asia and Europe, Anatolia has been the center of several civilizations since prehistoric times. The Anatolian hypothesis, first developed by British archaeologist Colin Renfrew in 1987, proposes that the dispersal of Proto-Indo-Europeans originated in Neolithic Anatolia. It is the main competitor to the Kurgan hypothesis, or steppe theory, the more favoured view academically. Neolithic settlements include Çatalhöyük, Çayönü, Nevali Cori, Aşıklı Höyük, Boncuklu Höyük, Hacilar, Göbekli Tepe, Karahan Tepe, Norşuntepe, Kosk, and Mersin.
Çatalhöyük (Central Turkey) is considered the most advanced of these, and Çayönü in the east the oldest (c. 7250–6750 BC).[contradictory] We have a good idea of the town layout at Çayönü, based on a central square with buildings constructed of stone and mud. Archeological finds include farming tools that suggest both crops and animal husbandry as well as domestication of the dog. Religion is represented by figurines of Cybele, the mother goddess. Hacilar (Western Turkey) followed Çayönü, and has been dated to 7040 BC.[6]
Straddling the Neolithic and early Bronze Age, the Chalcolithic era (c. 5500–3000 BC) is defined by the first metal implements made with copper. This age is represented in Anatolia by sites at Hacilar, Beycesultan, Canhasan, Mersin Yumuktepe, Elazig Tepecik, Malatya Degirmentepe, Norşuntepe, and Istanbul Fikirtepe.[7]
The Bronze Age (c. 3300–1200 BC) is characterised by the use of copper and its tin alloy, bronze, for manufacturing implements. Asia Minor was one of the first areas to develop bronze making.
Although the first habitation appears to have occurred as early as the 6th millennium BC during the Chalcolithic period, functioning settlements trading with each other occurred during the 3rd millennium BC. A settlement on a high ridge would become known as Büyükkaya, and later as the city of Hattush, the center of this civilization. Later still it would become the Hittite stronghold of Hattusha and is now Boğazköy. Remnants of the Hattian civilization have been found both under the lower city of Hattusha and in the higher areas of Büyükkaya and Büyükkale,[8]
Another settlement was established at Yarikkaya, about 2 km to the northeast.
The discovery of mineral deposits in this part of Anatolia allowed Anatolians to develop metallurgy, producing items such as the implements found in the royal graves at Alaca Höyük, about 25 km from Boğazköy, which it preceded, dating from 2400 to 2200 BC. Other Hattian centers include Hassum, Kanesh, Purushanda, and Zalwar.[9][10][11][12][13]
During this time the Hattians engaged in trade with city-states such as those of Sumer, which needed timber products from the Amanus mountains.
Anatolia had remained in the prehistoric period until it entered the sphere of influence of the Akkadian Empire in the 24th century BC under Sargon of Akkad, particularly in eastern Anatolia. However, the Akkadian Empire suffered problematic climate changes in Mesopotamia, as well as a reduction in available manpower that affected trade. This led to its fall around 2150 BC at the hands of the Gutians.[14]
The interest of the Akkadians in the region as far as it is known was for exporting various materials for manufacturing. Bronze metallurgy had spread to Anatolia from the Transcaucasian Kura-Araxes culture in the late 4th millennium BC.[15]
While Anatolia was well endowed with copper ores, there was no evidence of substantial workings of the tin required to make bronze in Bronze-Age Anatolia.[16]
At the origins of written history, the Anatolian plains inside the area ringed by the Kızılırmak River were occupied by the first defined civilization in Anatolia, a non-Indo-European indigenous people named the Hattians (c. 2500 BC – c. 2000 BC). During the middle Bronze Age, the Hattian civilization, including its capital of Hattush, continued to expand.[10]
The Anatolian middle Bronze Age influenced the early Minoan culture of Crete (3400 to 2200 BC)
as evidenced by archaeological findings at Knossos.[17]
The Hattians came into contact with Assyrians traders from Assur in Mesopotamia such as at Kanesh (Nesha) near modern Kültepe who provided them with the tin needed to make bronze. These trading posts or Karums (Akkadian for Port), have lent their name to a period, the Karum Period. The Karums, or Assyrian trading colonies, persisted in Anatolia until Hammurabi conquered Assyria and it fell under Babylonian domination in 1756 BC. These Karums represented separate residential areas where the traders lived, protected by the Hattites, and paying taxes in return. Meanwhile, the fortifications of Hattush were strengthened with construction of royal residences on Büyükkale.
After the Assyrians overthrew their Gutian neighbours (c. 2050 BC) they claimed the local resources, notably silver, for themselves. However the Assyrians brought writing to Anatolia, a necessary tool for trading and business. These transactions were recorded in Akkadian cuneiform on clay tablets. Records found at Kanesh use an advanced system of trading computations and credit lines. The records also indicate the names of the cities where the transaction occurred.[15]
The history of the Hittite civilization is known mostly from cuneiform texts found in the area of their empire, and from diplomatic and commercial correspondence found in various archives in Egypt and the Middle East.
Hattian civilization was also impacted by an invading Indo-European people, the Hittites, in the early 18th century BC, Hattush being burned to the ground in 1700 BC by King Anitta of Kussar after overthrowing King Piyushti. He then placed a curse on the site and set up his capital at Kanesh 160 km south east.[10]
The Hittites absorbed the Hattians over the next century, a process that was essentially complete by 1650 BC.
Eventually Hattusha became a Hittite centre by the second half of the 17th century BC, and King Hattusili I (1586–1556 BC) moved his capital back to Hattusha from Neša (Kanesh).
The Old Hittite Empire (17th–15th centuries BC) was at its height in the 16th century BC, encompassing central Anatolia, north-western Syria as far as Ugarit, and upper Mesopotamia. Kizzuwatna in southern Anatolia controlled the region separating the Hittite Empire from Syria, thereby greatly affecting trade routes. The peace was kept in accordance with both empires through treaties that established boundaries of control.
Following the reign of Telipinu (c. 1460 BC) the Hittite kingdom entered a relatively weak and poorly documented phase, known as the Middle Kingdom, from the reign of Telipinu's son-in-law, Alluwamna (mid-15th century BC) to that of Muwatalli I (c. 1400 BC).
King Tudhaliya I (early 14th century BC) ushered in a new era of Hittite power, often referred to as the Hittite Empire. The Kings took on a divine role in Hittite society and the Hittite peoples, often allied with neighbours such as the Kizzuwatna began to expand again, moving into Western Anatolia, absorbing the Luwian state of Arzawa and the Assuwa League.
It was not until the reign of King Suppiluliumas (c. 1344–1322 BC) that Kizzuwatna was taken over fully, although the Hittites still preserved their cultural accomplishments in Kummanni (now Şar, Turkey) and Lazawantiya, north of Cilicia.[18]
In the 13th century, after the reign of Hattusili III (c. 1267–1237 BC), Hittite power began to wane, threatened by Egypt to the South and Assyria to the East, effectively ending with Suppiluliuma II (c. 1207–1178 BC).
After 1180s BC, amid general turmoil in the Levant associated with the sudden arrival of the Sea Peoples, and the collapse of the Bronze Age the empire disintegrated into several independent Syro-Hittite (Neo-Hittite) city-states, some of which survived until as late as the 8th century BC. In the West, Greeks were arriving on the Anatolian coast, and the Kaskas along the northern Black Sea coast. Eventually Hattusha itself was destroyed around 1200 BC and the age of Empires shifted to that of regional states as the Bronze Age shifted into the Iron Age.
There is very little information about early Mycenaean presence in Anatolia. Miletus was clearly a center of Mycenaean presence in Asia Minor in the period c. 1450–1100 BC. The zone of intense Mycenaean settlement extends as far as Bodrum/Halicarnassus.[19]
The Mycenaean sphere of influence in Asia Minor is also relatively restricted geographically: Intense Mycenaean settlement is to be found in the archaeological records only for the region between the Peninsula of Halicarnassus in the south and Milet [Miletus] in the north (and in the islands off this coastline, between Rhodes in the south and Kos – possibly also Samos – in the north).[19]
Attarsiya was a 15th–14th century BC military leader who was probably Greek. He conducted the first recorded Mycenaean military activity on the Anatolian mainland. His activities are recorded in the Hittite archives of c. 1400 BC.[20]
British archaeologist J.M. Cook studied the Greek historical tradition about the Carians, and drew attention to the many similarities between the Carians and the Mycenaeans.[21]
The Iron Age (c. 1300–600 BC) was characterised by the widespread use of iron and steel. It is also an age known for the development of various alphabets and early literature. It formed the last phase of Pre-history, spanning the period between the collapse of the Bronze Age and the rise of classical civilisation. In Anatolia, the dissolution of the Hittite Empire was replaced by regional Neo-Hittite powers including Troad, Ionia, Lydia, Caria and Lycia in the west; Phrygia, centrally and Cimmeria and Urartu in the north east, while the Assyrians occupied much of the south east.
The Troad, on the Biga peninsula, was settled before 3000 BC. The site of Troy was occupied for more than three millennia, its archaeological layers numbered I-IX. Legends of the Trojan War may have a basis in historical events concerning Late Bronze Age Troy.[22][23][24]
Aeolis was an area of the north western Aegean coast, between Troad and Ionia, from the Hellespont to the Hermus River (Gediz), west of Mysia and Lydia. By the 8th century BC the twelve most important cities formed a league. In the 6th century the cities were progressively conquered by Lydia, and then Persia.
Ionia was part of a group of settlements on the central Aegean coast bounded by Lydia to the east, and Caria to the south, known as the Ionian league. Ionians had been expelled from the Peloponnesus by the Dorians, and were resettled on the Aegean coastline of Anatolia by the Athenians to whose land they had fled. By the time of the last Lydian king, Croesus (560–545 BC) Ionia fell under Lydian, and then Persian rule. With the defeat of Persia by the Greeks, Ionia again became independent until absorbed into the Roman province of Asia.
Lydia, or Maeonia as it was called before 687 BC, was a major part of the history of western Anatolia, beginning with the Atyad dynasty, who first appeared around 1300 BC. Lydia was situated to the west of Phrygia and east of the Aegean settlement of Ionia. The Lydians were Indo-European, speaking an Anatolian language related to Luwian and Hittite.
The Heraclids, managed to rule successively from 1185 to 687 BC despite a growing presence of Greek influences along the Mediterranean coast. As Greek cities such as Smyrna, Colophon, and Ephesus rose, the Heraclids became weaker and weaker. The last king, Candaules, was murdered by his friend and lance-bearer named Gyges, and he took over as ruler. Gyges waged war against the intruding Greeks, and soon faced by a grave problem as the Cimmerians began to pillage outlying cities within the kingdom. It was this wave of attacks that led to the incorporation of the formerly independent Phrygia and its capital Gordium into the Lydian domain. It was not until the successive rules of Sadyattes and Alyattes, ending in 560 BC, that the attacks of the Cimmerians ended for good.
Under the reign of the last Lydian king Croesus, Lydia reached its greatest expansion. Persia was invaded first at the Battle of Pteria ending without a victor. Progressing deeper into Persia, Croesus was thoroughly defeated in the Battle of Thymbra at the hands of the Persian Cyrus II in 546 BC.[25]
Following Croesus' defeat, Lydia fell under the hegemony of Persia, Greece, Rome and Byzantium until finally being absorbed into the Turkish lands.
Caria forms a region in Western Anatolia, south of Lydia, east of Ionia and north of Lycia. Partially Greek (Ionian and Dorian), and possibly partially Minoan. Caria became subject to Persia, Greece and Rome before being absorbed into Byzantium. Remnants of the Carian civilisation form a rich legacy in the south western Aegean. Caria managed to maintain a relative degree of independence during successive occupation, and its symbol, the double headed axe is seen as a mark of defiance and can be seen inscribed on many buildings. The mausoleum at Halicarnassus (modern Bodrum), the tomb of the Persian Satrap Mausolus, was considered one of the Seven Wonders of the Ancient World. Other important relics include that of Mylasa (Milas) at one time capital of Caria and administrative seat of Mausolus, Labranda in the mountains high above Mylasa and Euromos (Herakleia) near Lake Bafa.
Lycia formed the southernmost settlement in Western Anatolia on what is now the Teke peninsula on the western Mediterranean coast. There many historic Lycian sites include Xanthos, Patara, Myra, Pinara, Tlos, Olympos and Phaselis. Emerging at the end of the Bronze Age as a Neo-Hittite league of city states whose governance model still influences political systems today. Alternating between Persian and Greek rule it eventually was incorporated into Rome, Byzantium and the Turkish lands.
The west-central area of Anatolia became the domain of the Phrygian Kingdom following the fragmentation of the Hittite Empire during the 12th century BC, existing independently until the 7th century BC, and strongly featured in Greek mythology. Although their origin is disputed, their language more resembled Greek (Dorian) than the Hittites whom they succeeded. Possibly from the region of Thrace, the Phrygians eventually established their capital at Gordium (now Yassıhüyük) and an important religious center at Yazılıkaya. Known as the Mushki to the Assyrians, the Phrygian people lacked central control in their style of government, and yet established an extensive network of roads. They also held tightly onto a lot of the Hittite facets of culture and adapted them over time.[26]
Well known from ancient Greek and Roman writers is King Midas, the last king of the Phrygian Kingdom. The mythology of Midas revolves around his ability to turn objects to gold by mere touch, as granted by Dionysos, and his unfortunate encounter with Apollo from which his ears are turned into the ears of a donkey. The historical record of Midas shows that he lived approximately between 740 and 696 BC, and represented Phrygia as a great king. Most historians now consider him to be King Mita of the Mushki as noted in Assyrian accounts. The Assyrians thought of Mita as a dangerous foe, for Sargon II, their ruler at the time, was quite happy to negotiate a peace treaty in 709 BC. This treaty had no effect on the advancing Cimmerians in the East, who streamed into Phrygia and led to the downfall and suicide of King Midas in 696 BC.[27]
After Midas's death, Phrygia lost its independence, becoming respectively a vassal state of its western neighbour, Lydia, Persia, Greece, Rome and Byzantium, disappearing in the Turkish era.
Cimmeria was a region of north eastern Anatolia, appearing in the 8th century BC from the north and east, in the face of the eastern Scythian advance. They continued to move west, invading and subjugating Phrygia (696–695 BC), penetrating as far south as Cilicia, and west into Ionia after pillaging Lydia. Lydian campaigns between 637 and 626 BC effectively halted this advance. The Cimmerian influence progressively weakened and the last recorded mention is in 515 BC.
Urartu (Nairi, or the Kingdom of Van) existed in north-east Anatolia, centered around Lake Van (Nairi Sea), to the south of the Cimmerians and North of Assyria. Its prominence ran from its appearance in the 9th century until it was overrun by the Medes in the 6th century.
Urartu is first mentioned as a loose confederation of smaller entities in the Armenian Highlands in the 13th to 11th centuries BC, but was subject to recurrent Assyrian incursions before emerging as a powerful neighbour by the 9th century BC. This was facilitated by Assyria's weak position in the 8th century BC. Urartu continued to resist Assyrian attacks and reached it greatest extent under Argishti I (c. 785–760 BC). At that time it included present day Armenia, southern Georgia reaching almost to the Black Sea, west to the sources of the Euphrates and south to the sources of the Tigris.
Following this Urartu suffered a number of setbacks. King Tiglath Pileser III of Assyria conquered it in 745 BC. By 714 BC it was being ravaged by both Cimmerian and Assyrian raids. After 645 BC Scythian attacks provided further problems for Urartu forcing it to become dependent on Assyria. However Assyria itself fell to a combined attack of Scythians, Medes and Babylonians in 612 BC. While the details of Urartu's demise are debated, it effectively disappeared to be replaced by Armenia. It was a Persian Satrapy for a while from the 6th century BC before becoming an independent Armenia. To this day Urartu forms an important part of Armenian nationalist sentiment.
In the Iron Age Assyria extended to include south eastern Anatolia. Assyria, one of the great powers of the Mesopotamia region, had a long history from the 25th century BC (Bronze Age) until it final collapse in 612 BC at the end of the Iron Age. Assyria's Iron Age corresponds to the Middle Period (resurgence) and the Neo-Assyrian Empire in its last 300 years, and its territory centered on what is modern day Iraq.
Assyria influenced Anatolian politics and culture from when its traders first came into contact with Hattians in the late Bronze Age. By the 13th century BC Assyria was expanding to its north west at the expense of the Hittites, and to the north at the expense of Urartu. Assyrian expansion reached its height under Tukulti-Ninurta I (1244–1208 BC), following which it was weakened by internal dissent. The collapse of the Hittie Empire at the end of the Bronze Age coincided with an era of renewed Assyrian expansion under Ashur-resh-ishi I (1133–1116 BC) and soon Assyria had added the Anatolian lands in what is now Syria to its empire. Tiglath-Pileser I (1115–1077 BC) then commenced incursions against the Neo-Hittite Phrygians, followed by the Luwian kingdoms of Commagene, Cilicia and Cappadocia.
With the death of Tiglath-Pileser I Assyria entered a period of decline during what is referred to as the Ancient Dark Ages (1075–912 BC) in the region that corresponded to the collapse of the Bronze Age. The last 300 years of the Assyrian Empire (Neo-Assyrian Empire) from 911 to 627 BC saw a renewed expansion including attacks on the Neo-Hittite states to its north and west. Ashurnasirpal II (883–859 BC) extracted tribute from Phrygia while his successor Shalmaneser III (858–823 BC) also attacked Urartu forcing his Anatolian neighbours to pay tribute. After his death the land was torn by civil war. Assyrian power continued to wax and wane with periodic incursions into the Anatolian lands. Sennacherib (705–681 BC) encountered and drove back a new force in the region, the Greeks who attempted to settle Cilicia. His successor Esarhaddon (680–669 BC) was responsible for the final destruction of Urartu. Ashurbanipal (669-627 BC) then extended Assyrian influence still further placing Caria, Cilicia, Lydia and Cappadocia into vassalage.
However Assyria found its resources stretched to maintain the integrity of its vast empire and civil war again erupted following the death of Ashurbanipal. Vassal states stopped paying tribute, regaining independence. The weakened Assyrian state was now faced by a new threat, a coalition of Iranian peoples to its east and north, including Medes, Persians, Scythians and the Anatolian Cimmerians, who attacked Assyria in 616 BC. Ninevah, the capital, fell in 612 BC and the Assyrian Empire was finally swept away in 605 BC.
With the collapse of Assyria, ended not only the Iron Age, but also the era referred to as Pre-History, to make way for what has been variously described as Recorded History, or more specifically late Ancient History or Classical Civilisation. However these terms are not precise or universal and overlap.

Robert Bruce Foote (22 September 1834 – 29 December 1912) was a British geologist and archaeologist who conducted geological surveys of prehistoric locations in India for the Geological Survey of India. For his contributions to Indian archaeology, he is called the father of Indian prehistory.[1][2][3][4] He discovered the site of Attirampakkam (then part of the Madras Presidency, near Chennai), a Madrasian culture.[5][6]
Foote joined the Geological Survey of India (GSI) on 29 December 1858 and was posted in the Madras Presidency, Hyderabad State and Bombay Presidency.  An interest in Paleolithic life was inspired by the work of Joseph Prestwich in 1859. In 1863, after his archaeological survey began, he discovered India's first conclusive Paleolithic stone tool (a hand axe). He found the tool in southern India (Pallavaram, near Madras). After the discovery, he and William King went on to discover more such tools and settlements in Southern and Western India. In 1884, he discovered the 3.5 kilometres (2.2 mi) long Belum Caves, the second largest cave in the Indian subcontinent.[7] In 1887, he became a Director of the GSI, and retiring in 1891, he joined the state of Baroda.
As a geologist, one of his significant contributions[citation needed] to Indian geology was the "Geological Features of the South Mahratta Country and Adjacent Districts" (i.e., Border districts of Maharashtra, Karnataka and Andhra Pradesh States in India).
Foote built a valuable collection due to 40 years of geological and pre-historic expeditions in various parts of western and southern India. Foote's collection of antiquities was all sold to the Madras Government Museum in 1906, where it is quoted to be a valuable treasure.[8]
Later, he settled in Yercaud, where his father-in-law Reverend Peter Percival had worked and lived.[9]
He died on 29 December 1912 and was cremated at Calcutta; his ashes were deposited at Holy Trinity Church, Yercaud, Tamil Nadu, India. There is a memorial to him there.[10] Foote was a Fellow of the Geological Society, London, from 1867 and a Fellow of the Royal Anthropological Institute.
His grandson was Major General Henry Bowreman Foote, who was the recipient of the Victoria Cross for his contributions to realm's defence during the Second World War.

The timeline of human evolution outlines the major events in the evolutionary lineage of the modern human species, Homo sapiens, 
throughout the history of life, beginning some 4 billion years ago down to recent evolution within H. sapiens during and since the Last Glacial Period.
It includes brief explanations of the various taxonomic ranks in the human lineage. The timeline reflects the  mainstream views in modern taxonomy, based on the principle of phylogenetic nomenclature;
in cases of open questions with no  clear consensus, the main competing possibilities are briefly outlined.
A tabular overview of the taxonomic ranking   of Homo sapiens (with age estimates for each rank) is shown below.
For another billion years, prokaryotes would continue to diversify undisturbed.
The Holozoa lineage of eukaryotes evolves many features for making cell colonies, and finally leads to the ancestor of animals (metazoans) and choanoflagellates.[5][6]
Proterospongia (members of the Choanoflagellata) are the best living examples of what the ancestor of all animals may have looked like. They live in colonies, and show a primitive level of cellular specialization for different tasks.
Urmetazoan:
The first fossils that might represent animals appear in the 665-million-year-old rocks of the Trezona Formation of South Australia. These fossils are interpreted as being early sponges.[7] Multicellular animals may have existed from 800 Ma. Separation from the Porifera (sponges) lineage.
Eumetazoa/Diploblast: separation from the Ctenophora ("comb jellies") lineage.
Planulozoa/ParaHoxozoa: separation from the Placozoa and Cnidaria  lineages.
All diploblasts possess epithelia, nerves, muscles and connective tissue and mouths, and except for placozoans, have some form of symmetry, with their ancestors probably having radial symmetry like that of cnidarians. Diploblasts separated their early embryonic cells into two germ layers (ecto- and endoderm). Photoreceptive eye-spots evolve.
Urbilaterian:
the last common ancestor of xenacoelomorphs, protostomes (including the arthropod [insect, crustacean, spider], mollusc [squid, snail, clam] and annelid [earthworm] lineages) and the deuterostomes (including the vertebrate [human] lineage) (the last two are more related to each other and called Nephrozoa). Xenacoelomorphs all have a gonopore to expel gametes but nephrozoans merged it with their anus. Earliest development of bilateral symmetry, mesoderm, head (anterior cephalization) and various gut muscles (and thus peristalsis) and, in the Nephrozoa, nephridia (kidney precursors), coelom (or maybe pseudocoelom), distinct mouth and anus (evolution of through-gut), and possibly even nerve cords and blood vessels.[8] Reproductive tissue probably concentrates into a pair of gonads connecting just before the posterior orifice. "Cup-eyes" and balance organs evolve (the function of hearing added later as the more complex inner ear evolves in vertebrates). The nephrozoan through-gut had a wider portion in the front, called the pharynx. The integument or skin consists of an epithelial layer (epidermis) and a connective layer.
Most known animal phyla appeared in the fossil record as marine species during the Ediacaran-Cambrian explosion, probably caused by long scale oxygenation since around 585 Ma (sometimes called the Neoproterozoic Oxygenation Event or NOE) and also an influx of oceanic minerals. Deuterostomes, the last common ancestor of the Chordata [human] lineage, Hemichordata (acorn worms and graptolites) and Echinodermata (starfish, sea urchins, sea cucumbers, etc.), probably had both ventral and dorsal nerve cords like modern acorn worms.
An archaic survivor from this stage is the acorn worm, sporting an  open circulatory system (with less branched blood vessels) with a heart that also functions as a kidney. Acorn worms have a  plexus concentrated into both dorsal and ventral nerve cords. The dorsal cord reaches into the proboscis, and is partially separated from the epidermis in that region. This part of the dorsal nerve cord is often hollow, and may well be homologous with the brain of vertebrates.[9] Deuterostomes also evolved pharyngeal slits, which were probably used for filter feeding like in hemi- and proto-chordates.
The increased amount of oxygen causes many eukaryotes, including most animals, to become obligate aerobes.
The Chordata ancestor gave rise to the lancelets (Amphioxii) and Olfactores. Ancestral chordates evolved a post-anal tail, notochord, and endostyle (precursor of thyroid). The pharyngeal slits (or gills) are now supported by connective tissue and used for filter feeding and possibly breathing. The first of these basal chordates to be discovered by science was Pikaia gracilens.[10] Other, earlier chordate predecessors include Myllokunmingia fengjiaoa,[11] Yunnanozoon lividum,[12] and Haikouichthys ercaicunensis.[13] They probably lost their ventral nerve cord and evolved a special region of the dorsal one, called the brain, with glia becoming permanently associated with neurons. They probably evolved the first blood cells (probably early leukocytes, indicating advanced innate immunity), which they made around the pharynx and gut.[14] All chordates except tunicates sport an intricate, closed circulatory system, with highly branched blood vessels.
Olfactores, last common ancestor of tunicates and vertebrates in which olfaction (smell) evolved. Since lancelets lack a heart, it possibly emerged in this ancestor (previously the blood vessels themselves were contractile) though it could have been lost in lancelets after evolving in early deuterostomes (hemichordates and echinoderms have hearts).
The first vertebrates ("fish") appear: the Agnathans. They were jawless, had seven pairs of pharyngeal arches like their descendants today, and their endoskeletons were cartilaginous (then only consisting of the chondrocranium/braincase and vertebrae). The jawless Cyclostomata diverge at this stage. The connective tissue below the epidermis differentiates into the dermis and hypodermis.[15] They depended on gills for respiration and evolved the unique sense of taste (the remaining sense of the skin now called "touch"), endothelia, camera eyes and inner ears (capable of hearing and balancing; each consists of a lagena, an otolithic organ and two semicircular canals) as well as livers, thyroids, kidneys and two-chambered hearts (one atrium and one ventricle). They had a tail fin but lacked the paired (pectoral and pelvic) fins of more advanced fish. Brain divided into three parts (further division created distinct regions based on function). The pineal gland of the brain penetrates to the level of the skin on the head, making it seem like a third eye. They evolved the first erythrocytes and thrombocytes.[16]
The Placodermi were the first jawed fishes (Gnathostomata); their jaws evolved from the first gill/pharyngeal arch and they largely replaced their endoskeletal cartilage with bone and evolved pectoral and pelvic fins. Bones of the first gill arch became the upper and lower jaw, while those from the second arch became the hyomandibula, ceratohyal and basihyal; this closed two of the seven pairs of gills. The gap between the first and second arches just below the braincase (fused with upper jaw) created a pair of spiracles, which opened in the skin and led to the pharynx (water passed through them and left through gills). 
Placoderms had competition with the previous dominant animals, the cephalopods and sea scorpions, and rose to dominance themselves. A lineage of them probably evolved into the bony and cartilaginous fish, after evolving scales, teeth (which allowed the transition to full carnivory), stomachs, spleens, thymuses, myelin sheaths, hemoglobin and advanced, adaptive immunity (the latter two occurred independently in the lampreys and hagfish). Jawed fish also have a third, lateral semicircular canal and their otoliths are divided between a saccule and utricle.
Some freshwater lobe-finned fish (sarcopterygii) develop limbs and give rise to the Tetrapodomorpha. These fish evolved in shallow and swampy freshwater habitats, where they evolved large eyes and spiracles.
Primitive tetrapods ("fishapods") developed from tetrapodomorphs with a two-lobed brain in a flattened skull, a wide mouth and a medium snout, whose upward-facing eyes show that it was a bottom-dweller, and which had already developed adaptations of fins with fleshy bases and bones. (The "living fossil" coelacanth is a related lobe-finned fish without these shallow-water adaptations.) Tetrapod fishes used their fins as paddles in shallow-water habitats choked with plants and detritus. The universal tetrapod characteristics of front limbs that bend backward at the elbow and hind limbs that bend forward at the knee can plausibly be traced to early tetrapods living in shallow water.[18]
Panderichthys is a 90–130 cm (35–50 in) long fish from the Late Devonian period (380 Mya). It has a large tetrapod-like head. Panderichthys exhibits features transitional between lobe-finned fishes and early tetrapods.
Trackway impressions made by something that resembles Ichthyostega's limbs were formed 390 Ma  in Polish marine tidal sediments.  This suggests tetrapod evolution is older than the dated fossils of Panderichthys through to Ichthyostega.
Tiktaalik is a genus of sarcopterygian (lobe-finned) fishes from the late Devonian with many tetrapod-like features.  It shows a clear link between Panderichthys and Acanthostega.
Acanthostega is an extinct tetrapod, among the first animals to have recognizable limbs. It is a candidate for being one of the first vertebrates to be capable of coming onto land. It lacked wrists, and was generally poorly adapted for life on land. The limbs could not support the animal's weight. Acanthostega had both lungs and gills, also indicating it was a link between lobe-finned fish and terrestrial vertebrates. The dorsal pair of ribs form a rib cage to support the lungs, while the ventral pair disappears.
Ichthyostega is another extinct tetrapod. Being one of the first animals with only two pairs of limbs (also unique since they end in digits and have bones), Ichthyostega is seen as an intermediate between a fish and an amphibian. Ichthyostega had limbs but these probably were not used for walking. They may have spent very brief periods out of water and would have used their limbs to paw their way through the mud.[19] They both had more than five digits (eight or seven) at the end of each of their limbs, and their bodies were scaleless (except their bellies, where they remained as gastralia). Many evolutionary changes occurred at this stage: eyelids and tear glands evolved to keep the eyes wet out of water and the eyes became connected to the pharynx for draining the liquid; the hyomandibula (now called columella) shrank into the spiracle, which now also connected to the inner ear at one side and the pharynx at another, becoming the Eustachian tube (columella assisted in hearing); an early eardrum (a patch of connective tissue) evolved on the end of each tube (called the otic notch); and the ceratohyal and basihyal merged into the hyoid. These "fishapods" had more ossified and stronger bones to support themselves on land (especially skull and limb bones). Jaw bones fuse together while gill and opercular bones disappear.
Pederpes from around 350 Ma indicates that the standard number of 5 digits evolved at the Early Carboniferous, when modern tetrapods (or "amphibians") split in two directions (one leading to the extant amphibians and the other to amniotes). At this stage, our ancestors evolved vomeronasal organs, salivary glands, tongues, parathyroid glands, three-chambered hearts (with two atria and one ventricle) and bladders, and completely removed their gills by adulthood. The glottis evolves to prevent food going into the respiratory tract. Lungs and thin, moist skin allowed them to breathe; water was also needed to give birth to shell-less eggs and for early development. Dorsal, anal and tail fins all disappeared.
Lissamphibia (extant amphibians) retain many features of early amphibians but they have only four digits (caecilians have none).
From amphibians came the first amniotes: Hylonomus, a primitive reptile, is the earliest amniote known. It was 20 cm (8 in) long (including the tail) and probably would have looked rather similar to modern lizards. It had small sharp teeth and probably ate small millipedes and insects. It is a precursor of later amniotes (including both the reptiles and the ancestors of mammals). Alpha keratin first evolves here; it is used in the claws of modern amniotes, and hair in mammals, indicating claws and a different type of scales evolved in amniotes (complete loss of gills as well).[20]
Evolution of the amniotic egg allows the amniotes to reproduce on land and lay shelled eggs on dry land. They did not need to return to water for reproduction nor breathing. This adaptation and the desiccation-resistant scales gave them the capability to inhabit the uplands for the first time, albeit making them drink water through their mouths. At this stage, adrenal tissue may have concentrated into discrete glands.
Amniotes have advanced nervous systems, with twelve pairs of cranial nerves, unlike lower vertebrates. They also evolved true sternums but lost their eardrums and otic notches (hearing only by columella bone conduction).
The earliest synapsids, or "proto-mammals," are the pelycosaurs. The pelycosaurs were the first animals to have temporal fenestrae. Pelycosaurs were not therapsids but their ancestors. The therapsids were, in turn, the ancestors of mammals.
The therapsids had temporal fenestrae larger and more mammal-like than pelycosaurs, their teeth showed more serial differentiation, their gait was semi-erect and later forms had evolved a secondary palate. A secondary palate enables the animal to eat and breathe at the same time and is a sign of a more active, perhaps warm-blooded, way of life.[21] They had lost gastralia and, possibly, scales.
One subgroup of therapsids, the cynodonts, lose pineal eye and lumbar ribs and very likely became warm-blooded. The lower respiratory tract forms intricate branches in the lung parenchyma, ending in highly vascularized alveoli. Erythrocytes and thrombocytes lose their nuclei while lymphatic systems and advanced immunity emerge. They may have also had thicker dermis like mammals today.
The jaws of cynodonts resembled modern mammal jaws; the anterior portion, the dentary, held differentiated teeth. This group of animals likely contains a species which is the ancestor of all modern mammals. Their temporal fenestrae merged with their orbits. Their hindlimbs became erect and their posterior bones of the jaw progressively shrunk to the region of the columella.[22]
From Eucynodontia came the first mammals. Most early mammals were small shrew-like animals that fed on insects and had transitioned to nocturnality to avoid competition with the dominant archosaurs — this led to the loss of the vision of red and ultraviolet light (ancestral tetrachromacy of vertebrates reduced to dichromacy). Although there is no evidence in the fossil record, it is likely that these animals had a constant body temperature, hair and milk glands for their young (the glands stemmed from the milk line). The neocortex (part of the cerebrum) region of the brain evolves in Mammalia, at the reduction of the tectum (non-smell senses which were processed here became integrated into neocortex but smell became primary sense). Origin of the prostate gland and a pair of holes opening to the columella and nearby shrinking jaw bones; new eardrums stand in front of the columella and Eustachian tube. The skin becomes hairy, glandular (glands secreting sebum and sweat) and thermoregulatory. Teeth fully differentiate into incisors, canines, premolars and molars; mammals become diphyodont and possess developed diaphragms and males have internal penises. All mammals have four chambered hearts (with two atria and two ventricles) and lack cervical ribs (now mammals only have thoracic ribs).
Monotremes are an egg-laying group of mammals represented today by the platypus and echidna.  Recent genome sequencing of the platypus indicates that its sex genes are closer to those of birds than to those of the therian (live birthing) mammals. Comparing this to other mammals, it can be inferred that the first mammals to gain sexual differentiation through the existence or lack of SRY gene (found in the y-Chromosome) evolved only in the therians. Early mammals and possibly their eucynodontian ancestors had epipubic bones, which serve to hold the pouch in modern marsupials (in both sexes).
Evolution of live birth (viviparity), with early therians probably having pouches for keeping their undeveloped young like in modern marsupials. Nipples stemmed out of the therian milk lines. The posterior orifice separates into anal and urogenital openings; males possess an external penis.
Monotremes and therians independently detach the malleus and incus from the dentary (lower jaw) and combine them to the shrunken columella (now called stapes) in the tympanic cavity behind the eardrum (which is connected to the malleus and held by another bone detached from the dentary, the tympanic plus ectotympanic), and coil their lagena (cochlea) to advance their hearing, with therians further evolving an external pinna and erect forelimbs. Female placentalian mammals do not have pouches and epipubic bones but instead have a developed placenta which penetrates the uterus walls (unlike marsupials), allowing a longer gestation; they also have separated urinary and genital openings.[23]
A group of small, nocturnal, arboreal, insect-eating mammals called Euarchonta begins a speciation that will lead to the orders of primates, treeshrews and flying lemurs. They reduced the number of mammaries to only two pairs (on the chest). Primatomorpha is a subdivision of Euarchonta including primates and their ancestral stem-primates Plesiadapiformes. An early stem-primate, Plesiadapis, still had claws and eyes on the side of the head, making it faster on the ground than in the trees, but it began to spend long times on lower branches, feeding on fruits and leaves.
The Plesiadapiformes very likely contain the ancestor species of all primates.[24]  They first appeared in the fossil record around 66 million years ago, soon after the Cretaceous–Paleogene extinction event that eliminated about three-quarters of plant and animal species on Earth, including most dinosaurs.[25][26]
One of the last Plesiadapiformes is Carpolestes simpsoni, having grasping digits but not forward-facing eyes.
Simians split into infraorders Platyrrhini and Catarrhini. They fully transitioned to diurnality and lacked any claw and tapetum lucidum (which evolved many times in various vertebrates). They possibly evolved at least some of the paranasal sinuses, and transitioned from estrous cycle to menstrual cycle. The number of mammaries is now reduced to only one thoracic pair. Platyrrhines, New World monkeys, have prehensile tails and males are color blind. The individuals whose descendants would become Platyrrhini are conjectured to have migrated to South America either on a raft of vegetation or via a land bridge (the hypothesis now favored[27]). Catarrhines mostly stayed in Africa as the two continents drifted apart. Possible early ancestors of catarrhines include Aegyptopithecus and Saadanius.
Catarrhini splits into 2 superfamilies, Old World monkeys (Cercopithecoidea) and apes (Hominoidea). Human trichromatic color vision had its genetic origins in this period. Catarrhines lost the vomeronasal organ (or possibly reduced it to vestigial status).
Proconsul was an early genus of catarrhine primates. They had a mixture of Old World monkey and ape characteristics. Proconsul's monkey-like features include thin tooth enamel, a light build with a narrow chest and short forelimbs, and an arboreal quadrupedal lifestyle. Its ape-like features are its lack of a tail, ape-like elbows, and a slightly larger brain relative to body size.
Proconsul africanus is a possible ancestor of both great and lesser apes, including humans.
Pierolapithecus catalaunicus is thought to be a common ancestor of humans and the other great apes, or at least a species that brings us closer to a common ancestor than any previous fossil discovery. It had the special adaptations for tree climbing as do present-day humans and other great apes: a wide, flat rib cage, a stiff lower spine, flexible wrists, and shoulder blades that lie along its back.
Hominini: The latest common ancestor of humans and chimpanzees
is estimated to have lived between roughly 10 to 5 million years ago. Both chimpanzees and humans have a larynx that repositions during the first two years of life to a spot between the pharynx and the lungs, indicating that the common ancestors have this feature, a precondition for vocalized speech in humans.
Speciation may have begun shortly after 10 Ma, but late admixture between the lineages may have taken place until after 5 Ma. Candidates of Hominina or Homininae species which lived in this time period include
Graecopithecus (c. 7 Ma),
Sahelanthropus tchadensis (c. 7 Ma),
Orrorin tugenensis (c. 6 Ma).
Ardipithecus was arboreal, meaning it lived largely in the forest where it competed with other forest animals for food, no doubt including the contemporary ancestor of the chimpanzees. Ardipithecus was probably bipedal as evidenced by its bowl shaped pelvis, the angle of its foramen magnum and its thinner wrist bones, though its feet were still adapted for grasping rather than walking for long distances.
A member of the Australopithecus afarensis left human-like footprints on volcanic ash in Laetoli, northern Tanzania, providing strong evidence of full-time bipedalism. Australopithecus afarensis lived between 3.9 and 2.9 million years ago, and is considered one of the earliest hominins—those species that developed and comprised the lineage of Homo and Homo's closest relatives after the split from the line of the chimpanzees.
It is thought that A. afarensis was ancestral to both the genus Australopithecus and the genus Homo. Compared to the modern and extinct great apes, A. afarensis had reduced canines and molars, although they were still relatively larger than in modern humans. A. afarensis also has a relatively small brain size (380–430 cm3) and a prognathic (anterior-projecting) face.
Australopithecines have been found in savannah environments; they probably developed their diet to include scavenged meat.  Analyses of Australopithecus africanus lower vertebrae suggests that these bones changed in females to support bipedalism even during pregnancy.
Early Homo appears in East Africa,   speciating from australopithecine ancestors.
The Lower Paleolithic is defined by the beginning of use of stone tools.
Australopithecus garhi was using stone tools at about 2.5 Ma.
Homo habilis is the oldest species given the designation Homo, by Leakey et al. in 1964.
H. habilis is  intermediate between Australopithecus afarensis and H. erectus, and there have been suggestions to re-classify it within genus Australopithecus, as Australopithecus habilis.
LD 350-1 is now considered the earliest known specimen of the genus Homo, dating to 2.75–2.8 Ma, found in the Ledi-Geraru site in the Afar Region of Ethiopia. It is currently unassigned to a species, and it is unclear if it represents the ancestor to H. habilis and H. rudolfensis, which are estimated to have evolved around 2.4 Ma.[36]
Stone tools found at the Shangchen site in China and dated to 2.12 million years ago are considered the earliest known evidence of hominins outside Africa, surpassing Dmanisi hominins found in Georgia by 300,000 years, although whether these hominins  were an early species in the genus Homo or another hominin species is unknown.[37]
Homo erectus  derives from early Homo or late Australopithecus.
Homo habilis, although significantly different of anatomy and physiology, is thought to be the ancestor of Homo ergaster, or African Homo erectus; but it is also known to have coexisted with H. erectus for almost half a million years (until about 1.5 Ma).
From its earliest appearance at about 1.9 Ma, H. erectus is distributed in East Africa and Southwest Asia (Homo georgicus). 
H. erectus is  the first known species to develop control of fire, by about 1.5 Ma.
H. erectus later migrates throughout Eurasia, reaching Southeast Asia by 0.7 Ma.
It is described in a number of subspecies.[38] Early humans were social and initially scavenged, before becoming active hunters. The need to communicate and hunt prey efficiently in a new, fluctuating environment (where the locations of resources need to be memorized and told) may have driven the expansion of the brain from 2 to 0.8 Ma.
Evolution of dark skin at about 1.2 Ma.[39]
Homo antecessor may be a common ancestor of Homo sapiens and Neanderthals.[40][41] At present estimate, humans have approximately 20,000–25,000 genes and share 99% of their DNA with the now extinct Neanderthal[42] and 95–99% of their DNA with their closest living evolutionary relative, the chimpanzees.[43][44]  The human variant of the FOXP2 gene (linked to the control of speech) has been found to be identical in Neanderthals.[45]
Divergence of Neanderthal and Denisovan lineages from a common ancestor.[46]
Homo heidelbergensis (in Africa also known as Homo rhodesiensis) had long been thought to be a likely candidate for the last common ancestor of the Neanderthal and modern human lineages.
However, genetic evidence from the Sima de los Huesos fossils published in 2016 seems to suggest that H. heidelbergensis in its entirety should be included in the Neanderthal lineage, as "pre-Neanderthal" or "early Neanderthal", while the divergence time between the Neanderthal and modern lineages has been pushed back to before the emergence of H. heidelbergensis, to about 600,000 to 800,000 years ago, the approximate age of Homo antecessor.[47][48] Brain expansion (enlargement) between 0.8 and 0.2 Ma may have occurred due to the extinction of most African megafauna (which made humans feed from smaller prey and plants, which required greater intelligence due to greater speed of the former and uncertainty about whether the latter were poisonous or not), extreme climate variability after Mid-Pleistocene Transition (which intensified the situation, and resulted in frequent migrations), and in general selection for more social life (and intelligence) for greater chance of survival, reproductivity, and care for mothers. Solidified footprints dated to about 350 ka and associated with H. heidelbergensis were found in  southern Italy in 2003.[49]
H. sapiens lost the brow ridges from their hominid ancestors as well as the snout completely, though their noses evolve to be protruding (possibly from the time of H. erectus). By 200 ka, humans had stopped their brain expansion.
Neanderthals and Denisovans emerge from the northern Homo heidelbergensis lineage around 500-450 ka while sapients emerge  from the southern lineage around 350-300 ka.[50]
Fossils attributed to H. sapiens, along with stone tools, dated to approximately 300,000 years ago, found at Jebel Irhoud, Morocco[51] yield the earliest fossil evidence for anatomically modern Homo sapiens.
Modern human presence in East Africa (Gademotta), at 276 kya.[52] In July 2019, anthropologists reported the discovery of 210,000 year old remains of what may possibly have been a H. sapiens in Apidima Cave, Peloponnese, Greece.[53][54][55]
Patrilineal and matrilineal most recent common ancestors (MRCAs) of living humans roughly between 200 and 100 kya[56][57]
with some estimates on the patrilineal MRCA somewhat higher, ranging up to 250 to 500 kya.[58]
160,000 years ago, Homo sapiens idaltu in the Awash River Valley (near present-day Herto village, Ethiopia) practiced excarnation.[59]
Modern human presence in Southern Africa and West Africa.[60]
Appearance of mitochondrial haplogroup (mt-haplogroup) L2.
Early evidence for behavioral modernity.[61]
Appearance of mt-haplogroups M and N. Southern Dispersal migration out of Africa, Proto-Australoid peopling of Oceania.[62] Archaic admixture from Neanderthals in Eurasia,[63][64] from  Denisovans in Oceania with trace amounts in Eastern Eurasia,[65] and from an unspecified African lineage of archaic humans in Sub-Saharan Africa as well as an interbred species of Neanderthals and Denisovans in Asia and Oceania.[66][67][68][69]
Behavioral modernity develops by this time or earlier, according to the "great leap forward" theory.[70]
Extinction of Homo floresiensis.[71]
M168 mutation (carried by all non-African males).
Appearance of mt-haplogroups U and K.
Peopling of Europe, peopling of the North Asian Mammoth steppe. Paleolithic art.
Extinction of Neanderthals and other archaic human variants  (with possible survival of hybrid populations in Asia and Africa).
Appearance of Y-Haplogroup R2; mt-haplogroups J and X.
Last Glacial Maximum; Epipaleolithic / Mesolithic / Holocene.
Peopling of the Americas.
Appearance of: Y-Haplogroup R1a; mt-haplogroups V and T.
Various recent divergence associated with environmental pressures,
e.g.  light skin in Europeans and East Asians (KITLG, ASIP), after 30 ka;[72]
Inuit adaptation to high-fat diet and cold climate, 20 ka.[73]
Extinction of late surviving archaic humans at the beginning of the Holocene (12 ka).
Accelerated divergence due to selection pressures in populations participating in the Neolithic Revolution after 12 ka, e.g.
East Asian types of ADH1B associated with rice domestication,[74] or  lactase persistence.[75][76] A slight decrease in brain size occurred a few thousand years ago.[citation needed]

Homo sapiens is a distinct species of the hominid family of primates, which also includes all the great apes.[1] Over their evolutionary history, humans gradually developed traits such as bipedalism, dexterity, and complex language,[2] as well as interbreeding with other hominins (a tribe of the African hominid subfamily),[3] indicating that human evolution was not linear but weblike.[4][5][6][7] The study of the origins of humans involves several scientific disciplines, including physical and evolutionary anthropology, paleontology, and genetics; the field is also known by the terms anthropogeny, anthropogenesis, and anthropogony[8][9]—with the latter two sometimes used to refer to the related subject of hominization.
Primates diverged from other mammals about 85 million years ago (mya), in the Late Cretaceous period, with their earliest fossils appearing over 55 mya, during the Paleocene.[10] Primates produced successive clades leading to the ape superfamily, which gave rise to the hominid and the gibbon families; these diverged some 15–20 mya. African and Asian hominids (including orangutans) diverged about 14 mya. Hominins (including the Australopithecine and Panina subtribes) parted from the Gorillini tribe between 8 and 9 mya; Australopithecine (including the extinct biped ancestors of humans) separated from the Pan genus (containing chimpanzees and bonobos) 4–7 mya.[11] The Homo genus is evidenced by the appearance of H. habilis over 2 mya,[a] while anatomically modern humans emerged in Africa approximately 300,000 years ago.
The evolutionary history of primates can be traced back 65 million years.[12][13][14][15][16] One of the oldest known primate-like mammal species, the Plesiadapis, came from North America;[17][18][19][20][21][22] another, Archicebus, came from China.[23] Other similar basal primates were widespread in Eurasia and Africa during the tropical conditions of the Paleocene and Eocene.
David R. Begun[24] concluded that early primates flourished in Eurasia and that a lineage leading to the African apes and humans, including to Dryopithecus, migrated south from Europe or Western Asia into Africa. The surviving tropical population of primates—which is seen most completely in the Upper Eocene and lowermost Oligocene fossil beds of the Faiyum depression southwest of Cairo—gave rise to all extant primate species, including the lemurs of Madagascar, lorises of Southeast Asia, galagos or "bush babies" of Africa, and to the anthropoids, which are the Platyrrhines or New World monkeys, the Catarrhines or Old World monkeys, and the great apes, including humans and other hominids.
The earliest known catarrhine is Kamoyapithecus from the uppermost Oligocene at Eragaleit in the northern Great Rift Valley in Kenya, dated to 24 million years ago.[25] Its ancestry is thought to be species related to Aegyptopithecus, Propliopithecus, and Parapithecus from the Faiyum, at around 35 mya.[26] In 2010, Saadanius was described as a close relative of the last common ancestor of the crown catarrhines, and tentatively dated to 29–28 mya, helping to fill an 11-million-year gap in the fossil record.[27]
In the Early Miocene, about 22 million years ago, the many kinds of arboreally-adapted (tree-dwelling) primitive catarrhines from East Africa suggest a long history of prior diversification. Fossils at 20 million years ago include fragments attributed to Victoriapithecus, the earliest Old World monkey. Among the genera thought to be in the ape lineage leading up to 13 million years ago are Proconsul, Rangwapithecus, Dendropithecus, Limnopithecus, Nacholapithecus, Equatorius, Nyanzapithecus, Afropithecus, Heliopithecus, and Kenyapithecus, all from East Africa.
The presence of other generalized non-cercopithecids of Middle Miocene from sites far distant, such as Otavipithecus from cave deposits in Namibia, and Pierolapithecus and Dryopithecus from France, Spain and Austria, is evidence of a wide diversity of forms across Africa and the Mediterranean basin during the relatively warm and equable climatic regimes of the Early and Middle Miocene. The youngest of the Miocene hominoids, Oreopithecus, is from coal beds in Italy that have been dated to 9 million years ago.
Molecular evidence indicates that the lineage of gibbons diverged from the line of great apes some 18–12 mya, and that of orangutans (subfamily Ponginae)[b] diverged from the other great apes at about 12 million years; there are no fossils that clearly document the ancestry of gibbons, which may have originated in a so-far-unknown Southeast Asian hominoid population, but fossil proto-orangutans may be represented by Sivapithecus from India and Griphopithecus from Turkey, dated to around 10 mya.[28]
Hominidae subfamily Homininae (African hominids) diverged from Ponginae (orangutans) about 14 mya. Hominins (including humans and the Australopithecine and Panina subtribes) parted from the Gorillini tribe (gorillas) between 8 and 9 mya; Australopithecine (including the extinct biped ancestors of humans) separated from the Pan genus (containing chimpanzees and bonobos) 4–7 mya.[11] The Homo genus is evidenced by the appearance of H. habilis over 2 mya,[a] while anatomically modern humans emerged in Africa approximately 300,000 years ago.
Species close to the last common ancestor of gorillas, chimpanzees and humans may be represented by Nakalipithecus fossils found in Kenya and Ouranopithecus found in Greece. Molecular evidence suggests that between 8 and 4 million years ago, first the gorillas, and then the chimpanzees (genus Pan) split off from the line leading to the humans. Human DNA is approximately 98.4% identical to that of chimpanzees when comparing single nucleotide polymorphisms (see human evolutionary genetics). The fossil record, however, of gorillas and chimpanzees is limited; both poor preservation – rain forest soils tend to be acidic and dissolve bone – and sampling bias probably contribute to this problem.
Other hominins probably adapted to the drier environments outside the equatorial belt; and there they encountered antelope, hyenas, dogs, pigs, elephants, horses, and others. The equatorial belt contracted after about 8 million years ago, and there is very little fossil evidence for the split—thought to have occurred around that time—of the hominin lineage from the lineages of gorillas and chimpanzees. The earliest fossils argued by some to belong to the human lineage are Sahelanthropus tchadensis (7 Ma) and Orrorin tugenensis (6 Ma), followed by Ardipithecus (5.5–4.4 Ma), with species Ar. kadabba and Ar. ramidus.
It has been argued in a study of the life history of Ar. ramidus that the species provides evidence for a suite of anatomical and behavioral adaptations in very early hominins unlike any species of extant great ape.[30] This study demonstrated affinities between the skull morphology of Ar. ramidus and that of infant and juvenile chimpanzees, suggesting the species evolved a juvenalised or paedomorphic craniofacial morphology via heterochronic dissociation of growth trajectories. It was also argued that the species provides support for the notion that very early hominins, akin to bonobos (Pan paniscus) the less aggressive species of the genus Pan, may have evolved via the process of self-domestication. Consequently, arguing against the so-called "chimpanzee referential model"[31] the authors suggest it is no longer tenable to use chimpanzee (Pan troglodytes) social and mating behaviors in models of early hominin social evolution. When commenting on the absence of aggressive canine morphology in Ar. ramidus and the implications this has for the evolution of hominin social psychology, they wrote:
Of course Ar. ramidus differs significantly from bonobos, bonobos having retained a functional canine honing complex. However, the fact that Ar. ramidus shares with bonobos reduced sexual dimorphism, and a more paedomorphic form relative to chimpanzees, suggests that the developmental and social adaptations evident in bonobos may be of assistance in future reconstructions of early hominin social and sexual psychology. In fact the trend towards increased maternal care, female mate selection and self-domestication may have been stronger and more refined in Ar. ramidus than what we see in bonobos.[30]: 128
The authors argue that many of the basic human adaptations evolved in the ancient forest and woodland ecosystems of late Miocene and early Pliocene Africa. Consequently, they argue that humans may not represent evolution from a chimpanzee-like ancestor as has traditionally been supposed. This suggests many modern human adaptations represent phylogenetically deep traits and that the behavior and morphology of chimpanzees may have evolved subsequent to the split with the common ancestor they share with humans.
The genus Australopithecus evolved in eastern Africa around 4 million years ago before spreading throughout the continent and eventually becoming extinct 2 million years ago. During this time period various forms of australopiths existed, including Australopithecus anamensis, A. afarensis, A. sediba, and A. africanus. There is still some debate among academics whether certain African hominid species of this time, such as A. robustus and A. boisei, constitute members of the same genus; if so, they would be considered to be "robust australopiths" while the others would be considered "gracile australopiths". However, if these species do indeed constitute their own genus, then they may be given their own name, Paranthropus.
A new proposed species Australopithecus deyiremeda is claimed to have been discovered living at the same time period of A. afarensis. There is debate whether A. deyiremeda is a new species or is A. afarensis.[32] Australopithecus prometheus, otherwise known as Little Foot has recently been dated at 3.67 million years old through a new dating technique, making the genus Australopithecus as old as afarensis.[33] Given the opposable big toe found on Little Foot, it seems that the specimen was a good climber. It is thought given the night predators of the region that he built a nesting platform at night in the trees in a similar fashion to chimpanzees and gorillas.
The earliest documented representative of the genus Homo is Homo habilis, which evolved around 2.8 million years ago,[34] and is arguably the earliest species for which there is positive evidence of the use of stone tools. The brains of these early hominins were about the same size as that of a chimpanzee, although it has been suggested that this was the time in which the human SRGAP2 gene doubled, producing a more rapid wiring of the frontal cortex. During the next million years a process of rapid encephalization occurred, and with the arrival of Homo erectus and Homo ergaster in the fossil record, cranial capacity had doubled to 850 cm3.[35] (Such an increase in human brain size is equivalent to each generation having 125,000 more neurons than their parents.) It is believed that H. erectus and H. ergaster were the first to use fire and complex tools, and were the first of the hominin line to leave Africa, spreading throughout Africa, Asia, and Europe between 1.3 to 1.8 million years ago.
According to the recent African origin theory, modern humans evolved in Africa possibly from H. heidelbergensis, H. rhodesiensis or H. antecessor and migrated out of the continent some 50,000 to 100,000 years ago, gradually replacing local populations of H. erectus, Denisova hominins, H. floresiensis, H. luzonensis and H. neanderthalensis, whose ancestors had left Africa in earlier migrations.[36][37][38][39][40] Archaic Homo sapiens, the forerunner of anatomically modern humans, evolved in the Middle Paleolithic between 400,000 and 250,000 years ago.[41][42][43] Recent DNA evidence suggests that several haplotypes of Neanderthal origin are present among all non-African populations, and Neanderthals and other hominins, such as Denisovans, may have contributed up to 6% of their genome to present-day humans, suggestive of a limited interbreeding between these species.[44][45][46] According to some anthropologists, the transition to behavioral modernity with the development of symbolic culture, language, and specialized lithic technology happened around 50,000 years ago (beginning of the Upper Paleolithic),[47] although others point to evidence of a gradual change over a longer time span during the Middle Paleolithic.[48]
Homo sapiens is the only extant species of its genus, Homo. While some (extinct) Homo species might have been ancestors of Homo sapiens, many, perhaps most, were likely "cousins", having speciated away from the ancestral hominin line.[50][51] There is yet no consensus as to which of these groups should be considered a separate species and which should be subspecies; this may be due to the dearth of fossils or to the slight differences used to classify species in the genus Homo.[51] The Sahara pump theory (describing an occasionally passable "wet" Sahara desert) provides one possible explanation of the intermittent migration and speciation in the genus Homo.
Based on archaeological and paleontological evidence, it has been possible to infer, to some extent, the ancient dietary practices[52] of various Homo species and to study the role of diet in physical and behavioral evolution within Homo.[53][54][55][56][57]
Some anthropologists and archaeologists subscribe to the Toba catastrophe theory, which posits that the supereruption of Lake Toba on Sumatra in Indonesia some 70,000 years ago caused global starvation,[58] killing the majority of humans and creating a population bottleneck that affected the genetic inheritance of all humans today.[59] The genetic and archaeological evidence for this remains in question however.[60] A 2023 genetic study suggests that a similar human population bottleneck of between 1,000 and 100,000 survivors occurred "around 930,000 and 813,000 years ago ... lasted for about 117,000 years and brought human ancestors close to extinction."[61][62]
Homo habilis lived from about 2.8[34] to 1.4 Ma. The species evolved in South and East Africa in the Late Pliocene or Early Pleistocene, 2.5–2 Ma, when it diverged from the australopithecines with the development of smaller molars and larger brains. One of the first known hominins, it made tools from stone and perhaps animal bones, leading to its name homo habilis (Latin 'handy man') bestowed by discoverer Louis Leakey. Some scientists have proposed moving this species from Homo into Australopithecus due to the morphology of its skeleton being more adapted to living in trees rather than walking on two legs like later hominins.[63]
In May 2010, a new species, Homo gautengensis, was discovered in South Africa.[64]
These are proposed species names for fossils from about 1.9–1.6 Ma, whose relation to Homo habilis is not yet clear.
The first fossils of Homo erectus were discovered by Dutch physician Eugene Dubois in 1891 on the Indonesian island of Java. He originally named the material Anthropopithecus erectus (1892–1893, considered at this point as a chimpanzee-like fossil primate) and Pithecanthropus erectus (1893–1894, changing his mind as of based on its morphology, which he considered to be intermediate between that of humans and apes).[68] Years later, in the 20th century, the German physician and paleoanthropologist Franz Weidenreich (1873–1948) compared in detail the characters of Dubois' Java Man, then named Pithecanthropus erectus, with the characters of the Peking Man, then named Sinanthropus pekinensis. Weidenreich concluded in 1940 that because of their anatomical similarity with modern humans it was necessary to gather all these specimens of Java and China in a single species of the genus Homo, the species H. erectus.[69][70]
Homo erectus lived from about 1.8 Ma to about 70,000 years ago – which would indicate that they were probably wiped out by the Toba catastrophe; however, nearby H. floresiensis survived it. The early phase of H. erectus, from 1.8 to 1.25 Ma, is considered by some to be a separate species, H. ergaster, or as H. erectus ergaster, a subspecies of H. erectus. Many paleoanthropologists now use the term Homo ergaster for the non-Asian forms of this group, and reserve H. erectus only for those fossils that are found in Asia and meet certain skeletal and dental requirements which differ slightly from H. ergaster.
In Africa in the Early Pleistocene, 1.5–1 Ma, some populations of Homo habilis are thought to have evolved larger brains and to have made more elaborate stone tools; these differences and others are sufficient for anthropologists to classify them as a new species, Homo erectus—in Africa.[71] This species also may have used fire to cook meat. Richard Wrangham  notes that Homo seems to have been ground dwelling, with reduced intestinal length, smaller dentition, and "brains [swollen] to their current, horrendously fuel-inefficient size",[72] and hypothesizes that control of fire and cooking, which released increased nutritional value, was the key adaptation that separated Homo from tree-sleeping Australopithecines.[73]
These are proposed as species intermediate between H. erectus and H. heidelbergensis.
H. heidelbergensis ("Heidelberg Man") lived from about 800,000 to about 300,000 years ago. Also proposed as Homo sapiens heidelbergensis or Homo sapiens paleohungaricus.[77]
Homo neanderthalensis, alternatively designated as Homo sapiens neanderthalensis,[79] lived in Europe and Asia from 400,000[80] to about 28,000 years ago.[81]
There are a number of clear anatomical differences between anatomically modern humans (AMH) and Neanderthal specimens, many relating to the superior Neanderthal adaptation to cold environments. Neanderthal surface to volume ratio was even lower than that among modern Inuit populations, indicating superior retention of body heat.
Neanderthals also had significantly larger brains, as shown from brain endocasts, casting doubt on their intellectual inferiority to modern humans. However, the higher body mass of Neanderthals may have required larger brain mass for body control.[82] Also, recent research by Pearce, Stringer, and Dunbar has shown important differences in brain architecture. The larger size of the Neanderthal orbital chamber and occipital lobe suggests that they had a better visual acuity than modern humans, useful in the dimmer light of glacial Europe.
Neanderthals may have had less brain capacity available for social functions. Inferring social group size from endocranial volume (minus occipital lobe size) suggests that Neanderthal groups may have been limited to 120 individuals, compared to 144[citation needed][83] possible relationships for modern humans. Larger social groups could imply that modern humans had less risk of inbreeding within their clan, trade over larger areas (confirmed in the distribution of stone tools), and faster spread of social and technological innovations. All these may have all contributed to modern Homo sapiens replacing Neanderthal populations by 28,000 BP.[82]
Earlier evidence from sequencing mitochondrial DNA suggested that no significant gene flow occurred between H. neanderthalensis and H. sapiens, and that the two were separate species that shared a common ancestor about 660,000 years ago.[84][85][86] However, a sequencing of the Neanderthal genome in 2010 indicated that Neanderthals did indeed interbreed with anatomically modern humans c. 45,000-80,000 years ago, around the time modern humans migrated out from Africa, but before they dispersed throughout Europe, Asia and elsewhere.[87] The genetic sequencing of a 40,000-year-old human skeleton from Romania showed that 11% of its genome was Neanderthal, implying the individual had a Neanderthal ancestor 4–6 generations previously,[88] in addition to a contribution from earlier interbreeding in the Middle East. Though this interbred Romanian population seems not to have been ancestral to modern humans, the finding indicates that interbreeding happened repeatedly.[89]
All modern non-African humans have about 1% to 4% (or 1.5% to 2.6% by more recent data) of their DNA derived from Neanderthals.[90][87][91] This finding is consistent with recent studies indicating that the divergence of some human alleles dates to one Ma, although this interpretation has been questioned.[92][93] Neanderthals and AMH Homo sapiens could have co-existed in Europe for as long as 10,000 years, during which AMH populations exploded, vastly outnumbering Neanderthals, possibly outcompeting them by sheer numbers.[94]
In 2008, archaeologists working at the site of Denisova Cave in the Altai Mountains of Siberia uncovered a small bone fragment from the fifth finger of a juvenile member of another human species, the Denisovans.[95] Artifacts, including a bracelet, excavated in the cave at the same level were carbon dated to around 40,000 BP. As DNA had survived in the fossil fragment due to the cool climate of the Denisova Cave, both mtDNA and nuclear DNA were sequenced.[44][96]
While the divergence point of the mtDNA was unexpectedly deep in time,[97] the full genomic sequence suggested the Denisovans belonged to the same lineage as Neanderthals, with the two diverging shortly after their line split from the lineage that gave rise to modern humans.[44] Modern humans are known to have overlapped with Neanderthals in Europe and the Near East for possibly more than 40,000 years,[98] and the discovery raises the possibility that Neanderthals, Denisovans, and modern humans may have co-existed and interbred. The existence of this distant branch creates a much more complex picture of humankind during the Late Pleistocene than previously thought.[96][99] Evidence has also been found that as much as 6% of the DNA of some modern Melanesians derive from Denisovans, indicating limited interbreeding in Southeast Asia.[100][101]
Alleles thought to have originated in Neanderthals and Denisovans have been identified at several genetic loci in the genomes of modern humans outside Africa. Human leukocyte antigen (HLA) haplotypes from Denisovans and Neanderthal represent more than half the HLA alleles of modern Eurasians,[46] indicating strong positive selection for these introgressed alleles. Corinne Simoneti at Vanderbilt University, in Nashville and her team have found from medical records of 28,000 people of European descent that the presence of Neanderthal DNA segments may be associated with a higher rate of depression.[102]
The flow of genes from Neanderthal populations to modern humans was not all one way. Sergi Castellano of the Max Planck Institute for Evolutionary Anthropology reported in 2016 that while Denisovan and Neanderthal genomes are more related to each other than they are to us, Siberian Neanderthal genomes show more similarity to modern human genes than do European Neanderthal populations. This suggests Neanderthal populations interbred with modern humans around 100,000 years ago, probably somewhere in the Near East.[103]
Studies of a Neanderthal child at Gibraltar show from brain development and tooth eruption that Neanderthal children may have matured more rapidly than Homo sapiens.[104]
H. floresiensis, which lived from approximately 190,000 to 50,000 years before present (BP), has been nicknamed the hobbit for its small size, possibly a result of insular dwarfism.[105] H. floresiensis is intriguing both for its size and its age, being an example of a recent species of the genus Homo that exhibits derived traits not shared with modern humans. In other words, H. floresiensis shares a common ancestor with modern humans, but split from the modern human lineage and followed a distinct evolutionary path. The main find was a skeleton believed to be a woman of about 30 years of age. Found in 2003, it has been dated to approximately 18,000 years old. The living woman was estimated to be one meter in height, with a brain volume of just 380 cm3 (considered small for a chimpanzee and less than a third of the H. sapiens average of 1400 cm3).[105]
However, there is an ongoing debate over whether H. floresiensis is indeed a separate species.[106] Some scientists hold that H. floresiensis was a modern H. sapiens with pathological dwarfism.[107] This hypothesis is supported in part, because some modern humans who live on Flores, the Indonesian island where the skeleton was found, are pygmies. This, coupled with pathological dwarfism, could have resulted in a significantly diminutive human. The other major attack on H. floresiensis as a separate species is that it was found with tools only associated with H. sapiens.[107]
The hypothesis of pathological dwarfism, however, fails to explain additional anatomical features that are unlike those of modern humans (diseased or not) but much like those of ancient members of our genus. Aside from cranial features, these features include the form of bones in the wrist, forearm, shoulder, knees, and feet. Additionally, this hypothesis fails to explain the find of multiple examples of individuals with these same characteristics, indicating they were common to a large population, and not limited to one individual.[106]
In 2016, fossil teeth and a partial jaw from hominins assumed to be ancestral to H. floresiensis were discovered[108] at Mata Menge, about 74 km (46 mi) from Liang Bua. They date to about 700,000 years ago[109] and are noted by Australian archaeologist Gerrit van den Bergh for being even smaller than the later fossils.[110]
A small number of specimens from the island of Luzon, dated 50,000 to 67,000 years ago, have recently been assigned by their discoverers, based on dental characteristics, to a novel human species, H. luzonensis.[111]
H. sapiens (the adjective sapiens is Latin for "wise" or "intelligent") emerged in Africa around 300,000 years ago, likely derived from H. heidelbergensis or a related lineage.[112][113] In September 2019, scientists reported the computerized determination, based on 260 CT scans, of a virtual skull shape of the last common human ancestor to modern humans (H. sapiens), representative of the earliest modern humans, and suggested that modern humans arose between 260,000 and 350,000 years ago through a merging of populations in East and South Africa.[114][115]
Between 400,000 years ago and the second interglacial period in the Middle Pleistocene, around 250,000 years ago, the trend in intra-cranial volume expansion and the elaboration of stone tool technologies developed, providing evidence for a transition from H. erectus to H. sapiens. The direct evidence suggests there was a migration of H. erectus out of Africa, then a further speciation of H. sapiens from H. erectus in Africa. A subsequent migration (both within and out of Africa) eventually replaced the earlier dispersed H. erectus. This migration and origin theory is usually referred to as the "recent single-origin hypothesis" or "out of Africa" theory. H. sapiens interbred with archaic humans both in Africa and in Eurasia, in Eurasia notably with Neanderthals and Denisovans.[44][100]
The Toba catastrophe theory, which postulates a population bottleneck for H. sapiens about 70,000 years ago,[116] was controversial from its first proposal in the 1990s and by the 2010s had very little support.[117] Distinctive human genetic variability has arisen as the result of the founder effect, by archaic admixture and by recent evolutionary pressures.
Since Homo sapiens separated from its last common ancestor shared with chimpanzees, human evolution is characterized by a number of morphological, developmental, physiological, behavioral, and environmental changes.[9] Environmental (cultural) evolution discovered much later during the Pleistocene played a significant role in human evolution observed via human transitions between subsistence systems.[118][9] The most significant of these adaptations are bipedalism, increased brain size, lengthened ontogeny (gestation and infancy), and decreased sexual dimorphism. The relationship between these changes is the subject of ongoing debate.[119] Other significant morphological changes included the evolution of a power and precision grip, a change first occurring in H. erectus.[120]
Bipedalism, (walking on two legs), is the basic adaptation of the hominid and is considered the main cause behind a suite of skeletal changes shared by all bipedal hominids. The earliest hominin, of presumably primitive bipedalism, is considered to be either Sahelanthropus[121] or Orrorin, both of which arose some 6 to 7 million years ago. The non-bipedal knuckle-walkers, the gorillas and chimpanzees, diverged from the hominin line over a period covering the same time, so either Sahelanthropus or Orrorin may be our last shared ancestor. Ardipithecus, a full biped, arose approximately 5.6 million years ago.[122]
The early bipeds eventually evolved into the australopithecines and still later into the genus Homo. There are several theories of the adaptation value of bipedalism. It is possible that bipedalism was favored because it freed the hands for reaching and carrying food, saved energy during locomotion,[123] enabled long-distance running and hunting, provided an enhanced field of vision, and helped avoid hyperthermia by reducing the surface area exposed to direct sun; features all advantageous for thriving in the new savanna and woodland environment created as a result of the East African Rift Valley uplift versus the previous closed forest habitat.[123][124][125] A 2007 study provides support for the hypothesis that bipedalism evolved because it used less energy than quadrupedal knuckle-walking.[126][127] However, recent studies suggest that bipedality without the ability to use fire would not have allowed global dispersal.[128] This change in gait saw a lengthening of the legs proportionately when compared to the length of the arms, which were shortened through the removal of the need for brachiation. Another change is the shape of the big toe. Recent studies suggest that australopithecines still lived part of the time in trees as a result of maintaining a grasping big toe. This was progressively lost in habilines.
Anatomically, the evolution of bipedalism has been accompanied by a large number of skeletal changes, not just to the legs and pelvis, but also to the vertebral column, feet and ankles, and skull.[129] The femur evolved into a slightly more angular position to move the center of gravity toward the geometric center of the body. The knee and ankle joints became increasingly robust to better support increased weight. To support the increased weight on each vertebra in the upright position, the human vertebral column became S-shaped and the lumbar vertebrae became shorter and wider. In the feet the big toe moved into alignment with the other toes to help in forward locomotion. The arms and forearms shortened relative to the legs making it easier to run. The foramen magnum migrated under the skull and more anterior.[130]
The most significant changes occurred in the pelvic region, where the long downward facing iliac blade was shortened and widened as a requirement for keeping the center of gravity stable while walking;[28] bipedal hominids have a shorter but broader, bowl-like pelvis due to this. A drawback is that the birth canal of bipedal apes is smaller than in knuckle-walking apes, though there has been a widening of it in comparison to that of australopithecine and modern humans, thus permitting the passage of newborns due to the increase in cranial size. This is limited to the upper portion, since further increase can hinder normal bipedal movement.[131]
The shortening of the pelvis and smaller birth canal evolved as a requirement for bipedalism and had significant effects on the process of human birth, which is much more difficult in modern humans than in other primates. During human birth, because of the variation in size of the pelvic region, the fetal head must be in a transverse position (compared to the mother) during entry into the birth canal and rotate about 90 degrees upon exit.[132] The smaller birth canal became a limiting factor to brain size increases in early humans and prompted a shorter gestation period leading to the relative immaturity of human offspring, who are unable to walk much before 12 months and have greater neoteny, compared to other primates, who are mobile at a much earlier age.[125] The increased brain growth after birth and the increased dependency of children on mothers had a major effect upon the female reproductive cycle,[133] and the more frequent appearance of alloparenting in humans when compared with other hominids.[134] Delayed human sexual maturity also led to the evolution of menopause with one explanation, the grandmother hypothesis, providing that elderly women could better pass on their genes by taking care of their daughter's offspring, as compared to having more children of their own.[135][136]
The human species eventually developed a much larger brain than that of other primates—typically 1,330 cm3 (81 cu in) in modern humans, nearly three times the size of a chimpanzee or gorilla brain.[139] After a period of stasis with Australopithecus anamensis and Ardipithecus, species which had smaller brains as a result of their bipedal locomotion,[140] the pattern of encephalization started with Homo habilis, whose 600 cm3 (37 cu in) brain was slightly larger than that of chimpanzees. This evolution continued in Homo erectus with 800–1,100 cm3 (49–67 cu in), and reached a maximum in Neanderthals with 1,200–1,900 cm3 (73–116 cu in), larger even than modern Homo sapiens. This brain increase manifested during postnatal brain growth, far exceeding that of other apes (heterochrony). It also allowed for extended periods of social learning and language acquisition in juvenile humans, beginning as much as 2 million years ago. Encephalization may be due to a dependency on calorie-dense, difficult-to-acquire food.[141]
Furthermore, the changes in the structure of human brains may be even more significant than the increase in size.[142][143][144][53] Fossilized skulls shows the brain size in early humans fell within the range of modern humans 300,000 years ago, but only got its present-day brain shape between 100,000 and 35,000 years ago.[145]
The temporal lobes, which contain centers for language processing, have increased disproportionately, as has the prefrontal cortex, which has been related to complex decision-making and moderating social behavior.[139] Encephalization has been tied to increased starches[52] and meat[146][147] in the diet, however a 2022 meta study called into question the role of meat.[148] Other factors are the development of cooking,[149] and it has been proposed that intelligence increased as a response to an increased necessity for solving social problems as human society became more complex.[150] Changes in skull morphology, such as smaller mandibles and mandible muscle attachments, allowed more room for the brain to grow.[151]
The increase in volume of the neocortex also included a rapid increase in size of the cerebellum. Its function has traditionally been associated with balance and fine motor control, but more recently with speech and cognition. The great apes, including hominids, had a more pronounced cerebellum relative to the neocortex than other primates. It has been suggested that because of its function of sensory-motor control and learning complex muscular actions, the cerebellum may have underpinned human technological adaptations, including the preconditions of speech.[152][153][154][155]
The immediate survival advantage of encephalization is difficult to discern, as the major brain changes from Homo erectus to Homo heidelbergensis were not accompanied by major changes in technology. It has been suggested that the changes were mainly social and behavioural, including increased empathic abilities,[156][157] increases in size of social groups,[150][158][159] and increased behavioral plasticity.[160] Humans are unique in the ability to acquire information through social transmission and adapt that information.[161] The emerging field of cultural evolution studies human sociocultural change from an evolutionary perspective.[162]
The reduced degree of sexual dimorphism in humans is visible primarily in the reduction of the male canine tooth relative to other ape species (except gibbons) and reduced brow ridges and general robustness of males. Another important physiological change related to sexuality in humans was the evolution of hidden estrus. Humans are the only hominoids in which the female is fertile year round and in which no special signals of fertility are produced by the body (such as genital swelling or overt changes in proceptivity during estrus).[175]
Nonetheless, humans retain a degree of sexual dimorphism in the distribution of body hair and subcutaneous fat, and in the overall size, males being around 15% larger than females.[176] These changes taken together have been interpreted as a result of an increased emphasis on pair bonding as a possible solution to the requirement for increased parental investment due to the prolonged infancy of offspring.[177]
The ulnar opposition—the contact between the thumb and the tip of the little finger of the same hand—is unique to the genus Homo,[178] including Neanderthals, the Sima de los Huesos hominins and anatomically modern humans.[179][180] In other primates, the thumb is short and unable to touch the little finger.[179] The ulnar opposition facilitates the precision grip and power grip of the human hand, underlying all the skilled manipulations.
A number of other changes have also characterized the evolution of humans, among them an increased reliance on vision rather than smell (highly reduced olfactory bulb); a longer juvenile developmental period and higher infant dependency;[181] a smaller gut and small, misaligned teeth; faster basal metabolism;[182] loss of body hair;[183] an increase in
eccrine sweat gland density that is ten times higher than any other catarrhinian primates,[184] yet humans use 30% to 50% less water per day compared to chimps and gorillas;[185] more REM sleep but less sleep in total;[186] a change in the shape of the dental arcade from u-shaped to parabolic; development of a chin (found in Homo sapiens alone); styloid processes; and a descended larynx. As the human hand and arms adapted to the making of tools and were used less for climbing, the shoulder blades changed too. As a side effect, it allowed human ancestors to throw objects with greater force, speed and accuracy.[187]
The use of tools has been interpreted as a sign of intelligence, and it has been theorized that tool use may have stimulated certain aspects of human evolution, especially the continued expansion of the human brain.[189] Paleontology has yet to explain the expansion of this organ over millions of years despite being extremely demanding in terms of energy consumption. The brain of a modern human consumes, on average, about 13 watts (260 kilocalories per day), a fifth of the body's resting power consumption.[190] Increased tool use would allow hunting for energy-rich meat products, and would enable processing more energy-rich plant products. Researchers have suggested that early hominins were thus under evolutionary pressure to increase their capacity to create and use tools.[191]
Precisely when early humans started to use tools is difficult to determine, because the more primitive these tools are (for example, sharp-edged stones) the more difficult it is to decide whether they are natural objects or human artifacts.[189] There is some evidence that the australopithecines (4 Ma) may have used broken bones as tools, but this is debated.[192]
Many species make and use tools, but it is the human genus that dominates the areas of making and using more complex tools. The oldest known tools are flakes from West Turkana, Kenya, which date to 3.3 million years ago.[193] The next oldest stone tools are from Gona, Ethiopia, and are considered the beginning of the Oldowan technology. These tools date to about 2.6 million years ago.[194] A Homo fossil was found near some Oldowan tools, and its age was noted at 2.3 million years old, suggesting that maybe the Homo species did indeed create and use these tools. It is a possibility but does not yet represent solid evidence.[195] The third metacarpal styloid process enables the hand bone to lock into the wrist bones, allowing for greater amounts of pressure to be applied to the wrist and hand from a grasping thumb and fingers. It allows humans the dexterity and strength to make and use complex tools. This unique anatomical feature separates humans from other apes and other nonhuman primates, and is not seen in human fossils older than 1.8 million years.[196]
Bernard Wood noted that Paranthropus co-existed with the early Homo species in the area of the "Oldowan Industrial Complex" over roughly the same span of time. Although there is no direct evidence which identifies Paranthropus as the tool makers, their anatomy lends to indirect evidence of their capabilities in this area. Most paleoanthropologists agree that the early Homo species were indeed responsible for most of the Oldowan tools found. They argue that when most of the Oldowan tools were found in association with human fossils, Homo was always present, but Paranthropus was not.[195]
In 1994, Randall Susman used the anatomy of opposable thumbs as the basis for his argument that both the Homo and Paranthropus species were toolmakers. He compared bones and muscles of human and chimpanzee thumbs, finding that humans have 3 muscles which are lacking in chimpanzees. Humans also have thicker metacarpals with broader heads, allowing more precise grasping than the chimpanzee hand can perform. Susman posited that modern anatomy of the human opposable thumb is an evolutionary response to the requirements associated with making and handling tools and that both species were indeed toolmakers.[195]
Anthropologists describe modern human behavior to include cultural and behavioral traits such as specialization of tools, use of jewellery and images (such as cave drawings), organization of living space, rituals (such as grave gifts), specialized hunting techniques, exploration of less hospitable geographical areas, and barter trade networks, as well as more general traits such as language and complex symbolic thinking. Debate continues as to whether a "revolution" led to modern humans ("big bang of human consciousness"), or whether the evolution was more gradual.[48]
Until about 50,000–40,000 years ago, the use of stone tools seems to have progressed stepwise. Each phase (H. habilis, H. ergaster, H. neanderthalensis) marked a new technology, followed by very slow development until the next phase. Currently paleoanthropologists are debating whether these Homo species possessed some or many modern human behaviors. They seem to have been culturally conservative, maintaining the same technologies and foraging patterns over very long periods.
Around 50,000 BP, human culture started to evolve more rapidly. The transition to behavioral modernity has been characterized by some as a "Great Leap Forward",[197] or as the "Upper Palaeolithic Revolution",[198] due to the sudden appearance in the archaeological record of distinctive signs of modern behavior and big game hunting.[199] Evidence of behavioral modernity significantly earlier also exists from Africa, with older evidence of abstract imagery, widened subsistence strategies, more sophisticated tools and weapons, and other "modern" behaviors, and many scholars have recently argued that the transition to modernity occurred sooner than previously believed.[48][200][201][202]
Other scholars consider the transition to have been more gradual, noting that some features had already appeared among archaic African Homo sapiens 300,000–200,000 years ago.[203][204][205][206][207] Recent evidence suggests that the Australian Aboriginal population separated from the African population 75,000 years ago, and that they made a 160 km (99 mi) sea journey 60,000 years ago, which may diminish the significance of the Upper Paleolithic Revolution.[208]
Modern humans started burying their dead, making clothing from animal hides, hunting with more sophisticated techniques (such as using pit traps or driving animals off cliffs), and cave painting.[209] As human culture advanced, different populations innovated existing technologies: artifacts such as fish hooks, buttons, and bone needles show signs of cultural variation, which had not been seen prior to 50,000 BP. Typically, the older H. neanderthalensis populations did not vary in their technologies, although the Chatelperronian assemblages have been found to be Neanderthal imitations of H. sapiens Aurignacian technologies.[210]
Anatomically modern human populations continue to evolve, as they are affected by both natural selection and genetic drift. Although selection pressure on some traits, such as resistance to smallpox, has decreased in the modern age, humans are still undergoing natural selection for many other traits. Some of these are due to specific environmental pressures, while others are related to lifestyle changes since the development of agriculture (10,000 years ago), urbanization (5,000), and industrialization (250 years ago). It has been argued that human evolution has accelerated since the development of agriculture 10,000 years ago and civilization some 5,000 years ago, resulting, it is claimed, in substantial genetic differences between different current human populations,[211] and more recent research indicates that for some traits, the developments and innovations of human culture have driven a new form of selection that coexists with, and in some cases has largely replaced, natural selection.[212]
Particularly conspicuous is variation in superficial characteristics, such as Afro-textured hair, or the recent evolution of light skin and blond hair in some populations, which are attributed to differences in climate. Particularly strong selective pressures have resulted in high-altitude adaptation in humans, with different ones in different isolated populations. Studies of the genetic basis show that some developed very recently, with Tibetans evolving over 3,000 years to have high proportions of an allele of EPAS1 that is adaptive to high altitudes.
Other evolution is related to endemic diseases: the presence of malaria selects for sickle cell trait (the heterozygous form of sickle cell gene), while in the absence of malaria, the health effects of sickle-cell anemia select against this trait. For another example, the population at risk of the severe debilitating disease kuru has significant over-representation of an immune variant of the prion protein gene G127V versus non-immune alleles. The frequency of this genetic variant is due to the survival of immune persons.[214][215] Some reported trends remain unexplained and the subject of ongoing research in the novel field of evolutionary medicine: polycystic ovary syndrome (PCOS) reduces fertility and thus is expected to be subject to extremely strong negative selection, but its relative commonality in human populations suggests a counteracting selection pressure. The identity of that pressure remains the subject of some debate.[216]
Recent human evolution related to agriculture includes genetic resistance to infectious disease that has appeared in human populations by crossing the species barrier from domesticated animals,[217] as well as changes in metabolism due to changes in diet, such as lactase persistence.
Culturally-driven evolution can defy the expectations of natural selection:  while human populations experience some pressure that drives a selection for producing children at younger ages, the advent of effective contraception, higher education, and changing social norms have driven the observed selection in the opposite direction.[218] However, culturally-driven selection need not necessarily work counter or in opposition to natural selection:  some proposals to explain the high rate of recent human brain expansion indicate a kind of feedback whereupon the brain's increased social learning efficiency encourages cultural developments that in turn encourage more efficiency, which drive more complex cultural developments that demand still-greater efficiency, and so forth.[219] Culturally-driven evolution has an advantage in that in addition to the genetic effects, it can be observed also in the archaeological record:  the development of stone tools across the Palaeolithic period connects to culturally-driven cognitive development in the form of skill acquisition supported by the culture and the development of increasingly complex technologies and the cognitive ability to elaborate them.[220]
In contemporary times, since industrialization, some trends have been observed: for instance, menopause is evolving to occur later.[221] Other reported trends appear to include lengthening of the human reproductive period and reduction in cholesterol levels, blood glucose and blood pressure in some populations.[221]
The name Homo of the biological genus to which humans belong is Latin for 'human'.[e] It was chosen originally by Carl Linnaeus in his classification system.[f] The English word human is from the Latin humanus, the adjectival form of homo. The Latin homo derives from the Indo-European root *dhghem, or 'earth'.[222] Linnaeus and other scientists of his time also considered the great apes to be the closest relatives of humans based on morphological and anatomical similarities.[223]
The possibility of linking humans with earlier apes by descent became clear only after 1859 with the publication of Charles Darwin's On the Origin of Species, in which he argued for the idea of the evolution of new species from earlier ones. Darwin's book did not address the question of human evolution, saying only that "Light will be thrown on the origin of man and his history."[224]
The first debates about the nature of human evolution arose between Thomas Henry Huxley and Richard Owen. Huxley argued for human evolution from apes by illustrating many of the similarities and differences between humans and other apes, and did so particularly in his 1863 book Evidence as to Man's Place in Nature. Many of Darwin's early supporters (such as Alfred Russel Wallace and Charles Lyell) did not initially agree that the origin of the mental capacities and the moral sensibilities of humans could be explained by natural selection, though this later changed. Darwin applied the theory of evolution and sexual selection to humans in his 1871 book The Descent of Man, and Selection in Relation to Sex.[225]
A major problem in the 19th century was the lack of fossil intermediaries. Neanderthal remains were discovered in a limestone quarry in 1856, three years before the publication of On the Origin of Species, and Neanderthal fossils had been discovered in Gibraltar even earlier, but it was originally claimed that these were the remains of a modern human who had suffered some kind of illness.[226] Despite the 1891 discovery by Eugène Dubois of what is now called Homo erectus at Trinil, Java, it was only in the 1920s when such fossils were discovered in Africa, that intermediate species began to accumulate.[227] In 1925, Raymond Dart described Australopithecus africanus.[228] The type specimen was the Taung Child, an australopithecine infant which was discovered in a cave. The child's remains were a remarkably well-preserved tiny skull and an endocast of the brain.
Although the brain was small (410 cm3), its shape was rounded, unlike that of chimpanzees and gorillas, and more like a modern human brain. Also, the specimen showed short canine teeth, and the position of the foramen magnum (the hole in the skull where the spine enters) was evidence of bipedal locomotion. All of these traits convinced Dart that the Taung Child was a bipedal human ancestor, a transitional form between apes and humans.
During the 1960s and 1970s, hundreds of fossils were found in East Africa in the regions of the Olduvai Gorge and Lake Turkana. These searches were carried out by the Leakey family, with Louis Leakey and his wife Mary Leakey, and later their son Richard and daughter-in-law Meave, fossil hunters and paleoanthropologists. From the fossil beds of Olduvai and Lake Turkana they amassed specimens of the early hominins: the australopithecines and Homo species, and even H. erectus.
These finds cemented Africa as the cradle of humankind. In the late 1970s and the 1980s, Ethiopia emerged as the new hot spot of paleoanthropology after "Lucy", the most complete fossil member of the species Australopithecus afarensis, was found in 1974 by Donald Johanson near Hadar in the desertic Afar Triangle region of northern Ethiopia. Although the specimen had a small brain, the pelvis and leg bones were almost identical in function to those of modern humans, showing with certainty that these hominins had walked erect.[229] Lucy was classified as a new species, Australopithecus afarensis, which is thought to be more closely related to the genus Homo as a direct ancestor, or as a close relative of an unknown ancestor, than any other known hominid or hominin from this early time range.[230] (The specimen was nicknamed "Lucy" after the Beatles' song "Lucy in the Sky with Diamonds", which was played loudly and repeatedly in the camp during the excavations.)[231] The Afar Triangle area would later yield discovery of many more hominin fossils, particularly those uncovered or described by teams headed by Tim D. White in the 1990s, including Ardipithecus ramidus and A. kadabba.[232]
In 2013, fossil skeletons of Homo naledi, an extinct species of hominin assigned (provisionally) to the genus Homo, were found in the Rising Star Cave system, a site in South Africa's Cradle of Humankind region in Gauteng province near Johannesburg.[233][234] As of September 2015[update], fossils of at least fifteen individuals, amounting to 1,550 specimens, have been excavated from the cave.[234] The species is characterized by a body mass and stature similar to small-bodied human populations, a smaller endocranial volume similar to Australopithecus, and a cranial morphology (skull shape) similar to early Homo species. The skeletal anatomy combines primitive features known from australopithecines with features known from early hominins. The individuals show signs of having been deliberately disposed of within the cave near the time of death. The fossils were dated close to 250,000 years ago,[235] and thus are not ancestral but contemporary with the first appearance of larger-brained anatomically modern humans.[236]
The genetic revolution in studies of human evolution started when Vincent Sarich and Allan Wilson measured the strength of immunological cross-reactions of blood serum albumin between pairs of creatures, including humans and African apes (chimpanzees and gorillas).[237] The strength of the reaction could be expressed numerically as an immunological distance, which was in turn proportional to the number of amino acid differences between homologous proteins in different species. By constructing a calibration curve of the ID of species' pairs with known divergence times in the fossil record, the data could be used as a molecular clock to estimate the times of divergence of pairs with poorer or unknown fossil records.
In their seminal 1967 paper in Science, Sarich and Wilson estimated the divergence time of humans and apes as four to five million years ago,[237] at a time when standard interpretations of the fossil record gave this divergence as at least 10 to as much as 30 million years. Subsequent fossil discoveries, notably "Lucy", and reinterpretation of older fossil materials, notably Ramapithecus, showed the younger estimates to be correct and validated the albumin method.
Progress in DNA sequencing, specifically mitochondrial DNA (mtDNA) and then Y-chromosome DNA (Y-DNA) advanced the understanding of human origins.[124][238][239] Application of the molecular clock principle revolutionized the study of molecular evolution.
On the basis of a separation from the orangutan between 10 and 20 million years ago, earlier studies of the molecular clock suggested that there were about 76 mutations per generation that were not inherited by human children from their parents; this evidence supported the divergence time between hominins and chimpanzees noted above. However, a 2012 study in Iceland of 78 children and their parents suggests a mutation rate of only 36 mutations per generation; this datum extends the separation between humans and chimpanzees to an earlier period greater than 7 million years ago (Ma). Additional research with 226 offspring of wild chimpanzee populations in eight locations suggests that chimpanzees reproduce at age 26.5 years on average; which suggests the human divergence from chimpanzees occurred between 7 and 13 mya. And these data suggest that Ardipithecus (4.5 Ma), Orrorin (6 Ma) and Sahelanthropus (7 Ma) all may be on the hominid lineage, and even that the separation may have occurred outside the East African Rift region.
Furthermore, analysis of the two species' genes in 2006 provides evidence that after human ancestors had started to diverge from chimpanzees, interspecies mating between "proto-human" and "proto-chimpanzees" nonetheless occurred regularly enough to change certain genes in the new gene pool:
The research suggests:
In the 1990s, several teams of paleoanthropologists were working throughout Africa looking for evidence of the earliest divergence of the hominin lineage from the great apes. In 1994, Meave Leakey discovered Australopithecus anamensis. The find was overshadowed by Tim D. White's 1995 discovery of Ardipithecus ramidus, which pushed back the fossil record to 4.2 million years ago.
In 2000, Martin Pickford and Brigitte Senut discovered, in the Tugen Hills of Kenya, a 6-million-year-old bipedal hominin which they named Orrorin tugenensis. And in 2001, a team led by Michel Brunet discovered the skull of Sahelanthropus tchadensis which was dated as 7.2 million years ago, and which Brunet argued was a bipedal, and therefore a hominid—that is, a hominin (cf Hominidae; terms "hominids" and hominins).
Anthropologists in the 1980s were divided regarding some details of reproductive barriers and migratory dispersals of the genus Homo. Subsequently, genetics has been used to investigate and resolve these issues. According to the Sahara pump theory evidence suggests that the genus Homo have migrated out of Africa at least three and possibly four times (e.g. Homo erectus, Homo heidelbergensis and two or three times for Homo sapiens). Recent evidence suggests these dispersals are closely related to fluctuating periods of climate change.[245]
Recent evidence suggests that humans may have left Africa half a million years earlier than previously thought. A joint Franco-Indian team has found human artifacts in the Siwalk Hills north of New Delhi dating back at least 2.6 million years. This is earlier than the previous earliest finding of genus Homo at Dmanisi, in Georgia, dating to 1.85 million years. Although controversial, tools found at a Chinese cave strengthen the case that humans used tools as far back as 2.48 million years ago.[246] This suggests that the Asian "Chopper" tool tradition, found in Java and northern China may have left Africa before the appearance of the Acheulian hand axe.
Up until the genetic evidence became available, there were two dominant models for the dispersal of modern humans. The multiregional hypothesis proposed that the genus Homo contained only a single interconnected population as it does today (not separate species), and that its evolution took place worldwide continuously over the last couple of million years. This model was proposed in 1988 by Milford H. Wolpoff.[247][248] In contrast, the "out of Africa" model proposed that modern H. sapiens speciated in Africa recently (that is, approximately 200,000 years ago) and the subsequent migration through Eurasia resulted in the nearly complete replacement of other Homo species. This model has been developed by Chris Stringer and Peter Andrews.[249][250]
Sequencing mtDNA and Y-DNA sampled from a wide range of indigenous populations revealed ancestral information relating to both male and female genetic heritage, and strengthened the "out of Africa" theory and weakened the views of multiregional evolutionism.[251] Aligned in genetic tree differences were interpreted as supportive of a recent single origin.[252]
"Out of Africa" has thus gained much support from research using female mitochondrial DNA and the male Y chromosome. After analysing genealogy trees constructed using 133 types of mtDNA, researchers concluded that all were descended from a female African progenitor, dubbed Mitochondrial Eve. "Out of Africa" is also supported by the fact that mitochondrial genetic diversity is highest among African populations.[253]
A broad study of African genetic diversity, headed by Sarah Tishkoff, found the San people had the greatest genetic diversity among the 113 distinct populations sampled, making them one of 14 "ancestral population clusters". The research also located a possible origin of modern human migration in southwestern Africa, near the coastal border of Namibia and Angola.[254] The fossil evidence was insufficient for archaeologist Richard Leakey to resolve the debate about exactly where in Africa modern humans first appeared.[255] Studies of haplogroups in Y-chromosomal DNA and mitochondrial DNA have largely supported a recent African origin.[256] All the evidence from autosomal DNA also predominantly supports a Recent African origin. However, evidence for archaic admixture in modern humans, both in Africa and later, throughout Eurasia has recently been suggested by a number of studies.[257]
Recent sequencing of Neanderthal[90] and Denisovan[44] genomes shows that some admixture with these populations has occurred. All modern human groups outside Africa have 1–4% or (according to more recent research) about 1.5–2.6% Neanderthal alleles in their genome,[91] and some Melanesians have an additional 4–6% of Denisovan alleles. These new results do not contradict the "out of Africa" model, except in its strictest interpretation, although they make the situation more complex. After recovery from a genetic bottleneck that some researchers speculate might be linked to the Toba supervolcano catastrophe, a fairly small group left Africa and interbred with Neanderthals, probably in the Middle East, on the Eurasian steppe or even in North Africa before their departure. Their still predominantly African descendants spread to populate the world. A fraction in turn interbred with Denisovans, probably in southeastern Asia, before populating Melanesia.[100] HLA haplotypes of Neanderthal and Denisova origin have been identified in modern Eurasian and Oceanian populations.[46] The Denisovan EPAS1 gene has also been found in Tibetan populations.[258] Studies of the human genome using machine learning have identified additional genetic contributions in Eurasians from an "unknown" ancestral population potentially related to the Neanderthal-Denisovan lineage.[259]
There are still differing theories on whether there was a single exodus from Africa or several. A multiple dispersal model involves the Southern Dispersal theory,[260][261][262] which has gained support in recent years from genetic, linguistic and archaeological evidence. In this theory, there was a coastal dispersal of modern humans from the Horn of Africa crossing the Bab el Mandib to Yemen at a lower sea level around 70,000 years ago. This group helped to populate Southeast Asia and Oceania, explaining the discovery of early human sites in these areas much earlier than those in the Levant.[260] This group seems to have been dependent upon marine resources for their survival.
Stephen Oppenheimer has proposed a second wave of humans may have later dispersed through the Persian Gulf oases, and the Zagros mountains into the Middle East. Alternatively it may have come across the Sinai Peninsula into Asia, from shortly after 50,000 yrs BP, resulting in the bulk of the human populations of Eurasia. It has been suggested that this second group possibly possessed a more sophisticated "big game hunting" tool technology and was less dependent on coastal food sources than the original group. Much of the evidence for the first group's expansion would have been destroyed by the rising sea levels at the end of each glacial maximum.[260] The multiple dispersal model is contradicted by studies indicating that the populations of Eurasia and the populations of Southeast Asia and Oceania are all descended from the same mitochondrial DNA L3 lineages, which support a single migration out of Africa that gave rise to all non-African populations.[263]
On the basis of the early date of Badoshan Iranian Aurignacian, Oppenheimer suggests that this second dispersal may have occurred with a pluvial period about 50,000 years before the present, with modern human big-game hunting cultures spreading up the Zagros Mountains, carrying modern human genomes from Oman, throughout the Persian Gulf, northward into Armenia and Anatolia, with a variant travelling south into Israel and to Cyrenicia.[199]
Recent genetic evidence suggests that all modern non-African populations, including those of Eurasia and Oceania, are descended from a single wave that left Africa between 65,000 and 50,000 years ago.[264][265][266]
The evidence on which scientific accounts of human evolution are based comes from many fields of natural science. The main source of knowledge about the evolutionary process has traditionally been the fossil record, but since the development of genetics beginning in the 1970s, DNA analysis has come to occupy a place of comparable importance. The studies of ontogeny, phylogeny and especially evolutionary developmental biology of both vertebrates and invertebrates offer considerable insight into the evolution of all life, including how humans evolved. The specific study of the origin and life of humans is anthropology, particularly paleoanthropology which focuses on the study of human prehistory.[267]
The closest living relatives of humans are bonobos and chimpanzees (both genus Pan) and gorillas (genus Gorilla).[268] With the sequencing of both the human and chimpanzee genome, as of 2012[update] estimates of the similarity between their DNA sequences range between 95% and 99%.[268][269][31] It is also noteworthy that mice share around 97.5% of their working DNA with humans.[270]  By using the technique called the molecular clock which estimates the time required for the number of divergent mutations to accumulate between two lineages, the approximate date for the split between lineages can be calculated.
The gibbons (family Hylobatidae) and then the orangutans (genus Pongo) were the first groups to split from the line leading to the hominins, including humans—followed by gorillas (genus Gorilla), and, ultimately, by the chimpanzees (genus Pan). The splitting date between hominin and chimpanzee lineages is placed by some between 4 to 8 million years ago, that is, during the Late Miocene.[271][272][273][274] Speciation, however, appears to have been unusually drawn out. Initial divergence occurred sometime between 7 to 13 million years ago, but ongoing hybridization blurred the separation and delayed complete separation during several millions of years. Patterson (2006) dated the final divergence at 5 to 6 million years ago.[275]
Genetic evidence has also been employed to compare species within the genus Homo, investigating gene flow between early modern humans and Neanderthals, and to enhance the understanding of the early human migration patterns and splitting dates. By comparing the parts of the genome that are not under natural selection and which therefore accumulate mutations at a fairly steady rate, it is possible to reconstruct a genetic tree incorporating the entire human species since the last shared ancestor.
Each time a certain mutation (single-nucleotide polymorphism) appears in an individual and is passed on to his or her descendants, a haplogroup is formed including all of the descendants of the individual who will also carry that mutation. By comparing mitochondrial DNA which is inherited only from the mother, geneticists have concluded that the last female common ancestor whose genetic marker is found in all modern humans, the so-called mitochondrial Eve, must have lived around 200,000 years ago.
Human evolutionary genetics studies how human genomes differ among individuals, the evolutionary past that gave rise to them, and their current effects. Differences between genomes have anthropological, medical and forensic implications and applications. Genetic data can provide important insight into human evolution.
In May 2023, scientists reported a more complicated pathway of human evolution than previously understood. According to the studies, humans evolved from different places and times in Africa, instead of from a single location and period of time.[276][277]
There is little fossil evidence for the divergence of the gorilla, chimpanzee and hominin lineages.[278] The earliest fossils that have been proposed as members of the hominin lineage are Sahelanthropus tchadensis dating from 7 million years ago, Orrorin tugenensis dating from 5.7 million years ago, and Ardipithecus kadabba dating to 5.6 million years ago. Each of these have been argued to be a bipedal ancestor of later hominins but, in each case, the claims have been contested. It is also possible that one or more of these species are ancestors of another branch of African apes, or that they represent a shared ancestor between hominins and other apes.
The question then of the relationship between these early fossil species and the hominin lineage is still to be resolved. From these early species, the australopithecines arose around 4 million years ago and diverged into robust (also called Paranthropus) and gracile branches, one of which (possibly A. garhi) probably went on to become ancestors of the genus Homo. The australopithecine species that is best represented in the fossil record is Australopithecus afarensis with more than 100 fossil individuals represented, found from Northern Ethiopia (such as the famous "Lucy"), to Kenya, and South Africa. Fossils of robust australopithecines such as A. robustus (or alternatively Paranthropus robustus) and A./P. boisei are particularly abundant in South Africa at sites such as Kromdraai and Swartkrans, and around Lake Turkana in Kenya.
The earliest member of the genus Homo is Homo habilis which evolved around 2.8 million years ago.[34] H. habilis is the first species for which we have positive evidence of the use of stone tools. They developed the Oldowan lithic technology, named after the Olduvai Gorge in which the first specimens were found. Some scientists consider Homo rudolfensis, a larger bodied group of fossils with similar morphology to the original H. habilis fossils, to be a separate species, while others consider them to be part of H. habilis—simply representing intraspecies variation, or perhaps even sexual dimorphism. The brains of these early hominins were about the same size as that of a chimpanzee, and their main adaptation was bipedalism as an adaptation to terrestrial living.
During the next million years, a process of encephalization began and, by the arrival (about 1.9 million years ago) of H. erectus in the fossil record, cranial capacity had doubled. H. erectus were the first of the hominins to emigrate from Africa, and, from 1.8 to 1.3 million years ago, this species spread through Africa, Asia, and Europe. One population of H. erectus, also sometimes classified as separate species H. ergaster, remained in Africa and evolved into H. sapiens. It is believed that H. erectus and H. ergaster were the first to use fire and complex tools. In Eurasia, H. erectus evolved into species such as H. antecessor, H. heidelbergensis and H. neanderthalensis. The earliest fossils of anatomically modern humans are from the Middle Paleolithic, about 300–200,000 years ago such as the Herto and Omo remains of Ethiopia, Jebel Irhoud remains of Morocco, and Florisbad remains of South Africa; later fossils from the Skhul Cave in Israel and Southern Europe begin around 90,000 years ago (0.09 million years ago).
As modern humans spread out from Africa, they encountered other hominins such as H. neanderthalensis and the Denisovans, who may have evolved from populations of H. erectus that had left Africa around 2 million years ago. The nature of interaction between early humans and these sister species has been a long-standing source of controversy, the question being whether humans replaced these earlier species or whether they were in fact similar enough to interbreed, in which case these earlier populations may have contributed genetic material to modern humans.[279][280]
This migration out of Africa is estimated to have begun about 70–50,000 years BP and modern humans subsequently spread globally, replacing earlier hominins either through competition or hybridization. They inhabited Eurasia and Oceania by 40,000 years BP, and the Americas by at least 14,500 years BP.[281]
The hypothesis of interbreeding, also known as hybridization, admixture or hybrid-origin theory, has been discussed ever since the discovery of Neanderthal remains in the 19th century.[282] The linear view of human evolution began to be abandoned in the 1970s as different species of humans were discovered that made the linear concept increasingly unlikely. In the 21st century with the advent of molecular biology techniques and computerization, whole-genome sequencing of Neanderthal and human genome were performed, confirming recent admixture between different human species.[90] In 2010, evidence based on molecular biology was published, revealing unambiguous examples of interbreeding between archaic and modern humans during the Middle Paleolithic and early Upper Paleolithic. It has been demonstrated that interbreeding happened in several independent events that included Neanderthals and Denisovans, as well as several unidentified hominins.[283] Today, approximately 2% of DNA from all non-African populations (including Europeans, Asians, and Oceanians) is Neanderthal,[90] with traces of Denisovan heritage.[284] Also, 4–6% of modern Melanesian genetics are Denisovan.[284] Comparisons of the human genome to the genomes of Neandertals, Denisovans and apes can help identify features that set modern humans apart from other hominin species. In a 2016 comparative genomics study, a Harvard Medical School/UCLA research team made a world map on the distribution and made some predictions about where Denisovan and Neanderthal genes may be impacting modern human biology.[285][286]
For example, comparative studies in the mid-2010s found several traits related to neurological, immunological,[287] developmental, and metabolic phenotypes, that were developed by archaic humans to European and Asian environments and inherited to modern humans through admixture with local hominins.[288][289]
Although the narratives of human evolution are often contentious, several discoveries since 2010 show that human evolution should not be seen as a simple linear or branched progression, but a mix of related species.[44][5][6][7] In fact, genomic research has shown that hybridization between substantially diverged lineages is the rule, not the exception, in human evolution.[4] Furthermore, it is argued that hybridization was an essential creative force in the emergence of modern humans.[4]
Stone tools are first attested around 2.6 million years ago, when hominins in Eastern Africa used so-called core tools, choppers made out of round cores that had been split by simple strikes.[290] This marks the beginning of the Paleolithic, or Old Stone Age; its end is taken to be the end of the last Ice Age, around 10,000 years ago. The Paleolithic is subdivided into the Lower Paleolithic (Early Stone Age), ending around 350,000–300,000 years ago, the Middle Paleolithic (Middle Stone Age), until 50,000–30,000 years ago, and the Upper Paleolithic, (Late Stone Age), 50,000–10,000 years ago.
Archaeologists working in the Great Rift Valley in Kenya have discovered the oldest known stone tools in the world. Dated to around 3.3 million years ago, the implements are some 700,000 years older than stone tools from Ethiopia that previously held this distinction.[193][291][292][293]
The period from 700,000 to 300,000 years ago is also known as the Acheulean, when H. ergaster (or erectus) made large stone hand axes out of flint and quartzite, at first quite rough (Early Acheulian), later "retouched" by additional, more-subtle strikes at the sides of the flakes. After 350,000 BP the more refined so-called Levallois technique was developed, a series of consecutive strikes, by which scrapers, slicers ("racloirs"), needles, and flattened needles were made.[290] Finally, after about 50,000 BP, ever more refined and specialized flint tools were made by the Neanderthals and the immigrant Cro-Magnons (knives, blades, skimmers). Bone tools were also made by H. sapiens in Africa by 90,000–70,000 years ago[200][294] and are also known from early H. sapiens sites in Eurasia by about 50,000 years ago.
This list is in chronological order across the table by genus. Some species/subspecies names are well-established, and some are less established – especially in genus Homo. Please see articles for more information.

The Dong Son culture, Dongsonian culture,[1][2] or the Lạc Việt culture (named for modern village Đông Sơn, a village in Thanh Hóa, Vietnam) was a Bronze Age culture in ancient Vietnam centred at the Red River Valley of northern Vietnam from 1000 BC until the first century AD.[3]: 207  Vietnamese historians attribute the culture to the states of Văn Lang and Âu Lạc. Its influence spread to other parts of Southeast Asia, including Maritime Southeast Asia, from about 1000 BC to 1 BC.[4][5][6]
The Dong Son people were skilled at cultivating rice, keeping water buffalos and pigs, fishing and sailing in long dugout canoes. They also were skilled bronze casters, which is evidenced by the Dong Son drum found widely throughout northern Vietnam and Guangxi in China.[7]
To the south of the Dong Son culture was the Sa Huỳnh culture of the proto-Chams.
The Dongsonians spoke either Austroasiatic[8][9][10][11] or Northern Tai languages;[12] or were Austroasiatic-speakers with significant contact and admixture with Tai-speakers.[13]
Archaeogenetics have demonstrated that before the Dong Son period, the Red River Delta's inhabitants were predominantly Austroasiatic. Genetic data from Vietnam's Phùng Nguyên culture's Mán Bạc burial site demonstrated close proximity to modern Austroasiatic speakers such as the Khmer and Mlabri;[14][15] meanwhile, "mixed genetics" from Đông Sơn culture's Núi Nấp site showed affinity to "Dai from China, Tai-Kadai speakers from Thailand, and Austroasiatic speakers from Vietnam, including the Việt".[16] One study states that the majority of Dongsonians have cranial features characterized by narrow long faces, relatively flat glabellas and nasal roots and round orbits due to extensive admixture with their northern neighbors, including those from southern China.[17] Another study states that Núi Nấp populations have affinities with the Dushan and Baojianshan and that they can be modeled as a mixture of Dushan-related (~65%) and northern East Asian-related (~35%) ancestry.[18]
Ferlus (2009) showed that the inventions of pestle, oar, and a pan to cook sticky rice, which is the main characteristic of the Đông Sơn culture, correspond to the creation of new lexicons for these inventions in Northern Vietic (Việt–Mường) and Central Vietic (Cuoi-Toum).[19] The new vocabularies to denote these inventions were proven to be derivatives from original verbs rather than borrowed lexical items. The current distribution of Northern Vietic also correspond to the area of Dong Son culture. Thus, Ferlus conclude that the Dongsonian culture was of Vietic origin and they were the direct ancestors of modern Vietnamese people.[19]
The origins of Dong Son culture may be traced back to ancient bronze castings. Scholars traditionally traced the origins of bronze-casting technology to China but during the 1970s archaeological discoveries in Isan, Thailand found that the casting of bronze either began in Southeast Asia first then spread into China, or that it developed the practise independently from China. The Dong Son bronze industry therefore has a local origin in Southeast Asia rather than being introduced by migrations out of China. The Gò Mun culture gave rise to the Dong Son culture; the Dong Son was the culmination of the Bronze Age and the opening stage of the Iron Age.[20]
The bronze drums were used for war, "the chief summons the warriors of the tribe by beating the drum", when mourning, and during feasts.  "The scenes cast onto the drums would inform us that the Dong Son leaders had access to bronze founders of remarkable skill." Lost-wax casting was based on Chinese founders, but the scenes are local, including drummers and other musicians, warriors, rice processing, birds, deer, war vessels, and geometric designs.[3]: 200–202
The bronze drums were made in significant proportions in northern Vietnam, Laos and parts of Yunnan. The Dong Son bronze drums exhibit "remarkable skill".  The Cổ Loa drum weighs 72 kilograms (159 lb) and would have required the smelting of between 1 and 7 tonnes (1.1 and 7.7 tons) of copper ore.[3]: 200
Displays of the Đông Sơn drum surface can be seen in some of Vietnam's cultural institutions.[21]
Some of Dong Son bronze daggers closely resemble Scytho-Siberian styles.[2]

The prehistory of North Africa spans the period of earliest human presence in the region to gradual onset of  historicity in the Maghreb during classical antiquity. Early anatomically modern humans are known to have been present at Jebel Irhoud, in what is now Morocco, approximately 300,000 years ago.[1] The Nile Valley region, via ancient Egypt, contributed to the Neolithic, Bronze Age and Iron Age periods of the Old World, along with the ancient Near East.
Human habitation in North Africa has been greatly influenced by the climate of the Sahara (currently the world's largest warm desert), which has undergone enormous variations between wet and dry over the last few hundred thousand years.[2] This is due to a 41,000-year Axial tilt cycle in which the tilt of the earth changes between 22° and 24.5°.[3] At present (2000 AD), we are in a dry period, but it is expected that the Sahara will become green again in 15,000 years (17,000 AD).
During the last glacial period, the Sahara was much larger than it is today, extending south beyond its current boundaries.[4] The end of the glacial period brought more rain to the Sahara, from about 8000 BCE to 6000 BCE, perhaps because of low pressure areas over the collapsing ice sheets to the north.[5] Once the ice sheets were gone, the northern Sahara dried out. In the southern Sahara, the drying trend was initially counteracted by the monsoon, which brought rain further north than it does today. By around 4200 BCE, however, the monsoon retreated south to approximately where it is today,[6] leading to the gradual desertification of the Sahara.[7] The Sahara is presently as dry as it was about 13,000 years ago.[2]
These conditions are responsible for what has been called the Sahara pump theory. During periods of a wet or "Green Sahara", the Sahara becomes a savanna grassland and various flora and fauna become more common. Following inter-pluvial arid periods, the Sahara area then reverts to desert conditions and the flora and fauna are forced to retreat northwards to the Atlas Mountains, southwards into West Africa, or eastwards into the Nile Valley. This separates populations of some of the species in areas with different climates, forcing them to adapt, possibly giving rise to allopatric speciation.[citation needed]
The earliest inhabitants of central North Africa have left behind significant remains: early remnants of hominid occupation in North Africa, for example, were found in Ain el Hanech, in Setif (c. 200,000 BCE); in fact, more recent investigations have found signs of Oldowan technology, which has been dated between 2,000,000 BCE and 1,470,000 BCE.[8]
Early anatomically modern humans are known to have been present at Jebel Irhoud, in what is now Morocco, approximately 300,000 years ago.[1]
Human groups of Nazlet Sabaha, Egypt engaged in chert mining, as early as ~100,000 years ago, likely for use as tools.[9]
In the Sahara, Aterians camped near lakes, rivers, and springs, and engaged in the activity of hunting (e.g., antelope, buffalo, elephant, rhinoceros) and some gathering.[10] As a result of a hyper-aridification event of Saharan Africa, which occurred around the time of Europe's Würm glaciation event, Aterian hunter-gatherers may have migrated into areas of tropical Africa and coastal Africa.[10] More specifically, amid aridification in MIS 5 and regional change of climate in  MIS 4, in the Sahara and the Sahel, Aterians may have migrated southward into West Africa (e.g.,  Baie du Levrier, Mauritania; Tiemassas, Senegal; Lower Senegal River Valley).[11]
Affad 23 is an archaeological site located in the Affad region of southern Dongola Reach in northern Sudan,[12] which hosts "the well-preserved remains of prehistoric camps (relics of the oldest open-air hut in the world) and diverse hunting and gathering loci some 50,000 years old".[13][14][15]
The Iberomaurusian culture seems to have appeared around the time of the Last Glacial Maximum, sometime between c. 25,000 cal BP and 23,000 cal BP. It will have lasted until the early Holocene, c. 11,000 cal BP.[16]
Archaeological evidence has attested that population settlements occurred in Nubia as early as the Late Pleistocene and from the 5th millennium BCE onwards, whereas there is "no or scanty evidence" of human presence in the Egyptian Nile Valley during these periods, which may be due to problems in site preservation.[17]
The Capsian culture was a Mesolithic and Neolithic culture of the Maghreb that persisted between 8000 BCE and 2700 BCE.[18][19]
The engraved Central Saharan rock art of the Bubaline Period was created between 10,000 BP and 7500 BP.[20]
The engraved Central Saharan rock art of the Kel Essuf Period was created prior to 9800 BP.[20]
The painted Central Saharan rock art of the Round Head Period was created between 9800 BP and 7500 BP.[20]
Laboratory examination of the Uan Muhuggiag child mummy and Tin Hanakaten child, concludes that the Central Saharan peoples from the Epipaleolithic, Mesolithic, and Pastoral periods possessed dark skin complexions.[21]
Neolithic agriculturalists, who may have resided in Northeast Africa and the Near East, may have been the source population for lactase persistence variants, including –13910*T, and may have been subsequently supplanted by later migrations of peoples.[22] The Sub-Saharan West African Fulani, the North African Tuareg, and European agriculturalists, who are descendants of these Neolithic agriculturalists, share the lactase persistence variant –13910*T.[22] While shared by Fulani and Tuareg herders, compared to the Tuareg variant, the Fulani variant of –13910*T has undergone a longer period of haplotype differentiation.[22] The Fulani lactase persistence variant –13910*T may have spread, along with cattle pastoralism, between 9686 BP and 7534 BP, possibly around 8500 BP; corroborating this timeframe for the Fulani, by at least 7500 BP, there is evidence of herders engaging in the act of milking in the Central Sahara.[22] The engraved and painted Central Saharan rock art of the Pastoral Period was created between 7500 BP and 2800 BP.[20] One of the earliest Libyco-Berber inscriptions in Africa are found in Wadi Mertoutek, near or within a petroglyph, which may be the depiction of a bovid, and may be associated with a pastoral community during a period of pastoralism.[23]
Human remains were found by archaeologists in 2000 at a site known as Gobero in the Ténéré Desert of northeastern Niger.[24][25] The Gobero finds represent a uniquely preserved record of human habitation and burials from what is now called the Kiffian (7700 BCE – 6200 BCE) and the Tenerian (5200 BCE – 2500 BCE) cultures.[24]
The classic account of the riparian lifestyle of this period comes from investigations in Sudan during World War II by British archeologist Anthony Arkell.[26] Arkell's report described a Late Stone Age settlement on a sandbank of the Blue Nile which was then about 12 feet (3.7 m) higher than its present flood stage.[26] The countryside was clearly savanna, not the present-day desert, as evidenced by the bones of the most common species found in the middens — antelope, which require large expanses of seed-bearing grasses.[26] These people probably lived mainly on fish, however, and Arkell concluded, based on the totality of the evidence, that rainfall at the time was at least three times that of today.[26] The physical characteristics derived from skeletal remains suggested that these people were related to modern Nilotic peoples, such as the Nuer and Dinka.[26] Subsequent radiocarbon dating firmly established Arkell's site to between 7000 BCE and 5000 BCE.[26] Based on common patterns at his site and at French-excavated sites already reported from Chad, Mali and Niger (e.g., bone harpoons and a characteristic "wavy line" pottery), Arkell inferred "a common fishing and hunting culture spread by negroid people right across Africa at about the latitude of Khartoum at a time when the climate was so different that it was not desert."[26] Hunter-fishers, who created the wavy line pottery in 6700 BCE, were black African rather than Mediterranean in origin and showed signs of intentional cultivation of grain crops instead of simply gathering wild grains.[27]
Several scholars have argued that the Northeast African origins of the Egyptian civilisation derived from pastoral communities which emerged in both the Egyptian and Sudanese regions of the Nile Valley in the 5th millennium BCE.[28]
According to American historian and linguist, Christopher Ehret, the physical anthropological findings from the “major burial sites of those founding locales of ancient Egypt in the fourth millennium BCE, notably El-Badari as well as Naqada, show no demographic indebtedness to the Levant”. Ehret specified that these studies revealed cranial and dental affinities with "closest parallels" to other longtime populations in the surrounding areas of Northeastern Africa “such as Nubia and the northern Horn of Africa”. He further commented that the Naqada and Badarian populations did not migrate “from somewhere else but were descendants of the long-term inhabitants of these portions of Africa going back many millennia”. Ehret also cited existing, archaeological, linguistic and genetic data which he argued supported the demographic history.[29]
Dotted wavy line pottery and fishing cultures have also been located in the Lake Turkana region in poorly dated contexts.[30] By 3000 BCE, it does not appear that the Turkana Basin was populated with harpoon and dotted wavy line pottery users, but fishing remained an important part of peoples' diets into the late Holocene.[30]
The engraved Central Saharan rock art of the Caballine Period was created between 2800 BP and 1000 BP.[20][31]
The engraved and painted Central Saharan rock art of the Cameline Period was created from 2000 BP onward.[20][31]
As of about 5000 BC, the populations of North Africa were descended primarily from the Iberomaurusian and Capsian cultures, with a more recent intrusion being associated with the Neolithic Revolution.[32] The proto-Berber tribes evolved from these prehistoric communities during the late Bronze- and early Iron ages.[33]
The late-Neolithic Kehf el Baroud inhabitants were modelled as being of about 50% local North African ancestry and 50% Early European Farmer (EEF) ancestry. It was suggested that EEF ancestry had entered North Africa through Cardial Ware colonists from Iberia sometime between 5000 and 3000 BC. They were found to be closely related to the Guanches of the Canary Islands.[34]
In Ancient Egypt, the Bronze Age begins in the Protodynastic period, c. 3150 BCE. The archaic Early Bronze Age of Egypt, known as the Early Dynastic Period of Egypt,[35][36] immediately follows the unification of Lower and Upper Egypt, c. 3100 BCE. It is generally taken to include the First and Second Dynasties, lasting from the Protodynastic Period of Egypt until about 2686 BCE, or the beginning of the Old Kingdom. With the First Dynasty, the capital moved from Abydos to Memphis with a unified Egypt ruled by an Egyptian god-king. Abydos remained the major holy land in the south. The hallmarks of ancient Egyptian civilization, such as art, architecture and many aspects of religion, took shape during the Early Dynastic Period. Memphis in the Early Bronze Age was the largest city of the time. The Old Kingdom of the regional Bronze Age[35] is the name given to the period in the 3rd millennium BCE when Egypt attained its first continuous peak of civilization in complexity and achievement – the first of three "Kingdom" periods, which mark the high points of civilization in the lower Nile Valley (the others being Middle Kingdom and the New Kingdom).
The Maghreb transferred from the Mesolithic stage to the Neolithic stage between the 6th millennium BCE and 5th millennium BCE, then entered an intermediary period between Neolithic, Chalcolithic and the Bronze Age probably in the 2nd millennium BCE,[37] although they never truly transferred into either the Chalcolithic Age or the Bronze Age, remaining in between them and the Neolithic Age.[38]
The Iron Age in Egypt corresponds to the Third Intermediate Period of Egypt. Iron metal is singularly scarce in collections of Egyptian antiquities. Bronze remained the primary material there until the conquest by Neo-Assyrian Empire in 671 BCE. The explanation of this would seem to be that the relics are in most cases the paraphernalia of tombs, the funeral vessels and vases, and iron being considered an impure metal by the ancient Egyptians it was never used in their manufacture of these or for any religious purposes. It was attributed to Seth, the spirit of evil who according to Egyptian tradition governed the central deserts of Africa.[39] In the Black Pyramid of Abusir, dating before 2000 BCE, Gaston Maspero found some pieces of iron. In the funeral text of Pepi I, the metal is mentioned.[39] A sword bearing the name of pharaoh Merneptah as well as a battle axe with an iron blade and gold-decorated bronze shaft were both found in the excavation of Ugarit.[40] A dagger with an iron blade found in Tutankhamun's tomb, 13th century BCE, was recently examined and found to be of meteoric origin.[41][42][43]
Iron-working Phoenician colonization along the coast and trade with the inland caused the Maghreb to rapidly transfer from this intermediary stage to the Iron Age.

Prehistoric Korea is the era of human existence in the Korean Peninsula for which written records do not exist. It nonetheless constitutes the greatest segment of the Korean past and is the major object of study in the disciplines of archaeology, geology, and palaeontology.
Geological prehistory is the most ancient part of Korea's past. The oldest rocks in Korea date to the Precambrian.[1][citation needed] The Yeoncheon System corresponds to the Precambrian and is distributed around Seoul extending out to Yeoncheon-gun in a northeasterly direction. It is divided into upper and lower parts and is composed of biotite-quartz-feldspar schist, marble, lime-silicate, quartzite, graphite schist, mica-quartz-feldspar schist, mica schist, quartzite, augen gneiss, and garnet-bearing granitic gneiss. The Korean Peninsula had an active geological prehistory through the Mesozoic, when many mountain ranges were formed, and slowly became more stable in the Cenozoic. Major Mesozoic formations include the Gyeongsang Supergroup, a series of geological episodes in which biotite granites, shales, sandstones, conglomerates andesite, basalt, rhyolite, and tuff that were laid down over most of present-day Gyeongsang-do Province.
The remainder of this article describes the human prehistory of the Korean Peninsula.
Historians in Korea use the three-age system to classify Korean prehistory. The three-age system was applied during the post-Imperial Japanese occupation period as a way to refute the claims of Imperial Japanese archaeologists who insisted that, unlike Japan, Korea had "no Bronze Age" and because Korea has always had an earlier documented start of civilization than Japan and Bronze Age Korea even influenced the formation of pre-Bronze Age Japan to Iron Age Japan.[2]
There are some problems with the three-age system applied to the situation in Korea. This terminology was created to describe prehistoric Europe, where sedentism, pottery and agriculture go together to characterize the Neolithic stage. The periodization scheme used by Korean archaeologists proposes that the Neolithic began in 8000 BC and lasted until 1500 BC. This is despite the fact that palaeoethnobotanical studies  indicate that the first bona fide cultivation did not begin until circa 3500 BC. The period of 8000 to 3500 BC corresponds to the Mesolithic cultural stage, dominated by hunting and gathering of both terrestrial and marine resources.[3]
Korean archaeologists traditionally (until the 1990s) used a date of 1500 or 1000 BC as the beginning of the Bronze Age. This is in spite of bronze technology not being adopted in the southern portion of the Korean Peninsula until circa 700 BC, and the  archaeological record  indicates that bronze objects were not used in relatively large numbers until after 400 BC.
This does leave Korea with a proper Bronze Age, albeit a relatively short one, as bronze metallurgy began to be replaced by ferrous metallurgy soon after it had become widespread.[4]
The origins of this period are an open question but the antiquity of hominid occupation in Korea may date to as early as 500,000 BC. Yi and Clark are somewhat skeptical of dating the earliest occupation to the Lower Palaeolithic.[5]
At Seokjang-ri, an archaeological site near Gongju, Chungcheongnam-do Province, artifacts that appear to have an affinity with Lower Paleolithic stone tools were unearthed in the lower levels of the site. Bifacial chopper or chopping-tools were also excavated. Hand axes and cleavers produced by men in later eras were also uncovered.
From Jeommal Cave a tool, possibly for hunting, made from the radius of a hominid was unearthed, along with hunting and food preparation tools of animal bones. The shells of nuts collected for nourishment were also uncovered.
In Seokjang-ri and in other riverine sites, stone tools were found with definite traces of Palaeolithic tradition, made of fine-grain rocks such as quartzite, porphyry, obsidian, chert,[citation needed] and felsite manifest Acheulian, Mousteroid, and Levalloisian characteristics.[citation needed] Those of the chopper tradition are simpler in shape and chipped from quartz and pegmatite. Seokjang-ri's middle layers showed that  humans hunted with these bola or missile stones.
During the Middle Paleolithic Period, humans dwelt in caves at the Jeommal Site near Jecheon and at the Durubong Site near Cheongju. From these two cave sites, fossil remains of rhinoceros, cave bear, brown bear, hyena and numerous deer (Pseudaxi gray var.), all extinct species, were excavated.
The earliest radiocarbon dates for the Paleolithic indicate the antiquity of occupation on the Korean peninsula is between 40,000 and 30,000 BC.[6]
From an interesting habitation site at Locality 1 at Seokjang-ri, excavators claim that they excavated some human hairs of Mongoloid origin along with limonitic and manganese pigments near and around a hearth, as well as animal figurines such as a dog, tortoise and bear made of rock. Reports claim that these were carbon dated to some 20,000 years ago.
A distinctive technology of the Korean later Palaeolithic is a type of flaked stone tool known as stemmed points. Korean foragers used stemmed points for hunting in more challenging environments and local temperatures gradually decreased during the introduction of stemmed points. Stemmed point use was associated with more residential and less mobile behaviors and the appearance of stemmed points was probably not related to population dynamics.[7]
The Palaeolithic ends when pottery production begins c. 8000 BC.
The earliest known Korean pottery dates back to c. 8000 BC or before. This pottery is known as Yunggimun pottery (ko:융기문토기) is found in much of the peninsula. Some examples of Yunggimun-era sites are Gosan-ri in Jeju Province and Ubong-ri in Greater Ulsan. Jeulmun or Comb-pattern pottery (즐문토기) is found after 7000 BC, and pottery with comb-patterns over the whole vessel is found concentrated at sites in west–central Korea between 3500 and 2000 BC, a time when a number of settlements such as Amsa-dong and Chitam-ni existed. Jeulmun pottery bears basic design and form similarities to that of the Russian Maritime Province, Mongolia, the Amur and Sungari River basins of Manchuria, the Baiyue of southeastern China and the Jōmon culture in Japan.[8]
The people of the Jeulmun practiced a broad spectrum economy of hunting, gathering, foraging, and small-scale cultivation of wild plants. It was during the Jeulmun that the cultivation of millet and rice was introduced to the Korean peninsula from the Asian continent.
Agricultural societies and the earliest forms of social-political complexity emerged in the Mumun pottery period (c 1500–300 BC). People in southern Korea adopted intensive dry-field and paddy-field agriculture with a multitude of crops in the Early Mumun Period (1500–850 BC). The first societies led by chiefs emerged in the Middle Mumun (850–550 BC), and the first ostentatious elite burials can be traced to the Late Mumun (c 550–300 BC). Bronze production began in the Middle Mumun and became increasingly important in Mumun ceremonial and political society after 700 BC. The Mumun is the first time that villages rose, became large, and then fell: some important examples include Songgung-ni, Daepyeong, and Igeum-dong. The increasing presence of long-distance exchange, an increase in local conflicts, and the introduction of bronze and iron metallurgy are trends denoting the end of the Mumun around 300 BC.
The Bronze Age reaches Korea beginning about 800 BC, via Chinese transmission.[9] Bronze metallurgy does not become widespread until the 4th century BC and soon gives way to the transition to ferrous metallurgy, complete by about the 1st century BC.
The transition from the Late Bronze to Early Iron Age in Korea begins in the 4th century BC. This corresponds to the later stage of Gojoseon, the Jin state period in the south, and the Proto–Three Kingdoms period of the 1st to 4th century AD.[10]
The period that begins after 300 BC can be described as 'protohistoric', a time when some documentary sources seem to describe societies in the Korean peninsula. The historical polities described in ancient texts such as the Samguk sagi are an example.
The historical period in Korea begins in the late 4th to mid 5th centuries, when as a result of the transmission of Buddhism, the Korean Three Kingdoms modified Chinese writing to produce the earliest records in Old Korean.
Ancient texts such as the Samguk sagi, Samguk yusa, Book of the Later Han, and others have sometimes been used to interpret segments of Korean prehistory. The most well-known version of the founding legend that relates the origins of the Korean ethnicity explains that a mythical "first emperor", Dangun, was born from the child of the creator deity's son and his union with a female bear in human form. Dangun built the first city.[11] A significant amount of historical inquiry in the twentieth century was devoted to the interpretation of the accounts of Gojoseon (2333–108 BC), Gija Joseon (1122–194 BC), Wiman Joseon (194–108 BC), and others mentioned in historical texts.

Tungusic peoples are an ethnolinguistic group formed by the speakers of Tungusic languages (or Manchu–Tungus languages). They are native to Siberia, China, and Mongolia.
The Tungusic language family is divided into two main branches, Northern (Ewenic–Udegheic) and Southern Tungusic (Jurchenic–Nanaic).
The name Tungusic is artificial, and properly refers just to the linguistic family (Tungusic languages). It is derived from Russian Tungus (Тунгус), a Russian exonym for the Evenks (Ewenki). English usage of Tungusic was introduced by Friedrich Max Müller in the 1850s, based on earlier use of German Tungusik by Heinrich Julius Klaproth. The alternative term Manchu–Tungus is also in use (Тунгусо-маньчжурские 'Tunguso-Manchurian').
It is generally suggested that the homeland of the Tungusic people is in northeastern Manchuria, somewhere near the Amur River region. Genetic evidence collected from the Ulchsky District suggests a date for the expansion predating 3500 BC.[5]
The Tungusic expansion into Siberia displaced the indigenous Siberian languages, which are now grouped under the term Paleosiberian. Several theories suggest that the Pannonian Avars of the Avar Khaganate in Central, East and Southeast Europe were of Tungusic origin or of partially Tungusic origin (as a ruling class).[6]
Tungusic people on the Amur river like Udeghe, Ulchi and Nanai adopted Chinese influences in their religion and clothing with Chinese dragons on ceremonial robes, scroll and spiral bird and monster mask designs, Chinese New Year, using silk and cotton, iron cooking pots, and heated homes from China.[7]
The Manchu originally came from Manchuria, which is now Northeast China and the Russian Far East. Following the Manchu establishment of the Qing dynasty in the 17th century, they have been almost completely assimilated into the language and culture of the ethnic Han population of China.
The southern Tungusic Manchu farming sedentary lifestyle was very different from the nomadic hunter gatherer forager lifestyle of their more northern Tungusic relatives like the Warka, which left the Qing state to attempt to make them sedentarize and farm like Manchus.[8][9]
During the 17th century, the Tsardom of Russia was expanding east across Siberia, and into Tungusic-speaking lands, resulting in early border skirmishes with the Qing dynasty of China, leading up to the 1689 Treaty of Nerchinsk. The first published description of a Tungusic people to reach beyond Russia into the rest of Europe was by the Dutch traveler Isaac Massa in 1612. He passed along information from Russian reports after his stay in Moscow.[10]
"Tungusic" (Manchu-Tungus) peoples are divided into two main branches: northern and southern.
The southern branch is dominated by the Manchu (historically Jurchen). Qing emperors were Manchu, and the Manchu group has largely been sinicized (the Manchu language being moribund, with 20 native speakers reported as of 2007[11]).
The Sibe were possibly a Tungusic-speaking section of the (Mongolic) Shiwei and have been conquered by the expanding Manchu (Jurchen). Their language is mutually intelligible with Manchu. The Nanai (Goldi) are also derived from the Jurchen. The Orok (Ulta) are an offshoot of the Nanai. Other minor groups closely related to the Nanai are the Ulch, Oroch and Udege. The Udege live in the Primorsky Krai and Khabarovsk Krai in the Russian Federation.
The northern branch is mostly formed by the closely related ethnic groups of Evenks (Ewenki) and Evens. (Evenks and Evens are also grouped as "Evenic". Their ethnonyms are only distinguished by a different suffix - -n for Even and -nkī for Evenkī; endonymically, they even use the same adjective for themselves - ǝwǝdī, meaning "Even" in the Even language and "Evenkī" in the Evenkī language.) The Evenks live in the Evenk Autonomous Okrug of Russia in addition to many parts of eastern Siberia, especially Sakha Republic. The Evens are very closely related to the Evenks by language and culture, and they likewise inhabit various parts of eastern Siberia. People who classify themselves as Evenks in the Russian census tend to live toward the west and toward the south of eastern Siberia, whereas people who classify themselves as Evens tend to live toward the east and toward the north of eastern Siberia, with some degree of overlap in the middle (notably, in certain parts of Sakha Republic). Minor ethnic groups also in the northern branch are the Negidals and the Oroqen. The Oroqen, Solon, and Khamnigan inhabit some parts of Heilongjiang Province, Inner Mongolia in China, and Mongolia and may be considered as subgroups of the Evenk ethnicity, though the Solons and the Khamnigans in particular have interacted closely with Mongolic peoples (Mongol, Daur, Buryat), and they are ethnographically quite distinct from the Evenks in Russia.
The Taz people are unique among Tungusic peoples for having a Sinitic dialect as their native language. They are the result of intermarriages between Han Chinese men and Udege, Nanai, and Oroch women in Outer Manchuria during the Qing dynasty.
Tungusic peoples are:
Most Tungusic peoples, along with Mongolic peoples, have a mixture of 2 ancestral sources. One source is derived from Neolithic Yellow River Basin farmers whilst the other sources are related to Hlai, Tibetan and Neolithic Amur River Basin people.[12]
Previous studies argued for a potential shared ancestry between Tungusic, Mongolic, Turkic, Koreanic, and Japonic populations via Neolithic agriculturalist societies from Northeast China (e.g. the Liao civilization) as a part of the hypothetical Altaic language family. However, recent data contradicts this because while West Liao River ancestry was found among the "macro-Altaic" Koreans and Japanese, it was absent among the "micro-Altaic" Tungusic and Mongolic populations.[13]
The Manchu, the largest Tungusic-speaking population, displays increased genetic affinity with Han Chinese, and Koreans, compared to with other Tungusic peoples. The Manchu were therefore an exception to the coherent genetic structure of Tungusic-speaking populations, likely due to the large-scale population migrations and genetic admixtures with the Han Chinese in the past few hundred years.[14]
Tungusic peoples display primarily paternal haplogroups associated with Ancient Northeast Asians, and display high affinity to Mongolic peoples as well as other Northeastern Asian populations. Their primarily haplogroup is associated with the C-M217 clade and its subclades. The other dominant haplogroup is Haplogroup N-M231, which was found in Neolithic Northeastern Asian societies along the Liao river and widespreaded throughout Siberia. An exception are modern Manchu people which display higher frequency of Haplogroup O-M122.[15][16][17][18][19][20][21] 29/97 = 29.9% C-M86 in a sample of Mongols from northwest Mongolia,[22][23][24]
The maternal haplogroups of Tungusic peoples are primarily shared with other Northern East Asians. Maternal haplogroup diversity seems to reflect some amount of gene flow with peoples living around the Sea of Okhotsk (Koryaks, Nivkhs, Ainus, etc.) on the one hand and peoples living in Central Asia (Iranian, Turkic, Mongolic peoples) on the other.[25][26]
According to a total of 29 sample from the mtDNA studies of Xibo, Oroqen, and Hezhen from China:
283 samples from a mtDNA study of Tungusic Evenks, Evens, and Udeges in Russia published in 2013, their main mtDNA haplogroups are :

Polygenism is a theory of human origins which posits the view that humans are of different origins (polygenesis). This view is opposite to the idea of monogenism, which posits a single origin of humanity. Modern scientific views find little merit in any polygenic model due to an increased understanding of speciation in a human context, with the monogenic "Out of Africa" hypothesis and its variants being the most widely accepted models for human origins.[1] Polygenism has historically been heavily used in service of white supremacist ideas and practices, denying a common origin between European and non-European peoples.[2][3] It can be distinguished between Biblical polygenism, describing a Pre-Adamite or Co-Adamite origin of certain races in the context of the Genesis narrative of Adam and Eve, and scientific polygenism, attempting to find a taxonomic basis for ideas of racial science.
Many oral traditions feature polygenesis in their creation stories. For example, Bambuti mythology and other creation stories from the pygmies of Congo state that the supreme God of the pygmies, Khonvoum, created three different races of humans separately out of three kinds of clay: one black, one white, and one red.[4]  In some cultures, polygenism in the creation narrative served an etiological function.  These narratives provided an explanation as to why other people groups exist who are not affiliated with their tribe.  Moreover, distinctions made between the creation of foreign people groups and the tribe or ethnic group to which the creation myth pertains served to reinforce tribal or ethnic unity, the need to exercise wariness and caution when dealing with outsiders, or the unique nature of the relationship between that tribe and the deities of their religious system.
An example may be found in the creation myth of the Asmat people, a hunter-gatherer tribe situated along the south-western coast of New Guinea.  This creation myth asserts that the Asmat themselves came into being when a deity placed carved wooden statues in a ceremonial house and began to beat a drum.  The statues became living humans and began to dance.  Some time later, a great crocodile attempted to attack this ceremonial house, but was defeated by the power of the deity.  The crocodile was cut into several pieces and these were tossed in different directions.  Each piece became one of the foreign tribes known to the Asmat.[5]
The idea is also found in some ancient Greek and Roman literature. For example, the Roman Emperor Julian the Apostate in his Letter to a Priest wrote that he believed Zeus made multiple creations of man and women.[6] In his Against the Galilaens Julian presented his reasoning for this belief. Julian had noticed that the Germanics and Scythians (northern nations) were different in their bodies (i.e. complexion and other traits) to the Ethiopians. He therefore could not imagine such difference in physical attributes as having originated from common ancestry, and so maintained separate creations for different races.
In early classical and medieval geography the idea of polygenism surfaced because of the suggested possibility of there being inhabitants of the antipodes (Antichthones). These inhabitants were considered by some to have separate origins because of their geographical extremity.[7]
The religion of the Ainu people claims that the ancestors of the Ainu people arrived on Earth from the skies separate from the other races. See Ainu creation myth.
In their respective fundamentalist or Orthodox sects, Jewish people, Christians, and Muslims have embraced monogenism in the form that all modern humans ultimately are descended from a single mating pair, named Adam and Eve.  In this context, polygenism described all alternative explanations for the origin of humankind that involved more than two individual "first people".  This definition of polygenism is still employed among some Creationists and within the Roman Catholic Church (see Humani generis).
With the development of the evolutionary paradigm of human origins, it has become widely held within the scientific community that at no point did there exist a single "first man" and a single "first woman" who constituted the first true humans to whom all lineages of modern humans ultimately converge and that if Adam and Eve ever existed as distinct historical persons, they were members of a much larger population of the same species.[8]  However, a common scientific explanation of human origins asserts that the population directly ancestral to all modern humans remained united as a single population by constant gene flow.  Therefore, on the level of the entire human population, this explanation of human origin is classified as monogenism.  All modern humans share the same origin from this single ancestral population.
Modern polygenists do not accept either theological or scientific monogenism.  They believe that the variation among human racial types cannot be accounted for by monogenism or by evolutionary processes occurring since the proposed recent African origin of modern humans.  Polygenists reject the argument that human races must belong to a single species because they can interbreed. There are several polygenist hypotheses, including biblical creationist polygenism and polygenist evolution.[9][10]
To make polygenism compatible with the Biblical account in the early chapters of the Book of Genesis, some argument is needed to the effect that what is in the Bible is incomplete. Three standard positions are:
In Christian terms, polygenesis remained an uncommon Biblical interpretation until the mid-19th century, and was largely considered heretical; however, it has been pointed out by some modern scholars that, while Pre-Adamism was strongly rejected by most and deemed heretical, Co-Adamism was not received with the same degree of hostility.[12]
A major reason for the emergence of Biblical polygenism from around the 18th century was because it became noted that the number of races could not have developed within the commonly-accepted Biblical timeframe. Francis Dobbs (1750–1811), an eccentric member of the Irish Parliament, believed in a different kind of biblical polygenism. In his Concise View from History written in 1800 he maintained that there was a race resulting from a clandestine affair between Eve and the Devil (see Serpent Seed).
Polygenism was heavily criticized in the early 20th century by the Roman Catholic Church, and especially by Pope Pius XII in the encyclical Humani generis (1950), on the grounds that polygenism is incompatible with the doctrine of Original Sin.
Pre-Adamism claims there were already races of humans living before the creation of Adam. It traces back to Isaac La Peyrère in the 17th century.
Co-Adamism claims that there was more than one Adam – small groups of men, created at the same time in different places across the Earth – and therefore that the different races were separately created. The idea of co-Adamism has been traced back as far as Paracelsus in 1520.[13] Other 16th century advocates of co-Adamism included Thomas Harriot and Walter Raleigh, who theorised a different origin for the Native Americans.[14]
In 1591 Giordano Bruno argued that because no one could imagine that the Jews and the Ethiopians had the same ancestry, then God must have either created separate Adams or Africans were the descendants of pre-Adamite races.[15]
An anonymous Biblical paper supporting co-Adamism was published in 1732 entitled Co-adamitae or an Essay to Prove the Two Following. Paradoxes, viz. I. That There Were Other Men Created at the Same time with Adam, and II. That the Angels did not fall.[16]
Henry Home, Lord Kames was a believer in co-Adamism.[17] Home believed God had created different races on Earth in separate regions. In his book Sketches on the History of Man in 1734 Home claimed that the environment, climate, or state of society could not account for racial differences, so that the races must have come from distinct, separate stocks.[18]
Charles White was another advocate of co-Adamism, although he used less theology to support his views.[19] White's Account of the Regular Gradation in Man in 1799, provided the empirical science for polygenism. White defended the theory of polygeny by refuting French naturalist Georges-Louis Leclerc, Comte de Buffon's interfertility argument the theory that only the same species can interbreed – pointing to species hybrids such as foxes, wolves and jackals, which were separate groups that were still able to interbreed.[20]
Charles Hamilton Smith, a naturalist from England, was a polygenist: he believed races had been created separately. He published the book The Natural History of the Human Species in 1848. In the book he maintained that there had always been three fundamentally distinct human types: the Caucasian, the Mongolian and the Negro. He also referred to the polygenist Samuel George Morton's work in America.[21] Samuel Kneeland wrote an 84-page introduction to the American edition of the book where he laid out evidence which supports polygenist creationism and that the Bible is entirely compatible with multiple Adams.[22]
John William Colenso, a theologian and biblical scholar, was a polygenist who believed in co-Adamism. Colenso pointed to monuments and artifacts in Egypt to debunk monogenist beliefs that all races came from the same stock. For example, Ancient Egyptian representations of races showed exactly how the races looked in his time. Egyptological evidence indicated the existence of remarkable permanent differences in the shape of the skull, bodily form, colour and physiognomy between different races which are difficult to reconcile with biblical monogenesis. Colenso believed that racial variation between races was so great, that there was no way in which all the races could have come from the same stock just a few thousand years ago. He was unconvinced that climate could change racial variation and also believed, in common with other biblical polygenists, that monogenists had interpreted the Bible wrongly.[23]
Colenso said: "It seems most probable that the human race, as it now exists, had really sprung from more than one pair". Colenso denied that polygenism caused any kind of racist attitudes or practices; like many other polygenists he claimed monogenesis was the cause of slavery and racism. Colenso claimed that each race had sprung from a different pair of parents, and that all races were created equal by God.[23]
Biblical polygenists such as Colenso, Louis Agassiz, Josiah Clark Nott and George Gliddon maintained that many of the races on Earth, such as Africans and Asians, were not featured in the Table of Nations in Genesis 10.
Nott argued that its authors' knowledge was limited to their own region, and that the Bible does not concern the whole of the Earth's population. According to Nott, there are no verses in the Bible which support monogenism; and that the only passage the monogenists use is Acts 17:26, where (he wrote) the monogenists are wrong in their interpretation of this verse because the "one blood" of Paul's sermon only includes the nations he knew existed, which were local.[24]
According to Lansdown (2006), "Polygenism, the concept of different human species, was heretical and 'atheistic'; it was embraced only by the most isolated and heterodox thinkers".[25] Atheist polygenism was most notably supported by Ephraim Squier (1821–1888).[26] In Europe in the 19th century the general public had favored polygenism, as many believed it contradicted the Genesis account and thus was more scientific than religious monogenism.[27]
The British atheist leader Charles Bradlaugh was also interested in the theory of polygenesis. He found it useful to undermine Genesis accounts of creation.[28]
Scientific polygenism is a set of hypotheses resulting from the use of the scientific method to attempt explanation of the differences in traits between humans who live in different regions. Over the course of many centuries, polygenistic hypotheses have been dismissed by more accurate scientific theories.
During the late 17th century and early 18th century many countries first began to encounter different races from other countries due to colonial expansion, discovery, overseas exploration and increases in trade routes. Because of the encounters with different races, many people could not believe that they had the same ancestry as other races, because of the extreme racial differences. Many explorers and scientists visited other countries to observe and study different races and write down their findings. Later they went back to their own countries to publish books and journals on their findings and claim that the evidence supported polygenism.[29][30]
Polygenists of the 18th century included Voltaire and David Hume.[31] Voltaire in his 1734 book Traité de métaphysique wrote that "whites ... Negroes ... the yellow races ... are not descended from the same man".[32] Voltaire brought the subject up in his Essay on the Manner and Spirit of Nations and on the Principal Occurrences in History in 1756 (which was an early work of comparative history). He believed each race had separate origins because they were so racially diverse. Voltaire found biblical monogenism laughable, as he expressed:
It is a serious question among them whether the Africans are descended from monkeys or whether the monkeys come from them. Our wise men have said that man was created in the image of God. Now here is a lovely image of the Divine Maker: a flat and black nose with little or hardly any intelligence. A time will doubtless come when these animals will know how to cultivate the land well, beautify their houses and gardens, and know the paths of the stars: one needs time for everything.[33]
When comparing Caucasians to those with dark skin, Voltaire claimed they were different species:
The negro race is a species of men different from ours as the breed of spaniels is from that of greyhounds. The mucous membrane, or network, which nature has spread between the muscles and the skin, is white in us and black or copper-colored in them.[34]
Historians have suggested that Voltaire's support for polygenism was shaped by his financial investments in French colonial companies, including the Compagnie des Indies.[35]
John Atkins, an English naval surgeon, was one of the earliest scientists to be a proponent of the polygenist theory. In his book A Voyage to Guinea (1723) he said "I am persuaded that the black and white race have sprung from different coloured parents."[32]
In the last two decades of the 18th century polygenism was advocated in England by historian Edward Long and anatomist Charles White, in Germany by ethnographers Christoph Meiners and Georg Forster, and in France by Julien-Joseph Virey.[36]
Polygenism was most widespread during the 19th century.[37] The racial studies of Georges Cuvier, the French naturalist and zoologist, influenced scientific polygenism and scientific racism. These theories proposed a graduation from 'civilisation' to 'barbarism', at once justifying the European acquisition of foreign territories and highlighting the belief in the singular role of the Europeans as a civilizing force.
Georges Cuvier, a French naturalist and zoologist, believed there were three distinct races: the Caucasian ("white"), the Mongolian ("yellow") and the Ethiopian ("black"). Cuvier claimed Adam and Eve were "Caucasian", forming the original race of mankind, while the other two races originated by survivors escaping in different directions after a major catastrophe hit the Earth 5,000 years ago, with those survivors then living in complete isolation from each other.[38][39] Each race received marks for the beauty or ugliness of their skull and quality of their civilizations. According to Cuvier the white race was at the top and the black race was at the bottom.[40]
Cuvier wrote regarding "Caucasians":
The white race, with oval face, straight hair and nose, to which the civilised people of Europe belong and which appear to us the most beautiful of all, is also superior to others by its genius, courage and activity.[41]
Regarding those he termed "Ethiopian", Cuvier wrote:
The Negro race ... is marked by a black complexion, crisped or woolly hair, compressed cranium, and a flat nose. The projection of the lower parts of the face, and the thick lips, evidently approximate it to the monkey tribe; the hordes of which it consists have always remained in the most complete state of utter barbarism.[42]
Cuvier's racial studies held the main features of polygenism, which are as follows:[43]
Scientific polygenism became popular in France in the 1820s in response to James Cowles Prichard's Researches into the Physical History of Man (1813) which was considered a pioneering work of monogenism.[44] An anthropological school advocating polygenism arose to counter Prichard's monogenism in France.[45] Key French polygenists of this period included the naturalist Jean Baptiste Bory de Saint-Vincent and Louis-Antoine Desmoulins (1796–1828), a student of Georges Cuvier.[46]
Anders Retzius, a Swedish professor of anatomy, was a polygenist. Retzius studied many different skull types, and because the skulls were so different he believed that the races had a separate origin.[47]
Polygenist schools arose in the 1830s and 1840s across Europe. The Scottish anatomist and zoologist Robert Knox spent the latter half of his career advocating for polygenism; he argued in his 1850 work The Races of Men that "[r]acial natures ... were unchanging over thousands of years, and were so different that they should be called different species".[48] A colleague of Knox, James Hunt, was also a promoter of polygenism. In 1863, he published an article titled On the Negro's Place in Nature which was posthumously dedicated to Knox. In the controversial article, Hunt supported both polygenism and slavery in the Confederacy.[49]
John Crawfurd, a Scottish physician and colonial administrator, was a polygenist. He studied the geography of where different races were located, and believed that different races had been created separately by God in specific regional zones for climatic circumstance.[50]
Charles Caldwell, born in 1772 and died in 1853, was one of the earliest supporters of polygenism in America. Caldwell attacked the position that environment was the cause of racial differences and argued instead that four races, Caucasian, Mongolian, American Indian, and African, were four different species, created separately by God.[51]
Charles Pickering (naturalist) was a librarian and curator of the Academy of Natural Sciences. In 1843, he traveled to Africa and India to research human races. In 1848, Pickering published Races of Man and Their Geographical Distribution, which enumerated eleven races.
Polygenism came into mainstream scientific thought in America in the mid 19th century with the work of several corresponding natural scientists such as Samuel George Morton and Charles Pickering as well as Egyptologist George Gliddon, the surgeon Josiah Clark Nott and more prominently the paleontologist and geologist Louis Agassiz in the United States. All had contributed to a major ethnological work of 738 pages entitled Types of Mankind which was published in 1854[52] and was a great success; it was followed by a sequel Indigenous Races of the Earth (1857). These works sparked the first formal Polygenist vs. Monogenist debates across America, and advocates of the polygenism school became known as pluralists. Since Louis Agassiz backed the pluralists, polygenism received mainstream public approval and wide exposure during the 1840s-1860s. Numerous articles promoting polygenist views were published in the American Journal of Science and Arts during this time period.[53]
The archeologist Ephraim George Squier helped Morton's polygenism by excavating an ancient cranium from the midwestern mounds and sending a drawing of it to Morton. Morton found its similarities striking to Central and South American crania, confirming his belief that the American Indian nations had a common and indigenous origin. Morton's polygenism explicitly stated the Mound Builders were an American Indian race of great antiquity, they did not migrate from Asia, and their physical form has remained essentially unchanged in their descendants.[54] Both Squier and Gliddon demonstrated for Morton the permanence of racial characteristics, and the suitability of each race to the region for which it had been created.
American Indians supported Morton's conclusions, whilst some white archaeologists supported Morton. Others such as William Pidgeon did not accept Morton's conclusions because at the time some white archaeologists such as Pidgeon could not believe that Native Americans had created the archaeological remains they saw around them; instead William Pidgeon wrote a book called Traditions of Dee-Coo-Dah and Antiquarian Researches in 1858.[55] In the book Pidgeon attempts to prove that a vanished race, culturally superior to and existing earlier than the American Indians, occupied America first and that The Mound Builders were not Native Americans. Pidgeon's book was revealed mostly to be a hoax. The famed archaeologist Theodore H. Lewis later revealed that Pidgeon had fabricated most of his research and distorted much of the rest of it, mapping mounds where none existed and changing the arrangement of existing mound groups to suit his needs.[56] Morton's work gained more support because his work was considered to be evidence of true objective science unlike others such as Pidgeon. Morton won his reputation as the great data-gatherer and objectivist of American Science. Oliver Wendell Holmes praised Morton for "the severe and cautious character" of his works, which "from their very nature are permanent data for all future students of ethnology".[57]
By 1850 Agassiz had developed a unique form of co-Adamism. God he believed had created several different zoological provinces with different races in them, but also fauna and animals specific to those regions. An essay of Agassiz promoting this theory with maps of the zoological zones was attached as a preface to Types of Mankind in collaboration with Morton, Gliddon, Nott and others.[58] Agassiz's theory developed some support amongst Christians, and he often wrote articles in Christian magazines claiming his views on polygenism were compatible with the bible.[59] Christian fundamentalists however who held to Young Earth Creationism and strict monogenism (i.e. everyone on Earth from Adam and Eve) attacked his views, as well as those of Gliddon and Nott.[60]
Unlike Josiah Nott, the slave-owner from Alabama,[61] Agassiz was never a supporter of slavery. He claimed his views had nothing to do with politics.[62]
The notion that races were separate and came together by hybridism, rather than being variations from a common stock, was cast into doubt with the publication of Darwin's Origin of Species in 1859, which Agassiz opposed till his death. Yet the influence of polygenism persisted for many years. For example, the Hamitic Hypothesis, which argued that certain African populations were the descendants of a proto-white invasion in the ancient past, was influenced by polygenism and continued to hold sway in linguistics and anthropology until the 1950s.[63] Darwin did not address man's origin directly at this stage, and the argument continued for a number of years, with the creation of the Anthropological Society of London in 1863 in the shadow of the American Civil War, in opposition to the abolitionist Ethnological Society. The Anthropologicals had the Confederate agent Henry Hotze permanently on their council.[64] The two societies did not heal their differences until they merged in 1871 to form the Anthropological Institute.
Georges Pouchet, the French naturalist and anatomist, was a polygenist. Pouchet made contributions in several scientific fields, and specialised in comparative anatomy of fishes and whales. He was a prime advocate of polygeny, and was the author of an anthropological work titled De la Pluralité des Races Humaines (1858), which was translated into English as The Plurality of the Human Race in 1864 by the Anthropological Society.
John Thurnam with Joseph Barnard Davis published a work in two volumes under the title of Crania Britannica in 1865, important for craniometry. Thurnam and Davis were both believers in polygenism, in the form that different races had been created separately. Davis was a collector of crania, and had over 1700 specimens.[65] Because of the racial differences of the crania, Davis and Thurnam believed that proofs of polygenism were to be found in studying the skull types of different races. Davis also wrote Thesaurus craniorum: catalogue of the skulls of the various races of man (1875).
Although it took many years, polygenism, which required species to be created in specific geographic locations and to remain immutable, has been almost entirely replaced among scientists by Darwin's theory of evolution from a common ancestor.  Persistent antagonism to Darwinian theory is today primarily a matter of religious or political viewpoint.[66]
At least as late as 1919, the Journal of the American Medical Association published articles that seriously engaged with the possibility that Black and White people might have had separate origins.[67]
Polygenist evolution is the belief that humans evolved independently from separate species of apes. This can be traced back to Carl Vogt in 1864. Polygenist evolution allowed polygenists to link each race to an altogether different ape. This was proposed by Hermann Klaatsch and F. G. Crookshank.[68]
Carl Vogt believed that the inhabitants of Africa were related to the ape, and were separate from those of European descent. In Chapter VII of his Lectures of man (1864) he compared both ethnicities, describing them as "two extreme human types". The difference between them, he claimed, are greater than those between two species of ape; and this proves the two are a separate species altogether.[69]
In an unusual blend of contemporary evolutionary thinking and pre-Adamism, the theistic evolutionist and geologist Alexander Winchell argued in his 1878 book Adamites and Preadamites for the pre-Adamic origins of the human race on the basis that Africans were too racially inferior to have developed from the Biblical Adam. Winchell also believed that the laws of evolution operated according to the will of God.[70]
Before Darwin published his theory of evolution and common descent in his Origin of Species (1859), scientific theories or models of Polygenism (such as Agassiz's) were strictly creationist. Even after Darwin's book was published, Agassiz still stuck to his scientific form of polygenist creationism and denounced the idea of evolution. However, by the late 19th century most scientific polygenists had abandoned Agassiz's creationism and began to embrace polygenist forms of evolution. This even included many students of Agassiz, including Nathaniel Shaler, who had studied under Agassiz at Harvard.[71] Shaler continued to believe in polygenism, but believed the different races evolved from different primates. The prominent French anthropologist Paul Broca by 1878 had also converted from creationist polygenism to accepting a form of polygenist evolution.[72]
In his work The Descent of Man (1871), Charles Darwin and some of his supporters argued for the monogenesis of the human species, seeing the common origin of all humans as essential for evolutionary theory. This is known as the single-origin hypothesis. Darwin even dedicated a chapter of his The Descent of Man to an attempt to refute the polygenists and support common ancestry of all races. Polygenist evolution views however continued into the early 20th century, and still found support amongst prominent scientists.
Alfred Russel Wallace was also an advocate of polygenist evolution, claiming that the physical differences in races must have occurred at such a remote time in the past before humans had any intelligence, when natural selection was still operative on human bodies.  The differentiation into separate races with distinct physical traits must have happened so soon after humans had just appeared on Earth that for all practical purposes all races had always been distinct.[73]
In contrast to most of Darwin's supporters, Ernst Haeckel put forward a doctrine of evolutionary polygenism based on the ideas of the linguist and polygenist August Schleicher, in which several different language groups had arisen separately from speechless prehuman Urmenschen, which themselves had evolved from simian ancestors. These separate languages had completed the transition from animals to man, and, under the influence of each main branch of languages, humans had evolved as separate species, which could be subdivided into races. Haeckel divided human beings into ten races, of which the Caucasian was the highest and the primitives were doomed to extinction.[74]
Haeckel claimed that Africans have stronger and more freely movable toes than any other race, which he interpreted as evidence that members of the race were closely related to apes. Reasoning that apes use their toes to cling tightly to tree branches, Haeckel compared Africans to "four-handed" apes. Haeckel also believed that Blacks were savages and that Whites were significantly more civilized.[69]

Polynesians are an ethnolinguistic group comprising closely related ethnic groups native to Polynesia, which encompasses the islands within the Polynesian Triangle in the Pacific Ocean. They trace their early prehistoric origins to Island Southeast Asia and are part of the larger Austronesian ethnolinguistic group, with an Urheimat in Taiwan. They speak the Polynesian languages, a branch of the Oceanic subfamily within the Austronesian language family. The Indigenous Māori people form the largest Polynesian population,[9] followed by Samoans, Native Hawaiians, Tahitians, Tongans, and Cook Islands Māori.[citation needed]
As of 2012[update], there were an estimated 2 million ethnic Polynesians (both full and part) worldwide. The vast majority either inhabit independent Polynesian nation-states (Samoa, Niue, Cook Islands, Tonga, and Tuvalu) or form minorities in countries such as Australia, Chile (Easter Island), New Zealand, France (French Polynesia and Wallis and Futuna), and the United States (Hawaii and American Samoa), as well as in the British Overseas Territory of the Pitcairn Islands. New Zealand had the highest population of Polynesians, estimated at 110,000 in the 18th century.[10]
Polynesians have acquired a reputation as great navigators, with their canoes reaching the most remote corners of the Pacific and allowing the settlement of islands as far apart as Hawaii, Rapanui (Easter Island), and Aotearoa (New Zealand).[11] The people of Polynesia accomplished this voyaging using ancient navigation skills, including reading stars, currents, clouds, and bird movements—skills that have been passed down through successive generations to the present day.[12]
Polynesians, including Samoans, Tongans, Niueans, Cook Islands Māori, Tahitian Mā'ohi, Hawaiian Māoli, Marquesans, and New Zealand Māori, are a subset of the Austronesian peoples. They share the same origins as the indigenous peoples of Taiwan, Maritime Southeast Asia, Micronesia, and Madagascar.[13] This is supported by genetic,[14] linguistic[15] and archaeological evidence.[16]
There are multiple hypotheses regarding the ultimate origin and mode of dispersal of the Austronesian peoples, but the most widely accepted theory is that modern Austronesians originated from migrations out of Taiwan between 3000 and 1000 BC. Using relatively advanced maritime innovations such as the catamaran, outrigger boats, and crab claw sails, they rapidly colonized the islands of both the Indian and Pacific oceans. They were the first humans to cross vast distances of water on ocean-going boats.[18] Despite the popularity of rejected hypotheses, such as Thor Heyerdahl's belief that Polynesians are descendants of "bearded white men" who sailed on primitive rafts from South America,[19][20] Polynesians are believed to have originated from a branch of the Austronesian migrations in Island Melanesia.
The direct ancestors of the Polynesians are believed to be the Neolithic Lapita culture. This group emerged in Island Melanesia and Micronesia around 1500 BC from a convergence of Austronesian migration waves, originating from both Island Southeast Asia to the west and an earlier Austronesian migration to Micronesia to the north. The culture was distinguished by dentate-stamped pottery. However, their eastward expansion halted when they reached the western Polynesian islands of Fiji, Samoa, and Tonga by around 900 BC. This remained the furthest extent of the Austronesian expansion in the Pacific for approximately 1,500 years, during which the Lapita culture in these islands abruptly lost the technology of pottery-making for unknown reasons. They resumed their eastward migrations around 700 AD, spreading to the Cook Islands, French Polynesia, and the Marquesas. From here, they expanded further to Hawaii by 900 AD, Easter Island by 1000 AD, and finally New Zealand by 1200 AD.[21][22]
Analysis by Kayser et al. (2008) found that only 21% of the Polynesian autosomal gene pool is of Australo-Melanesian origin, with the remaining 79% being of Austronesian origin.[23] Another study by Friedlaender et al. (2008) also confirmed that some Polynesians are genetically closer to Micronesians, Taiwanese Aborigines, and Islander Southeast Asians. The study concluded that Polynesians moved through Melanesia fairly rapidly, allowing only limited admixture between Austronesians and Papuans.[24] Polynesians predominantly belong to Haplogroup B (mtDNA), particularly to mtDNA B4a1a1 (the Polynesian motif). The high frequencies of mtDNA B4 in Polynesians are the result of genetic drift and represent the descendants of a few Austronesian females who mixed with Papuan males.[25] The Polynesian population experienced a founder effect and genetic drift due to the small number of ancestors.[26][27]  As a result of the founder effect, Polynesians are distinctively different both genotypically and phenotypically from the parent population, due to the establishment of a new population by a very small number of individuals from a larger population, which also causes a loss of genetic variation.[28][29]
Soares et al. (2008) argued for an older pre-Holocene Sundaland origin in Island Southeast Asia (ISEA) based on mitochondrial DNA.[30] The "out of Taiwan" model was challenged by a study from Leeds University published in Molecular Biology and Evolution. Examination of mitochondrial DNA lineages indicates that they have been evolving in ISEA for longer than previously believed. Ancestors of the Polynesians arrived in the Bismarck Archipelago of Papua New Guinea at least 6,000 to 8,000 years ago.[31]
A 2014 study by Lipson et al., using whole genome data, supports the findings of Kayser et al. Modern Polynesians were shown to have lower levels of admixture with Australo-Melanesians than Austronesians in Island Melanesia. Nonetheless, both groups show admixture, along with other Austronesian populations outside of Taiwan, indicating varying degrees of intermarriage between the incoming Neolithic Austronesian settlers and the preexisting Paleolithic Australo-Melanesian populations of Island Southeast Asia and Melanesia.[32][33][34]
Studies from 2016 and 2017 also support the idea that the earliest Lapita settlers mostly bypassed New Guinea, coming directly from Taiwan or the northern Philippines. The intermarriage and admixture with Australo-Melanesian Papuans evident in the genetics of modern Polynesians (as well as Islander Melanesians) occurred after the settlement of Tonga and Vanuatu.[35][36][37]
A 2020 study found that Polynesians and the Indigenous peoples of South America came in contact around 1200, centuries before Europeans interacted with either group.[38][39]
There are an estimated 3.2 million ethnic Polynesians and many of partial Polynesian descent worldwide, the majority of whom live in Polynesia, the United States, Australia, and New Zealand.[40] The Polynesian peoples are listed below in their distinctive ethnic and cultural groupings, with estimates of the larger groups provided:
Polynesia:
Polynesian outliers:

Peter Stafford Bellwood (born Leicester, England, 1943) is Emeritus Professor of Archaeology in the School of Archaeology and Anthropology at the Australian National University (ANU) in Canberra.[1] He is well known for his Out of Taiwan model regarding the spread of Austronesian languages.[2]
Peter Bellwood received his BA and PhD from King's College, Cambridge, in 1966 and 1980 respectively. His areas of specialization include the  human population history of Southeast Asia and the Pacific from archaeological, linguistic and biological perspectives; the worldwide origins of agriculture and resulting cultural, linguistic and biological developments; and the prehistory of human migration. He is currently[update] researching with Philip J. Piper and Lam My Dzung on an archaeological fieldwork project, funded by the Australian Research Council, on Neolithic sites in Vietnam.[3][4]
Professor Bellwood was the Secretary-General of the Indo-Pacific Prehistory Association (1990 to 2009) and was formerly the Editor of the Bulletin of the Indo-Pacific Prehistory Association (now the Journal of Indo-Pacific Archaeology).[4]
His books have been translated into French, Greek, Russian, Turkish, Chinese, Japanese, Vietnamese and Indonesian. Further translations are in progress into Chinese (Complex and Simplified).
Bellwood is a member of the Editorial Advisory Board of the archaeology journal Antiquity.[5]
Peter Bellwood is a Fellow of the Australian Academy of the Humanities, a Corresponding Fellow of the British Academy, and an Honorary Fellow of the Associazione Internationale di Studi sul Mediterraneo e l'Oriente (Rome).[3]
In July 2021 Peter Bellwood won the International Cosmos Prize in Osaka, Japan, being the first Australian recipient.[6]

Prehistoric music (previously called primitive music) is a term in the history of music for all music produced in preliterate cultures (prehistory), beginning somewhere in very late geological history. Prehistoric music is followed by ancient music in different parts of the world, but still exists in isolated areas. However, it is more common to refer to the "prehistoric" music which still survives as folk, indigenous or traditional music. Prehistoric music is studied alongside other periods within music archaeology.[citation needed]
Findings from Paleolithic archaeology sites suggest that prehistoric people used carving and piercing tools to create instruments. Archeologists have found Paleolithic flutes carved from bones in which lateral holes have been pierced. The disputed Divje Babe flute, a perforated cave bear femur, is at least 40,000 years old. Instruments such as the seven-holed flute and various types of stringed instruments, such as the Ravanahatha, have been recovered from the Indus Valley civilization archaeological sites.[1] India has one of the oldest musical traditions in the world—references to Indian classical music (marga) are found in the Vedas, ancient Hindu scriptures.[2]
Many languages traditionally have terms for music that include dance, religion, or cult.[citation needed] The context in which prehistoric music took place has also become a subject of study and debate, as the sound made by music in prehistory would have been somewhat different depending on the acoustics present. Some cultures include sound mimesis within their music; often, this feature is related to shamanistic beliefs or practice.[3][4] It may also serve entertainment[5][6] or practical functions, for example in hunting scenarios.[5]
It is likely that the first musical instrument was the human voice itself, which can make a vast array of sounds, from singing, humming and whistling through to clicking, coughing and yawning.[7] The oldest known Neanderthal hyoid bone with the modern human form has been dated to be 60,000 years old,[8] predating the oldest known Paleolithic bone flute by some 20,000 years,[9] but the true chronology may date back much further.
Theoretically, music may have existed prior to the Paleolithic era. Anthropological and archaeological research suggest that music first arose when stone tools first began to be used by hominins.[citation needed] The noises produced by work, such as pounding seed and roots into a meal, are a likely source of rhythm created by early humans. The first rhythm instruments or percussion instruments most likely involved the clapping of hands, stones hit together, or other things that are useful to create rhythm. There are bone flutes and pipes which are unambiguously paleolithic. Additionally, pierced phalanges (usually interpreted as "phalangeal whistles"), bullroarers, and rasps have also been discovered. The latter musical finds date back as far as the Paleolithic era, although there is some ambiguity over archaeological finds which can be variously interpreted as either musical or non-musical instruments/tools.[10]
Another possible origin of music is motherese, the vocal-gestural communication between mothers and infants. This form of communication involves melodic, rhythmic and movement patterns as well as the communication of intention and meaning, and in this sense is similar to music.[11]
Geoffrey Miller suggests musical displays play a role in "demonstrating fitness to mate." Based on the ideas of honest signal and the handicap principle, Miller suggested that music and dancing, as energetically costly activities, demonstrated the physical and psychological prowess of the singing and dancing individual.[12] Similarly, communal singing occurs among both sexes in cooperatively breeding songbirds of Australia and Africa, such as magpies[13] and white-browed sparrow-weavers.[14]
The field of archaeoacoustics uses acoustic techniques to explore prehistoric sounds, soundscapes, and instruments; it has included the study of ringing rocks and lithophones, of the acoustics of ritual sites such as chamber tombs and stone circles, and the exploration of prehistoric instruments using acoustic testing. Such work has included acoustic field tests to capture and analyze the impulse response of archaeological sites; acoustic tests of lithophones or 'rock gongs'; and reconstructions of soundscapes as experimental archaeology.
In prehistoric Egypt, music and chanting were commonly used in magic and rituals. The ancient Egyptians credited the goddess Bat with the invention of music. The cult of Bat was eventually syncretised into that of Hathor because both were depicted as cows. Hathor's music was believed to have been used by Osiris as part of his effort to civilise the world. The lion-goddess Bastet was also considered a goddess of music. Rhythms during this time were unvaried and music served to create rhythm. Small shells were used as whistles.[15]: 26–30  During the predynastic period of Egyptian history, funerary chants continued to play an important role in Egyptian religion and were accompanied by clappers or a flute. Despite the lack of physical evidence in some cases, Egyptologists theorise that the development of certain instruments known of the Old Kingdom period, such as the end-blown flute, took place during this time.[15]: 33–34
Excavations in 1969 found a 90-115,000 year old bone flute fragment in the Haua Fteah cave in Libya. It has one manmade punctured hole, which resembles similar bone flutes found in Europe and the Mediterranean. The exact species the bone comes from is unknown, but it seems to have come from a large bird.[16]
The peoples of Southern Africa in the South Africa, Zimbabwe, and Zambia region used bone, clay, and metal for creating instruments, as idiophones and aerophones were the two types of instruments that were made. Spinning disks, bone tubes, and a bullroarer were found in the Southern and Western Capes of South Africa that date back from 2525±85 BP - 1732 AD. There were also many more bone tubes found in the Matjes River which may have been used for flutes, trumpets, whistles, bells, and mbira keys.[17] Numerous mbira keys were found in Zimbabwe that date back to 210±90 BP - Later Iron Age.[17]
In 1986, several gudi (lit. "bone flutes") were found in Jiahu in Henan Province, China. They date to about 7000 BCE. They have between six and nine holes each and were made from the hollow bones of the red-crowned crane. At the time of  discovery, one was found to be still playable. This playable bone flute is capable of using both the five- or seven-note Xia Zhi scale and the six-note Qing Shang scale of the ancient Chinese musical system.[18]
India has the oldest musical traditions in the world. References to Indian classical music (marga) are found in the Vedas, ancient scriptures of the Hindu tradition.[2] Instruments such as the seven-holed flute and various types of stringed instruments have been recovered from the Indus Valley Civilisation archaeological sites.[19]
The peoples of Israel had prehistoric bones that were specifically aerophones. Several of these bones were excavated at Eynan-Mallaha and date back to 10,730 and 9760 cal BC. Smaller bird bones were preferred to bigger ones due to the difference in sound, although they are more difficult to play as a result of their size.[20] The pitch of the tone the flutes produce are believed to mimic the call of several birds. It is likely that the flute was used for music and dance rather than hunting, since it is limited by the small range of birds imitated. It is common for birds to be used as an inspiration for music such as the Sun Dance of the Plains Indians in which dancers used whistles to mimic eagles, or the Kaluli people who wore rainforest birds' feathers as ornaments.[20]
Two deer antlers were discovered in the Go O Chua site of southern Vietnam which were used as stringed instruments, they are dated to be at minimum 2,000 years old. One discovered in 1997, and the other in 2008. The instrument has a single string which was attached on both ends of the antler, with the burr of the antler forming a bridge.[21] The instrument is similar in form to a Đàn brố, or a K'ni. These are the first stringed instruments archaeologically discovered in Vietnam.[21]
Several lithophones were also found across the country which would have been laid down on strings with wooden or bamboo frames and struck to make noise.[21]
Australian Aboriginal and Torres Strait Islander music includes the music of Aboriginal Australians and Torres Strait Islanders. Music has formed an integral part of the social, cultural and ceremonial observances of these people, down through the millennia of their individual and collective histories to the present day, and has existed for 40,000 years.[22][23][24][25] The traditional forms include many aspects of performance and musical instrumentation which are unique to particular regions or Indigenous Australian groups; there are equally elements of musical tradition which are common or widespread through much of the Australian continent, and even beyond. The culture of the Torres Strait Islanders is related to that of adjacent parts of New Guinea and so their music is also related. Music is a vital part of Indigenous Australians' cultural maintenance.[26]
A didgeridoo is a type of musical instrument that, according to western musicological classification, falls into the category of aerophone. It is one of the oldest instruments to date. It consists of a long tube, without finger holes, through which the player blows. It is sometimes fitted with a mouthpiece of beeswax. Didgeridoos are traditionally made of eucalyptus, but contemporary materials such as PVC piping are used. In traditional situations it is played only by men, usually as an accompaniment to ceremonial or recreational singing, or, much more rarely, as a solo instrument. Skilled players use the technique of circular breathing to achieve a continuous sound, and also employ techniques for inducing multiple harmonic resonances. Traditionally the instrument was not widespread around the country, but was only used by Aboriginal groups in the most northerly areas.[citation needed]
A clapstick is a type of musical instrument that, according to western musicological classification, falls into the category of percussion. Unlike drumsticks, which are generally used to strike a drum, clapsticks are intended for striking one stick on another, and people as well. They are of oval shape with paintings of snakes, lizards, birds and more.
Used as a hand-held free reed instrument.
A bullroarer consists of a weighted airfoil (a rectangular thin slat of wood about 15 cm (6 in) to 60 cm (24 in) long and about 1.25 cm (0.5 in) to 5 cm (2 in) wide) attached to a long cord. Typically, the wood slat is trimmed down to a sharp edge around the edges, and serrations along the length of the wooden slat may or may not be used, depending on the cultural traditions of the region in question.
The cord is given a slight initial twist, and the roarer is then swung in a large circle in a horizontal plane, or in a smaller circle in a vertical plane. The aerodynamics of the roarer will keep it spinning about its axis even after the initial twist has unwound. The cord winds fully first in one direction and then the other, alternating.
It makes a characteristic roaring vibrato sound with notable sound modulations occurring from the rotation of the roarer along its longitudinal axis, and the choice of whether a shorter or longer length of cord is used to spin the bullroarer. By modifying the expansiveness of its circuit and the speed given it, and by changing the plane in which the bullroarer is whirled from horizontal to vertical or vice versa, the modulation of the sound produced can be controlled, making the coding of information possible.
The low-frequency component of the sound travels extremely long distances, clearly audible over many miles on a quiet night.
The use of bullroarers has also been documented in ancient Greece, Britain, Ireland, Scandinavia, Mali, New Zealand, and the Americas (see Bullroarer).  Banks Island Eskimos were still using bullroarers circa 1963 (59-year-old "Susie" being documented scaring off four polar bears armed with only three seal hooks and vocals.[27]  Aleut, Eskimo and Inuit used bullroarers occasionally as a children's toy or musical instruments, but preferred drums and rattles.[28]
Clay bells were found in Austria and Hungary which date to the early neolithic period. One is from the Starčevo site in Gellénháza, Hungary, and the other is from the Brunn site located on the outskirts of Vienna which was excavated in 1999. Unlike modern bells these bells lack a clapper. They were suspended by string and most likely struck with wooden sticks or animal bones.[29] Both bells were recreated and played, but neither were loud enough to be used as instruments, which might be why they were destroyed and thrown away.[29]
A one-of-a-kind Upper Paleolithic era Seashell Horn was discovered in the Marsoulas cave in 1931, which is made of a Charonia lampus shell. Dating back to the early Magdalenian period, it was modified to be played as a wind instrument by blowing air through the mouthpiece located at the apex. There are engravings on the inside of the lip, while unclear what the engravings represent, it is clear that they were intentional.[30]
In 2008, archaeologists discovered a bone flute in the Hohle Fels cave near Ulm, Germany.[31][32][33] The five-holed flute has a V-shaped mouthpiece and is made from a vulture wing bone. The researchers involved in the discovery officially published their findings in the journal Nature in June 2009. It is one of several similar instruments found in the area, which date to around 42,000 years ago, making this the oldest confirmed finds of any musical instruments in history.[34] The Hohle Fels flute was found next to the Venus of Hohle Fels and a short distance from the oldest known human carving.[35] On announcing the discovery, scientists suggested that the "finds demonstrate the presence of a well-established musical tradition at the time when modern humans colonized Europe".[31] Scientists have also suggested that the discovery of the flute may help to explain why early humans survived, while Neanderthals became extinct.[34]
On the island of Keros (Κέρος), two marble statues from the late Neolithic culture called Early Cycladic culture (2900–2000 BCE) were discovered together in a single grave in the 19th century. They depict a standing double flute player and a sitting musician playing a triangular-shaped lyre or harp. The harpist is approximately 23 cm (9 in) high and dates to around 2700–2500 BCE. He expresses concentration and intense feelings and tilts his head up to the light. The meaning of these and many other figures is not known; perhaps they were used to ward off evil spirits, had religious significance, served as toys, or depicted figures from mythology.
The oldest known wooden pipes were discovered in Wicklow, Ireland, in the winter of 2003, carbon-dated at around 2167±30 BCE. A wood-lined pit contained a group of six flutes made from yew wood, between 30 and 50 cm (12 and 20 in) long, tapered at one end, but without any finger holes. They may once have been strapped together.[36]
A clay egg-shaped rattle, bottle-shaped rattles, and pan pipes made of bone were all discovered in Slovakia. They are dated back to 300-800 AD, during the Migration Period. Music culture in Slovakia had not formed until the 9th century while these instruments go back to 4-6th century AD, so while they cannot be connected to Slovak culture they prove that music had existed in this region at that time.[37] They may have been used for ceremonies, rituals, or cults for dancing and singing to ward off evil spirits or call to the gods for help.[37]
The oldest flute ever discovered may be the so-called Divje Babe flute, found in the Cerkno Hills, Slovenia in 1995, though this is disputed.[38] The item in question is a fragment of the femur of a juvenile cave bear, and has been dated to about 43,000 years ago.[39][40] However, whether it is truly a musical instrument or simply a carnivore-chewed bone is a matter of ongoing debate.[38] In 2012, some flutes that were discovered years earlier in the Geißenklösterle cave received a new high-resolution carbon-dating examination yielding an age of 42,000 to 43,000 years.[41]
For thousands of years, Canada has been inhabited by Indigenous Peoples from a variety of different cultures and of several major linguistic groupings. Each of the Indigenous communities had (and have) their own unique musical traditions. Chanting – singing is widely popular, with  many of its performers also using a variety of musical instruments.[42] They used the materials at hand to make their instruments for thousands of years before Europeans immigrated to the new world.[43] They made gourds and animal horns into rattles which were elaborately carved and beautifully painted.[44] In woodland areas, they made horns of birchbark along with drumsticks of carved antlers and wood.[43] Drums were generally made of carved wood and animal hides.[45] These musical instruments provide the background for songs and dances.[45]

Prehistoric technology is technology that predates recorded history. History is the study of the past using written records. Anything prior to the first written accounts of history is prehistoric, including earlier technologies. About 2.5 million years before writing was developed, technology began with the earliest hominids who used stone tools, which they first used to hunt food, and later to cook.
There are several factors that made the evolution of prehistoric technology possible or necessary. One of the key factors is behavioral modernity of the highly developed brain of Homo sapiens  capable of abstract reasoning, language, introspection, and problem-solving. The advent of agriculture resulted in lifestyle changes from nomadic lifestyles to ones lived in homes, with domesticated animals, and land farmed using more varied and sophisticated tools. Art, architecture, music and religion evolved over the course of the prehistoric periods.
The Stone Age is a broad prehistoric period during which stone was widely used in the manufacture of implements with a sharp edge, a point, or a percussion surface. The period lasted roughly 2.5 million years, from the time of early hominids to Homo sapiens in the later Pleistocene era, and largely ended between 6000 and 2000 BCE with the advent of metalworking.[1]
The Stone Age lifestyle was that of hunter-gatherers who traveled to hunt game and gather wild plants, with minimal changes in technology. As the last glacial period of the current ice age neared its end (about 12,500 years ago), large animals like the mammoth and bison antiquus became extinct and the climate changed.  Humans adapted by maximizing the resources in local environments, gathering and eating a wider range of wild plants and hunting or catching smaller game.  Domestication of plants and animals with early stages in the Old World (Afro-Eurasia) Mesolithic and New World (Americas) Archaic periods led to significant changes and reliance on agriculture in the Old World Neolithic and New World Formative stage.  The agricultural life led to more settled existences and significant technological advancements.[2][nb 1]
Although Paleolithic cultures left no written records, the shift from nomadic life to settlement and agriculture can be inferred from a range of archaeological evidence. Such evidence includes ancient tools,[3] cave paintings, and other prehistoric art, such as the Venus of Willendorf. Human remains also provide direct evidence, both through the examination of bones, and the study of mummies. Though concrete evidence is limited, scientists and historians have been able to form significant inferences about the lifestyle and culture of various prehistoric peoples, and the role technology played in their lives.[citation needed]
The Lower Paleolithic period was the earliest subdivision of the Paleolithic or Old Stone Age. It spans the time from around 2.5 million years ago when the first evidence of craft and use of stone tools by hominids appears in the current archaeological record, until around 300,000 years ago, spanning the Oldowan ("mode 1") and Acheulean ("mode 2") lithic technology.[citation needed]
Early humans (hominids) used stone tool technology, such as a hand axe that was similar to that used by primates, which are found to have intelligence levels of modern children aged 3 to 5 years. Intelligence and use of technology did not change much for millions of years. The first "Homo" species began with Homo habilis about 2.4 to 1.5 million years ago.[4] Homo habilis ("handy man') created stone tools called Oldowan tools.[5][6][7] Homo ergaster lived in eastern and southern Africa about 2.5 to 1.7 million years ago and used more diverse and sophisticated stone tools than its predecessor, Homo habilis, including having refined the inherited Oldowan tools and developed the first Acheulean bifacial axes.[8]
Homo erectus ("upright man") lived about 1.8 to 1.3 million years ago in West Asia and Africa and is thought to be the first hominid to hunt in coordinated groups, use complex tools, and care for infirm or weaker companions.[9][10] Homo antecessor the earliest hominid in Northern Europe lived from 1.2 million to 800,000 years ago and used stone tools.[11][12] Homo heidelbergensis lived between 600,000 and 400,000  years ago and used stone tool technology similar the Acheulean tools used by Homo erectus.[13]
European and Asian sites dating back 1.5 million years ago seem to indicate controlled use of fire by Homo erectus.  A northern Israel site from about 690,000 to 790,000 years ago suggests that man could light fires.[14] Homo heidelbergensis may have been the first species to bury their dead about 500,000 years ago.[15]
The Middle Paleolithic period occurred in Europe and the Near East, during which the Neanderthals lived (c. 300,000–28,000 years ago). The earliest evidence (Mungo Man) of settlement in Australia dates to around 40,000 years ago when modern humans likely crossed from Asia by island-hopping.  The Bhimbetka rock shelters exhibit the earliest traces of human life in India, some of which are approximately 30,000 years old.[citation needed]
Homo neanderthalensis used Mousterian Stone tools that date back to around 300,000 years ago[16] and include smaller, knife-like and scraper tools.[citation needed] They buried their dead in shallow graves along with stone tools and animal bones, although the reasons and significance of the burials are disputed.[17][18]
Homo sapiens, the only living species in the genus Homo, originated in Africa about 200,000 years ago. As compared to their predecessors, Homo sapiens had a more complex brain structure, which provided better coordination for manipulating objects and far greater use of tools.[19] There was art created during this period. Intentional burial, particularly with grave goods, may be one of the earliest detectable forms of religious practice since it may signify a "concern for the dead that transcends daily life."[20] The earliest undisputed human burial so far dates back 130,000 years.  Human skeletal remains stained with red ochre were discovered in the Skhul cave at Qafzeh, Israel with a variety of grave goods.[21]
During the Upper Paleolithic Revolution, advancements in human intelligence and technology changed radically with the advent of behavioral modernity between 60,000 and 30,000 years ago.[4] Behavioral modernity is a set of traits that distinguish Homo sapiens from extinct hominid lineages. Homo sapiens reached full behavioral modernity around 50,000 years ago due to a highly developed brain capable of abstract reasoning, language, introspection, and problem-solving.[19][22]
Aurignacian tools, such as stone-bladed tools, tools made of antlers, and tools made of bones were created during this period.[23] People began creating clothing. What appear to be sewing needles were found around 40,000 years ago and[24] dyed flax fibers dated 36,000 BP were found in a prehistoric cave in the Republic of Georgia.[25][26] Human beings may have begun wearing clothing as far back as 190,000 years ago.[27]
Cultural aspects emerged, such as art of the Upper Paleolithic period, which included cave painting, sculpture such as the Venus figurines, carvings and engravings of bone and ivory.  The most common subject matter was large animals that were hunted by the people of the time.
The Cave of Altamira and Paleolithic Cave Art of Northern Spain and Côa Valley Paleolithic Art are examples of such artwork. Musical instruments such as flutes emerged during this period.[citation needed]
The Mesolithic period was a transitional era between the Paleolithic hunter-gatherers, beginning with the Holocene warm period around 11,660 BP and ending with the Neolithic introduction of farming, the date of which varied in each geographical region.  Adaptation was required during this period due to climate changes that affected environment and the types of available food.[citation needed]
Small stone tools called microliths, including small bladelets and microburins, emerged during this period.[28] For instance, spears or arrows were found at the earliest known Mesolithic battle site at Cemetery 117 in the Sudan.[29] Holmegaard bows were found in the bogs of Northern Europe dating from the Mesolithic period.[30] These microliths point to the use of projectile technology since they are widely assumed to have formed the tips and barbs of arrows.[31] This is demonstrated by mesolithic assemblages found in southwest Germany, which revealed two types of projectiles used: arrows with transverse, trapezoidal stone tips and large barbed antler "harpoons".[32] These implements indicate the nature of human adaptation to the environment during the period, describing the Mesolithic societies as hunter-gatherers.[33]
The Neolithic Revolution was the first agricultural revolution, representing a transition from hunting and gathering nomadic life to an agriculture existence.  It evolved independently in six separate locations worldwide circa 10,000–7,000 years BP (8,000–5,000 BC). The earliest known evidence exists in the tropical and subtropical areas of southwestern/southern Asia, northern/central Africa and Central America.[34]
There are some key defining characteristics. The introduction of agriculture resulted in a shift from nomadic to more sedentary lifestyles,[35] and the use of agricultural tools such as the plough, digging stick and hoe made agricultural labor more efficient.[citation needed] Animals were domesticated, including dogs.[34][35] Another defining characteristic of the period was the emergence of pottery,[35] and, in the late Neolithic period, the wheel was introduced for making pottery.[36]
Neolithic architecture included houses and villages built of mud-brick and wattle and daub and the construction of storage facilities, tombs and monuments.[37] Copper metalworking was employed as early as 9000 BC in the Middle East;[38] and a copper pendant found in northern Iraq dated to 8700 BC.[39] Ground and polished stone tools continued to be created and used during the Neolithic period.[35]
Numeric record keeping evolved from a system of counting using small clay tokens that began in Sumer about 8000 BC.[40]
The Stone Age developed into the Bronze Age after the Neolithic Revolution. The Neolithic Revolution involved radical changes in agricultural technology which included development of agriculture, animal domestication, and the adoption of permanent settlements.[citation needed]
The Bronze Age is characterised by metal smelting of copper and its alloy bronze, an alloy of tin and copper, to create implements and weapons. Polished stone tools continued to be used due to their abundance compared with the less common metals (especially tin).[citation needed]
This technological trend apparently began in the Fertile Crescent, and spread outward.[citation needed]
The Iron Age involved the adoption of iron or steel smelting technology, either by casting or forging. Iron replaced bronze,[41][42] and made it possible to produce tools which were stronger, lighter and cheaper to make than bronze equivalents.[43] The best tools and weapons were made from steel.[44]
Other societal changes often accompanied the introduction of iron, including practice changes in art, religion and agriculture. The Iron Age ends with the beginning of the historic periods, generally marked by the development of written language that enabled creation of historic records.[42][44]
The timing of the adoption of iron depended upon "the availability of iron ore and the state of knowledge".[41][42] Iron was smelted in Egypt about 6000 B.C. and iron replaced bronze in the Middle East about 1500 B.C. Chinese began casting iron about 5000 B.C. and their methods for casting iron was the precursor to modern steel manufacturing methods. Most of Asia, however, did not adopt production of iron until the historic period.[41]
In Europe, iron was introduced about 1100 B.C. and had replaced bronze for creating weapons and tools by 500 B.C. They made iron through the forging smelting process and integrated casting in the Middle Ages.[41] Large hill forts or oppida were built either as a refuge in time of war, or sometimes as permanent settlements. Agricultural practices were made more efficient with more effective and varied iron tools.[45]
Iron was extracted from metal ore starting about 2000 B.C. in Africa.[41]
The New World periods began with the crossing of the Paleo-Indians, Athabaskan, Aleuts and Eskimos along the Bering Land Bridge onto the North American continent.[46]
The Paleo-Indians  were the first people who entered, and subsequently inhabited, the Americas during the final glacial episodes of the late Pleistocene period. Evidence suggests big-game hunters crossed the Bering Strait from Asia into North America over a land and ice bridge (Beringia), that existed between 45,000 BCE – 12,000 BCE,[47] following herds of large herbivores far into Alaska.[48]
In their book, Method and Theory in American Archaeology, Gordon Willey and Philip Phillips defined five cultural stages for the Americas, including the three prehistoric Lithic,  Archaic and Formative stages.  The historic stages are the Classic and Post-Classic stages.[49][50]
The Lithic period occurred from 12,000 to 6,000 years before present and included the Clovis, Folsom and Plano cultures.[50] Clovis culture was considered the first culture to use projectile points to hunt on the North American continent. Since then, a pre-Clovis site was found in Manis, Washington that found use of projectile points to hunt mastodons.[51]
The Archaic period in the Americas was dated from 8,000 to 2,000 years before present.[50] People were hunters of small game, such as deer, antelope and rabbits, and gatherers of wild plants, moving seasonally to hunting and gathering sites.  Late in the Archaic period, about 200-500 A.D., corn was introduced into the diet and pottery-making became an occupation for storing and curing food.[52]
The Formative stage followed the Archaic period in the Americas and continued until there was contact by European people. Some of the cultures from that period include that of the Ancient Pueblo People, Mississippian culture and Olmec cultures.[50]
Cultures of the Formative Stage are supposed to possess the technologies of pottery, weaving, and developed food production.  Social organization is supposed to involve permanent towns and villages, as well as the first ceremonial centers.  Ideologically, an early priestly class or theocracy is often present or in development.[53]

The prehistory of the Netherlands was heavily influenced by the region's constantly changing, low-lying geography. Inhabited by humans for at least 37,000 years, the landscape underwent significant transformations, from the last ice age's tundra climate to the emergence of various Paleolithic groups. The region witnessed the development of the Swifterbant culture, which was closely linked to rivers and open water, while the Mesolithic era saw the creation of the world's oldest recovered canoe, the Pesse canoe. The arrival of agriculture around 5000–4000 BC marked the beginning of the Linear Pottery culture, which gradually transformed prehistoric communities.
A succession of cultural groups, such as the Funnelbeaker, Corded Ware, and Bell Beaker cultures, left their mark on the area. The Bronze Age heralded increased prosperity and trade, with the construction of notable monuments such as the dolmens in Drenthe. The Iron Age, on the other hand, brought about the spread of Germanic and Celtic influences in the region, exemplified by the Elp and Hilversum cultures. The pre-Roman period was characterized by a complex interplay of different cultures and ethnicities, including the emergence of early Frisians, Saxons and Salian Franks.
The prehistory of the area that is now the Netherlands was largely shaped by its constantly shifting, low-lying geography.
The oldest human (Neanderthal) traces, believed to be about 70,000 years old, were found in higher soils near Maastricht.
The area that is now the Netherlands was inhabited by early humans at least 37,000 years ago, as attested by flint tools discovered in Woerden in 2010.[1] In 2009 a fragment of a 40,000-year-old Neanderthal skull was found in sand dredged from the North Sea floor off the coast of Zeeland.[2]
During the last ice age, the Netherlands had a tundra climate with scarce vegetation and the inhabitants survived as hunter-gatherers. After the end of the ice age, various Paleolithic groups inhabited the area. It is known that around 8000 BC a Mesolithic tribe resided near Burgumer Mar (Friesland). Another group residing elsewhere is known to have made canoes. The oldest recovered canoe in the world is the Pesse canoe.[3][4] According to C14 dating analysis it was constructed somewhere between 8200 BC and 7600 BC.[4] This canoe is exhibited in the Drents Museum in Assen.
At the end of the Ice Age, the nomadic late Upper Palaeolithic Hamburg culture (13,000–10,000 BC) hunted reindeer in the area, using spears. The later Ahrensburg culture (11,200–9,500 BC) used bow and arrow. From Mesolithic Maglemosian-like tribes (c. 8000 BC), the world's oldest canoe was found in Drenthe.[5]
Autochthonous hunter-gatherers from the Swifterbant culture are attested from around 5600 BC onwards.[6] They are strongly linked to rivers and open water and were related to the southern Scandinavian Ertebølle culture (5300–4000 BC). To the west, the same tribes might have built hunting camps to hunt winter game, including seals.
There are a number of theories about the arrival of agriculture in the Netherlands. The first is that between 4800 and 4500 BC, the Swifterbant people started to adopt from the neighbouring Linear Pottery culture the practice of animal husbandry, and between 4300 and 4000 BC the practice of agriculture.[7] The Funnelbeaker culture (4300–2800 BC), related to the Swifterbant culture, erected the dolmens, large stone grave monuments found in Drenthe. There was a quick and smooth transition from the Funnelbeaker farming culture to the pan-European Corded Ware pastoralist culture (c. 2950 BC). In the southwest, the Seine-Oise-Marne culture — related to the Vlaardingen culture (c. 2600 BC), an apparently more primitive culture of hunter-gatherers — survived well into the Neolithic period, until it too was succeeded by the Corded Ware culture.
The second is that agriculture arrived in the Netherlands somewhere around 5000 BC with the Linear Pottery culture, who were probably central European farmers. Agriculture was practiced only on the loess plateau in the very south (southern Limburg), but even there it was not established permanently. Farms did not develop in the rest of the Netherlands.
There is also a third theory that there is some evidence of small settlements in the rest of the country. These people made the switch to animal husbandry sometime between 4800 BC and 4500 BC. Dutch archaeologist Leendert Louwe Kooijmans wrote, "It is becoming increasingly clear that the agricultural transformation of prehistoric communities was a purely indigenous process that took place very gradually."[6] This transformation took place as early as 4300 BC–4000 BC[8] and featured the introduction of grains in small quantities into a traditional broad-spectrum economy.[9]
The Funnelbeaker culture was a farming culture extending from Denmark through northern Germany into the northern Netherlands. In this period of Dutch prehistory, the first notable remains were erected: the dolmens, large stone grave monuments. They are found in Drenthe, and were probably built between 4100 BC and 3200 BC.
To the west, the Vlaardingen culture (around 2600 BC), an apparently more primitive culture of hunter-gatherers survived well into the Neolithic period.
Around 2950 BCE there was a transition from the Funnelbeaker farming culture to the Corded Ware pastoralist culture, a large archeological horizon appearing in western and central Europe, that is associated with the advance of Indo-European languages. This transition was probably caused by developments[clarification needed] in eastern Germany, and it occurred within two generations.[10]
The Bell Beaker culture was also present in the Netherlands.[11][12] from around 2700 to 2100 BC.[13] It introduced metalwork in copper, gold and later bronze and opened international trade routes not seen before, reflected in copper artifacts. Finds of rare bronze objects suggest that Drenthe was a trading centre in the Bronze Age (2000–800 BC). The Bell Beaker culture developed locally into the Barbed-Wire Beaker culture (2100–1800 BC) and later the Elp culture (1800–800 BC),[14] a Middle Bronze Age archaeological culture with earthenware low-quality pottery as a marker. The initial phase of the Elp culture was characterised by tumuli (1800–1200 BC). The subsequent phase was that of cremating the dead and placing their ashes in urns which were then buried in fields, following the customs of the Urnfield culture (1200–800 BC). The southern region became dominated by the related Hilversum culture (1800–800 BC), with apparently cultural ties with Britain of the previous Barbed-Wire Beaker culture.
The Corded Ware and Bell Beaker cultures were not indigenous to the Netherlands but were pan-European in nature, extending across much of northern and central Europe.
The first evidence of the use of the wheel dates from this period, about 2400 BC.[citation needed] This culture also experimented with working with copper. Evidence of this, including stone anvils, copper knives, and a copper spearhead, was found on the Veluwe.[citation needed] Copper finds show that there was trade with other areas in Europe, as natural copper is not found in Dutch soil.
The Bronze Age probably started somewhere around 2000 BC and lasted until around 800 BC. The earliest bronze tools were in the Wageningen hoard, found in the grave of a Bronze Age metalworker.[15] More Bronze Age objects from later periods have been found in Epe, Drouwen and elsewhere.[citation needed] Broken bronze objects found in Voorschoten were apparently destined for recycling.[citation needed] This indicates how valuable bronze was considered in the Bronze Age. Typical bronze objects from this period included knives, swords, axes, fibulae and bracelets.
Most of the Bronze Age objects found in the Netherlands have been found in Drenthe.[citation needed] One item shows that trading networks during this period extended a far distance. Large bronze situlae (buckets) found in Drenthe were manufactured somewhere in eastern France or in Switzerland. They were used for mixing wine with water (a Roman/Greek custom). The many finds in Drenthe of rare and valuable objects, such as tin-bead necklaces, suggest that Drenthe was a trading centre in the Netherlands in the Bronze Age.[citation needed]
The Bell Beaker cultures (2700–2100) locally developed into the Bronze Age Barbed-Wire Beaker culture (2100–1800). In the second millennium BC, the region was the boundary[citation needed] between the Atlantic and Nordic horizons and was split into a northern and a southern region, roughly divided by the course of the Rhine.
In the north, the Elp culture (c. 1800 to 800 BC)[16] was a Bronze Age archaeological culture having earthenware pottery of low quality known as "Kümmerkeramik" (or "Grobkeramik") as a marker. The initial phase was characterized by tumuli (1800–1200 BC) that were strongly tied to contemporary tumuli in northern Germany and Scandinavia, and were apparently related to the Tumulus culture (1600–1200 BC) in central Europe. This phase was followed by a subsequent change featuring Urnfield (cremation) burial customs (1200–800 BC). The southern region became dominated by the Hilversum culture (1800–800), which apparently inherited the cultural ties with Britain of the previous Barbed-Wire Beaker culture.
The Iron Age brought a measure of prosperity to the people living in the area of the present-day Netherlands. Iron ore was available throughout the country, including bog iron extracted from the ore in peat bogs (moeras ijzererts) in the north, the natural iron-bearing balls found in the Veluwe and the red iron ore near the rivers in Brabant. Smiths travelled from small settlement to settlement with bronze and iron, fabricating tools on demand, including axes, knives, pins, arrowheads and swords. Some evidence[citation needed] even suggests the making of Damascus steel swords using an advanced method of forging that combined the flexibility of iron with the strength of steel.
Iron ore brought a measure of prosperity and was available throughout the country, including bog iron. Smiths travelled from settlement to settlement with bronze and iron, fabricating tools on demand. The King's grave of Oss (700 BC) was found in a burial mound, the largest of its kind in Western Europe and containing an iron sword with an inlay of gold and coral.
In Oss, a grave dating from around 500 BC was found in a burial mound 52 metres wide (and thus the largest of its kind in western Europe). Dubbed the "king's grave" (Vorstengraf (Oss)), it contained extraordinary objects, including an iron sword with an inlay of gold and coral.
In the centuries just before the arrival of the Romans, northern areas formerly occupied by the Elp culture emerged as the probably Germanic Harpstedt culture[17] while the southern parts were influenced by the Hallstatt culture and assimilated into the Celtic La Tène culture. The contemporary southern and western migration of Germanic groups and the northern expansion of the Hallstatt culture drew these peoples into each other's sphere of influence.[18] This is consistent with Caesar's account of the Rhine forming the boundary between Celtic and Germanic tribes.
The Germanic tribes originally inhabited southern Scandinavia, Jutland peninsula and northern Germany,[19] but subsequent Iron Age cultures of the same region, like Wessenstedt (800–600 BC) and Jastorf, also belonged to this grouping.[20]
The climate deteriorating in Scandinavia around 850 BC to 760 BC and later and faster around 650 BC might have triggered migrations. Archaeological evidence suggests around 750 BC a relatively uniform Germanic people came to the Netherlands from the Vistula and southern Scandinavia.[19] In the west, the newcomers settled the coastal floodplains for the first time, since in adjacent higher grounds the population had increased and the soil had become exhausted.[21]
By the time this migration was complete, around 250 BC, a few general cultural and linguistic groupings had emerged.[22][23]
One grouping – labelled the "North Sea Germanic" – inhabited the northern part of the Netherlands (north of the great rivers) and extending along the North Sea and into Jutland. This group is also sometimes referred to as the "Ingvaeones". Included in this group are the peoples who would later develop into, among others, the early Frisians and the early Saxons.[23]
A second grouping, which scholars subsequently dubbed the "Weser–Rhine Germanic" (or "Rhine–Weser Germanic"), extended along the middle Rhine and Weser and inhabited the southern part of the Netherlands (south of the great rivers). This group, also sometimes referred to as the "Istvaeones", consisted of tribes that would eventually develop into the Salian Franks.[23]
The Celtic culture had its origins in the central European Hallstatt culture (c. 800–450 BC), named for the rich grave finds in Hallstatt, Austria.[24] By the later La Tène period (c. 450 BC up to the Roman conquest), this Celtic culture had, whether by diffusion or migration, expanded over a wide range, including into the southern area of the Netherlands. This would have been the northern reach of the Gauls.
In March 2005 17 Celtic coins were found in Echt (Limburg). The silver coins, mixed with copper and gold, date from around 50 BC to 20 AD. In October 2008 a hoard of 39 gold coins and 70 silver Celtic coins was found in the Amby area of Maastricht.[25] The gold coins were attributed to the Eburones people.[26] Celtic objects have also been found in the area of Zutphen.[27]
Although it is rare for hoards to be found, in past decades loose Celtic coins and other objects have been found throughout the central, eastern and southern part of the Netherlands. According to archaeologists these finds confirmed that at least the Meuse (Dutch: Maas) river valley in the Netherlands was within the influence of the La Tène culture. Dutch archaeologists even speculate that Zutphen (which lies in the centre of the country) was a Celtic area before the Romans arrived, not a Germanic one at all.[27]
Scholars debate the actual extent of the Celtic influence.[21][28] The Celtic influence and contacts between Gaulish and early Germanic culture along the Rhine is assumed to be the source of a number of Celtic loanwords in Proto-Germanic.[citation needed] But according to Belgian linguist Luc van Durme, toponymic evidence of a former Celtic presence in the Low Countries is near to utterly absent.[29] Although there were Celts in the Netherlands, Iron Age innovations did not involve substantial Celtic intrusions and featured a local development from Bronze Age culture.[21]
Some scholars (De Laet, Gysseling, Hachmann, Kossack & Kuhn) have speculated that a separate ethnic identity, neither Germanic nor Celtic, survived in the Netherlands until the Roman period. They see the Netherlands as having been part of an Iron Age "Nordwestblock" stretching from the Somme to the Weser.[30][31] Their view is that this culture, which had its own language, was being absorbed by the Celts to the south and the Germanic peoples from the east as late as the immediate pre-Roman period.
The first author to describe the coast of Holland and Flanders was the Greek geographer Pytheas, who noted in c. 325 BC that in these regions, "more people died in the struggle against water than in the struggle against men."[32]

The history of Ukraine spans thousands of years, tracing its roots to the Pontic steppe—one of the key centers of the Chalcolithic and Bronze Ages, Indo-European migrations, and early horse domestication. In antiquity, the region was home to the Scythians, followed by the gradual expansion of Slavic tribes. The northern Black Sea coast saw the influence of Greek and Roman colonies, leaving a lasting cultural legacy. Over time, these diverse influences contributed to the development of early political and cultural structures.[1][2][3]
Ukraine enters into written history with the establishment of the medieval state of Kievan Rus'. In Dnieper Ukraine, the tribe of Polans played a key role in the formation of the state, adopting the name Rus' by the 9th century. The term is believed to have connections to the Varangians, who contributed to the state’s early political and military structure.[4][5][6] By the 10th–11th centuries, Kievan Rus' had grown into one of the most powerful and culturally advanced states in Europe, reaching its golden age under Volodymyr the Great and Yaroslav the Wise, who introduced Christianity and strengthened political institutions. However, internal conflicts among Kyivan rulers, along with increasing pressure from Turkic nomads in Southern Ukraine, gradually weakened the state.[7] In the 13th century, Kievan Rus' suffered devastating destruction during the Mongol invasion, particularly in its Dnieper heartlands. While much of the former Rus' territory fell under Mongol control, the Kingdom of Ruthenia (Galicia-Volhynia) emerged as its successor, continuing political and cultural traditions under King Daniel.[8]
In the 14th and 15th centuries, the majority of Ukrainian territories became part of Grand Duchy of Lithuania, Ruthenia and Samogitia, while Galicia and Transcarpathia came under Polish and Hungarian rule. Lithuania kept the local Ruthenian traditions, and was gradually influenced by Ruthenian language, law and culture, until Lithuania itself came under Polish influence, following the Union of Krewo and Union of Lublin, resulting in two countries merging into Polish-Lithuanian Commonwealth, leaving Ukrainian lands under the dominance of Polish crown.[9] Meanwhile Southern Ukraine was dominated by Golden Horde and then Crimean Khanate, which came under protection of the Ottoman Empire, major regional power in and around Black Sea, which also had some of its own directly-administrated areas as well.[10]
In the 17th century, the Cossack rebellion led by Bohdan Khmelnytsky marked a turning point in the Ukraine' history. The uprising, which began in 1648, was fueled by grievances against the Polish-Lithuanian Commonwealth's nobility, religious tensions, and social inequalities. This rebellion led to the creation of the Cossack Hetmanate, a semi-autonomous polity in central and eastern Ukraine. In 1654, the Cossack Hetmanate allied with the Tsardom of Moscow through the Pereiaslav Agreement. The nature of this alliance has been widely debated by historians. Some argue that it established a protectorate relationship, with Russia offering military support in exchange for loyalty, while others believe it symbolized the subordination of the Hetmanate to the Tsar. The ambiguity of the treaty's terms and differing interpretations contributed to tensions over the following decades.[11] Over time, the relationship between the Cossack Hetmanate and Russia evolved, with Russia increasingly asserting dominance. This process intensified in the late 17th and 18th centuries, especially after the Truce of Andrusovo, which divided Ukraine between the Polish-Lithuanian Commonwealth and Russia.[12]
The Cossack Hetmanate's autonomy was progressively eroded, culminating in its abolition by Catherine the Great in the late 18th century. Simultaneously, the Polish-Lithuanian Commonwealth's internal decline and external pressures from neighboring powers facilitated the partitions of Poland. These partitions allowed the Russian Empire to incorporate vast Ukrainian territories, including those previously under Polish control. Western Ukraine, however, came under the rule of the Habsburg monarchy. This division set the stage for the different historical trajectories of Ukrainian lands under Russian and Austrian influence.[13]: 199
The 20th century began with a renewed struggle for Ukrainian statehood. Following the collapse of empires during World War I, the Ukrainian People’s Republic (UPR) was proclaimed in 1917 with Kyiv as its capital. Meanwhile, in the western territories, the West Ukrainian People’s Republic (WUPR) was established in 1918, centered in Lviv. Both republics sought to unite, forming the Unification Act (Act Zluky) on 22 January 1919.[14] However, their independence was short-lived. The UPR faced constant military conflict with Bolshevik forces, Poland, and White Army factions. By 1921, following the Soviet-Ukrainian War, Ukrainian lands were divided: the eastern territories became the Ukrainian Soviet Socialist Republic (part of the USSR), while western Ukraine was absorbed by Poland, Romania, and Czechoslovakia.[13]: 537
Under Soviet rule, initial policies of Ukrainianization gave way to oppressive Russification. The Holodomor famine of 1932–1933, a man-made disaster, caused the deaths of 4-5 millions Ukrainians.[15]: §§ 8.1.3  During World War II, Ukraine endured brutal occupations by both Nazi Germany and the Soviet Union. The Ukrainian Insurgent Army (UPA) fought for independence, though parts of Ukrainian society also collaborated with occupying forces. Post-war, Soviet control was reestablished, and Crimea was transferred to Ukraine in 1954.
Ukraine became independent when the Soviet Union dissolved in 1991. This started a period of transition to a market economy, in which Ukraine suffered an eight-year recession.[16] Subsequently however, the economy experienced a high increase in GDP growth until it plunged during the Great Recession.[17] This period was marked by economic challenges, the rise of nationalism, and growing tensions with Russian Federation. In 2013, the Euromaidan protests began in response to President Viktor Yanukovych’s rejection of an EU association agreement. The Revolution of Dignity followed, leading to Yanukovych’s ousting. Russia annexed Crimea in 2014 and supported separatist movements in Donbas, initiating the ongoing Russo-Ukrainian War. This escalated on 24 February 2022, with Russia’s full-scale invasion, marking a critical phase in Ukraine’s fight for sovereignty and territorial integrity.
The discovery of 1.4-million-year-old stone tools in Korolevo, located in western Ukraine, marks one of the earliest securely dated presences of hominins in Europe. These tools offer crucial insights into the behaviors and adaptive strategies of early members of the genus Homo, likely Homo erectus, as they expanded into the continent during the Lower Paleolithic period.[18] The Neanderthal presence in Ukraine is well-documented through archaeological findings, particularly at the Molodova sites, which are located in the modern-day Chernivtsi region in southwestern Ukraine. These sites, dating to the Middle Paleolithic period (c. 45000 – c. 43000 BC), provide significant evidence of Neanderthal activity. Molodova is known for its rich cultural layers attributed to the Mousterian tradition, showcasing the use of fire, mammoth bone processing, and possible construction of primitive shelters.[19] These sites, located along the Dniester River, are particularly notable for their evidence of advanced behavior. One remarkable feature is the discovery of a dwelling constructed from mammoth bones, a testament to Neanderthal ingenuity and adaptation to harsh Ice Age environments. These structures highlight their ability to organize resources and create durable shelters.[20][21]
The Crimean Mountains hold significant evidence of early modern humans (Homo sapiens) during the Upper Paleolithic period. The Buran-Kaya cave sites have yielded artifacts, such as tools and ornaments, along with skeletal remains, dating to approximately 32,000 BC. These Gravettian settlements reflect a sophisticated hunter-gatherer culture, known for their blade-based lithic technologies and artistic expression. The region likely served as a vital hub for human activity, offering both shelter and access to diverse ecological resources.[22] The archaeological record in Ukraine highlights the area's importance as a crossroads for early human populations migrating through Europe. From the earliest stone tools to evidence of complex social structures and artistic traditions, Ukraine offers a valuable lens into the evolution and adaptation of early humans over hundreds of thousands of years.
The Cucuteni–Trypillia culture (c. 5050 – c. 2950 BC), centered in modern-day Romania, Moldova, and Ukraine, represents one of the most advanced Neolithic civilizations in Europe. These people were known for their impressive settlements, some of which were among the largest in the world at the time, with populations reaching up to 10,000 inhabitants. They built concentric layouts of houses, often using clay and wood, demonstrating advanced planning. They are renowned for their intricate painted ceramics, featuring geometric and symbolic designs in red, black, and white. Their economy was primarily agrarian, complemented by animal husbandry and small-scale trade. Archaeological findings suggest a focus on fertility cults, as evidenced by numerous figurines and symbolic artifacts. The Cucuteni–Trypillia culture began to decline around 3000 BC, possibly due to environmental changes, resource depletion, or pressures from neighboring steppe cultures.[23]
The Sredny Stog culture (c. 4500 – c. 3500 BC) emerged to the east of the Cucuteni–Trypillian zone, on the Pontic–Caspian steppe. It marked a transitional phase between the Neolithic and the emergence of the early Bronze Age steppe cultures. This culture is among the earliest to show evidence of horse domestication, which became a defining feature of steppe societies. Early indications of mound burial (kurgans) began to appear, reflecting evolving social hierarchies and ritual practices. The Sredny Stog culture played a significant role in the genesis of the later Yamna culture.[24]
The Yamna culture (c. 3300 – c. 2600 BC), also known as the Pit Grave culture, was a dominant force in the early Bronze Age across the Pontic–Caspian steppe. This culture is often linked with the spread of Indo-European languages and reflects a shift toward a more mobile, pastoralist lifestyle. The Yamna culture is characterized by its kurgans, often accompanied by grave goods such as weapons, ornaments, and animal sacrifices. The economy relied heavily on livestock, including sheep, cattle, and horses, which supported a semi-nomadic lifestyle. They utilized copper and bronze tools and weapons, demonstrating early metallurgical skills. Many scholars associate the Yamna culture with Proto-Indo-European speakers, as their migrations and cultural diffusion likely influenced vast areas of Europe and Asia.[25]
Following the Yamna culture, the Catacomb culture (c. 2500 – c. 1950 BC) emerged, encompassing much of the same geographic area. It is distinguished by its unique burial practices, where bodies were interred in specially carved niches within grave pits. They further advanced bronze metallurgy, producing a variety of tools, weapons, and ornaments. The Catacomb culture maintained many traditions of the Yamna culture while also interacting with neighboring groups to the west and south.[26]
During the Iron Age, the region witnessed the rise and interaction of diverse peoples and cultures. Following earlier Bronze Age societies, the Dacians, alongside nomadic groups such as the Cimmerians (associated with the Novocherkassk archaeological culture), Scythians, and Sarmatians, dominated the landscape. Among these, the Scythians established a powerful kingdom that flourished between 750 and 250 BC, characterized by their mastery of mounted warfare and trade networks spanning vast territories.[27]
One of the notable events in Scythian history was the campaign of Darius the Great in 513 BC. The Achaemenid Persian king led an expedition aimed at subjugating the Scythians. While the Scythians employed their signature scorched-earth and guerrilla tactics to evade outright defeat, the campaign resulted in the Persian domination of several Thracian peoples and regions along the Black Sea’s northern coast. These territories, encompassing parts of modern-day Bulgaria, Romania, Ukraine, and southern Russia, were incorporated into the vast Achaemenid sphere of influence, though direct control remained tenuous.[28][29]
Meanwhile, Greek colonization left a lasting imprint on the region. Beginning in the 7th or 6th century BC during the Archaic period, Greek settlers established colonies along the northern Black Sea coast, including Crimea and parts of modern Ukraine. These colonies, such as Chersonesus and Olbia, served as hubs of trade, cultural exchange, and Hellenic influence. The Bosporan Kingdom, a Greco-Scythian polity formed in this context, became a regional power, blending Greek traditions with local elements. It thrived until the 4th century AD, when invasions by the Goths and later the Huns disrupted its stability.[30][31]
The Roman Empire, expanding its reach into the region, briefly annexed the Bosporan Kingdom from 62 to 68 AD under Emperor Nero. During this period, the reigning Bosporan king, Tiberius Julius Cotys I, was deposed, and the kingdom was directly administered by Rome. Following Nero’s rule, the Bosporan Kingdom was restored as a Roman client state, retaining local governance but under Roman military oversight. This arrangement ensured the region remained within Rome’s economic and strategic sphere during the middle of the 1st century AD.[32][33][34]
In the 3rd century AD, the Goths, a Germanic people originally from Scandinavia, began migrating toward Eastern Europe. By approximately 250 to 375 AD, they had settled in the area of modern Ukraine, which they referred to as Oium. This region is linked to the archaeological Chernyakhov culture, known for its unique mix of local and Gothic influences.[35]
The Goths in this region soon divided into two main groups: the Visigoths (Western Goths) and the Ostrogoths (Eastern Goths). The Ostrogoths established a stronghold in Ukraine but faced significant changes in the 370s with the arrival of the Huns, a nomadic group from Central Asia. The Huns were powerful warriors and ultimately brought the Ostrogoths under their control, leading to major shifts in Gothic society and governance.[36]
To the north of the Ostrogothic territory was the Kyiv culture, a Slavic archaeological culture that thrived from the 2nd to the 5th centuries AD. As the Huns expanded their influence across Eastern Europe, this culture also came under threat. Many Slavic and other local groups were affected by the Hunnic invasions, resulting in changes in settlement patterns and local governance.[37]
In 454 AD, a decisive battle known as the Battle of Nedao marked a turning point. The Ostrogoths, along with other Germanic tribes, rebelled against the Huns and contributed to their defeat. Following this victory, the Ostrogoths were permitted by the Romans to settle in Pannonia (modern-day Hungary), marking their departure from the Ukrainian lands.[38]
Meanwhile, the Black Sea’s northeastern shores were dotted with ancient Greek colonies, such as Tyras, Pontic Olbia, and Hermonassa. Established as early as the 6th century BC, these colonies developed into important cultural and trading centers under Roman and later Byzantine rule. These cities continued to thrive until the 6th century AD, when Byzantine influence began to wane.[39]
In the early 6th century AD, the Bosporan Kingdom on the Crimean Peninsula came under the rule of a Hunnic king named Gordas. Gordas maintained friendly relations with the Byzantine emperor Justinian I, but this ended when Gordas was killed in a local revolt around 527 AD. Justinian seized the opportunity to intervene, occupying the Bosporan Kingdom and further extending Byzantine influence over the region. However, even into the 12th century, Byzantine emperors continued to claim dominion over the Crimean region, known as the Cimmerian Bosporus.[40][41]
Following the power vacuum left by the fall of Hunnic and Gothic dominance, the Early Slavs began to expand over much of the territory that is now Ukraine during the 5th century, continuing their migration into the Balkans in the 6th century. The exact origins of the Early Slavs remain uncertain, though several theories suggest they may have originated near the Polesia region, a marshy area between modern-day Belarus and Ukraine. This period marks a transition from the Kyiv culture to the establishment of Slavic tribes across Eastern Europe.[42]
In the 5th and 6th centuries, the Antes Union (a tribal confederation) is generally believed to have been situated in present-day Ukraine. The Antes are considered ancestors of several Slavic tribes that would later form the Ukrainians, including the Polans, Severians, Drevlians, White Croats, Dulebes, Ulichs, and Tivertsi. The migration of these tribes from Ukraine throughout the Balkans contributed to the foundation of several South Slavic nations, while northern migrations, reaching as far as Lake Ilmen, gave rise to the Ilmen Slavs, Krivichs, and Radimichs.[43]
The collapse of the Antes Union in 602, following a devastating raid by the Pannonian Avars, led to a fragmentation of the early Slavic federation. Despite this, many of these tribes maintained their distinct identities until the formation of larger political entities in the early second millennium, such as Kievan Rus', which began to consolidate Slavic lands.[43]
The Early Slavs were primarily agrarian, relying on subsistence farming, and lived in semi-nomadic communities. Over time, they developed a complex social structure, with local chieftains leading tribal groups. They also practiced paganism, with a pantheon of gods tied to nature, such as Perun (god of thunder) and Dazhbog (sun god). By the time of the Antes Union's decline, the cultural and religious practices of the Slavs had already begun to influence neighboring peoples, laying the groundwork for the Slavic cultural sphere that would emerge later in Eastern and Southeastern Europe.[44]
In the 7th century, the territory of modern Ukraine was at the core of the state of the Bulgars, often referred to as Old Great Bulgaria. This state, with its capital at Phanagoria (located in what is now the Taman Peninsula), controlled a significant part of the northern Black Sea region. The Bulgars, a semi-nomadic people from Central Asia, were known for their sophisticated society, military organization, and far-reaching influence.[35]
By the end of the 7th century, the Bulgars faced increasing pressure from neighboring tribes and empires. Most of the Bulgar tribes migrated in various directions—some settled in the Balkans, where they eventually established the First Bulgarian Empire. Other groups moved towards the Volga region, forming Volga Bulgaria, which became a prominent center of trade and culture. The remaining parts of Old Great Bulgaria were eventually absorbed by the Khazars, another semi-nomadic people from Central Asia.[35]
The Khazars founded the Khazar Khaganate, a powerful and influential state near the Caspian Sea and the Caucasus. The Khaganate's territory expanded to include parts of modern-day western Kazakhstan, eastern Ukraine, southern Russia, and northern Azerbaijan. The Khazars were noted for their religious tolerance and political pragmatism, famously adopting Judaism as their state religion in the 8th century, although Christianity, Islam, and other faiths were also practiced within their borders.[45]
The Khazars played a key role in the politics and economy of Eastern Europe and the Pontic–Caspian steppe. Their control over trade routes contributed to the establishment of the Pax Khazarica, a period of relative peace and stability that fostered safe long-distance trade. This stability allowed traders, including the Radhanite Jews, to operate along vast routes that stretched from China to the Byzantine Empire. These trade routes facilitated the exchange of goods, culture, and ideas across Eurasia.[46]
The origins of the Kievan state and the etymology of its name, Rus', continue to be subjects of scholarly debate. One theory, often called the 'Norman theory', posits that the term Rus' originated from the Scandinavian Varangians, who were active in trade and military ventures across Eastern Europe in the early medieval period. Proponents of this theory argue that the Varangians, also known as the Rus', initially brought a political and military influence that shaped the emerging state structure, with the term Rus' eventually becoming synonymous with the region and its people.[47]
In contrast, the 'anti-Norman theory' suggests that the term Rus' has indigenous Slavic roots, developing independently of Scandinavian influence. Advocates of this theory assert that Eastern Slavic tribes already had established political and social frameworks before any contact with the Varangians, and that these tribes were naturally progressing toward political consolidation. According to this perspective, the name Rus' could be linked to the Ros River, a historically significant waterway flowing through present-day Ukraine, which was home to various Slavic communities. In this view, Rus' may have referred to a collective identity tied to a specific geographic region or a union of native tribes, rather than to foreign invaders or rulers.[48][49]
The first reliable mention of the Rus' dates back to the year 839 in the Frankish chronicle Annals of St. Bertin, where members of an embassy from the north, arriving in the Byzantine Empire, referred to themselves as Rus'.[50] The second notable mention of the Rus' occurred in 860, when they launched a bold and unexpected naval raid on Constantinople. Commanding a fleet across the Black Sea, the Rus' forces struck at the very heart of the Byzantine Empire, quickly reaching the city’s outskirts. According to accounts from Greek eyewitnesses, the Rus' not only managed to pillage the suburbs of Constantinople but also instilled widespread fear among its residents. The Byzantine defenses were unprepared, allowing the Rus' to withdraw without significant resistance.[51]
The earliest source about the history of the Dnieper Ukraine region is the Tale of Bygone Years (or Primary Chronicle), written no earlier than the 11th century. In its 'legendary' part, it narrates the Rus' raid on Constantinople and the formation of a state centered in Kiev during the second half of the 9th century. The Chronicle, in particular, mentions the names of the leaders of the raid on Constantinople — Askold and Dir — and calls them retainers of the Scandinavian Rurik dynasty. According to the Chronicle, a representative of this dynasty, Oleg the Wise, allegedly came to Kiev from Novgorod in 882, killed Askold and Dir, and took control of the Kiev state. This narrative contains chronological errors—for example, it incorrectly dates the Constantinople raid to 867—and lacks corroboration from archaeological evidence, which suggests that Novgorod itself was only established in the 10th century. Therefore, modern historians view the Chronicle's account of the 9th century as largely speculative and likely a later construction by the Chronicle's author.[52][53]
Scholars associate the state-building processes in the Middle Dnieper region with the emergence of the well-known trade route from Scandinavia to Constantinople, known as the 'Route from the Varangians to the Greeks'. A significant section of this route ran along the Dnieper River, and Kiev was an important transshipment point, allowing control over trade along the Dnieper, Pripyat, and Desna rivers.[55] The Middle Dnieper region began to serve as a political, cultural, and economic center for the East Slavic world. It eventually came to be known as the Rus' land in the narrow sense of this term.[54] According to The Tale of Bygone Years, the East Slavic tribe of the Polans, inhabitants of the Middle Dnieper region, adopted the name Rus' for their land and were regarded as the most advanced among the East Slavs. Thus, they played a central role in forming a new tribal union centered around Kiev.[56]
From the first half of the 10th century, the first confirmed ruler of the Kievan state, as documented in foreign sources, was Igor the Old, whom the Primary Chronicle identifies as a prince. Information in the Chronicle regarding governance during this period is considered relatively reliable. The princely retinue played a significant role in governance, accompanying rulers on campaigns and collecting tribute from subjugated local Slavic tribes. The collected tribute (such as furs, honey, hides, wax, and slaves) was mainly exported to Byzantium, with the proceeds used to purchase weapons, luxury goods, and wine—constituting the core of imports. When trade conditions no longer met the prince's expectations, he led an unsuccessful campaign against Constantinople in 941, which eventually resulted in a new Rus-Byzantine trade treaty in 944. The Tale of Bygone Years recounts Igor's attempt to levy additional tribute from the subdued Slavs, which led to a rebellion by the Drevlians, who killed him in 945.[57]
Following Igor's death in 945, his widow, Princess Olga, assumed the role of regent for their young son, Sviatoslav, who was still too young to rule. Olga is best known for her calculated and fierce retaliation against the Drevlians, a campaign that culminated in the annexation of their lands into the expanding Kievan Rus'. Olga also implemented significant reforms, particularly by restructuring the tribute collection system known as poliudie. This new system made revenue collection more systematic and centralized, reducing the likelihood of rebellion by ensuring a more balanced distribution of power. In a landmark decision, Olga became the first ruler of Kievan Rus' to embrace Christianity around 957 during a diplomatic visit to Constantinople. However, despite her conversion, the state under her rule remained predominantly pagan, with Christianity gaining influence only later under her descendants.[58][59]
When Sviatoslav assumed rule over Kievan Rus', he launched an ambitious military expansion. His most notable achievement was the decisive defeat of the Khazar Khaganate, a once-dominant regional power that had controlled key trade routes for centuries. The fall of the Khazars opened opportunities for Kievan Rus' to extend its influence into the Caucasus and beyond. In addition to his campaigns in the east, Sviatoslav waged numerous battles against the Byzantine Empire, seeking to establish Kievan Rus' as a formidable power in the Balkans. His primary goal was to create a lasting base in Bulgaria, strengthening his strategic position in southeastern Europe. However, his ambitions in the Balkans were thwarted after a Byzantine counterattack. In 972, while returning from his Balkan campaign, Sviatoslav was ambushed and killed by the Pechenegs near the Dnieper River.[60][61]
After Sviatoslav's death, a power struggle ensued among his sons. Yaropolk, Sviatoslav's eldest son, assumed the title of Grand Prince of Kiev and worked to consolidate power across the vast territory, leading to conflicts with his brothers. Oleg, Sviatoslav's second son, governed the Drevlian lands, and rivalry between him and Yaropolk escalated into open warfare. During one of their clashes around 977, Oleg was killed, further intensifying the conflict. Initially, Vladimir, Sviatoslav's youngest son, fled to avoid being entangled in the conflict. However, after Oleg's death, he returned with a Varangian army. By 980, Vladimir had defeated Yaropolk and consolidated power, becoming the sole ruler of Kievan Rus'.[62]
During the reign of Vladimir the Great, Kievan Rus' expanded significantly, notably through the conquest and annexation of Red Ruthenia, Transcarpathia, and Korsun. These territorial gains made Kievan Rus' the largest state in Europe at the time, covering over 800,000 square kilometers and boasting a population of more than 5 million. The socio-economic structure of Kievan Rus' was similar to that of other European states of the period, characterized by a natural economy, communal land ownership, and reliance on slash-and-burn agriculture, and animal husbandry.[63]
Vladimir reformed local governance by abolishing the traditional tribal autonomies and installing his own appointed governors, further centralizing his authority. Under his rule, Kiev emerged as one of the wealthiest commercial centers in Europe during the 10th and 11th centuries, benefiting from its strategic location on trade routes and its growing political influence. Initially, Vladimir supported the worship of Slavic deities such as Perun. However, in 988, he made a historic decision to convert Kievan Rus' to Christianity, adopting Eastern Orthodoxy from the Byzantine Empire. He personally led the mass baptism of the people of Kiev in the Pochaina River and built the first stone church in Kievan Rus' — Church of the Tithes. He also introduced the Charter on Church Courts and Tithes, thereby securing a strong relationship between the church and state. Unlike the earlier attempts of his grandmother, Princess Olga, Vladimir's reforms had a lasting impact, deeply influencing the religious and cultural development of the region.[64]
Vladimir's rule also saw administrative, monetary, and military reforms. He appointed governors and entrusted his sons with ruling major cities, also minted his own gold (Zlatnik) and silver (Srebrenik) coins, and granted borderlands to loyal vassals in exchange for military defense. To protect the realm, Vladimir oversaw the construction of an extensive system of defensive fortifications, known as the Serpent's Walls, which stretched for 1,000 kilometers, safeguarding Kievan Rus' from external threats.[65]
Despite these accomplishments, Kievan Rus' entered a turbulent period of internal strife following Vladimir's death in 1015, as his sons fought for control. This era of internecine conflict lasted until 1019, when Yaroslav the Wise emerged victorious and assumed the throne. Ruling jointly with his brother Mstislav until 1036, Yaroslav presided over what is often regarded as the golden age of Kievan Rus'. One of Yaroslav's key achievements was his decisive defeat of the Pechenegs, who had long been a threat to the state. To commemorate this victory, Yaroslav ordered the construction of the Saint Sophia Cathedral in Kiev, a structure that still stands as a symbol of this prosperous era. He also founded the Kyiv Pechersk Lavra and supported the election of  Hilarion of Kiev as the Kyiv Metropolitan, marking a significant step in the independence of the Kievan church. Yaroslav's reign was also notable for the introduction of the first written legal code — Rus' Truth, which established a foundation for legal governance in the state. Additionally, Yaroslav engaged in extensive marriage diplomacy, forming alliances by arranging marriages between his daughters and European royalty.[66][67] However, after Yaroslav's death, Kievan Rus' began to fragment, as his sons divided the territory among themselves, leading to further internecine conflicts and the eventual decline of centralized power.[68]
The feudal fragmentation of Kievan Rus' began in the late 11th century, driven by a complex interplay of internal and external factors. One significant factor was the rotational succession system, allowing power to pass among male relatives instead of directly from father to son. This system often sparked conflicts, as competing claims to power emerged. With princes often granted specific territories, autonomous principalities began to emerge clearly. Regional cities such as Chernihiv, Polotsk, and Novgorod gained power and asserted their independence, with local elites increasing their influence. By this time, the authority of the Grand Prince of Kiev was notably weakening. Regional princes, bolstered by local resources and armies, began to resist central control. Additionally, the vast geography and economic diversity of Rus' impeded unity, as various regions developed their own trade routes and systems. External threats from nomadic groups, such as the Pechenegs and Polovtsy (Cumans), further exacerbated regionalism, compelling local rulers to focus on defending their territories.[69][70]
After Yaroslav the Wise's death, his sons divided the lands, which significantly accelerated political fragmentation. Although his succession plan aimed to prevent conflict, it ultimately sowed the seeds of feudal division. The Council of Liubech, convened by several princes, including Vladimir Monomakh, sought to settle disputes and clarify the inheritance of principalities. This agreement formalized Kievan Rus's division into regional hereditary principalities, thereby legalizing fragmentation. The adage "Let everyone hold his own" emerged, signifying that each prince would govern his own territory without interference from others.[71]
Vladimir Monomakh, who served as Grand Prince of Kiev from 1113 to 1125, is remembered as one of Kievan Rus' most capable leaders. He ascended the throne amid internal strife and external threats, yet worked diligently to stabilize the fragmented territories of Rus'. Monomakh's reign achieved relative unity through efforts to reduce feuds among princes and reinforce central authority. His military successes, particularly against the Polovtsy (Cumans), secured the region's borders. Diplomatically, he forged strong ties with Europe, notably through his marriage to Gytha of Wessex, the daughter of English King Harold II. As an educated ruler, Monomakh authored The Instruction of Vladimir Monomakh, offering insights on governance, morality, and leadership. His reign is often regarded as one of the last periods of unity before Rus' descended further into division.[72][73]
Monomakh's son, Mstislav the Great, preserved some unity during his reign from 1125 to 1132. However, following his death, the principality fractured. The division of lands among Monomakh's sons and other relatives resulted in multiple competing power centers.[74][75] The process of feudal fragmentation marked a turning point in the history of Rus', culminating in its eventual decline and the rise of smaller, more independent states. These would later evolve into the medieval powers of the Kingdom of Ruthenia, the Principality of Polotsk, the Novgorod Republic, and others.[76][77][78][79]
In 1222, a new wave of nomads—the warlike Mongols—arrived in the Black Sea steppes and defeated the Polovtsians as part of the Mongol Empire's westward expansion. The Polovtsians, who had family ties with the Rus' princes, turned to Rus' for help. A joint Rus'-Polovtsian force marched into the steppes, where the Battle of the Kalka River took place in 1223. Following the battle, the Mongols retreated eastward for approximately 15 years.[80]
In the late 1230s, the Mongols returned with significantly larger forces under the leadership of Batu Khan and his general Subutai. The Mongols first attacked the eastern regions of the state, using superior siege tactics to overwhelm local defenses. Between 1239 and 1240, the Mongols shifted their focus to the southern territories. In 1240, they laid siege to Kiev, which ultimately fell after a brutal assault, marking the final collapse of Kievan Rus' as a unified political entity.[81][82]
As a result of this invasion, Kievan Rus' was extensively destroyed, depopulated, and fragmented. Following their conquest, the Mongols established dominion over the region through the Golden Horde, transforming most of the Rus' principalities into tributary states. Under Mongol rule, the Rus' were required to pay heavy taxes and submit to Mongol authority. The invasion severely stunted the region's political, cultural, and economic development, and the effects of Mongol domination reverberated for centuries. Despite its devastation, the Eastern Orthodox Church played an important role in preserving cultural identity during Mongol rule.[83][84]
The Principality—later the Kingdom of Galicia–Volhynia (Ruthenia)—emerged from the disintegration of Kievan Rus'. Its rulers continued the political and cultural legacy of Kiev, preserving the traditions and governance of the Rus' state even as Kiev fell to Mongol control.[8][85] Previously, Vladimir the Great had established the cities of Halych and Volodymyr as regional capitals, setting the foundation for future political entities in the region. The area was originally inhabited by the Dulebes, Tivertsi, and White Croats tribes.[86] Initially, Volhynia and Galicia existed as separate principalities, each ruled by descendants of Yaroslav the Wise. Galicia was governed by the Rostislavich dynasty, while Volhynia was initially ruled by the Igorevichs and eventually by the Iziaslavich dynasty.[87] During the reign of Yaroslav Osmomysl (1153–1187), Galicia expanded its influence, extending as far as the Black Sea.[87]
Rivalry between the rulers of these principalities often revolved around efforts to assert dominance over one another. This conflict was finally resolved by Roman the Great (1197–1205), who not only succeeded in uniting Galicia and Volhynia under his rule but also briefly extended his authority over Kiev. Roman's consolidation of power laid the groundwork for the rise of the Principality of Galicia–Volhynia, which became a significant political force in the region.[88]
Following Roman's death, a period of unrest ensued, lasting until his son Daniel Romanovich reclaimed the throne in 1238. Daniel successfully restored his father's state, including Kiev. In 1240, the Mongol Empire, led by Batu Khan, unleashed devastating invasions across Kievan Rus'. Cities like Kiev were sacked, leaving much of the region in ruins. Daniel's decisive victory at the Battle of Yaroslavl in 1245 ended internal conflicts and firmly secured his control over the state. By 1246, Daniel was compelled to recognize Mongol supremacy to safeguard his state. Though this submission was humiliating, it allowed Daniel to retain some degree of autonomy, provided he paid tribute to the Mongol khan.[89]
Despite this subjugation, Daniel remained determined to free Ruthenia from Mongol dominance. He sought support from Western Europe, forging diplomatic ties with the Papacy, Poland, Hungary, and the Holy Roman Empire. In 1253, Daniel was crowned King of Ruthenia (Latin: Rex Rusiae) by a papal legate in Drohiczyn, receiving formal recognition from Pope Innocent IV. This coronation symbolized Ruthenia's alignment with Western Europe and its Christian identity, while still adhering to the Orthodox faith. The crowning also carried the hope of an anti-Mongol alliance, though such a coalition never materialized due to political divisions in Western Europe.[90] King Daniel also founded numerous cities that became hubs of trade, culture, and military strength. Among his most notable achievements were the establishments of Lviv, named after his son Lev, and Kholm. These urban centers not only revitalized the kingdom's economy but also helped Daniel consolidate his political authority over the region.[91][92]
After King Daniel Romanovych's death in 1264, the kingdom was inherited by his son, Lev Danylovych. Lev I ruled from 1269 to 1301 and relocated the capital from Kholm to Lviv. He continued his father's policies of defending the kingdom against external threats, particularly from the Mongols, while also striving to maintain alliances with Poland and Lithuania to counterbalance Mongol influence. Although Lev managed to preserve the state's territorial integrity, the kingdom's power gradually eroded under the relentless pressure from the Mongol Golden Horde and ongoing internal political challenges.[93][94] Following Lev's death in 1301, his son, Yuri Lvovych, ascended the throne and ruled until 1308. During his reign, Yuri I succeeded in maintaining relative stability, yet the kingdom's political position had significantly weakened compared to the time of Daniel. Despite his efforts, Yuri's reign was short-lived, and after his death, the kingdom entered a period of fragmentation.[95]
Yuri's sons, Andrew and Lev II, attempted to co-rule, but their efforts to sustain the kingdom's unity and strength ultimately failed. The Mongol Golden Horde continued to exert substantial influence over the weakened Kingdom of Ruthenia throughout the 14th century.[96][97] After the deaths of the co-rulers around 1323, the kingdom found itself without strong leadership. Yuri II Boleslav, the last ruler of the Kingdom of Ruthenia, took power following the demise of the Romanovych line. Reigning from 1323 to 1340, he was a member of the Polish Piast dynasty, chosen as king due to his maternal connection to the Ruthenian royal family—his mother, Maria, was the daughter of Lev I.[98] Upon ascending the throne, Boleslav converted to Orthodox Christianity and adopted the name Yuriy to garner support from the local Orthodox nobility. However, his reign was fraught with tension due to his Catholic background, which clashed with the predominantly Orthodox Ruthenian elite. During his rule, Yuriy II struggled to balance the competing interests of Poland, Lithuania, and the Mongol Golden Horde, while also introducing pro-Catholic policies that alienated the local nobility. His rule culminated in his assassination in 1340 by Ruthenian nobles discontented with his leadership. His death triggered a fierce power struggle between Poland and Lithuania for control over the region. Soon after, Galicia was annexed by Poland, and Volhynia fell under Lithuanian rule, marking the end of the independent Kingdom of Ruthenia.[99]
From the 13th century onwards, parts of Ukraine’s Black Sea called Genoese Gazaria came under the influence of the Republic of Genoa, which established fortified trading colonies. These included key settlements such as Caffa (modern Feodosia), Soldaia (modern Sudak), and others in today’s Odesa Oblast. These fortresses, heavily guarded by Genoese garrisons, functioned as hubs for maritime trade and ensured Genoa's control over Black Sea commerce. The Genoese dominance, however, faced challenges from neighboring powers and ended with the Ottoman conquest in 1475.[100][101][102]
In 1322, Pope John XXII established a Catholic diocese in Caffa (modern-day Feodosia), marking the only Catholic stronghold within Mongol-controlled territories. It served as a central religious authority from the Balkans to the capital of the Golden Horde, Sarai. The Genoese settlements were not only economic centers but also a point of cultural and religious interaction between Europe, the Mongol Empire, and the Middle East.[103]
By the mid-14th century, the Grand Duchy of Lithuania expanded into the territories of modern Ukraine. After the decisive Battle of the Blue Waters against the Golden Horde (1362/63), Lithuania annexed Polotsk, Volhynia, Chernihiv, and Kyiv. Lithuanian rulers adhered to the principle “We do not disturb the old, nor do we introduce the new”, which allowed local Ruthenian (Ukrainian) traditions, religion, and administration to remain largely intact. The Lithuanian rulers styled themselves as “rulers of Rus’”, integrating Ruthenian traditions and governance into their system. This integration included Ruthenian aristocrats, like the Olelkovich family, who became influential in the Lithuanian administration. Old Church Slavonic and Ruthenian served as primary administrative languages alongside Latin.[104]
Simultaneously, Poland began asserting influence over western Ukraine. Red Ruthenia, parts of Volhynia, and Podolia were incorporated into the Kingdom of Poland, and the Polish monarch adopted the title "lord and heir of Ruthenia" (Latin: Russiae dominus et Heres).[105] Meanwhile, Kingdom of Hungary also maintained influence, particularly over Zakarpattia (Transcarpathia) and parts of Bessarabia. The Hungarian Crown controlled these regions, fostering Magyarization and Catholic influence while integrating them into the Hungary’s political and economic system.[106]
Since the 14th century, Poland and Lithuania had developed a unique and evolving relationship, often defined by dynastic unions, military alliances, and shared geopolitical interests, particularly in countering external threats. These included the aggressive expansion of the Teutonic Knights in the Baltic region and the rise of the Principality of Moscow to the east. The need for mutual support in the face of these common enemies gradually led the two states toward closer cooperation. This relationship reached a decisive turning point with the Union of Krewo in 1385, which marked the beginning of a new political era. Through the marriage of Jadwiga of Poland, the Queen of Poland, and Władysław II Jagiełło, the Grand Duke of Lithuania, the Lithuanian Grand Duchy was brought under Polish influence. This union was not just a political arrangement but a dynastic consolidation, with Jagiełło converting to Christianity and agreeing to unite Lithuania with Poland under his rule. This union established the Jagiellonian dynasty, which would go on to rule both Poland and Lithuania for several generations. The creation of this shared monarchy laid the foundation for an increasingly intertwined relationship between the two states, leading to greater cooperation in military, political, and cultural matters.[107][108]
In the early 15th century, tensions within the Grand Duchy of Lithuania, Ruthenia and Samogitia over the governance of the Rus' lands intensified. These tensions escalated into a power struggle following the death of Grand Duke Vytautas in 1430. Vytautas had been a powerful and unifying figure, and his passing left a power vacuum that deepened divisions within the Lithuanian elite. Two main contenders for the throne emerged: Švitrigaila, who had the support of the Orthodox Rus' nobility, and Sigismund Kęstutaitis, who represented the Catholic Lithuanian elite.[109] In 1432, Sigismund Kęstutaitis declared himself the new Grand Duke of Lithuania. To consolidate his rule and gain favor with the Rus' lands, he proclaimed the creation of a new political entity—the Grand Principality of Rus' (1432—1435) [uk]. This new formation was intended to include several important regions, such as Volhynia, Podolia, Siveria, Berestia, and Kyivshchyna, with Kyiv as its capital. Despite this declaration, Švitrigaila remained formally the head of the Grand Duchy of Lithuania and sought to transform the Rus' lands into an independent state under his control. His ambition to sever the ties with the rest of the Grand Duchy sparked a civil war that raged from 1432 to 1435, plunging Lithuania into internal conflict.[110]
Švitrigaila's forces, which included early Cossack, sought external support from the Teutonic Order, which had its own interests in weakening both Poland and Lithuania. The Teutonic Order's involvement further complicated the political situation. However, Sigismund Kęstutaitis found strong backing from Polish King Władysław II Jagiełło (Jogaila), as well as from the majority of the Lithuanian nobility. This external support proved pivotal in the conflict, tipping the scales in Sigismund's favor. The turning point in the war came with the Battle of Wiłkomierz (modern-day Ukmergė, Lithuania) on September 1, 1435. This battle was one of the largest and most decisive military engagements in Eastern Europe during the 15th century. Švitrigaila's forces were decisively defeated, suffering a crushing blow that effectively ended the Grand Principality of Rus'. The defeat led to the collapse of Švitrigaila's short-lived state and ensured the continuation of Lithuanian rule over the Rus' lands. With the victory, Sigismund Kęstutaitis solidified his power as Grand Duke, and the Rus' territories were once again integrated into the Grand Duchy of Lithuania. However, these lands would now be firmly under Lithuanian control, with no 
significant autonomy for the Rus' people. The failure of the Grand Principality of Rus' also marked a significant shift in the governance of the region, as the aspirations of the Rus' nobility for greater independence were suppressed.[110]
Southwestern Crimea saw the emergence of the Principality of Theodoro, also known as Gothia, a Byzantine successor state. Centered in Doros (modern Mangup), Theodoro was a multi-ethnic entity populated by Greeks, Goths, Alans, and others. Its rulers maintained close ties with the Byzantine Empire and the Empire of Trebizond. The principality acted as a buffer between Genoese colonies and the expanding Crimean Khanate. Despite its strong fortifications, Theodoro was conquered by the Ottoman Empire in 1475, as the Ottomans sought full dominance over the Black Sea region.[111]
The increasing dominance of the Polish nobility over Ukrainian lands in the late 15th century sparked resistance, most notably Mukha’s Rebellion in 1490. Led by Petro Mukha and supported by Moldavian Prince Stephen the Great, the uprising involved early Cossacks, Hutsuls, and Romanians (Moldavians). Mukha’s forces captured several towns in Pokuttya and advanced toward Lviv, though they failed to take the city. The rebellion, while ultimately suppressed, revealed growing dissatisfaction among Ukrainians under foreign rule and foreshadowed future uprisings in the region.[112]
As the Golden Horde declined in the 15th century, the Crimean Khanate emerged as a dominant power in the southern Ukrainian steppe. Centered around the city of Bakhchisarai, the Khanate controlled much of the Black Sea coastline, including key ports and strategic territories in what is today southern Ukraine. This rise to power marked a significant shift in the region, as the Crimean Khanate not only played a key military and political role but also became a vital player in the economy of the Black Sea and beyond.[113][114]
From the early 16th century to the late 18th century, the Crimean Khanate was deeply involved in the region’s slave trade, which became one of its main economic drivers. The Khanate, often acting as an intermediary between the steppes and the Ottoman Empire, exported an estimated 2 million slaves to slavery in the Ottoman Empire and the Middle East between 1500 and 1700. These slaves, many of whom were captured in raids on neighboring lands, were primarily sold to the Ottoman Empire, where they were used in various sectors, including the military, households, and harems.[115]
Despite its growing power, the Crimean Khanate remained a vassal state of the Ottoman Empire throughout much of its history. This relationship provided the Khanate with military support and protection, but also limited its autonomy. The Crimean Khanate’s role as a vassal did not diminish its influence, however, and it continued to exert significant control over the Black Sea region until the late 18th century.[116]
After the Union of Lublin in 1569, which united the Kingdom of Poland and the Grand Duchy of Lithuania into the Polish–Lithuanian Commonwealth, Ukrainian lands transitioned under Polish administration, becoming part of the Crown of the Kingdom of Poland. The union was driven by the need to counter external threats, primarily the growing aggression of Tsardom of Moscow and persistent raids by the Teutonic Order, prompting both states to consolidate their resources and strengthen their defenses. This political shift marked a significant transformation of Ukraine's social, economic, and cultural landscape. Polish authorities initiated large-scale colonization efforts, resulting in the foundation of numerous new towns and villages, particularly in the steppe regions. Settlers brought new legal norms, governance practices, and economic models, integrating Ukrainian lands more closely into the Commonwealth's political system.[117][118]
At the same time, Renaissance ideas began to permeate Ukrainian society, driven by the spread of new schools and educational institutions, which were often established by the Jesuits. The arrival of Polish settlers in large numbers led to cultural assimilation. Over time, a significant portion of the Ukrainian nobility became 'polonized', adopting Polish customs, language, and converting to Roman Catholicism. However, the majority of Ruthenian-speaking peasants remained loyal to the Eastern Orthodox Church, which caused growing social and religious tensions. Polish influence extended not only politically but also culturally. Some polonized Ukrainian nobles, such as Stanisław Orzechowski, made notable contributions to Polish intellectual and cultural life, writing influential works on theology, law, and politics. These changes contributed to the fragmentation of Ukrainian society, as the gap between the Catholic, Polish-speaking nobility and the Orthodox, Ruthenian-speaking peasants widened, creating the foundations for future conflicts.[119][120]
The incorporation of Kyiv, Volhynia, Podolia, and Bratslav lands into the Crown also strengthened internal connections between various Ukrainian regions. New trade routes and urban centers facilitated greater economic, social, and cultural interaction across these previously fragmented territories, fostering regional cohesion. However, despite the spread of Polish culture and governance, the Ukrainian population maintained a distinct identity rooted in Orthodox traditions and Ruthenian language, which became a key factor in their resistance to cultural assimilation.[121]
At the same time, the harsh conditions imposed on Ruthenian peasants by the Polish nobility sparked widespread resistance. As serfdom expanded and economic exploitation intensified, many peasants fled to the borderlands of the Dnipro region, seeking freedom and a better life. These frontier settlers became known as Cossacks, forming a distinct social and military group known for their martial prowess, independence, and deep ties to the Orthodox faith. The Cossacks quickly gained a reputation as skilled warriors, adept at defending the southeastern borders of the Commonwealth from Tatar raids. Recognizing their military value, the Polish authorities enlisted Cossacks into the Commonwealth’s army, particularly for the defense of frontier regions, and granted them limited privileges through the creation of the Registered Cossacks in the late 16th century. Petro Konashevych-Sahaidachny, one of the most prominent Cossack leaders, played a decisive role in the Battle of Khotyn in 1621, where the Commonwealth, with Cossack support, repelled a massive Ottoman army. His leadership and diplomatic efforts also strengthened ties with the Orthodox Church, as he sought to restore the Church’s influence in Ukrainian society.[122]
However, despite their significant contributions to the Commonwealth’s military victories, the Cossacks were denied substantial political or social autonomy. The Szlachta (Polish nobility), which dominated the Commonwealth’s political system, viewed the Cossacks as a disruptive and uncontrollable force, refusing to recognize them as a distinct social class. Instead, the nobility made repeated attempts to reduce the Cossacks to the status of serfs, depriving them of their privileges and freedoms. This ongoing marginalization and denial of rights led to a series of Cossack uprisings, as they sought to defend their autonomy, way of life, and religious identity. Notable rebellions included the Kosiński Uprising (1591-1593) and the Nalyvaiko Uprising (1594-1596), both of which were brutally suppressed by the Polish authorities. These rebellions, however, underscored the growing discontent among the Cossacks and highlighted the tension between the Commonwealth’s central government and the frontier population.[123]
By the early 17th century, the situation became increasingly volatile, with Cossack demands for recognition of their rights being continuously ignored. The Commonwealth's refusal to integrate the Cossacks politically and respect their distinct cultural and religious identity ultimately set the stage for larger conflicts, most notably the Khmelnytsky Uprising of 1648, also known as the Cossack–Polish War,[124] which profoundly reshaped the political landscape of Eastern Europe.[125]
The Cossack Hetmanate, also known as the Hetmanate of Ukraine, was a self-governing Cossack state that existed between 1649 and 1764. It arose during a particularly volatile era in Eastern European history, following the seismic upheaval of the Khmelnytsky Uprising. This revolt, led by the charismatic and astute military leader Bohdan Khmelnytsky, sought to liberate Ukrainian lands from the dominion of the Polish-Lithuanian Commonwealth. The rebellion not only destabilized one of Europe’s most prominent states at the time but also triggered a broader period of chaos in the region, known in Ukrainian history as the "Ruin", while in Polish historiography it is referred to as "the Deluge".[127]
The establishment of the Cossack Hetmanate in 1649 marked a pivotal shift in the political landscape of Eastern Europe. Officially recognized through the Treaty of Zboriv that same year, the Hetmanate emerged as a semi-autonomous entity within the Polish-Lithuanian Commonwealth. Yet, this fragile self-governance was immediately tested by a turbulent geopolitical environment. The state found itself entangled in a web of diplomatic and military conflicts involving major regional powers, including the Ottoman Empire, the Commonwealth, the Crimean Khanate, and the Tsardom of Moscow. For these competing entities, the Hetmanate was not just a potential ally, but also a strategic prize, which complicated its quest for sovereignty and stability.[127]
As ruler of the Hetmanate, Khmelnytsky engaged in state-building across multiple spheres: military, administration, finance, economics, and culture. He invested the Zaporozhian Host under the leadership of its hetman with supreme power in the new Ruthenian state, and he unified all the spheres of Ukrainian society under his authority. This involved building a government system and a developed military and civilian administration out of Cossack officers and Ruthenian nobles, as well as the establishment of an elite within the Cossack Hetman state.[128]
In 1654, under the continuous pressure of Poland and in pursuit of a more secure route to sovereignty, the Cossack leadership, headed by Bohdan Khmelnytsky, entered into the Pereyaslav Agreement with the Tsardom of Russia. This pact positioned the Hetmanate under the Russian protection, signaling a significant shift in the region’s balance of power. While the agreement initially promised mutual military support and guaranteed a degree of autonomy for the Cossacks, it also laid the groundwork for Russia's gradual encroachment on Ukrainian governance. Over time, the aspirations of the Hetmanate for true independence were systematically undermined, as the Tsardom’s ambitions to consolidate control over the territory took precedence.[129][127]
Efforts to reclaim autonomy and negotiate favorable terms with neighboring powers continued, culminating in the 1658 Treaty of Hadiach. This ambitious agreement aimed to elevate the Hetmanate to the status of the Grand Principality of Rus', an equal partner within the Commonwealth alongside Poland and Lithuania. While the treaty represented a significant step toward reconciliation, it was ultimately derailed by fierce Russian opposition and deep-seated divisions within the Ukrainian leadership. Moreover, the agreement failed to stabilize the region, as ongoing conflicts, including Russian military campaigns and internal unrest, plunged the Hetmanate into over a decade of turmoil, leaving its aspirations for autonomy unfulfilled.[130][131]
The Truce of Andrusovo in 1667 marked a pivotal moment in Ukrainian history, as the Polish-Lithuanian Commonwealth and the Tsardom of Russia formalized an agreement to partition the Hetmanate's territories along the Dnieper River. Under the terms of the treaty, Right-Bank Ukraine was returned to Polish control, while Left-Bank Ukraine, including Kyiv, was ceded to Russia. Although the Hetmanate retained a nominal degree of autonomy on the Left Bank, this autonomy was heavily constrained by Russian oversight. The division not only symbolized the beginning of a new long-term foreign domination of Ukrainian lands, but also violated the unity of the Cossack state. This fragmentation further weakened the Hetmanate's capacity to resist external pressures and maintain its independence, leaving it increasingly vulnerable to the ambitions of neighboring powers.[127]
The late 17th century was characterized by civil strife, foreign intervention, and territorial shifts. From 1657 to 1686, the region was embroiled in nearly constant conflict, with neighboring powers, including the Ottoman Empire, Poland, and Russia, capitalizing on Ukraine's vulnerability. During this period, Hetman Petro Doroshenko, a prominent Cossack leader, sought to consolidate control over key territories such as Kyiv and Bratslav. However, his ambitions were thwarted as these areas fell under Turkish occupation, further complicating the geopolitical landscape.[132]
In the broader context, the Treaty of Karlowitz in 1699 played a pivotal role in redefining territorial control. It resulted in the return of much of the land previously held by the Ottomans to Polish authority. Despite this, Ukrainian Cossack forces, especially in the frontier regions of Zaporizhzhia and Sloboda Ukraine, managed to maintain a degree of autonomy. Semi-independent Cossack republics in these areas continued to challenge both Polish and Russian dominance, asserting their independence and preserving distinct cultural and political identities.[132]
In addition to political turbulence, the Hetmanate played a pivotal role in the cultural and intellectual development of both Ukraine and Russia during the late 17th and early 18th centuries. Ukrainian clerics, scholars, and intellectuals, many of whom were educated at the esteemed Kyiv-Mohyla Academy, served as key agents of cultural exchange and reform. Their influence was particularly pronounced during the reign of Peter the Great, as they introduced new ideas and approaches that helped modernize Russian society. Figures such as Theophan Prokopovich and Stefan Yavorsky, both leading members of the Most Holy Synod, played an important role in shaping religious and educational reforms in Russia. The intellectual movement often referred to as the 'Ukrainian school' left a lasting imprint on Russian literature, theology, and pedagogy, becoming a dominant force in the region's cultural landscape.[133][134]
A critical turning point in Ukraine's religious history occurred in 1686, when the Metropolis of Kyiv was annexed by the Moscow Patriarchate. This event symbolized the consolidation of Moscow's influence over Ukrainian religious and cultural life. This annexation was a significant step towards the gradual erosion of Ukrainian ecclesiastical independence. Nonetheless, the Hetmanate’s leadership continued to assert their autonomy, navigating complex political realities to safeguard their unique identity and governance. The interplay between cooperation and resistance defined this era, highlighting Ukraine's dual role as both a cultural sponsor of Moscovy's reforms and a defender of its own sovereignty.[135]
The Hetmanate reached its peak of independence under the leadership of Hetman Ivan Mazepa, a complex and ambitious figure in Ukrainian history. Seeking to assert the autonomy of the Hetmanate, Mazepa pursued an alliance with the Swedish Empire during the Great Northern War (1700–1721), hoping to break free from Russian dominance. His decision to align with Charles XII of Sweden was a bold but perilous move, rooted in the desire to safeguard Ukrainian sovereignty and escape the tightening grip of Moscow.[136]
The turning point came in 1708, when the Moscovy’s army, under orders from Tsar Peter I, sacked the Hetmanate’s capital city, Baturyn. The brutal assault resulted in the massacre of thousands of defenders and civilians, with many burned alive or executed in other horrific ways. This act of retribution was a devastating blow to Mazepa’s efforts, as it not only destroyed the administrative and military center of the Hetmanate but also served as a warning to other potential dissenters.[137] Mazepa's rebellion culminated in the catastrophic defeat at the Battle of Poltava in 1709, where the combined Swedish-Ukrainian forces were decisively crushed by Peter’s army. The aftermath of Poltava marked the end of Mazepa’s aspirations for Ukrainian independence and significantly tightened Russia's control over the Hetmanate. The rebellion’s failure also signaled a shift in the balance of power in Eastern Europe, consolidating Russia’s dominance in the region and diminishing the Hetmanate’s autonomy.[138][139]
After the defeat at Poltava, Ivan Skoropadsky led the Hetmanate (1708–1722) during a challenging period of diminishing autonomy. While he sought to navigate the growing pressures from Russia, his efforts were significantly undermined in 1722 when Peter I established the Little Russian Collegium. This administrative body, composed largely of imperial officials, was tasked with supervising the Hetmanate’s governance, effectively curtailing Skoropadsky’s authority and undermining traditional Cossack institutions. His death in 1722 left the Hetmanate vulnerable to further imperial centralization.[140] Following Skoropadsky’s death, Pavlo Polubotok, serving as acting Hetman, attempted to resist Moscow’s encroachments. Polubotok appealed to Peter I to honor the Hetmanate’s earlier agreements, particularly those guaranteeing autonomy. However, his resistance led to his arrest and imprisonment in the Peter and Paul Fortress, where he died in 1724. Polubotok’s fate symbolized the futility of overt resistance during this phase, as the imperial government tightened its grip on the region.[141]
A brief resurgence of autonomy occurred under Danylo Apostol (1727–1734), whose election marked a temporary relaxation of imperial policies following Peter I’s death. Apostol negotiated limited autonomy for the Hetmanate, securing greater control over local taxation and governance. He also implemented administrative and judicial reforms aimed at strengthening internal stability. Despite these achievements, Apostol’s death led to another period of direct imperial administration under the Governing Council of the Hetman Office, a body dominated by Russian officials. This period (1734–1750) deepened the integration of the Hetmanate into the imperial administrative structure.[142]
In 1750, the Hetmanate was restored under Kyrylo Rozumovsky, a politically savvy leader with close ties to the Russian court. Rozumovsky sought to modernize the Hetmanate by reforming its administration, judiciary, and military while working to strengthen its autonomy. He relocated the Hetmanate’s capital to Hlukhiv, transforming it into a political and cultural center. Rozumovsky also initiated ambitious projects, including plans to establish a university, reflecting his vision of the Hetmanate as a semi-autonomous, modernized polity within the Russian Empire. However, his growing influence and calls for greater independence alarmed Catherine the Great, who was determined to centralize power.[143][144]
In 1764, Catherine the Great formally abolished the Hetmanate, transferring its governance to the Little Russian Collegium. This move marked the end of the Hetmanate’s semi-autonomous status and its complete incorporation into the administrative structure of the Russian Empire. The abolition not only dismantled the Hetmanate’s unique political and military institutions but also signified the culmination of a broader imperial strategy to suppress regional autonomy in favor of centralized governance.[145]
During the 18th century, the Russian Empire transitioned from providing nominal "protection" to exerting direct control over central Ukraine, progressively eroding the autonomy of the Cossacks. The Cossack uprisings, which had once been directed against the Polish-Lithuanian Commonwealth, now shifted focus to the Russian authorities. By the late 18th century, these uprisings had largely subsided, their potency undermined by the overwhelming might of the Russian Empire and deep divisions within the Cossack leadership.[146]
The Russo-Turkish War of 1768–1774 marked a pivotal moment for the Ukrainian Cossacks and their role within the Russian Empire. The Cossacks had provided crucial support during the war, significantly contributing to Russian victories. The conflict culminated in the Treaty of Küçük Kaynarca, which granted Russia significant territorial acquisitions along the Black Sea coast, further consolidating its influence in the region. This success reduced the strategic importance of the Zaporozhian Sich, as the borderlands the Cossacks had long defended were now firmly under Russian control. The geopolitical realignment weakened the Cossacks’ traditional role, setting the stage for their eventual demise.[147]
In 1775, Empress Catherine the Great took decisive action to eliminate what she perceived as a potential threat to her centralized rule. The Zaporozhian Sich, with its semi-autonomous status and militarized society, stood as a symbol of independence that clashed with the empire’s drive toward unification. Acting on her orders, General Peter Tekeli led a massive military force to suppress the Sich. On June 15, 1775, Tekeli’s forces, composed of 45,000 troops—including 8 cavalry regiments, 10 infantry regiments, 20 hussar squadrons, 17 pike squadrons, and 13 squadrons of Don Cossacks—overwhelmed the Sich. Defended by a mere 3,000 Cossacks, the Sich fell with little resistance. The destruction of the Zaporozhian Sich marked the end of an era, effectively dismantling one of the last vestiges of Ukrainian self-rule.[148]
The last Kosh Otaman, Petro Kalnyshevsky, was arrested and exiled to the Solovetsky Islands in the far north, where he spent the remainder of his life in harsh conditions, dying in captivity. Following the destruction, a smaller group of Cossacks fled to the lands of the Ottoman Empire, settling beyond the Danube, where they formed the Danubian Sich. Others relocated to the Kuban region in the Russian Empire, where they helped establish the Kuban Cossack Host. However, the majority of the Ukrainian Cossacks were deported to remote parts of the Russian Empire, including Siberia, in a move that sought to break their power and disperse their population. In addition to the loss of Cossack autonomy, the Russian troops seized over 30,000 documents, along with weapons and other valuables, representing the history of Ukraine from the 16th to the 18th centuries. These items were stored in the fortress of St. Elizabeth, which had been the primary military base of the Russian Imperial Army in Ukraine. These historical artifacts remained in St. Elizabeth’s fortress until they were eventually transferred to Kyiv in 1918, providing valuable insights into the rich history of the Ukrainian Cossacks.[149][150][151]
Meanwhile, in right-bank Ukraine, dissatisfaction with Polish rule had been growing for decades. Increased corvée (forced labor) obligations, along with the abuse of power by Polish magnates, nobles, and their Jewish stewards, led to widespread unrest. The peasants, many of whom were Orthodox Christians, resented the imposition of taxes and labor on their lands, as well as the presence of Roman Catholic and Uniate clergy. The resulting discontent gave rise to a series of Haidamak uprisings, in which bands of rebels attacked and looted towns, targeting the estates of nobles, clergy, and Jewish populations. Major uprisings occurred in 1734, 1750, and the largest—known as Koliyivschyna—took place in 1768. This revolt was a culmination of decades of accumulated grievances, sparked by rumors of impending changes in religious and social policies that threatened the Orthodox community. The rebellion saw widespread attacks across towns and estates, resulting in significant loss of life and property. While the Haidamaks initially achieved some successes, the uprising was ultimately brutally suppressed by Polish forces with the assistance of Russian troops.[152]
By the end of the century, Catherine the Great’s policies had reshaped the political landscape of Ukraine. The Cossack's role in defending the borderlands was no longer necessary, as the Russian Empire now controlled the Black Sea and Crimea. In 1783, the Crimean Khanate was formally annexed by Russia, cementing control over the northern Black Sea region. A few years earlier, in 1778, a mass emigration of Christians from Crimea occurred, further altering the demographic balance of the region. Finally, in 1793, following the Second Partition of Poland, right-bank Ukraine was officially incorporated into the Russian Empire. By the end of Catherine’s reign, most of Ukraine had fallen under Russian control, bringing an end to the centuries-old struggle for autonomy and leaving the Ukrainian people under the rule of the imperial power.[153]
Under the reign of Alexander I (1801–1825), the Russian presence in Ukraine was largely limited to the imperial army and its bureaucracy. However, by the reign of Nicholas I (1825–1855), Russia had established a centralized administration in Ukraine. After suppressing the November Uprising of 1830, the tsarist regime implemented Russification policies, particularly on the Right-bank Ukraine. These policies not only suppressed the Ukrainian language but also aimed to weaken local traditions by emphasizing loyalty to the Russian Orthodox Church, which actively promoted Russification in contrast to the Uniate Church's earlier influence in Western Ukraine.[154]
The 1861 emancipation of serfs had a profound effect on Ukraine, as 42% of its population were serfs. By the late 19th century, heavy taxes, rapid population growth, and land shortages led to widespread impoverishment among the peasantry. Many rural Ukrainians began migrating to cities, where industrialization and urban development created new economic opportunities but also fostered a growing sense of class consciousness. The construction of the first railway track in 1866 not only integrated Ukraine’s economy into the Russian imperial system but also facilitated the mobility of workers and goods. Despite their hardships, Ukrainian peasants and workers contributed significantly to the empire’s economy, with the steppe regions producing 20% of the world's wheat and 80% of the empire's sugar.[154]
The Ukrainian national revival began in the late 18th century with the emergence of modern Ukrainian literature, spearheaded by Ivan Kotliarevsky. Prominent 19th-century Ukrainian authors included Taras Shevchenko, Mykhailo Kotsiubynsky, and Lesya Ukrainka in the Russian Empire, and Ivan Franko in Austria-Hungary. The rise of a Ukrainian intelligentsia, increasingly composed of commoners and peasants, challenged the dominance of the traditional nobility and fueled the movement for national rights and social justice. However, fearing the rise of separatism, the Russian government imposed strict limits on Ukrainian language and culture. In 1863, the Valuev Circular banned the use of Ukrainian in religious and educational texts. Further repression came with the Ems Ukaz in 1876, which prohibited Ukrainian-language publications, the import of Ukrainian books from abroad, the use of Ukrainian in theater, and even public readings. Ukrainian schools were also suppressed. These policies prompted many Ukrainian intellectuals, such as Mykhailo Drahomanov and Mykhailo Hrushevsky, to flee to Austrian-ruled Western Ukraine.[155] In addition, the development of underground educational organizations, such as the "Prosvita" society, played a critical role in preserving Ukrainian culture. Despite the restrictions, Ukrainians within the Russian Empire sometimes succeeded in advancing within the system, often blending loyalty to the tsar with a subtle promotion of their heritage.
The fate of Ukrainians under the Austrian Empire was markedly different. In Austrian-ruled Galicia, Ukrainians found themselves in a delicate position within the broader Russian-Austrian rivalry for influence in Central and Southern Europe. Unlike in the Russian Empire, Galicia’s ruling elite were primarily of Austrian or Polish descent, while the Ruthenian population remained predominantly peasant. During the 19th century, Russophilia was initially common among Galicia’s Slavic population. However, the influx of Ukrainian intellectuals fleeing Russian repression, combined with Austrian intervention, gradually replaced Russophilia with a growing Ukrainophilia. This sentiment spread back into Russian-ruled Ukraine, fueling the national revival.[154]
The 2.4 million Ukrainians under Habsburg rule lived primarily in Eastern Galicia, with 95% of them being peasants. The region remained one of the poorest in Europe, with persistent land shortages and limited industrialization. Nevertheless, the Greek Catholic Church in Galicia became a key institution in preserving Ukrainian culture and fostering national identity. The first Ukrainian-language newspaper, Zoria Halytska, launched in 1848, symbolized the growing national awakening. Many Ukrainians from Galicia and other Austrian territories also emigrated to North America and South America during this period, seeking economic opportunities and escaping poverty.[156]
The late 19th century witnessed a slow but steady growth of Ukrainian urban populations and the beginnings of a political awakening. Ukrainians in Galicia formed the Supreme Ruthenian Council and began advocating for autonomy and reforms, such as land redistribution. In Russian Ukraine, underground networks spread literature, education, and national ideas among the peasantry, contributing to the resilience of Ukrainian identity under challenging conditions.
World War I and the wave of revolutions that swept across Europe—including the October Revolution in Russia—shattered empires such as the Austro-Hungarian and Russian Empires, leaving Ukraine caught in the midst of geopolitical upheaval. Between 1917 and 1919, several Ukrainian republics declared independence, marking the emergence of a complex array of states and territories seeking sovereignty. Among these were the Ukrainian People's Republic, the Ukrainian State, the West Ukrainian People's Republic, the Makhnovshchina, the Kholodny Yar Republic, and the Kuban People's Republic. Concurrently, a number of Bolshevik revolutionary committees, or revkoms, sought to establish Soviet power, leading to the formation of various Soviet-aligned entities, including the Ukrainian People's Republic of Soviets, the Odessa Soviet Republic, the Donetsk–Krivoy Rog Soviet Republic, the Ukrainian Soviet Republic, the Taurida Soviet Socialist Republic, the Galician Soviet Socialist Republic, and the Ukrainian Soviet Socialist Republic. Each of these republics and regimes represented different visions for Ukraine's future, reflecting the era's ideological and territorial conflicts that would profoundly impact the region.
The Ukrainian People's Republic (UPR) was officially proclaimed on November 20, 1917, amidst the turmoil of the Russian Revolution and the disintegration of the Russian Empire. Initially, the Ukrainian Central Council (Rada), comprising influential Ukrainian political figures, pursued autonomy within a federated Russia. However, as the political situation in Russia grew increasingly unstable, the UPR took a decisive step by declaring full independence on January 22, 1918.[157]
From its inception, the fledgling UPR faced significant challenges. Internally, political divisions among various factions, including socialists, nationalists, and federalists, complicated governance and decision-making. Economically, the nascent republic struggled with the transition from imperial control to an independent administration, resulting in shortages, inflation, and a weakened infrastructure. Externally, the UPR faced threats from multiple sides, primarily from the Bolsheviks, who regarded Ukraine as essential to their revolutionary agenda. As a result, they launched a series of military campaigns to assert control over Ukrainian territories, triggering prolonged and intense conflicts with UPR forces.[158]
As the Bolsheviks sought to expand their influence across the former territories of the Russian Empire, Ukraine became a significant battleground. In December 1917, amidst the chaos of the Russian Revolution and the collapse of imperial power, the Ukrainian People's Republic of Soviets was proclaimed. This was a direct challenge to the Ukrainian People's Republic, which had declared its independence from the Russian Empire earlier that year. The UPR, led by nationalists and democrats, sought to build an independent Ukrainian state. In contrast, the Bolshevik-backed People's Republic of Soviets aimed to bring Ukraine under Soviet control and align it with the goals of the Russian Bolsheviks.[159]
In March 1918, this newly established republic merged with two other short-lived Soviet republics in the region: the Donetsk–Krivoy Rog Soviet Republic and the Odessa Soviet Republic. These republics were formed by local Bolshevik groups seeking to establish Soviet power across key industrial and strategic regions of Ukraine. The result of this merger was the Ukrainian Soviet Republic, a state that was aligned with Soviet Russia and part of the larger efforts of the Bolsheviks to secure control over Ukraine during the chaotic period of civil war and foreign intervention. This period was marked by fierce conflicts between various Ukrainian factions, including the Ukrainian People's Republic, anarchists, and foreign powers, alongside the advancing Bolshevik forces, contributing to the overall instability of the region. When the Bolshevik troops retreated from the territory, on April 18, 1918, the Ukrainian Soviet Republic was officially dissolved.[160][161]
Amid growing unrest, a coup d'état led by General Pavlo Skoropadskyi on April 29, 1918, dismantled the UPR and established the Ukrainian State, also known as the Hetmanate. Skoropadskyi, a former officer in the Russian Imperial Army, assumed the title of Hetman of all Ukraine, aiming to create a strong, centralized state with close ties to the German Empire and Austria-Hungary. His vision for the Hetmanate included restoring order, promoting economic development, and implementing agrarian reforms to stabilize the economy and society.[162]
While the Hetmanate initially brought some stability, Skoropadskyi's alignment with the Central Powers and his authoritarian policies alienated many Ukrainians. Nationalists, socialists, and peasant groups grew increasingly discontented, perceiving his regime as prioritizing foreign interests over Ukrainian sovereignty. Additionally, Skoropadskyi's agricultural reforms, which often favored large landowners and reinstated some pre-revolutionary land policies, furthered resentment among the rural populace.[163]
As the Central Powers began to crumble in late 1918 with their defeat in World War I, opposition to Skoropadskyi's rule surged. In November 1918, a coalition of anti-Hetmanate forces known as The Directorate, led by Symon Petliura, Volodymyr Vynnychenko, and other prominent Ukrainian leaders, initiated a successful uprising against Skoropadskyi’s government. By December 1918, Skoropadskyi was forced to abdicate, dissolving the Ukrainian State and restoring the Ukrainian People's Republic.[164]
Simultaneously, a separate Ukrainian state was established in the western part of the country. The West Ukrainian People's Republic (WUPR) was proclaimed on October 19, 1918, following the disintegration of the Austro-Hungarian Empire. This new state centered around Eastern Galicia, including the key city of Lviv, Transcarpathia and extended into parts of Bukovina, areas with substantial Ukrainian populations. The WUPR government, led by Yevhen Petrushevych, aspired to build an independent Ukrainian state in Western Ukraine, distinct from both Polish and Russian influences.[165][166]
The WUPR quickly developed its administrative structures, forming the Ukrainian Galician Army to defend its territories and introducing social and economic reforms to stabilize the new state. However, the WUPR’s claim over Eastern Galicia sparked immediate conflict with the re-established Polish state, which also sought control over the region. This territorial dispute erupted into the Polish-Ukrainian War (1918-1919), beginning with fierce fighting in Lviv, where Polish paramilitary forces resisted Ukrainian authority.[167]
Although Ukrainian forces initially gained some ground, the Polish Army, bolstered by material support from the Entente Powers, soon regained momentum. By mid-1919, the Polish Army launched a major offensive, pushing back the Ukrainian Galician Army and regaining control over contested areas. Facing overwhelming opposition, WUPR forces retreated into Ukrainian People's Republic (UPR) territory by July 1919, marking the end of the WUPR as an independent state.[168]
In an effort to consolidate their positions during a tumultuous period, UPR and WUPR formally united on January 22, 1919, through the signing of the Unification Act (Act Zluky). This historic declaration symbolized the unification of the two republics into a single Ukrainian state. Despite its significance as a milestone for Ukrainian national aspirations, the unification remained largely symbolic in practice. The lack of effective integration between the UPR and WUPR resulted in minimal military coordination or mutual support. Both entities were preoccupied with their respective military challenges: the UPR was engaged in a desperate struggle against advancing Bolshevik forces, while the WUPR was embroiled in a conflict with Polish troops over territorial claims in Eastern Galicia.[14]
The Makhnovshchina, which existed from 1918 to 1921, was a revolutionary anarchist movement in southern Ukraine led by Nestor Makhno. It emerged during the chaos of the Russian Civil War. The Makhnovists aimed to establish a stateless, self-managed society based on anarchist principles, where peasants and workers controlled the land and factories. Makhno's forces fought against various powers, including the Bolsheviks, the White Army, and foreign invaders. Despite initial successes, the movement was eventually crushed by the Bolsheviks.[169]
The Kholodny Yar Republic, which existed from 1919 to 1922, was a small Ukrainian insurgent state located in the Chyhyryn region. It was formed by local resistance fighters and peasants who opposed both the Bolshevik Red Army and White Army forces during the Russian Civil War. Inspired by Ukrainian independence and nationalist ideals, the republic remained independent for a few years but was eventually overwhelmed by Bolshevik forces.[170][171]
In May 1919, in central Ukraine began the Hryhoriv Uprising, largest anti-soviet Uprising in Ukraine, which was brutally suppressed by regular troops.[172]
The Kuban People's Republic, which existed from 1918 to 1920, was a short-lived state established by Cossacks in the Kuban region, near the Black Sea. In the wake of the Russian Empire's collapse during the revolution, the Kuban Cossacks declared independence and formed the Kuban Rada to govern. Their goal was to preserve their cultural identity and retain control over their land amid the Russian Civil War. The republic allied with the White Army against the Bolsheviks and sought to unite with the Ukrainian People's Republic (UPR) due to shared Cossack heritage and mutual strategic interests. However, these negotiations never culminated in a formal union, as the republic, isolated at the time, fell to the advancing Bolshevik Red Army in 1920, ending its brief independence.[173][174][175]
After being driven out of Kyiv by Bolshevik forces in early 1919, the UPR government, led by Symon Petliura, continued to resist Bolshevik advances and Polish encroachment. By 1920, facing insurmountable odds and a deteriorating military position, Petliura sought an alliance with Poland. In April 1920, the Treaty of Warsaw was signed, under which the UPR agreed to recognize Polish control over Western Ukraine in exchange for Polish military support against the Bolsheviks.[176]
The joint Polish-Ukrainian campaign initially achieved some success, including the temporary recapture of Kyiv in May 1920. However, the Bolshevik counter-offensive soon pushed back the allied forces. The situation for the UPR became even more precarious when Poland sought a peace agreement with Soviet Russia, culminating in the signing of the Treaty of Riga in March 1921. The treaty effectively partitioned Ukraine, leaving most of its territory under Soviet control and the western parts under Polish administration.[177]
With the signing of the Treaty of Riga, the UPR government went into exile, primarily in Poland and other European countries. Ukrainian leaders continued their efforts to advocate for Ukrainian independence in the international arena, but without a territorial base or significant military forces, their influence was limited. Symon Petliura, a key figure in the UPR, continued his political activities in exile until his assassination in Paris in 1926.[178]
Canadian scholar Orest Subtelny says:
Historian Paul Kubicek says:
As both the UPR and WUPR faced defeat, the Bolshevik forces consolidated their control over Ukraine. On December 30, 1919, the Bolsheviks proclaimed the establishment of the Ukrainian Socialist Soviet Republic (Ukrainian SSR), positioning it as a satellite of the Russian Soviet Federative Socialist Republic (RSFSR). The new Soviet government aimed to establish complete Soviet authority over all Ukrainian territories, incorporating Ukraine into the broader framework of Soviet expansion.[15]
The creation of the Ukrainian SSR marked the beginning of Soviet rule in Ukraine. Over the next two years, the Red Army systematically subdued remaining Ukrainian forces, nationalist resistance movements, and other anti-Bolshevik factions. By 1921, Bolshevik forces had largely crushed organized resistance, paving the way for the integration of the Ukrainian SSR as one of the founding republics of the Soviet Union in 1922.[181]
In the 1920s, the Soviet government implemented a policy of "Ukrainization" as part of its broader strategy to strengthen support for the Soviet regime in non-Russian republics. This policy encouraged the use of the Ukrainian language in education, government, and media. Ukrainian culture and history were promoted to win over the local population and intellectual elite. Ukrainization allowed a degree of cultural revival after years of Russian dominance in Ukraine. Ukrainian literature, theater, and arts experienced significant growth, and schools began teaching in the Ukrainian language. However, this policy was carefully controlled by the Communist Party, ensuring that cultural development aligned with Soviet ideology.[182]
Following the devastation of war and revolution, the Soviet government introduced the New Economic Policy (NEP) to stabilize the economy. It represented a temporary retreat from pure socialist policies, allowing some elements of private enterprise and market mechanisms to function alongside state-controlled industries. The NEP had a mixed impact on Ukraine. On one hand, it allowed limited economic recovery, especially in agriculture and small-scale industry. Peasants were permitted to sell surplus products on the market, and small businesses could operate under certain conditions. On the other hand, large-scale industries remained under state control, and the heavy industrial sector, which Ukraine relied on, remained inefficient and slow to recover. While the NEP offered some relief to peasants, many remained suspicious of Soviet power, particularly after the harsh grain requisition policies during the civil war. Tensions between the peasantry and the Soviet regime continued to simmer.[183]
During this period, the Communist Party tightened its control over Ukraine. The Communist Party of Ukraine (CPU) became a key instrument in enforcing Soviet policies and maintaining order. Political power was highly centralized, with decisions made in Moscow dictating policy in Ukraine. Despite the relative cultural freedom of Ukrainization, any political opposition to the Soviet regime was harshly repressed. Former nationalists, intellectuals, and opponents of Soviet power were marginalized, and any movement toward true Ukrainian autonomy was quickly suppressed.
In the early Soviet years, there was a strong emphasis on rebuilding Ukraine's war-ravaged economy. Ukraine was a critical industrial center, especially in coal, steel, and machinery production. While some infrastructure was rebuilt, economic challenges remained due to the inefficiency of state control and the lingering effects of war. Ukraine, being an agriculturally rich region, faced difficulties as the peasants were subjected to state control over grain production. Despite the NEP, rural areas continued to suffer from poverty, which would later fuel resistance to Soviet policies.[184]
By the late 1920s, the NEP was being phased out as the Soviet Union prepared for a shift towards more centralized and state-controlled economic policies under Stalin. The focus was moving toward heavy industrialization and forced collectivization, setting the stage for the dramatic and tragic events of the 1930s, including the Holodomor. Although Ukrainization was relatively successful in the 1920s, by the end of the decade, Stalin's regime began to reverse this policy, with a focus on Russian centralization. The coming years would see a crackdown on Ukrainian nationalism and culture as part of Stalin's larger efforts to solidify control over the Soviet republics.
In 1929, Joseph Stalin launched a campaign of forced collectivization across the Soviet Union, including Ukraine. The policy aimed to consolidate individual peasant farms into large, state-controlled collective farms (kolkhozes) to increase agricultural productivity and secure grain supplies for rapid industrialization. Ukrainian peasants, particularly wealthier ones known as "kulaks", resisted collectivization. The Soviet regime responded with brutal force, seizing land, livestock, and grain, and deporting or executing those who resisted. Collectivization led to widespread chaos in rural areas. Agricultural output plummeted due to poor planning, lack of incentives, and resistance from the peasantry. The disruption of traditional farming practices and the state's requisition of grain exacerbated food shortages.[185]
In 1932-33, Holodomor, derived from the Ukrainian words for "hunger" (holod) and "extermination" (moryty), was a man-made famine that resulted from the Soviet government's grain requisition policies and punitive measures against those who resisted collectivization. Millions of Ukrainians died from starvation during the Holodomor. Entire villages were decimated, and the event remains one of the most tragic episodes in Ukrainian history. The Soviet government denied the famine's existence and continued exporting grain during the crisis. The Holodomor not only devastated the rural population but also weakened Ukrainian national identity and culture. It served as a stark warning against any resistance to Soviet authority.[15]: §§ 8.1.3 [186][187]
Stalin's economic strategy included a series of Five-Year Plans aimed at rapidly industrializing the Soviet Union. Ukraine, with its rich natural resources and strategic location, was a key focus of these plans. Ukraine became a major center for heavy industry, particularly in coal mining, steel production, and machine building. Cities like Kharkiv, Dnipropetrovsk (now Dnipro), and Stalino (now Donetsk) were transformed into industrial hubs. The rapid growth of industry led to significant urbanization. Millions of Ukrainians moved from rural areas to cities in search of work, fundamentally altering the demographic and social landscape.[188]
Throughout the 1930s, Stalin's regime became increasingly marked by paranoia and a relentless drive to eradicate any perceived threats to his authority. This climate of suspicion fueled widespread political repression across the Soviet Union, profoundly impacting every layer of society in Ukraine. The purges specifically targeted Ukrainian intellectuals, artists, political leaders, and ordinary citizens suspected of harboring nationalist sympathies or potential dissenting views. Stalin’s objective was clear: to eliminate any possible source of opposition to Soviet rule, no matter how tenuous or imagined.[189]
The Great Purge, reaching its zenith between 1936 and 1938, devastated Ukraine. During this period, tens of thousands were arrested, tortured, and executed, or sent to forced labor camps (the Gulag) in remote Soviet regions. The Ukrainian intelligentsia, initially supported during the Soviet policy of Ukrainization in the 1920s, became a particular target as they were increasingly viewed as a threat to Soviet ideological conformity. In a systematic crackdown, the NKVD, Stalin’s secret police, dismantled the Ukrainian cultural and intellectual community. Most members of this intelligentsia were either imprisoned, executed, or driven to despair and suicide. One notable site, the Slovo Building in Kharkiv, where many prominent Ukrainian intellectuals resided, became infamous as a place where residents were closely surveilled, then rounded up in these purges.[190][191]
The terror also took a horrific toll on Kyiv, which became the capital of the Ukrainian SSR in 1934, replacing Kharkiv. Tens of thousands of Kyiv’s citizens were abducted by Soviet security forces, tortured, and summarily executed on fabricated charges. Victims were accused of treason, espionage, or nationalist activities without evidence and sentenced to death in sham trials. Their bodies were secretly buried in Bykivnia, a wooded area near Kyiv, which later became one of the largest mass grave sites in Ukraine. After Ukraine’s independence and the declassification of KGB archives, thousands of graves were discovered in Bykivnia, leading to the establishment of the Bykivnia Graves Memorial Complex. Soviet authorities had long denied the truth, claiming instead that Nazi atrocities had caused the mass burials.[192][193]
These purges were marked by infamous show trials, where prominent figures were coerced, often through brutal interrogation, into confessing to invented charges of anti-Soviet activity. The loss of Ukraine's educated and skilled citizens stifled intellectual, cultural, and social progress for decades, creating a legacy of fear that has hampered Ukraine's development and left a scar that is remembered in Ukraine to this day.[194]
In October 1938, following the Munich Agreement, Carpatho-Ukraine, also known as Subcarpathian Ruthenia, gained autonomy within Czechoslovakia. This allowed the formation of a local government led by Avhustyn Voloshyn. However, this period of autonomy was brief.[195]
In March 1939, as Czechoslovakia disintegrated under pressure from Nazi Germany, Carpathian Ukraine declared independence as the Republic of Carpatho-Ukraine. The government envisioned this small, mountainous region as the nucleus of a future independent Ukrainian state. This independence was short-lived. Within days, Hungarian forces, supported by Nazi Germany, invaded and occupied the region. The occupation was brutal, and many Ukrainian leaders were arrested or executed. Carpathian Ukraine remained under Hungarian control.[196]
On 1 September 1939, World War II began with Nazi Germany’s invasion of western Poland. Sixteen days later, the Soviet Union invaded eastern Poland under the terms of the Molotov-Ribbentrop Pact, dividing Eastern Europe into spheres of influence between Nazi Germany and the Soviet Union. The eastern part of Poland, which included Western Ukraine (Galicia and Volhynia), was annexed into the Ukrainian Soviet Socialist Republic. As Soviet forces occupied these territories, they quickly implemented Sovietization policies, repressing nationalist movements and religious institutions, which fueled local resentment.[197]
On 22 June 1941, Nazi Germany and its allies launched Operation Barbarossa, invading the Soviet Union. Ukraine became one of the main battlegrounds during the conflict, as Nazi forces occupied large parts of the country, including major cities such as Kyiv, Odesa, and Lviv. The German occupation, while initially seen by some as a potential liberation from the oppressive Soviet regime, quickly turned brutal. Nazi ideology viewed Ukraine as a critical part of its plan for Lebensraum (living space) and exploitation of resources.[198][199][200][201]
Around 4.5 to 6 million Ukrainians fought in the Soviet Red Army against Nazi Germany, contributing significantly to the eventual Soviet victory. At the same time, Ukraine became a center of partisan resistance. Some Ukrainians collaborated with the Germans, hoping to secure independence, while others joined the resistance movement. The Ukrainian Insurgent Army (UPA), formed by the Organization of Ukrainian Nationalists (OUN), fought for an independent Ukraine, engaging in conflict with both the Nazis and the Soviet forces. This dual struggle was motivated by a desire to free Ukraine from foreign domination, but the complexity of alliances and enmities made this a multi-sided war.[202]
Meanwhile, some factions within the Ukrainian nationalist movement, such as the Ukrainian National Democratic Alliance (UNDA), sought autonomy within a pro-Polish framework before the war. However, Polish policies of forced assimilation marginalized these efforts, leading to growing tensions between Poles and Ukrainians. During the German occupation, these tensions escalated into violent ethnic conflicts in Volhynia and Eastern Galicia, known as the Volhynian-Galician tragedy. The Ukrainian Insurgent Army (UPA) and Polish underground forces, including the Armia Krajowa, engaged in a simultaneous campaign of mutual violence during 1943–1944. The UPA targeted Polish civilians in Volhynia and Eastern Galicia, resulting in the deaths of up to 100,000 Poles, while Polish forces carried out attacks on Ukrainian civilians, killing tens of thousands of Ukrainians. These events, driven by competing nationalist ambitions, caused immense suffering on both sides and remain a deeply painful chapter in Ukrainian-Polish history.[203]
The Nazi occupation of Ukraine was marked by extreme brutality, especially towards Jews. Around 1.5 million Jews were murdered during the Holocaust in Ukraine, with atrocities such as the Babi Yar massacre, where tens of thousands of Jews were executed near Kyiv. The occupation also saw widespread repression of other groups, including Roma, communists, and Ukrainian nationalists.[204]
By 1943, following the Battle of Stalingrad, the tide of the war began to turn in favor of the Soviet Union. Soviet forces began to push the Germans out of Ukraine, and by 1944, the entire country was back under Soviet control. However, Soviet "liberation" did not bring freedom for many Ukrainians. The Soviet government imposed harsh reprisals against those suspected of collaboration with the Nazis or support for Ukrainian independence. The NKVD (Soviet secret police) conducted mass arrests, deportations, and executions. Small groups of UPA partisans continued their armed resistance against the Soviet regime well into the late 1940s and early 1950s, particularly in Western Ukraine, though the Soviet authorities eventually crushed this insurgency.[205]
Carpathian Ukraine, during the Hungarian occupation, faced significant repression, particularly against its Jewish and Ukrainian populations. Thousands of Jews from the region were deported to Nazi concentration camps, and many Ukrainian nationalists were imprisoned or killed. In 1944, the Soviet Red Army "liberated" Carpathian Ukraine from Hungarian and German forces.[206]
In addition to the annexation of Galicia and Volhynia, several other territories were incorporated into the Ukrainian SSR as a result of the Molotov-Ribbentrop Pact and subsequent wartime events. These included Northern Bukovina, Carpathian Ukraine (Transcarpathia), and parts of Northern and South Bessarabia, regions with a predominantly Ukrainian population.[207][208]
After World War II, amendments to the Constitution of the Ukrainian SSR were accepted, which allowed it to act as a separate subject of international law in some cases and to a certain extent, remaining a part of the Soviet Union at the same time. In particular, these amendments allowed the Ukrainian SSR to become one of the founding members of the United Nations (UN) together with the Soviet Union and the Byelorussian SSR. This was part of a deal with the United States to ensure a degree of balance in the General Assembly, which, the USSR opined, was unbalanced in favor of the Western Bloc. In its capacity as a member of the UN, the Ukrainian SSR was an elected member of the United Nations Security Council in 1948–1949 and 1984–1985.[209][210]
However, this period was also marked by severe repression and social upheaval. The famine of 1946-1947 devastated large parts of Ukraine, as the Soviet government requisitioned the entirety of the grain harvest, exacerbating food shortages. Western Ukraine, however, was less affected by the famine, largely due to the resistance efforts of the Ukrainian Insurgent Army (UIA). In response, the Soviet regime launched Operation "West" in 1947, forcibly deporting over 77,000 individuals—men, women, and children—from Western Ukraine to Siberia. These deportees later played a critical role in organizing uprisings within the Soviet Gulag system, including the notable Norilsk Uprising, which hastened the decline of the forced labor camp network.[211][212][213]
After Stalin's death in 1953 and the rise of Nikita Khrushchev to power, a significant number of political prisoners were released from the Gulag, including many Ukrainian nationalists and intellectuals. However, those deemed a threat to Soviet authority often remained under close surveillance. While some victims of Stalinist purges were formally rehabilitated, this process was selective and incomplete, with many individuals still excluded from full reintegration into society.[214]
Khrushchev, focusing on agricultural development, emphasized the importance of Ukraine’s fertile soil in Soviet food production. This was particularly significant in the context of the Virgin Lands Campaign, which saw resources and personnel diverted to develop agricultural lands in Kazakhstan and Siberia. While the campaign had long-term effects on the Ukrainian agricultural sector, Ukrainian collective farms remained inefficient, plagued by bureaucratic mismanagement that hampered the potential benefits of these reforms. During this period, industrial growth continued, with particular emphasis on heavy industries like steel production and mining. However, outdated infrastructure and poor planning often undermined progress, and economic inefficiency persisted throughout the country.[215]
The Khrushchev Thaw, a period of relative cultural liberalization, also allowed for a limited expression of Ukrainian identity, especially through literature, the arts, and historical studies. Prominent Ukrainian writers such as Oles Honchar and Lina Kostenko emerged, reflecting themes of national identity and social change. Their work offered subtle resistance to Soviet ideologies, though they had to navigate a highly censored environment. Industrialization also sparked rapid urbanization, with many Ukrainians moving to cities for factory and mining jobs. This period saw a rise in literacy rates and access to education, contributing to the development of a more technically skilled population. However, the curricula remained ideologically driven, prioritizing loyalty to the Communist Party over independent thought or national identity.[216]
In 1954, the Crimean Peninsula was transferred from the Russian Soviet Federative Socialist Republic (RSFSR) to the Ukrainian SSR. The transfer was largely administrative, as both the RSFSR and the Ukrainian SSR were part of the Soviet Union, and it was part of Khrushchev's broader strategy, rather than a gesture of genuine autonomy for Ukraine.[217] Meanwhile, Ukrainian nationalists, including remnants of the Organization of Ukrainian Nationalists (OUN) and the Ukrainian Insurgent Army (UPA), continued to face persecution. While the UPA’s armed resistance had been largely crushed by the early 1950s, nationalist sentiments remained a clandestine force, particularly among intellectuals and rural populations. Those who strayed too far from Soviet ideology faced censorship, harassment, and sometimes imprisonment. Despite the brief cultural revival and improvements in living standards, the era was marked by continued economic inefficiency and political repression. The limited freedoms allowed under Khrushchev's rule fostered a new generation of Ukrainian intellectuals who began to explore national identity in ways that would later fuel the dissident movement. However, despite these small steps toward cultural and intellectual freedom, Ukraine remained firmly under Soviet control, with little political autonomy or room for genuine national development.[218][219]
The transfer of power from Nikita Khrushchev to Leonid Brezhnev in 1964 was a result of a coup orchestrated by the Communist Party leadership. Khrushchev's policies, such as de-Stalinization and economic reforms, had created dissatisfaction among party elites due to their perceived instability and inefficiency. In October 1964, during a Politburo meeting, Khrushchev was accused of policy failures and forced to resign. Leonid Brezhnev, who had gained the trust of party conservatives, replaced Khrushchev as General Secretary. Brezhnev’s leadership marked a return to collective decision-making, stability, and more conservative policies, emphasizing continuity and avoiding the bold reforms associated with Khrushchev.[220]
Under Brezhnev, often associated with the "stagnation" period, Ukraine faced significant challenges. Centralized control from Moscow left Ukrainian Communist Party leaders, such as Volodymyr Shcherbytsky, as mere executors of Kremlin policies. Russification policies intensified, sidelining the Ukrainian language and culture in favor of Soviet Russian dominance. Ukrainian literature, art, and history were heavily censored, with works reflecting national identity banned or rewritten. Political repression was severe. The KGB targeted dissidents, intellectuals, and cultural figures advocating for Ukrainian autonomy or identity. Figures like Vasyl Stus, Ivan Dziuba, and Viacheslav Chornovil faced harassment, imprisonment, or exile. Despite repression, underground movements persisted, using samizdat to circulate banned works and raise awareness of Soviet human rights violations.[220][221]
Economically, Ukraine remained an industrial powerhouse, producing steel, coal, and machinery, and serving as the Soviet Union's "breadbasket". However, inefficient planning, outdated infrastructure, and environmental degradation plagued the economy. Poor agricultural yields and bureaucratic mismanagement led to food shortages. Urbanization grew, but housing shortages and inadequate services highlighted the stagnation. Environmental issues worsened, particularly in industrial regions like Donbas, where pollution severely impacted public health. Rural areas were neglected, fueling urban migration. Despite Ukraine’s contributions to the Soviet space program and industrial output, minimal modernization occurred.[220]
The suppression of Ukrainian identity and the struggles of dissidents sowed seeds of resistance. Activists like Vasyl Stus and the Ukrainian Helsinki Group exposed Soviet human rights abuses, though they faced harsh punishments. From the 1960s through the 1980s, Ukraine became a focal point for dissident activity within the USSR. A disproportionately high number of Ukrainian intellectuals, activists, and cultural figures were imprisoned, exiled, or subjected to punitive psychiatry for opposing the regime. These movements not only highlighted systemic oppression in the Soviet Union, but also laid the foundation for a national awakening that ultimately fueled Ukraine's quest for independence.[222]
After the death of Leonid Brezhnev in 1982, the Soviet Union experienced a period of short-lived leadership under Yuri Andropov (1982–1984) and Konstantin Chernenko (1984–1985), before Mikhail Gorbachev assumed power in 1985. Gorbachev's introduction of glasnost (openness) and perestroika (restructuring) marked a turning point, fostering an atmosphere of reform and amplifying public dissatisfaction with Soviet governance. Glasnost opened the door for greater freedom of expression, allowing Ukrainian intellectuals, activists, and dissidents to publicly address long-suppressed issues such as Russification, environmental degradation, and historical atrocities like the Holodomor. During this time, organizations like the Ukrainian Helsinki Group, and movements like Rukh (the People’s Movement of Ukraine, established in 1989) emerged as key advocates for greater autonomy, cultural revival, and ultimately, independence. As a major industrial and agricultural hub within the USSR, Ukraine bore the brunt of the broader Soviet economic stagnation. Inefficient central planning, a lack of innovation, and overexploitation of resources resulted in widespread economic inefficiencies and severe environmental damage.[223][224]
On 26 April 1986, the Ukrainian town of Pripyat became the site of one of the worst nuclear disasters in history when Reactor 4 of the Chernobyl Nuclear Power Plant exploded. This released a significant amount of radioactive material into the atmosphere, which was carried across Europe by wind currents. The resulting radioactive fallout contaminated vast areas of northern Ukraine and neighboring Belarus. The immediate aftermath of the explosion was devastating. Two plant workers died on the night of the accident, and in the weeks that followed, 28 emergency workers succumbed to acute radiation sickness. The disaster forced the evacuation of over 100,000 people from Pripyat and surrounding areas, leaving behind ghost towns and a contaminated Chernobyl exclusion zone that remains uninhabitable to this day. This tragedy had profound environmental, health, and political consequences. The Chernobyl disaster galvanized local independence movements, such as Rukh, which gained significant momentum in the late 1980s and contributed to the eventual dissolution of the Soviet Union.[225]
The late 1980s witnessed a cultural awakening in Ukraine, characterized by a renewed interest in the Ukrainian language, traditions, and history. Efforts to revive Ukrainian cultural identity challenged decades of Soviet policies aimed at suppressing it. The reestablishment of the Ukrainian Greek Catholic Church and movements advocating for an independent Ukrainian Orthodox Church underscored growing dissatisfaction with Soviet atheism and fueled nationalist sentiment. These developments signaled a rising tide of Ukrainian self-awareness and a determination to reclaim national identity and sovereignty.[226][227]
By 1990, calls for Ukrainian sovereignty reached a fever pitch, fueled by decades of cultural suppression, economic exploitation, and a growing national consciousness among Ukrainians. On 21 January 1990, one of the most iconic demonstrations of unity and determination occurred when over 300,000 Ukrainians formed the "human chain" stretching from Kyiv to Lviv. This symbolic act, known as the "Chain of Unity", marked the anniversary of the Unification Act of the Ukrainian People's Republic and the West Ukrainian People's Republic in 1919. It highlighted the nation's steadfast commitment to reclaiming its independence, emphasizing the unity between eastern and western Ukraine.[228]
In March 1991, a referendum on preserving the Soviet Union was held, and in Ukraine, the majority of voters approved the new Union Treaty, supporting the idea of joining the Soviet Union based on Ukraine's Declaration of State Sovereignty. Voters were specifically asked, "Do you agree that Ukraine should be part of a Union of Soviet sovereign states on the basis of the Declaration of State Sovereignty of Ukraine?" The proposal was approved by 81.7% of voters.[229]
However, a few months later, events occurred that radically changed the situation. In August 1991, the so-called August Coup took place in Moscow, an attempt by conservative communists to seize power and regain tight control within the USSR. The coup failed, but it seriously undermined trust in the central government in the USSR, provoking a wave of declarations of independence among the republics.[230]
On August 24, 1991, the Verkhovna Rada (Ukrainian Parliament) declared Ukraine’s independence from the Soviet Union, a decisive step taken in the wake of the failed August Coup.[231] This historic decision was reinforced by a nationwide referendum on December 1, 1991, where 90.32% of voters supported independence, with majorities in every region (including 54.19% in Crimea).[232] That same day, Ukraine held its first presidential election, a pivotal moment in its post-Soviet history. Leonid Kravchuk, a former high-ranking Soviet official, won the election, becoming first President of Ukraine. During his tenure, Kravchuk worked to maintain stability, distance Ukraine from Moscow’s influence, and manage internal political challenges.[233]
The dissolution of the USSR was formalized in the Białowieża Forest in Belarus, where the leaders of Ukraine, Belarus, and Russia—Leonid Kravchuk, Stanislav Shushkevich, and Boris Yeltsin—signed the Belovezha Accords on 8 December 1991. These accords declared the Soviet Union defunct and established the Commonwealth of Independent States (CIS) as a loose association of former Soviet republics. By 26 December 1991, the USSR officially ceased to exist, and Ukraine's independence gained de jure recognition from the international community.[234]
After declaring independence, Ukraine began a complex post-Soviet transition, shaping its identity as a new independent nation. From 1991 to 1996, Ukraine experienced significant political, economic, and social transformations aimed at establishing itself as a sovereign state on the global stage.[235]
In the early years of independence, the Verkhovna Rada played a key role in establishing the country’s legal and political framework. As Ukraine’s legislative body, the Rada was responsible for drafting and passing laws to build the foundation of Ukraine’s political and economic structures. However, it faced ideological divides as members debated Ukraine’s path—whether to orient toward Western integration or maintain stronger ties with Russian Federation. These debates mirrored broader societal divides and significantly influenced legislative reform.[236]
To solidify its national identity, Ukraine adopted state symbols that resonated with historical and cultural significance. The Parliament selected the blue-and-yellow flag and the tryzub (trident) as national emblems, which became powerful representations of Ukrainian sovereignty and unity.[237][238]
A significant historical moment of this period was the transfer of powers from the President of the Ukrainian People's Republic in exile, Mykola Plaviuk, to the newly elected President of independent Ukraine, Leonid Kravchuk, during the solemn session of the Verkhovna Rada on August 22, 1992 in Kyiv. This transfer, while largely symbolic, marked a continuity of the Ukrainian struggle for independence, linking the efforts of past leaders to those of the new government. This act represented the culmination of over 70 years of Ukrainian national aspirations and signaled a deepening commitment to the state’s sovereignty and historical continuity.[239][240] In his declaration, Plaviuk proclaimed that the current Ukrainian state is the lawful successor to the Ukrainian People's Republic and a continuation of its authority and state traditions.[241][239][a]
The Budapest Memorandum on Security Assurances, signed on December 5, 1994, by Ukraine, Russia, the United States, and the United Kingdom, was a critical agreement in post-Soviet geopolitics. It formalized the process by which Ukraine surrendered the world’s third-largest nuclear arsenal, inherited after the Soviet Union's collapse, which included approximately 1,900 strategic nuclear warheads. Although these weapons were stationed on Ukrainian territory, they were operationally controlled by Moscow, and Ukraine’s possession of such an arsenal posed significant concerns for global non-proliferation efforts. In return for surrendering the weapons, the signatory powers provided assurances of Ukraine's sovereignty, territorial integrity, and independence. The memorandum guaranteed that no force or threats of force would be used against Ukraine, and no economic or political pressure would be employed to undermine its status. It also affirmed that the United Nations Security Council would intervene in case of aggression against Ukraine. However, the assurances were political commitments, not legally binding guarantees, which made their enforcement dependent on the goodwill of the signatories.[243][244]
The transition to a market economy was compounded by inflation and political instability. The absence of immediate reforms led to widespread frustration, culminating in a snap presidential election in 1994, where Leonid Kravchuk was succeeded by Leonid Kuchma, an engineer and former Soviet official. Kuchma’s presidency aimed at modernizing Ukraine’s economy and fostering a balanced relationship with both Russia and Western Europe, a delicate approach to preserving Ukraine’s independence amid complex geopolitical pressures.[245]
Trying to stabilize the economy, back in 1992, Ukraine introduced a temporary currency, the Ukrainian karbovanets. This currency was intended to serve as a stopgap measure until a more permanent solution could be implemented. The karbovanets quickly devalued, contributing to growing economic instability.[246] In 1996, Ukraine introduced the Hryvnia as the national currency, marking a significant milestone in the country’s economic transition and further solidifying its independence.[247][248] It is named after a measure of weight used in Kievan Rus'.[249]
Despite disputes in the Verkhovna Rada, which at the time struggled to reach consensus on reform initiatives and reconcile the interests of pro-Western and pro-Russian factions, Ukraine took a decisive step in defining its legal structure by adopting the Constitution of Ukraine on June 28, 1996. This document established Ukraine as a democratic, law-based state with a presidential-parliamentary system, clearly delineating the separation of powers among the executive, legislative, and judicial branches. Drafted with input from various political factions and scholars, the Constitution enshrined Ukraine’s commitment to legal governance and human rights, becoming a cornerstone for Ukraine’s development as an independent nation.[250]
At the end of its transition, Ukraine created the main components of its independence. With its own currency, Constitution, national symbols, and a growing sense of national identity, Ukraine began to chart its course as a sovereign state. Despite the fact that the post-Soviet transition period caused numerous economic and political challenges, this formative period played an important role in shaping the direction and identity of modern Ukraine.
The third presidential election in Ukraine took place in 1999, resulting in a victory for Leonid Kuchma, who defeated Petro Symonenko in the run-off. This secured Kuchma a second consecutive term. However, his second term was plagued by widespread controversies, including allegations of authoritarianism, pervasive corruption scandals, curtailment of media freedoms, and large-scale public protests that challenged his leadership and legitimacy.[251]
One of the darkest episodes of Kuchma’s presidency was the "Cassette Scandal", which erupted after recordings allegedly made by his former bodyguard, Mykola Melnychenko, were leaked. These recordings implicated Kuchma in severe abuses of power, including involvement in the murder of journalist Georgiy Gongadze, as well as corruption and electoral manipulations. The scandal provoked massive public outrage, culminating in the "Ukraine without Kuchma" protests of 2000–2001. These protests, marked by their intensity and broad support, severely undermined Kuchma's standing both domestically and internationally.[252][253]
During his presidency, Kuchma’s administration was accused of suppressing opposition media outlets and harassing journalists and political opponents. High-profile figures like Viacheslav Chornovil died under mysterious circumstances, further fueling suspicions of state complicity.[254][255][256] Historian Serhy Yekelchyk observed that Kuchma's government "employed electoral fraud freely", particularly during the 1999 presidential elections and the 2000 constitutional referendum.[257]
Amid these challenges, Viktor Yushchenko, a respected economist and reformer, rose to prominence. His tenure as the Governor of the National Bank of Ukraine earned him recognition for his professionalism and integrity, which led to his appointment as Prime Minister in 1999, during Kuchma's re-election campaign. Yushchenko was perceived as a technocratic leader capable of addressing Ukraine’s economic stagnation and corruption.[258]
Initially, Yushchenko's government embarked on an ambitious reform agenda. These reforms included fiscal discipline, restructuring of key industries, and efforts to stabilize the economy, which had suffered during Kuchma’s first term. However, Yushchenko's policies soon clashed with the entrenched oligarchic networks that had flourished under Kuchma's protection. These oligarchic factions, wielding significant influence in Parliament and the Kuchma administration, actively resisted reforms that threatened their monopolistic practices and access to state resources.[259]
One of the most contentious episodes of Yushchenko’s premiership was the dismissal of his deputy prime minister, Yulia Tymoshenko, in 2001. Tymoshenko, who spearheaded anti-corruption initiatives and energy sector reforms, posed a significant challenge to oligarchic interests. Under pressure from Kuchma and oligarchic allies, Yushchenko was forced to dismiss Tymoshenko, a move that symbolized the constraints on reform under Kuchma’s presidency. Shortly after, the Verkhovna Rada, dominated by pro-Kuchma factions, passed a vote of no confidence in Yushchenko’s government, effectively ending his tenure as Prime Minister. Kuchma’s lack of support during this political crisis highlighted the deepening rift between the two leaders.[260][261]
Following his dismissal, Yushchenko became a potent symbol of reform and anti-corruption, gaining significant public support. In 2002, he founded the "Our Ukraine" (Nasha Ukrayina) political coalition, which championed pro-Western and democratic ideals. This bloc emerged as a key opposition force against Kuchma’s policies and set the stage for the pivotal 2004 presidential election. In this election, Yushchenko, as the leading opposition candidate, challenged Viktor Yanukovych, Kuchma’s chosen successor, in a contest that would shape Ukraine’s political trajectory for years to come.[262]
In 2004, Ukrainian President Leonid Kuchma announced he would not seek re-election after serving two terms in office. This decision created a political vacuum that set the stage for a highly contested 2004 presidential election between two main candidates. Viktor Yanukovych, the incumbent Prime Minister, was supported by Kuchma and the Russian Federation. He advocated closer ties with Russia. On the other hand, Viktor Yushchenko, the opposition leader, campaigned as a reformist, emphasizing democratic changes and closer integration with the European Union.[262][263]
The campaign highlighted deep regional and cultural divisions within Ukraine. The western and central regions of the country rallied behind Yushchenko, while the eastern and southern regions strongly supported Yanukovych. A prominent figure in Yushchenko's coalition was Yulia Tymoshenko, a charismatic and polarizing politician. Known for her iconic braided hairstyle and fiery rhetoric, Tymoshenko played a pivotal role in galvanizing support for the opposition. Her speeches and leadership became symbols of the movement for change.[264]
The second round of the presidential election, held on November 21, 2004, officially declared Viktor Yanukovych as the winner. However, widespread allegations of electoral fraud, including ballot-stuffing, voter intimidation, and falsification of results, led to a political crisis. The situation was further intensified by the attempted poisoning of Viktor Yushchenko with dioxin during the campaign, which left him severely disfigured. This attack garnered widespread sympathy for Yushchenko and galvanized his supporters.[265][266]
The announcement of Yanukovych’s victory sparked massive protests, marking the beginning of the Orange Revolution, a landmark event in Ukraine’s modern history. From November 22, 2004, hundreds of thousands of Ukrainians gathered at Kyiv’s Maidan Nezalezhnosti (Independence Square), demanding justice, democracy, and fair elections. The protesters adopted the color orange, symbolizing Yushchenko’s campaign and the broader ideals of hope and resistance to corruption and authoritarianism.[262]
Yulia Tymoshenko emerged as a central figure in the revolution, inspiring crowds with her impassioned speeches and coordinating efforts to sustain the movement. The protests remained largely peaceful, with participants emphasizing nonviolence and civil disobedience, despite provocations aimed at inciting unrest. On December 3, 2004, Ukraine’s Supreme Court invalidated the election results due to evidence of fraud and ordered a re-run of the runoff. This repeat election, held on December 26, 2004, resulted in Viktor Yushchenko's victory with 52% of the vote. Yanukovych resigned as Prime Minister, and his cabinet was dismissed on January 5, 2005. Yushchenko's inauguration as president marked the culmination of the Orange Revolution.[267][262]
In February 2005, Yulia Tymoshenko was appointed Prime Minister, cementing her role as a key political figure in post-revolution Ukraine. However, the Orange coalition faced internal struggles during Yushchenko’s presidency. Relations between Yushchenko and Tymoshenko began to deteriorate, weakening the government’s ability to implement reforms. In September 2005, Yushchenko dismissed Tymoshenko from her position as Prime Minister, creating a rift that would have lasting effects on Ukrainian politics.[262][268]
Under Yushchenko, Ukraine’s foreign policy shifted toward strengthening ties with the European Union, often at the expense of its relationship with Russia. This realignment caused tensions, particularly over energy issues. In 2005, a dispute over natural gas prices with Russia resulted in shortages across Europe, as Ukraine served as a critical transit country for gas supplies. A compromise was reached in January 2006, but the incident underscored the fragility of Ukraine's relations with its eastern neighbor.[269][270]
The 2006 parliamentary elections in Ukraine reflected a deeply fragmented political landscape. The elections were marked by the re-emergence of Viktor Yanukovych, a rival of President Viktor Yushchenko and a proponent of closer ties with Russia. Yanukovych’s Party of Regions secured significant support, enabling him to become Prime Minister. This marked a shift in Ukraine’s political orientation, with Yanukovych advocating for a more pro-Russian agenda in contrast to Yushchenko’s pro-European stance. The elections exacerbated tensions between the presidency and parliament, as the balance of power between the two branches of government became a central issue. Frequent political deadlocks ensued, paralyzing governance and delaying key reforms. The strained relationship between Yushchenko and Yanukovych led to a standoff, which ultimately resulted in snap elections in 2007. The snap elections led to the formation of a coalition opposed to Yanukovych’s agenda. Yulia Tymoshenko returned as Prime Minister, yet the political environment remained fraught with instability. Conflicts between Yushchenko and Tymoshenko, which had begun earlier, persisted, further weakening the government and contributing to widespread public frustration.[271][272][273]
Tensions reached a breaking point in September 2008, triggering a major political crisis. The crisis began when Yushchenko’s Our Ukraine–People’s Self-Defense Bloc (NU-NS) withdrew from the governing coalition after the Bloc Yulia Tymoshenko (BYuT) sided with the opposition Party of Regions in supporting a bill on 4 September 2008 that sought to curtail the president’s powers in favor of the prime minister and parliament. Yushchenko saw this as a direct challenge to his authority, accusing Tymoshenko of betraying the coalition’s principles and aligning with pro-Russian forces. On 16 September 2008, the official collapse of the BYuT/NU-NS coalition was announced. As attempts to restore the alliance failed, this led to a deepening political stalemate.[274]
The crisis finally ended on 9 December 2008, when the Orange Coalition was reformed, now including Lytvyn Bloc. This followed the election of Volodymyr Lytvyn as parliamentary speaker on 8 December, securing his faction’s support for a new governing majority. Shortly after, on 16 December 2008, a new government was formed, representing a 245-seat parliamentary majority composed of the Lytvyn Bloc, the Bloc Yulia Tymoshenko, and Our Ukraine–People's Self-Defense Bloc. Though this coalition temporarily stabilized the political landscape, deep divisions among Ukraine’s leadership persisted, continuing to undermine governance and public confidence.[275]
Ukraine was severely impacted by the global financial crisis of 2008-2009. The crisis led to a sharp economic downturn, with GDP contracting significantly as demand for Ukraine’s key exports, such as steel, plummeted. The government faced a budgetary crisis and had to rely on a bailout from the International Monetary Fund (IMF) to stabilize the economy. The IMF package came with stringent conditions, including fiscal austerity measures and structural reforms, which sparked domestic controversy. Adding to Ukraine’s economic woes was an escalating energy dispute with Russia. Ukraine, as a major transit country for Russian natural gas to Europe, found itself at the center of geopolitical tensions. Disagreements over gas pricing and transit fees between Ukraine’s Naftogaz and Russia’s Gazprom reached a boiling point in 2009.[276]
The 2009 gas conflict was a major standoff that highlighted Ukraine’s vulnerability and its dependence on Russian energy. The dispute centered on allegations of unpaid bills and demands by Russia for higher gas prices. In January 2009, Gazprom cut off gas supplies to Europe via Ukraine, plunging several European countries into an energy crisis during the winter. Negotiations eventually resulted in a new agreement, but the episode underscored the fragility of Ukraine’s energy security. The crisis damaged Ukraine’s reputation as a reliable transit country and strained its relations with both Russia and the European Union. It also placed significant financial pressure on the government, further destabilizing the economy.[277]
Throughout this period, the rivalry between Yushchenko and Tymoshenko continued to destabilize Ukrainian politics. Their clashes over policy, governance, and political priorities created a climate of dysfunction. Tymoshenko accused Yushchenko of obstructing her efforts to address economic and social issues, while Yushchenko criticized her populist policies as reckless and counterproductive. These internal conflicts not only hindered progress but also deepened public disillusionment with the political elite.
By the time of Ukraine's 2010 presidential election, the alliance between Viktor Yushchenko and Yulia Tymoshenko, key figures of the Orange Revolution, had disintegrated. Tymoshenko competed against both Yushchenko and Viktor Yanukovych in a tightly contested three-way race.[278][252] With Yushchenko’s approval rating drastically weakened, many pro-Orange voters abstained, leading to a runoff between Tymoshenko and Yanukovych.[279] In the final round, Yanukovych secured the presidency with 48% of the vote, while Tymoshenko received 45%.[280]
Upon taking office, Yanukovych and his Party of Regions swiftly moved to consolidate power.[281] In March 2010, the newly formed parliamentary majority, the "Coalition of Stability and Reforms", dismissed Tymoshenko as Prime Minister, replacing her with Yanukovych’s longtime ally, Mykola Azarov.[282] His administration pursued closer ties with Russia, notably signing the controversial Kharkiv Pact, which extended Russia’s Black Sea Fleet lease in Sevastopol until 2042 in exchange for discounted gas prices. The deal sparked protests from pro-European and nationalist groups, who saw it as a threat to Ukraine’s sovereignty.[283][284]
In the 2012 parliamentary elections, the Party of Regions further strengthened its grip on power, securing the largest number of seats despite widespread allegations of vote-rigging, administrative pressure, and misuse of state resources. The opposition remained fragmented, though it made notable gains. The newly formed UDAR party, led by heavyweight boxing champion Vitali Klitschko, positioned itself as a pro-European force, while the nationalist Svoboda party, previously on the political margins, unexpectedly surpassed the electoral threshold, capitalizing on growing dissatisfaction with Yanukovych’s rule.[285]
The largest opposition bloc, Batkivshchyna, led by Arseniy Yatsenyuk in Tymoshenko’s absence, struggled to counterbalance the ruling party’s dominance. By this time, Tymoshenko had been imprisoned since 2011 on charges of abuse of office, a case widely condemned by Western governments and human rights organizations as politically motivated. Her imprisonment, along with the selective prosecution of other opposition figures, was cited as a key reason for the European Union's reluctance to deepen ties with Ukraine under Yanukovych’s leadership.[286]
In November 2013, President Yanukovych did not sign the Ukraine–European Union Association Agreement and instead pursued closer ties with Russia.[287][288] This move sparked protests on the streets of Kyiv and, ultimately, the Revolution of Dignity. Protesters set up camps in Kyiv's Maidan Nezalezhnosti (Independence Square),[289] and in December 2013 and January 2014 protesters started taking over various government buildings, first in Kyiv, and later in Western Ukraine.[290] Battles between protesters and police resulted in about 80 deaths in February 2014.[291][292]
Following the violence, the Ukrainian parliament on 22 February voted to remove Yanukovych from power (on the grounds that his whereabouts were unknown and he thus could not fulfil his duties), and to free Yulia Tymoshenko from prison. On the same day, Yanukovych supporter Volodymyr Rybak resigned as speaker of the Parliament, and was replaced by Tymoshenko loyalist Oleksandr Turchynov, who was subsequently installed as interim President.[293] Yanukovych had fled Kyiv, and subsequently gave a press conference in the Russian city of Rostov-on-Don.[294]
In March 2014, the Annexation of Crimea by the Russian Federation occurred. Although official results of a referendum on Crimean reunification with Russia were reported as showing a large majority in favor of the proposition, the vote was organized under Russian military occupation and was denounced by the European Union and the United States as illegal.[295]
The Crimean crisis was followed by pro-Russian unrest in east Ukraine and south Ukraine.[296] In April 2014 Ukrainian separatists self-proclaimed the Donetsk People's Republic and Luhansk People's Republic and held referendums on 11 May 2014; the separatists claimed nearly 90% voted in favor of independence.[297][296] Later in April 2014, fighting between the Ukrainian army and pro-Ukrainian volunteer battalions on one side, and forces supporting the Donetsk and Lugansk People's Republics on the other side, escalated into the war in Donbas.[296][298] By December 2014, more than 6,400 people had died in this conflict, and according to United Nations figures it led to over half a million people becoming internally displaced within Ukraine and two hundred thousand refugees to flee to (mostly) Russia and other neighboring countries.[299][300][301][302] During the same period, political (including adoption of the law on lustration and the law on decommunization) and economic reforms started.[303] On 25 May 2014, Petro Poroshenko was elected president[304] in the first round of the presidential election. By the second half of 2015, independent observers noted that reforms in Ukraine had considerably slowed down, corruption did not subside, and the economy of Ukraine was still in a deep crisis.[303][305][306][307] By December 2015, more than 9,100 people had died (largely civilians) in the war in Donbas,[308] according to United Nations figures.[309]
The Budapest Memorandum's fragility became evident in 2014 when Russia annexed Crimea and began supporting separatist movements in Ukraine's Donetsk and Luhansk regions. These actions violated the agreement’s commitments to respect Ukraine's borders and sovereignty. Russia justified its actions by claiming they were protecting Russian-speaking populations, a rationale widely rejected by the international community. Despite protests from Ukraine and Western powers, no direct action was taken to compel Russia to adhere to the memorandum. The crisis exposed the limitations of non-binding agreements, leaving Ukraine in a precarious position and reshaping the global conversation about security assurances and the reliability of international commitments.[310][311][better source needed]
On 1 January 2016, Ukraine joined the DCFTA with the EU. Ukrainian citizens were granted visa-free travel to the Schengen Area for up to 90 days during any 180-day period on 11 June 2017, and the Association Agreement formally came into effect on 1 September 2017.[312] Significant achievements in the foreign policy arena include support for anti-Russian sanctions, obtaining a visa-free regime with the countries of the European Union, and better recognition of the need to overcome extremely difficult tasks within the country. However, the old local authorities did not want any changes; they were cleansed of anti-Maidan activists (lustration), but only in part. The fight against corruption was launched, but was limited to sentences of petty officials and electronic declarations, and the newly established NABU and NACP were marked by scandals in their work. Judicial reform was combined with the appointment of old, compromised judges. The investigation of crimes against Maidan residents was delayed. In order to counteract the massive global Russian anti-Ukrainian propaganda of the "information war", the Ministry of Information Policy was created, which for 5 years did not show effective work, except for the ban on Kaspersky Lab, Dr.Web, 1С, Mail.ru, Yandex and Russian social networks VKontakte or Odnoklassniki and propaganda media. In 2017, the president signed the law "On Education", which met with opposition from national minorities, and quarreled with the Government of Hungary.[citation needed].[313]
On 19 May 2018, Poroshenko signed a Decree which put into effect the decision of the National Security and Defense Council on the final termination of Ukraine's participation in the statutory bodies of the Commonwealth of Independent States.[314][315] As of February 2019, Ukraine minimized its participation in the Commonwealth of Independent States to a critical minimum and effectively completed its withdrawal. The Verkhovna Rada of Ukraine did not ratify the accession, i.e. Ukraine has never been a member of the CIS.[316]
The Kerch Strait incident occurred on 25 November 2018 when the Russian Federal Security Service (FSB) coast guard fired upon and captured three Ukrainian Navy vessels attempting to pass from the Black Sea into the Sea of Azov through the Kerch Strait on their way to the port of Mariupol.[317][318]
On 6 January 2019, in Fener, a delegation of the Orthodox Church of Ukraine with the participation of President of Ukraine Petro Poroshenko received a Tomos on autocephaly. The Tomos was presented to the head of the OCU, Metropolitan Epiphanius, during a joint liturgy with the Ecumenical Patriarch.[319] The next day, Tomos was brought to Ukraine for a demonstration at St. Sophia Cathedral. On 9 January, all members of the Synod of the Constantinople Orthodox Church signed the Tomos during the scheduled meeting of the Synod.[citation needed]
On 21 February 2019, the Constitution of Ukraine was amended, with the norms on the strategic course of Ukraine for membership in the European Union and NATO being enshrined in the preamble of the Basic Law, three articles and transitional provisions.[320]
On 21 April 2019, Volodymyr Zelenskyy was elected president in the second round of the presidential election. Early parliamentary elections on 21 July allowed the newly formed pro-presidential Servant of the People party to win an absolute majority of seats for the first time in the history of independent Ukraine (248). Dmytro Razumkov, the party's chairman, was elected speaker of parliament. The majority was able to form a government on 29 August on its own, without forming coalitions, and approved Oleksii Honcharuk as prime minister.[321] On 4 March 2020, due to a 1.5% drop in GDP (instead of a 4.5% increase at the time of the election), the Verkhovna Rada fired Honcharuk's government and Denys Shmyhal[322] became the new Prime Minister.[323]
On 28 July 2020, in Lublin, Lithuania, Poland and Ukraine created the Lublin Triangle initiative, which aims to create further cooperation between the three historical countries of the Polish–Lithuanian Commonwealth and further Ukraine's integration and accession to the EU and NATO.[324]
On 2 February 2021, a presidential decree banned the television broadcasting of the pro-Russian TV channels 112 Ukraine, NewsOne and ZIK.[325][326] The decision of the National Security and Defense Council and the Presidential Decree of 19 February 2021 imposed sanctions on 8 individuals and 19 legal entities, including Putin's pro-Russian politician and Putin's godfather Viktor Medvedchuk and his wife Oksana Marchenko.[327][328]
On 17 May 2021, the Association Trio was formed by signing a joint memorandum between the Foreign Ministers of Georgia, Moldova and Ukraine. Association Trio is tripartite format for the enhanced cooperation, coordination, and dialogue between the three countries (that have signed the Association Agreement with the EU) with the European Union on issues of common interest related to European integration, enhancing cooperation within the framework of the Eastern Partnership, and committing to the prospect of joining the European Union.[329]
At the June 2021 Brussels Summit, NATO leaders reiterated the decision taken at the 2008 Bucharest Summit that Ukraine would become a member of the Alliance with the Membership Action Plan (MAP) as an integral part of the process and Ukraine's right to determine its own future and foreign policy without outside interference.[330]
Throughout 2021, Russian forces built up along the Russia-Ukraine Border, in occupied Crimea and Donbas, and in Belarus.[331] On 24 February 2022, Russian forces invaded Ukraine.[332] Russia quickly occupied much of the east and south of the country, but failed to advance past the city of Mykolaiv towards Odesa, and were forced to retreat from the north after failing to occupy Kyiv, Chernihiv, Sumy, and Kharkiv.[333] After failing to gain further territories and being driven out of Kharkiv Oblast by a fast-paced Ukrainian counteroffensive,[334] Russia officially annexed the Donetsk People's Republic and the Luhansk People's Republic, along with most of the Kherson and Zaporizhzhia Oblasts on 30 September. The invasion was met with international condemnation. The United Nations General Assembly passed a resolution condemning the invasion and demanding a full Russian withdrawal in March 2022. The International Court of Justice ordered Russia to suspend military operations and the Council of Europe expelled Russia. Many countries imposed sanctions on Russia and its ally Belarus, and provided humanitarian and military aid to Ukraine. The Baltic states all declared Russia a terrorist state. Protests occurred around the world, along with mass arrests of anti-war protesters in Russia, which also enacted a law enabling greater media censorship. Over 1,000 companies closed their operations in Russia and Belarus as a result of the invasion.[335]
On the eve of the Russian invasion of Ukraine in 2022, the country was the poorest in Europe,[336] a handicap whose cause was attributed to high corruption levels[337] and the slow pace of economic liberalization and institutional reform.[338][339][340][341] Russia's invasion of the country damaged Ukraine's economy and future prospects of improvement to such an extent, that the GDP of the country was projected to shrink by as much as 35% in its first year alone after the invasion.[342]
Ukraine was originally preparing to formally apply for EU membership in 2024, but instead signed an application for membership in February 2022.[343]
Knowledge about Ukraine in other parts of the world came chiefly from Russian secondary sources until relatively recently. After the second half of the seventeenth century, when Muscovy and later the Russian Empire came to control much of Ukrainian territory, Russian writers included Ukraine as part of Russian history. This included referring to medieval Kievan Rus' as "Kievan Russia" and its Old East Slavic culture and inhabitants as "Kievan Russian" or "Old Russian". Later Ukraine or its parts were called "Little Russia", "South Russia", "West Russia" (with Belarus), or "New Russia" (the Black Sea coast and southeastern steppe). But parts of Ukraine beyond Russia's reach were called Ruthenia and its people Ruthenians. The names chosen to refer to Ukraine and Ukrainians have often reflected a certain political position, and sometimes even to deny the existence of Ukrainian nationality.[35]: 10–11  The Russian point-of-view of Ukrainian history became the prevailing one in Western academia, and although the bias was identified as early as the 1950s, many scholars of Slavic studies and history believe significant changes are still necessary to correct the Moscow-centric view.[344]
The scholarly study of Ukraine's history emerged from romantic impulses in the late 19th century when German Romanticism spread to Eastern Europe. The outstanding leaders were Volodymyr Antonovych (1834–1908), based in Kiev, and his student Mykhailo Hrushevsky (1866–1934).[345] The first serious challenge to the Russian view of Ukraine was Hrushevsky's 1904 article "The Traditional Scheme of 'Russian' History and the Problem of the Rational organization of the History of the Eastern Slavs".[346] For the first time full-scale scholarly studies based on archival sources, modern research techniques, and modern historical theories became possible. However, the demands of government officials—Tsarist, to a lesser degree Austro-Hungarian and Polish, and later Soviet—made it difficult to disseminate ideas that ran counter to the central government. Therefore, exile schools of historians emerged in central Europe and Canada after 1920.[citation needed]
Strikingly different interpretations of the medieval state of Kievan Rus' appear in the four schools of historiography within Ukraine: Russophile, Sovietophile, Eastern Slavic, and Ukrainophile. In the Soviet Union, there was a radical break after 1921, led by Mikhail Pokrovsky. Until 1934, history was generally not regarded as chauvinistic, but was rewritten in the style of Marxist historiography. National "pasts" were rewritten as social and national liberation for non-Russians, and social liberation for Russians, in a process that ended in 1917. Under Stalin, the state and its official historiography were given a distinct Russian character and a certain Russocentrism. Imperial history was rewritten such that non-Russian love caused an emulation and deference to "join" the Russian people by becoming part of the (tsarist) Russian state, and in return, Russian state interests were driven by altruism and concern for neighboring people.[347] Russophile and Sovietophile schools have become marginalized in independent Ukraine, with the Ukrainophile school being dominant in the early 21st century. The Ukrainophile school promotes an identity that is mutually exclusive of Russia. It has come to dominate the nation's educational system, security forces, and national symbols and monuments, although it has been dismissed as nationalist by Western historians. The East Slavic school, an eclectic compromise between Ukrainophiles and Russophilism, has a weaker ideological and symbolic base, although it is preferred by Ukraine's centrist former elites.[348]
Many historians in recent years have sought alternatives to national histories, and Ukrainian history invited approaches that looked beyond a national paradigm. Multiethnic history recognises the numerous peoples in Ukraine; transnational history portrays Ukraine as a border zone for various empires; and area studies categorises Ukraine as part of East-Central Europe or, less often, as part of Eurasia. Serhii Plokhy argues that looking beyond the country's national history has made possible a richer understanding of Ukraine, its people, and the surrounding regions.[349] since 2015, there has been renewed interest in integrating a "territorial-civic" and "linguistic-ethnic" history of Ukraine. For example, the history of the Crimean Tatars and the more distant history of the Crimea peninsula is now integrated into Ukrainian school history. This is part of the constitutionally mandated "people of Ukraine" rather than "Ukrainian people". Slowly, the histories of Poles and Jews are also being reintegrated. However, due to the current political climate caused by territorial sovereignty breaches by Russia, the role of Russians as "co-host" has been greatly minimized, and there are still unresolved difficult issues of the past, for example, the role of Ukrainians during the Holodomor.[350]: 98
After 1991, historical memory was a powerful tool in the political mobilization and legitimation of the post-Soviet Ukrainian state, as well as the division of selectively used memory along the lines of the political division of Ukrainian society. Ukraine did not experience the restorationist paradigm typical of some other post-Soviet nations, for example the three Baltic countries—Lithuania, Latvia, and Estonia—although the multifaceted history of independence, the Orthodox Church in Ukraine, Soviet-era repressions, mass famine, and World War II collaboration were used to provide a different constitutive frame for developing Ukrainian nationhood. The politics of identity (which includes the production of history textbooks and the authorization of commemorative practices) has remained fragmented and tailored to reflect the ideological anxieties and concerns of individual regions of Ukraine.[351]
In Soviet Ukraine, twentieth-century historians were strictly limited in the range of models and topics they could cover, with Moscow insisting on an official Marxist approach. However, émigré Ukrainians in Canada developed an independent scholarship that ignored Marxism, and shared the Western tendencies in historiography.[352] George W. Simpson and Orest Subtelny were leaders promoting Ukrainian studies in Canadian academe.[353] The lack of independence in Ukraine meant that traditional historiographical emphases on diplomacy and politics were handicapped. The flourishing of social history after 1960 opened many new approaches for researchers in Canada; Subtelny used the modernization model. Later historiographical trends were quickly adapted to the Ukrainian evidence, with special focus on Ukrainian nationalism. The new cultural history, post-colonial studies, and the "linguistic turn" augmenting, if not replacing social history, allowed for multiple angles of approach. By 1991, historians in Canada had freely explored a wide range of approaches regarding the emergence of a national identity. After independence, a high priority in Canada was assisting in the freeing of Ukrainian scholarship from Soviet-Marxist orthodoxy—which downplayed Ukrainian nationalism and insisted that true Ukrainians were always trying to reunite with Russia. Independence from Moscow meant freedom from an orthodoxy that was never well suited to Ukrainian developments. Scholars in Ukraine welcomed the "national paradigm" that Canadian historians had helped develop. Since 1991, the study of Ukrainian nation-building became an increasingly global and collaborative enterprise, with scholars from Ukraine studying and working in Canada, and with conferences on related topics attracting scholars from around the world.[354]
Media related to History of Ukraine at Wikimedia Commons

The  Hominini (hominins) form a taxonomic tribe of the subfamily Homininae (hominines). They comprise two extant genera: Homo (humans) and Pan (chimpanzees and bonobos), and in standard usage exclude the genus Gorilla (gorillas), which is grouped separately within the subfamily Homininae.
The term Hominini was originally introduced by Camille Arambourg (1948), who combined the categories of Hominina and Simiina pursuant to Gray's classifications (1825).
Traditionally, chimpanzees, gorillas and orangutans were grouped together, excluding humans, as pongids. Since Gray's classifications, evidence  accumulating from genetic phylogeny confirmed that humans, chimpanzees, and gorillas are more closely related to each other than to the orangutan.[3] The orangutans were reassigned to the family Hominidae (great apes), which already included humans; and the gorillas were grouped as a separate tribe (Gorillini) of the subfamily Homininae.[3] Still, details of this reassignment remain contested, and of publishing since (on tribe Hominini), not every source excludes gorillas and not every source includes chimpanzees.
Humans are the only extant species in the Australopithecine branch (subtribe), which also contains many extinct close relatives of humans.
Concerning membership, when Hominini is taken to exclude Pan, Panini ("panins")[4] may refer to the tribe containing Pan as its only genus.[5][6] Or perhaps place Pan with other dryopithecine genera, making the whole tribe or subtribe of Panini or Panina together. Minority dissenting nomenclatures include Gorilla in Hominini and Pan in Homo (Goodman et al. 1998), or both Pan and Gorilla in Homo (Watson et al. 2001).
By convention, the adjectival term "hominin" (or nominalized "hominins") refers to the tribe Hominini, whereas the members of the subtribe Hominina (and thus all archaic human species) are referred to as "homininan" ("homininans").[7][8][9] This follows the proposal by Mann and Weiss (1996), which presents tribe Hominini as including both Pan and Homo, placed in separate subtribes. The genus Pan is referred to subtribe Panina, and genus Homo is included in the subtribe Hominina (see below).[10]
The alternative convention uses "hominin" to exclude members of Panina: for Homo; or for human and australopithecine species. This alternative convention is referenced in e.g. Coyne (2009)[11] and in Dunbar (2014).[6] Potts (2010) in addition uses the name Hominini in a different sense, as excluding Pan, and uses "hominins" for this, while a separate tribe (rather than subtribe) for chimpanzees is introduced, under the name Panini.[5] In this recent convention, contra Arambourg, the term "hominin" is applied to Homo, Australopithecus, Ardipithecus, and others that arose after the split from the line that led to chimpanzees (see cladogram below);[12][13] that is, they distinguish fossil members on the human side of the split, as "hominins", from those on the chimpanzee side, as "not hominins" (or "non-hominin hominids").[11]
This cladogram shows the clade of superfamily Hominoidea and its descendant clades, focused on the division of Hominini (omitting detail on clades not ancestral to Hominini). The family Hominidae ("hominids") comprises the tribes Ponginae (including orangutans), Gorillini (including gorillas) and Hominini, the latter two forming the subfamily of Homininae. Hominini is divided into Panina (chimpanzees) and Australopithecina (australopithecines). The Hominina (humans) are usually held to have emerged within the Australopithecina (which would roughly correspond to the alternative definition of Hominini according to the alternative definition which excludes Pan).
Genetic analysis combined with fossil evidence indicates that hominoids diverged from the Old World monkeys about 25 million years ago (Mya), near the Oligocene-Miocene boundary.[14] The most recent common ancestors (MRCA) of the subfamilies Homininae and Ponginae lived about 15 million years ago. The best-known fossil genus of Ponginae is Sivapithecus, consisting of several species from 12.5 million to 8.5 million years ago. It differs from orangutans in dentition and postcranial morphology.[15] In the following cladogram, the approximate time the clades radiated newer clades is indicated in millions of years ago (Mya).
Hylobatidae (gibbons)
Ponginae (orangutans)
Gorillini (gorillas)
Panina (chimpanzees)
Ardipithecus (†)
Praeanthropus (†)
Australopithecus/Paranthropus robustus (†2)
Australopithecus garhi (†2.5)
Homo (humans)
Both Sahelanthropus and Orrorin existed during the estimated duration of the ancestral chimpanzee–human speciation events, within the range of eight to four million years ago (Mya). Very few fossil specimens have been found that can be considered directly ancestral to genus Pan. News of the first fossil chimpanzee, found in Kenya, was published in 2005. However, it is dated to very recent times—between 545 and 284 thousand years ago.[16] The divergence of a "proto-human" or "pre-human" lineage separate from Pan appears to have been a process of complex speciation-hybridization rather than a clean split, taking place over the period of anywhere between 13 Mya (close to the age of the tribe Hominini itself) and some 4 Mya. Different chromosomes appear to have split at different times, with broad-scale hybridization activity occurring between the two emerging lineages as late as the period 6.3 to 5.4 Mya, according to Patterson et al. (2006),[17] This research group noted that one hypothetical late hybridization period was based in particular on the similarity of X chromosomes in the proto-humans and stem chimpanzees, suggesting that the final divergence was even as recent as 4 Mya. Wakeley (2008) rejected these hypotheses; he suggested alternative explanations, including selection pressure on the X chromosome in the ancestral populations prior to the chimpanzee–human last common ancestor (CHLCA).[18]
Most DNA studies find that humans and Pan are 99% identical,[19][20] but one study found only 94% commonality, with some of the difference occurring in non-coding DNA.[21] It is most likely that the australopithecines, dating from 4.4 to 3 Mya, evolved into the earliest members of genus Homo.[22][23] In the year 2000, the discovery of Orrorin tugenensis, dated as early as 6.2 Mya, briefly challenged critical elements of that hypothesis,[24] as it suggested that Homo did not in fact derive from australopithecine ancestors.[25]
All the listed fossil genera are evaluated for two traits that could identify them as hominins:
Some, including Paranthropus, Ardipithecus, and Australopithecus, are broadly thought to be ancestral and closely related to Homo;[26] others, especially earlier genera, including Sahelanthropus (and perhaps Orrorin), are supported by one community of scientists but doubted by another.[27][28]
Extant species are in bold.

Recent human evolution refers to evolutionary adaptation, sexual and natural selection, and genetic drift within Homo sapiens populations, since their separation and dispersal in the Middle Paleolithic about 50,000 years ago. Contrary to popular belief, not only are humans still evolving, their evolution since the dawn of agriculture is faster than ever before.[1][2][3][4] It has been proposed that human culture acts as a selective force in human evolution and has accelerated it;[5] however, this is disputed.[6][7] With a sufficiently large data set and modern research methods, scientists can study the changes in the frequency of an allele occurring in a tiny subset of the population over a single lifetime, the shortest meaningful time scale in evolution.[8] Comparing a given gene with that of other species enables geneticists to determine whether it is rapidly evolving in humans alone. For example, while human DNA is on average 98% identical to chimp DNA, the so-called Human Accelerated Region 1 (HAR1), involved in the development of the brain, is only 85% similar.[2]
Following the peopling of Africa some 130,000 years ago, and the recent Out-of-Africa expansion some 70,000 to 50,000 years ago, some sub-populations of Homo sapiens have been geographically isolated for tens of thousands of years prior to the early modern Age of Discovery. Combined with archaic admixture, this has resulted in relatively significant genetic variation. Selection pressures were especially severe for populations affected by the Last Glacial Maximum (LGM) in Eurasia, and for sedentary farming populations since the Neolithic, or New Stone Age.[9]
Single nucleotide polymorphisms (SNP, pronounced 'snip'), or mutations of a single genetic code "letter" in an allele that spread across a population, in functional parts of the genome can potentially modify virtually any conceivable trait, from height and eye color to susceptibility to diabetes and schizophrenia. Approximately 2% of the human genome codes for proteins and a slightly larger fraction is involved in gene regulation. But most of the rest of the genome has no known function. If the environment remains stable, the beneficial mutations will spread throughout the local population over many generations until it becomes a dominant trait. An extremely beneficial allele could become ubiquitous in a population in as little as a few centuries whereas those that are less advantageous typically take millennia.[10]
Human traits that emerged recently include the ability to free-dive for long periods of time,[11] adaptations for living in high altitudes where oxygen concentrations are low,[2] resistance to contagious diseases (such as malaria),[12] light skin,[13] blue eyes,[14] lactase persistence (or the ability to digest milk after weaning),[15][16] lower blood pressure and cholesterol levels,[17][18] retention of the median artery,[19] reduced prevalence of Alzheimer's disease,[8] lower susceptibility to diabetes,[20] genetic longevity,[20] shrinking brain sizes,[21][22] and changes in the timing of menarche and menopause.[23]
Genetic evidence suggests that a species dubbed Homo heidelbergensis is the last common ancestor of Neanderthals, Denisovans, and Homo sapiens. This common ancestor lived between 600,000 and 750,000 years ago, likely in either Europe or Africa. Members of this species migrated throughout Europe, the Middle East, and Africa and became the Neanderthals in Western Asia and Europe while another group moved further east and evolved into the Denisovans, named after the Denisova Cave in Russia where the first known fossils of them were discovered. In Africa, members of this group eventually became anatomically modern humans. Migrations and geographical isolation notwithstanding, the three descendant groups of Homo heidelbergensis later met and interbred.[24]
Archaeological research suggests that as prehistoric humans swept across Europe 45,000 years ago, Neanderthals went extinct. Even so, there is evidence of interbreeding between the two groups as humans expanded their presence in the continent. While prehistoric humans carried 3–6% Neanderthal DNA, modern humans have only about 2%. This seems to suggest selection against Neanderthal-derived traits.[26] For example, the neighborhood of the gene FOXP2, affecting speech and language, shows no signs of Neanderthal inheritance whatsoever.[27]
Introgression of genetic variants acquired by Neanderthal admixture has different distributions in Europeans and East Asians, pointing to differences in selective pressures.[28] Though East Asians inherit more Neanderthal DNA than Europeans,[27] East Asians, South Asians, Australo-Melanesians, Native Americans, and Europeans all share Neanderthal DNA, so hybridization likely occurred between Neanderthals and their common ancestors coming out of Africa.[29] Their differences also suggest separate hybridization events for the ancestors of East Asians and other Eurasians.[27]
Following the genome sequencing of three Vindija Neanderthals, a draft sequence of the Neanderthal genome was published and revealed that Neanderthals shared more alleles with Eurasian populations—such as French, Han Chinese, and Papua New Guinean—than with sub-Saharan African populations, such as Yoruba and San. According to the authors of the study, the observed excess of genetic similarity is best explained by recent gene flow from Neanderthals to modern humans after the migration out of Africa.[30] But gene flow did not go one way. The fact that some of the ancestors of modern humans in Europe migrated back into Africa means that modern Africans also carry some genetic materials from Neanderthals. In particular, Africans share 7.2% Neanderthal DNA with Europeans but only 2% with East Asians.[29]
Some climatic adaptations, such as high-altitude adaptation in humans, are thought to have been acquired by archaic admixture. An ethnic group known as the Sherpas from Nepal is believed to have inherited an allele called EPAS1, which allows them to breathe easily at high altitudes, from the Denisovans.[24] A 2014 study reported that Neanderthal-derived variants found in East Asian populations showed clustering in functional groups related to immune and haematopoietic pathways, while European populations showed clustering in functional groups related to the lipid catabolic process.[note 1] A 2017 study found correlation of Neanderthal admixture in modern European populations with traits such as skin tone, hair color, height, sleeping patterns, mood and smoking addiction.[31] A 2020 study of Africans unveiled Neanderthal haplotypes, or alleles that tend to be inherited together, linked to immunity and ultraviolet sensitivity.[29]
The gene microcephalin (MCPH1), involved in the development of the brain, likely originated from a Homo lineage separate from that of anatomically modern humans, but was introduced to them around 37,000 years ago, and has become much more common ever since, reaching around 70% of the human population at present. Neanderthals were suggested as one possible origin of this gene.[32]
But later studies did not find this gene in the Neanderthal genome[33][34] nor has it been found to be associated with cognitive ability in modern people.[35][36][37]
The promotion of beneficial traits acquired from admixture is known as adaptive introgression.[29]
A study concluded only 1.5–7% of "regions" of the modern human genome to be specific to modern humans. These regions have neither been altered by archaic hominin DNA due to admixture (only a small portion of archaic DNA is inherited per individual but a large portion is inherited across populations overall) nor are shared with Neanderthals or Denisovans in any of the genomes of the used datasets. They also found two bursts of changes specific to modern human genomes which involve genes related to brain development and function.[38][39]
Victorian naturalist Charles Darwin was the first to propose the out-of-Africa hypothesis for the peopling of the world,[40] but the story of prehistoric human migration is now understood to be much more complex thanks to twenty-first-century advances in genomic sequencing.[40][41][42] There were multiple waves of dispersal of anatomically modern humans out of Africa,[43][44][45] with the most recent one dating back to 70,000 to 50,000 years ago.[46][47][48][49] Earlier waves of human migrants might have gone extinct or returned to Africa.[45][50] Moreover, a combination of gene flow from Eurasia back into Africa and higher rates of genetic drift among East Asians compared to Europeans led these human populations to diverge from one another at different times.[40]
Around 65,000 to 50,000 years ago, a variety of new technologies, such as projectile weapons, fish hooks, porcelain, and sewing needles, made their appearance.[51] Bird-bone flutes were invented 30,000 to 35,000 years ago,[52] indicating the arrival of music.[51] Artistic creativity also flowered, as can be seen with Venus figurines and cave paintings.[51] Cave paintings of not just actual animals but also imaginary creatures that could reliably be attributed to Homo sapiens have been found in different parts of the world. Radioactive dating suggests that the oldest of the ones that have been found, as of 2019, are 44,000 years old.[53] For researchers, these artworks and inventions represent a milestone in the evolution of human intelligence, the roots of story-telling, paving the way for spirituality and religion.[51][53] Experts believe this sudden "great leap forward"—as anthropologist Jared Diamond calls it—was due to climate change. Around 60,000 years ago, during the middle of an ice age, it was extremely cold in the far north, but ice sheets sucked up much of the moisture in Africa, making the continent even drier and droughts much more common. The result was a genetic bottleneck, pushing Homo sapiens to the brink of extinction, and a mass exodus from Africa. Nevertheless, it remains uncertain (as of 2003) whether or not this was due to some favorable genetic mutations, for example in the FOXP2 gene, linked to language and speech.[54] A combination of archaeological and genetic evidence suggests that humans migrated along Southern Asia and down to Australia 50,000 years ago, to the Middle East and then to Europe 35,000 years ago, and finally to the Americas via the Siberian Arctic 15,000 years ago.[54]
DNA analyses conducted since 2007 revealed the acceleration of evolution with regards to defenses against disease, skin color, nose shapes, hair color and type, and body shape since about 40,000 years ago, continuing a trend of active selection since humans emigrated from Africa 100,000 years ago. Humans living in colder climates tend to be more heavily built compared to those in warmer climates because having a smaller surface area compared to volume makes it easier to retain heat.[note 2] People from warmer climates tend to have thicker lips, which have large surface areas, enabling them to keep cool. With regards to nose shapes, humans residing in hot and dry places tend to have narrow and protruding noses in order to reduce loss of moisture. Humans living in hot and humid places tend to have flat and broad noses that moisturize inhaled air and retain moisture from exhaled air.[dubious – discuss][citation needed] Humans dwelling in cold and dry places tend to have small, narrow, and long noses in order to warm and moisturize inhaled air. As for hair types, humans from regions with colder climates tend to have straight hair so that the head and neck are kept warm. Straight hair also allows cool moisture to quickly fall off the head. On the other hand, tight and curly hair increases the exposed areas of the scalp, easing the evaporation of sweat and allowing heat to be radiated away while keeping itself off the neck and shoulders. Epicanthic eye folds are believed to be an adaptation protecting the eye from overexposure to ultraviolet radiation, and is presumed to be a particular trait in archaic humans from eastern and southeast Asia. A cold-adaptive explanation for the epicanthic fold is today seen as outdated by some, as epicanthic folds appear in some African populations. Dr. Frank Poirier, a physical anthropologist at Ohio State University, concluded that the epicanthic fold in fact may be an adaptation for tropical regions, and was already part of the natural diversity found among early modern humans.[55][56]
Physiological or phenotypical changes have been traced to Upper Paleolithic mutations, such as the East Asian variant of the EDAR gene, dated to about 35,000 years ago in Southern or Central China. Traits affected by the mutation are sweat glands, teeth, hair thickness and breast tissue.[58] While Africans and Europeans carry the ancestral version of the gene, most East Asians have the mutated version. By testing the gene on mice, Yana G. Kamberov and Pardis C. Sabeti and their colleagues at the Broad Institute found that the mutated version brings thicker hair shafts, more sweat glands, and less breast tissue. East Asian women are known for having comparatively small breasts and East Asians in general tend to have thick hair. The research team calculated that this gene originated in Southern China, which was warm and humid, meaning having more sweat glands would be advantageous to the hunter-gatherers who lived there.[58] A subsequent study from 2021, based on ancient DNA samples, has suggested that the derived variant became dominant among "Ancient Northern East Asians" shortly after the Last Glacial Maximum in Northeast Asia, around 19,000 years ago. Ancient remains from Northern East Asia, such as the Tianyuan Man (40,000 years old) and the AR33K (33,000 years old) specimen lacked the derived EDAR allele, while ancient East Asian remains after the LGM carry the derived EDAR allele.[59][60] The frequency of 370A is most highly elevated in North Asian and East Asian populations.[61]
The most recent Ice Age peaked in intensity between 19,000 and 25,000 years ago and ended about 12,000 years ago. As the glaciers that once covered Scandinavia all the way down to Northern France retreated, humans began returning to Northern Europe from the Southwest, modern-day Spain. But about 14,000 years ago, humans from Southeastern Europe, especially Greece and Turkey, began migrating to the rest of the continent, displacing the first group of humans. Analysis of genomic data revealed that all Europeans since 37,000 years ago have descended from a single founding population that survived the Ice Age, with specimens found in various parts of the continent, such as Belgium. Although this human population was displaced 33,000 years ago, a genetically related group began spreading across Europe 19,000 years ago.[26] Recent divergence of Eurasian lineages was sped up significantly during the Last Glacial Maximum (LGM), the Mesolithic and the Neolithic, due to increased selection pressures and founder effects associated with migration.[62] Alleles predictive of light skin have been found in Neanderthals,[63] but the alleles for light skin in Europeans and East Asians, KITLG and ASIP, are (as of 2012) thought to have not been acquired by archaic admixture but recent mutations since the LGM.[62] Hair, eye, and skin pigmentation phenotypes associated with humans of European descent emerged during the LGM, from about 19,000 years ago.[13] The associated TYRP1 SLC24A5 and SLC45A2 alleles emerge around 19,000 years ago, still during the LGM, most likely in the Caucasus.[62][64] Within the last 20,000 years or so, lighter skin has evolved in East Asia, Europe, North America and Southern Africa. In general, people living in higher latitudes tend to have lighter skin.[3] The HERC2 variation for blue eyes first appears around 14,000 years ago in Italy and the Caucasus.[65]
Inuit adaptation to high-fat diet and cold climate has been traced to a mutation dated the Last Glacial Maximum (20,000 years ago).[66] Average cranial capacity among modern male human populations varies in the range of 1,200 to 1,450 cm3. Larger cranial volumes are associated with cooler climatic regions, with the largest averages being found in populations of Siberia and the Arctic.[note 3][68] Humans living in Northern Asia and the Arctic have evolved the ability to develop thick layers of fat on their faces to keep warm. Moreover, the Inuit tend to have flat and broad faces, an adaptation that reduces the likelihood of frostbites.[69] Both Neanderthal and Cro-Magnons had somewhat larger cranial volumes on average than modern Europeans, suggesting the relaxation of selection pressures for larger brain volume after the end of the LGM.[67]
Australian Aboriginals living in the Central Desert, where the temperature can drop below freezing at night, have evolved the ability to reduce their core temperatures without shivering.[69]
The advent of agriculture has played a key role in the evolutionary history of humanity. Early farming communities benefited from new and comparatively stable sources of food, but were also exposed to new and initially devastating diseases such as tuberculosis, measles, and smallpox. Eventually, genetic resistance to such diseases evolved and humans living today are descendants of those who survived the agricultural revolution and reproduced.[70][5] The pioneers of agriculture faced tooth cavities, protein deficiency and general malnutrition, resulting in shorter statures.[5] Diseases are one of the strongest forces of evolution acting on Homo sapiens. As this species migrated throughout Africa and began colonizing new lands outside the continent around 100,000 years ago, they came into contact with and helped spread a variety of pathogens with deadly consequences. In addition, the dawn of agriculture led to the rise of major disease outbreaks. Malaria is the oldest known of human contagions, traced to West Africa around 100,000 years ago, before humans began migrating out of the continent. Malarial infections surged around 10,000 years ago, raising the selective pressures upon the affected populations, leading to the evolution of resistance.[12]
Examples for adaptations related to agriculture and animal domestication include East Asian types of ADH1B associated with rice domestication,[71] and lactase persistence.[72][73]
As Europeans and East Asians migrated out of Africa, those groups were maladapted and came under stronger selective pressures.[5]
Around 11,000 years ago, as agriculture was replacing hunting and gathering in the Middle East, people invented ways to reduce the concentrations of lactose in milk by fermenting it to make yogurt and cheese. People lost the ability to digest lactose as they matured and as such lost the ability to consume milk. Thousands of years later, a genetic mutation enabled people living in Europe at the time to continue producing lactase, an enzyme that digests lactose, throughout their lives, allowing them to drink milk after weaning and survive bad harvests.[15]
Today, lactase persistence can be found in 90% or more of the populations in Northwestern and Northern Central Europe, and in pockets of Western and Southeastern Africa, Saudi Arabia, and South Asia. It is not as common in Southern Europe (40%) because Neolithic farmers had already settled there before the mutation existed. It is rarer in inland Southeast Asia and Southern Africa. While all Europeans with lactase persistence share a common ancestor for this ability, pockets of lactase persistence outside Europe are likely due to separate mutations. The European mutation, called the LP allele, is traced to modern-day Hungary, 7,500 years ago. In the twenty-first century, about 35% of the human population is capable of digesting lactose after the age of seven or eight.[15] Before this mutation, dairy farming was already widespread in Europe.[74]
A Finnish research team reported that the European mutation that allows for lactase persistence is not found among the milk-drinking and dairy-farming Africans, however. Sarah Tishkoff and her students confirmed this by analyzing DNA samples from Tanzania, Kenya, and Sudan, where lactase persistence evolved independently. The uniformity of the mutations surrounding the lactase gene suggests that lactase persistence spread rapidly throughout this part of Africa. According to Tishkoff's data, this mutation first appeared between 3,000 and 7,000 years ago. This mutation provides some protection against drought and enables people to drink milk without diarrhea, which causes dehydration.[16]
Lactase persistence is a rare ability among mammals.[74] Because it involves a single gene, it is a simple example of convergent evolution in humans. Other examples of convergent evolution, such as the light skin of Europeans and East Asians or the various means of resistance to malaria, are much more complicated.[16]
The light skin pigmentation characteristic of modern Europeans is estimated to have spread across Europe in a "selective sweep" during the Mesolithic (5,000 years ago).[13] Signals for selection in favor of light skin among Europeans was one of the most pronounced, comparable to those for resistance to malaria or lactose tolerance.[75] However, Dan Ju and Ian Mathieson caution in a study addressing 40,000 years of modern human history, "we can assess the extent to which they carried the same light pigmentation alleles that are present today," but explain that c. 40,000 BP Early Upper Paleolithic hunter-gatherers "may have carried different alleles that we cannot now detect", and as a result "we cannot confidently make statements about the skin pigmentation of ancient populations.”[76]
Eumelanin, which is responsible for pigmentation in human skin, protects against ultraviolet radiation while also limiting vitamin D synthesis.[77] Variations in skin color, due to the levels of melanin, are caused by at least 25 different genes, and variations evolved independently of each other to meet different environmental needs.[77] Over the millennia, human skin colors have evolved to be well-suited to their local environments. Having too much melanin can lead to vitamin D deficiency and bone deformities while having too little makes the person more vulnerable to skin cancer.[77] Indeed, Europeans have evolved lighter skin in order to combat vitamin D deficiency in regions with low levels of sunlight. Today, they and their descendants in places with intense sunlight such as Australia are highly vulnerable to sunburn and skin cancer. On the other hand, Inuit have a diet rich in vitamin D and consequently have not needed lighter skin.[78]
Blue eyes are an adaptation for living in regions where the amounts of light are limited because they allow more light to come in than brown eyes.[69] They also seem to have undergone both sexual and frequency-dependent selection.[79][80][75] A research program by geneticist Hans Eiberg and his team at the University of Copenhagen from the 1990s to 2000s investigating the origins of blue eyes revealed that a mutation in the gene OCA2 is responsible for this trait. According to them, all humans initially had brown eyes and the OCA2 mutation took place between 6,000 and 10,000 years ago. It dilutes the production of melanin, responsible for the pigmentation of human hair, eye, and skin color. The mutation does not completely switch off melanin production, however, as that would leave the individual with a condition known as albinism. Variations in eye color from brown to green can be explained via the variation in the amounts of melanin produced in the iris. While brown-eyed individuals share a large area in their DNA controlling melanin production, blue-eyed individuals have only a small region. By examining mitochondrial DNA of people from multiple countries, Eiberg and his team concluded blue-eyed individuals all share a common ancestor.[14]
In 2018, an international team of researchers from Israel and the United States announced their genetic analysis of 6,500-year-old excavated human remains in Israel's Upper Galilee region revealed a number of traits not found in the humans who had previously inhabited the area, including blue eyes. They concluded that the region experienced a significant demographic shift 6,000 years ago due to migration from Anatolia and the Zagros mountains (in modern-day Turkey and Iran) and that this change contributed to the development of the Chalcolithic culture in the region.[81]
Resistance to malaria is a well-known example of recent human evolution. This disease attacks humans early in life. Thus humans who are resistant enjoy a higher chance of surviving and reproducing. While humans have evolved multiple defenses against malaria, sickle cell anemia—a condition in which red blood cells are deformed into sickle shapes, thereby restricting blood flow—is perhaps the best known. Sickle cell anemia makes it more difficult for the malarial parasite to infect red blood cells. This mechanism of defense against malaria emerged independently in Africa and in Pakistan and India. Within 4,000 years it has spread to 10–15% of the populations of these places.[82] Another mutation that enabled humans to resist malaria that is strongly favored by natural selection and has spread rapidly in Africa is the inability to synthesize the enzyme glucose-6-phosphate dehydrogenase, or G6PD.[16]
A combination of poor sanitation and high population densities proved ideal for the spread of contagious diseases which was deadly for the residents of ancient cities. Evolutionary thinking would suggest that people living in places with long-standing urbanization dating back millennia would have evolved resistance to certain diseases, such as tuberculosis and leprosy. Using DNA analysis and archeological findings, scientists from the University College London and the Royal Holloway studied samples from 17 sites in Europe, Asia, and Africa. They learned that, indeed, long-term exposure to pathogens has led to resistance spreading across urban populations. Urbanization is therefore a selective force that has influenced human evolution.[83] The allele in question is named SLC11A1 1729+55del4. Scientists found that among the residents of places that have been settled for thousands of years, such as Susa in Iran, this allele is ubiquitous whereas in places with just a few centuries of urbanization, such as Yakutsk in Siberia, only 70–80% of the population have it.[84]
Evolution to resist infection of pathogens also increased inflammatory disease risk in post-Neolithic Europeans over the last 10,000 years. A study of ancient DNA estimated nature, strength, and time of onset of selections due to pathogens and also found that "the bulk of genetic adaptation occurred after the start of the Bronze Age, <4,500 years ago".[85][86]
Adaptations have also been found in modern populations living in extreme climatic conditions such as the Arctic, as well as immunological adaptations such as resistance against prion caused brain disease in populations practicing mortuary cannibalism, or the consumption of human corpses.[87][88] Inuit have the ability to thrive on the lipid-rich diets consisting of Arctic mammals. Human populations living in regions of high altitudes, such as the Tibetan Plateau, Ethiopia, and the Andes benefit from a mutation that enhances the concentration of oxygen in their blood.[2] This is achieved by having more capillaries, increasing their capacity for carrying oxygen.[3] This mutation is believed to be around 3,000 years old.[2]
A recent adaptation has been proposed for the Austronesian Sama-Bajau, also known as the Sea Gypsies or Sea Nomads, developed under selection pressures associated with subsisting on free-diving over the past thousand years or so.[11][89] As maritime hunter-gatherers, the ability to dive for long periods of times plays a crucial role in their survival. Due to the mammalian dive reflex, the spleen contracts when the mammal dives and releases oxygen-carrying red blood cells. Over time, individuals with larger spleens were more likely to survive the lengthy free-dives, and thus reproduce. By contrast, communities centered around farming show no signs of evolving to have larger spleens. Because the Sama-Bajau show no interest in abandoning this lifestyle, there is no reason to believe further adaptation will not occur.[18]
Advances in the biology of genomes have enabled geneticists to investigate the course of human evolution within centuries. Jonathan Pritchard and a postdoctoral fellow, Yair Field, counted the singletons, or changes of single DNA bases, which are likely to be recent because they are rare and have not spread throughout the population. Since alleles bring neighboring DNA regions with them as they move around the genome, the number of singletons can be used to roughly estimate how quickly the allele has changed its frequency. This approach can unveil evolution within the last 2,000 years or a hundred human generations. Armed with this technique and data from the UK10K project, Pritchard and his team found that alleles for lactase persistence, blond hair, and blue eyes have spread rapidly among Britons within the last two millennia or so. Britain's cloudy skies may have played a role in that the genes for light hair could also cause light skin, reducing the chances of vitamin D deficiency. Sexual selection could also favor blond hair. The technique also enabled them to track the selection of polygenic traits—those affected by a multitude of genes, rather than just one—such as height, infant head circumferences, and female hip sizes (crucial for giving birth).[23] They found that natural selection has been favoring increased height and larger head and female hip sizes among Britons. Moreover, lactase persistence showed signs of active selection during the same period. However, evidence for the selection of polygenic traits is weaker than those affected only by one gene.[90]
A 2012 paper studied the DNA sequence of around 6,500 Americans of European and African descent and confirmed earlier work indicating that the majority of changes to a single letter in the sequence (single nucleotide variants) were accumulated within the last 5,000-10,000 years. Almost three quarters arose in the last 5,000 years or so. About 14% of the variants are potentially harmful, and among those, 86% were 5,000 years old or younger. The researchers also found that European Americans had accumulated a much larger number of mutations than African Americans. This is likely a consequence of their ancestors' migration out of Africa, which resulted in a genetic bottleneck; there were few mates available. Despite the subsequent exponential growth in population, natural selection has not had enough time to eradicate the harmful mutations. While humans today carry far more mutations than their ancestors did 5,000 years ago, they are not necessarily more vulnerable to illnesses because these might be caused by multiple mutations. It does, however, confirm earlier research suggesting that common diseases are not caused by common gene variants.[91] In any case, the fact that the human gene pool has accumulated so many mutations over such a short period of time—in evolutionary terms—and that the human population has exploded in that time mean that humanity is more evolvable than ever before. Natural selection might eventually catch up with the variations in the gene pool, as theoretical models suggest that evolutionary pressures increase as a function of population size.[92]
A study published in 2021 states that the populations of the Cape Verde islands off the coast of West Africa  have speedily evolved resistance to malaria within roughly the last 20 generations, since the  start of human habitation there. As expected, the residents of the Island of Santiago, where malaria is most prevalent, show the highest prevalence of resistance. This is one of the most rapid cases of change to the human genome measured.[93][94]
Geneticist Steve Jones told the BBC that during the sixteenth century, only a third of English babies survived until the age of 21, compared to 99% in the twenty-first century. Medical advances, especially those made in the twentieth century, made this change possible. Yet while people from the developed world today are living longer and healthier lives, many are choosing to have just a few or no children at all, meaning evolutionary forces continue to act on the human gene pool, just in a different way.[95]
Natural selection affects only 8% of the human genome, meaning mutations in the remaining parts of the genome can change their frequency by pure chance through neutral selection. If natural selective pressures are reduced, then more mutations survive, which could increase their frequency and the rate of evolution. For humans, a large source of heritable mutations is sperm; a man accumulates more and more mutations in his sperm as he ages. Hence, men delaying reproduction can affect human evolution.[2]
A 2012 study led by Augustin Kong suggests that the number of de novo (new) mutations increases by about two per year of delayed reproduction by the father and that the total number of paternal mutations doubles every 16.5 years.[96]
For a long time, medicine has reduced the fatality of genetic defects and contagious diseases, allowing more and more humans to survive and reproduce, but it has also enabled maladaptive traits that would otherwise be culled to accumulate in the gene pool. This is not a problem as long as access to modern healthcare is maintained. But natural selective pressures will mount considerably if that is taken away.[18] Nevertheless, dependence on medicine rather than genetic adaptations will likely be the driving force behind humanity's fight against diseases for the foreseeable future. Moreover, while the introduction of antibiotics initially reduced the mortality rates due to infectious diseases by significant amounts, overuse has led to the rise of antibiotic-resistant strains of bacteria, making many illnesses major causes of death once again.[70]
Human jaws and teeth have been shrinking in proportion with the decrease in body size in the last 30,000 years as a result of new diets and technology. There are many individuals today who do not have enough space in their mouths for their third molars (or wisdom teeth) due to reduced jaw sizes. In the twentieth century, the trend toward smaller teeth appeared to have been slightly reversed due to the introduction of fluoride, which thickens dental enamel, thereby enlarging the teeth.[69]
Recent research suggests that menopause is evolving to occur later. Other reported trends appear to include lengthening of the human reproductive period and reduction in cholesterol levels, blood glucose and blood pressure in some populations.[17]
Population geneticist Emmanuel Milot and his team studied recent human evolution in an isolated Canadian island using 140 years of church records. They found that selection favored younger age at first birth among women.[8] In particular, the average age at first birth of women from Coudres Island (Île aux Coudres), 80 km (50 mi) northeast of Québec City, decreased by four years between 1800 and 1930. Women who started having children sooner generally ended up with more children in total who survive until adulthood. In other words, for these French-Canadian women, reproductive success was associated with lower age at first childbirth. Maternal age at first birth is a highly heritable trait.[97]
Human evolution continues during the modern era, including among industrialized nations. Things like access to contraception and the freedom from predators do not stop natural selection.[98] Among developed countries, where life expectancy is high and infant mortality rates are low, selective pressures are the strongest on traits that influence the number of children a human has. It is speculated that alleles influencing sexual behavior would be subject to strong selection, though the details of how genes can affect said behavior remain unclear.[10]
Historically, as a by-product of the ability to walk upright, humans evolved to have narrower hips and birth canals and to have larger heads. Compared to other close relatives such as chimpanzees, childbirth is a highly challenging and potentially fatal experience for humans. Thus began an evolutionary tug-of-war (see Obstetrical dilemma). For babies, having larger heads proved beneficial as long as their mothers' hips were wide enough. If not, both mother and child typically died. This is an example of balancing selection, or the removal of extreme traits. In this case, heads that were too large or hips that were too small were selected against. This evolutionary tug-of-war attained an equilibrium, making these traits remain more or less constant over time while allowing for genetic variation to flourish, thus paving the way for rapid evolution should selective forces shift their direction.[99]
All this changed in the twentieth century as Caesarean sections (also known as C-sections) became safer and more common in some parts of the world.[100] Larger head sizes continue to be favored while selective pressures against smaller hip sizes have diminished. Projecting forward, this means that human heads would continue to grow while hip sizes would not. As a result of increasing fetopelvic disproportion, C-sections would become more and more common in a positive feedback loop, though not necessarily to the extent that natural childbirth would become obsolete.[99][100]
Paleoanthropologist Briana Pobiner of the Smithsonian Institution noted that cultural factors could play a role in the widely different rates of C-sections across the developed and developing worlds. Daghni Rajasingam of the Royal College of Obstetricians observed that the increasing rates of diabetes and obesity among women of reproductive age also boost the demand for C-sections.[100] Biologist Philipp Mitteroecker from the University of Vienna and his team estimated that about six percent of all births worldwide were obstructed and required medical intervention. In the United Kingdom, one quarter of all births involved the C-section while in the United States, the number was one in three. Mitteroecker and colleagues discovered that the rate of C-sections has gone up 10% to 20% since the mid-twentieth century. They argued that because the availability of safe Cesarean sections significantly reduced maternal and infant mortality rates in the developed world, they have induced an evolutionary change. However, "It's not easy to foresee what this will mean for the future of humans and birth," Mitteroecker told The Independent. This is because the increase in baby sizes is limited by the mother's metabolic capacity and modern medicine, which makes it more likely that neonates who are born prematurely or are underweight to survive.[101]
Researchers participating in the Framingham Heart Study, which began in 1948 and was intended to investigate the cause of heart disease among women and their descendants in Framingham, Massachusetts, found evidence for selective pressures against high blood pressure due to the modern Western diet, which contains high amounts of salt, known for raising blood pressure. They also found evidence for selection against hypercholesterolemia, or high levels of cholesterol in the blood.[18] Evolutionary geneticist Stephen Stearns and his colleagues reported signs that women were gradually becoming shorter and heavier. Stearns argued that human culture and changes humans have made on their natural environments are driving human evolution rather than putting the process to a halt.[95] The data indicates that the women were not eating more; rather, the ones who were heavier tended to have more children.[102] Stearns and his team also discovered that the subjects of the study tended to reach menopause later; they estimated that if the environment remains the same, the average age at menopause will increase by about a year in 200 years, or about ten generations. All these traits have medium to high heritability.[10] Given the starting date of the study, the spread of these adaptations can be observed in just a few generations.[18]
By analyzing genomic data of 60,000 individuals of Caucasian descent from Kaiser Permanente in Northern California, and of 150,000 people in the UK Biobank, evolutionary geneticist Joseph Pickrell and evolutionary biologist Molly Przeworski were able to identify signs of biological evolution among living human generations. For the purposes of studying evolution, one lifetime is the shortest possible time scale. An allele associated with difficulty withdrawing from tobacco smoking dropped in frequency among the British but not among the Northern Californians. This suggests that heavy smokers—who were common in Britain during the 1950s but not in Northern California—were selected against. A set of alleles linked to later menarche was more common among women who lived for longer. An allele called ApoE4, linked to Alzheimer's disease, fell in frequency as carriers tended to not live for very long.[23] In fact, these were the only traits that reduced life expectancy Pickrell and Przeworski found, which suggests that other harmful traits probably have already been eradicated. Only among older people are the effects of Alzheimer's disease and smoking visible. Moreover, smoking is a relatively recent trend. It is not entirely clear why such traits bring evolutionary disadvantages, however, since older people have already had children. Scientists proposed that either they also bring about harmful effects in youth or that they reduce an individual's inclusive fitness, or the tendency of organisms that share the same genes to help each other. Thus, mutations that make it difficult for grandparents to help raise their grandchildren are unlikely to propagate throughout the population.[8] Pickrell and Przeworski also investigated 42 traits determined by multiple alleles rather than just one, such as the timing of puberty. They found that later puberty and older age of first birth were correlated with higher life expectancy.[8]
Larger sample sizes allow for the study of rarer mutations. Pickrell and Przeworski told The Atlantic that a sample of half a million individuals would enable them to study mutations that occur among only 2% of the population, which would provide finer details of recent human evolution.[8] While studies of short time scales such as these are vulnerable to random statistical fluctuations, they can improve understanding of the factors that affect survival and reproduction among contemporary human populations.[23]
Evolutionary geneticist Jaleal Sanjak and his team analyzed genetic and medical information from more than 200,000 women over the age of 45 and 150,000 men over the age of 50—people who have passed their reproductive years—from the UK Biobank and identified 13 traits among women and ten among men that were linked to having children at a younger age, having a higher body-mass index,[note 4] fewer years of education, and lower levels of fluid intelligence, or the capacity for logical reasoning and problem solving. Sanjak noted, however, that it was not known whether having children actually made women heavier or being heavier made it easier to reproduce. Because taller men and shorter women tended to have more children and because the genes associated with height affect men and women equally, the average height of the population will likely remain the same. Among women who had children later, those with higher levels of education had more children.[98]
Evolutionary biologist Hakhamanesh Mostafavi led a 2017 study that analyzed data of 215,000 individuals from just a few generations in the United Kingdom and the United States and found a number of genetic changes that affect longevity. The ApoE allele linked to Alzheimer's disease was rare among women aged 70 and over while the frequency of the CHRNA3 gene associated with smoking addiction among men fell among middle-aged men and up. Because this is not itself evidence of evolution, since natural selection only cares about successful reproduction not longevity, scientists have proposed a number of explanations. Men who live longer tend to have more children. Men and women who survive until old age can help take care of both their children and grandchildren, in benefits their descendants down the generations. This explanation is known as the grandmother hypothesis. It is also possible that Alzheimer's disease and smoking addiction are also harmful earlier in life, but the effects are more subtle and larger sample sizes are required in order to study them. Mostafavi and his team also found that mutations causing health problems such as asthma, having a high body-mass index and high cholesterol levels were more common among those with shorter lifespans while mutations leading to delayed puberty and reproduction were more common among long living individuals. According to geneticist Jonathan Pritchard, while the link between fertility and longevity was identified in previous studies, those did not entirely rule out the effects of educational and financial status—people who rank high in both tend to have children later in life; this seems to suggest the existence of an evolutionary trade-off between longevity and fertility.[103]
In South Africa, where large numbers of people are infected with HIV, some have genes that help them combat this virus, making it more likely that they would survive and pass this trait onto their children.[104] If the virus persists, humans living in this part of the world could become resistant to it in as little as hundreds of years. However, because HIV evolves more quickly than humans, it will more likely be dealt with technologically rather than genetically.[10]
A 2017 study by researchers from Northwestern University unveiled a mutation among the Old Order Amish living in Berne, Indiana, that suppressed their chances of having diabetes and extends their life expectancy by about ten years on average. That mutation occurred in the gene called Serpine1, which codes for the production of the protein PAI-1 (plasminogen activator inhibitor), which regulates blood clotting and plays a role in the aging process. About 24% of the people sampled carried this mutation and had a life expectancy of 85, higher than the community average of 75. Researchers also found the telomeres—non-functional ends of human chromosomes—of those with the mutation to be longer than those without. Because telomeres shorten as the person ages, those with longer telomeres tend to live longer. At present, the Amish live in 22 U.S. states plus the Canadian province of Ontario. They live simple lifestyles that date back centuries and generally insulate themselves from modern North American society. They are mostly indifferent towards modern medicine, but scientists do have a healthy relationship with the Amish community in Berne. Their detailed genealogical records make them ideal subjects for research.[20]
In 2020, Teghan Lucas, Maciej Henneberg, Jaliya Kumaratilake gave evidence that a growing share of the human population retained the median artery in their forearms. This structure forms during fetal development but dissolves once two other arteries, the radial and ulnar arteries, develop. The median artery allows for more blood flow and could be used as a replacement in certain surgeries. Their statistical analysis suggested that the retention of the median artery was under extremely strong selection within the last 250 years or so. People have been studying this structure and its prevalence since the eighteenth century.[19][105]
Multidisciplinary research suggests that ongoing evolution could help explain the rise of certain medical conditions such as autism and autoimmune disorders. Autism and schizophrenia may be due to genes inherited from the mother and the father which are over-expressed and which fight a tug-of-war in the child's body. Allergies, asthma, and autoimmune disorders appear linked to higher standards of sanitation, which prevent the immune systems of modern humans from being exposed to various parasites and pathogens the way their ancestors' were, making them hypersensitive and more likely to overreact. The human body is not built from a professionally engineered blue print but a system shaped over long periods of time by evolution with all kinds of trade-offs and imperfections. Understanding the evolution of the human body can help medical doctors better understand and treat various disorders. Research in evolutionary medicine suggests that diseases are prevalent because natural selection favors reproduction over health and longevity. In addition, biological evolution is slower than cultural evolution and humans evolve more slowly than pathogens.[106]
Whereas in the ancestral past, humans lived in geographically isolated communities where inbreeding was rather common,[70] modern transportation technologies have made it much easier for people to travel great distances and facilitated further genetic mixing, giving rise to additional variations in the human gene pool.[102] It also enables the spread of diseases worldwide, which can have an effect on human evolution.[70] Furthermore, climate change may trigger the mass migration of not just humans but also diseases affecting humans.[77] Besides the selection and flow of genes and alleles, another mechanism of biological evolution is epigenetics, or changes not to the DNA sequence itself, but rather the way it is expressed. Scientists already know that chronic illnesses and stress are epigenetic mechanisms.[3]

The prehistory of Southeast Europe, defined roughly as the territory of the wider Southeast Europe (including the territories of the modern countries of Albania, Bosnia and Herzegovina, Bulgaria, Croatia, Cyprus, Greece, Kosovo, Moldova, Montenegro, North Macedonia, Romania, Serbia, Slovenia, and European Turkey) covers the period from the Upper Paleolithic, beginning with the presence of Homo sapiens in the area some 44,000 years ago, until the appearance of the first written records in Classical Antiquity, in Greece. First Greek language is Linear A and follows Linear B, which is a syllabic script that was used for writing in Mycenaean Greek, the earliest attested form of the Greek language. The script predates the Greek alphabet by several centuries. The oldest Mycenaean writing dates to about 1400 BC.[1] It is descended from the older Linear A, an undeciphered earlier script used for writing the Minoan language, as is the later Cypriot syllabary, which also recorded Greek. Linear B, found mainly in the palace archives at Knossos, Kydonia,[2] Pylos, Thebes and Mycenae,[3] but disappeared with the fall of the Mycenean civilisation during the Late Bronze Age collapse.
Human prehistory in Southeast Europe is conventionally divided into smaller periods, such as Upper Paleolithic, Holocene Mesolithic/Epipaleolithic, Neolithic Revolution, expansion of Proto-Indo-Europeans, and Protohistory. The changes between these are gradual. For example, depending on interpretation, protohistory might or might not include Bronze Age Greece (3000–1200 BC),[4] Minoan, Mycenaean, Thracian and Venetic cultures. By one interpretation of the historiography criterion, Southeast Europe enters protohistory only with Homer (See also Historicity of the Iliad, and Geography of the Odyssey). At any rate, the period ends before Herodotus in the 5th century BC.[5]
The earliest evidence of human occupation discovered in the region, in Kozarnika cave (Bulgaria), date from at least 1.5 million years ago.[6]
There is evidence of human presence in the Southeastern Europe from the Lower Paleolithic onwards, but the number of sites is limited. According to Douglass W. Bailey:[7]
it is important to recognize that the Southeastern Europe Upper Palaeolithic was a long period containing little significant internal change. Thus, regional transition was not as dramatic as in other European regions. Crucial changes that define the earliest emergence of Homo sapiens sapiens are presented at Bacho Kiro at 44,000 BC. The Bulgarian key Palaeolithic caves named Bacho Kiro and Temnata Dupka with early Upper Palaeolithic material correlate that the transition was gradual.
The Palaeolithic period, literally the “Old Stone Age”, is an ancient cultural level of human development characterized by the use of unpolished chipped stone tools. The transition from Middle to Upper Palaeolithic is directly related to the development of behavioural modernity by hominids around 40,000 years BP. To denote the great significance and degree of change, this dramatic shift from Middle to Upper Palaeolithic is sometimes called the Upper Palaeolithic Revolution.
In the late Pleistocene, various components of the transition–material culture and environmental features (climate, flora, and fauna) indicate continual change, differing from contemporary points in other parts of Europe. The aforementioned aspects leave some doubt that the term Upper Palaeolithic Revolution is appropriate to the Balkans.
In general, continual evolutionary changes are the first crucial characteristic of the transition to the Upper Palaeolithic in the region. The notion of the Upper Palaeolithic Revolution that has been developed for core European regions is not applicable to the region. What is the reason? This particularly significant moment and its origins are defined and enlightened by other characteristics of the transition to upper Old Stone Age. The environment, climate, flora and fauna corroborate the implications.
During the last interglacial period and the most recent glaciation of the Pleistocene (from 131,000 till 12,000 BP), Europe was very different from the regional glaciation. The glaciations did not affect southeastern Europe to the extent that they did in the northern and central regions. The evidence of forest and steppe indicate the influence was not so drastic; some species of flora and fauna survived only in this part of Europe. The region today still abounds in species endemic only to this part of Europe.
The notion of gradual transition (or evolution) best defines southeastern Europe from about 50,000 BP. In this sense, the material culture and natural environment of the region of the late Pleistocene and the early Holocene were distinct from other parts of Europe. Douglass W. Bailey writes in Balkan Prehistory: Exclusion, Incorporation and Identity: “Less dramatic changes to climate, flora and fauna resulted in less dramatic adaptive, or reactive, developments in material culture.”
Thus, in speaking about southeastern Europe, many classic conceptions and systematizations of human development during the Palaeolithic (and then by implication the Mesolithic) should not be considered correct in all cases. In this regard, the absence of Upper Palaeolithic cave art in the region does not seem to be surprising. Civilisations develop new and distinctive characteristics as they respond to new challenges in their environment.
In 2002, some of the oldest modern human (Homo sapiens sapiens) remains in Europe were discovered in the "Cave With Bones" (Peștera cu Oase), near Anina, Romania.[8] Nicknamed "John of Anina" (Ion din Anina), the remains (the lower jaw) are approximately 37,800 years old.
These are some of Europe's oldest remains of Homo sapiens, so they are likely to represent the first such people to have entered the continent.[9] According to some researchers, the particular interest of the discovery resides in the fact that it presents a mixture of archaic, early modern human and Neanderthal morphological features,[10] indicating considerable Neanderthal/modern human admixture,[11] which in turn suggests that, upon their arrival in Europe, modern humans met and interbred with Neanderthals. Recent reanalysis of some of these fossils has challenged the view that these remains represent evidence of interbreeding.[12] A second expedition by Erik Trinkaus and Ricardo Rodrigo, discovered further fragments (for example, a skull dated ~36,000, nicknamed "Vasile").
Two human fossil remains found in the Muierii (Peştera Muierilor) and the Cioclovina caves in Romania have been radiocarbon dated using the technique of the accelerator mass spectrometry to the age of ~ 30,000 years BP (see Human fossil bones from the Muierii Cave and the Cioclovina Cave, Romania).
The first skull, scapula and tibia remains were found in 1952 in Baia de Fier, in the Muierii Cave, Gorj County in the Oltenia province, by Constantin Nicolaescu-Plopşor.
In 1941 another skull was found at the Cioclovina Cave near Commune Bosorod, Hunedoara County, in Transylvania. The anthropologist, Francisc Rainer, and the geologist, Ion Th. Simionescu, published a study of this skull.
The physical analysis of these fossils was begun in the summer of the year 2000 by Emilian Alexandrescu, archaeologist at the Vasile Pârvan Institute of Archaeology in Bucharest, and Agata Olariu, physicist at the Institute of Physics and Nuclear Engineering-Horia Hulubei, Bucharest, where samples were taken. One sample of bone was taken from the skull from Cioclovina; samples were also taken from the scapula and tibia remains from Muierii Cave. The work continued at the University of Lund, AMS group, by Göran Skog, Kristina Stenström and Ragnar Hellborg. The samples of bones were dated by radiocarbon method applied at the AMS system of the Lund University.[13]
The human fossil remains from Muierii Cave, Baia de Fier, have been dated to 30,150 ± 800 years BP, and the skull from the Cioclovina Cave has been dated to 29,000 ± 700 years BP.[14][15][16]
The Mesolithic period began at the end of the Pleistocene epoch (10th millennium BC) and ended with the Neolithic introduction of farming, the date of which varied in each geographical region. According to Douglass W. Bailey:[17]
It is equally important to recognize that the Balkan upper Palaeolithic was a long period containing little significant internal change. The Mesolithic may not have existed in the Balkans for the same reasons that cave art and mobiliary art never appeared: the changes in climate and flora and fauna were gradual and not drastic. (…) Furthermore, one of the reasons that we do not distinguish separate industries in the Balkans as Mesolithic is because the lithic industries of the early Holocene were very firmly of a gradually developing late Palaeolithic tradition
The Mesolithic is the transitional period between the Upper Palaeolithic hunter-gathering existence and the development of farming and pottery production during the Postglacial Neolithic. The duration of the classical Palaeolithic, which lasted until about 10,000 years ago, is applicable to Southeastern Europe. It ended with the Mesolithic (duration is two to four millennia) or, where an early Neolithisation was peculiar to, with the Epipalaeolithic.
In regions with limited glacial impact (e.g. Southeastern Europe), the term Epipalaeolithic is preferable. Regions that experienced less environmental impact during the last ice age have a much less apparent and straightforward change, and occasionally are marked by an absence of sites from the Mesolithic era.[17]
There is lithic evidence of the Iron Gates mesolithic culture, which is notable for its early urbanization, at Lepenski Vir. Iron Gates mesolithic sites are found in modern Serbia, south-west Romania and Montenegro. At Ostrovul Banului, the Cuina Turcului rock shelter in the Danube gorges and in the nearby caves of Climente, there are finds that people of that time made relatively advanced bone and lithic tools (i.e. end-scrapers, blade lets, and flakes).
The single site with materials related to the Mesolithic era in Bulgaria is Pobíti Kámǎni. There has been no other lithic evidence of this period found in Bulgaria. There is a 4,000-year gap between the latest Upper Palaeolithic material (13,600 BP at Témnata Dupka) and the earliest Neolithic evidence presented at Gǎlǎbnik (the beginning of the 7th millennium BC).
At Odmut in Montenegro there is evidence of human activity in the Mesolithic period. The research on the period has been supplemented with Greek Mesolithic finds, well represented by sites such as Frachthi Cave. Other sites are Theopetra Cave and Sesklo in Thessaly that represent the Middle and Upper Palaeolithic as well as the early Neolithic period. Yet southern and coastal sites in Greece, which contained materials from the Mesolithic, are less known.
Activities began to be concentrated around individual sites where people displayed personal and group identities using various decorations: wearing ornaments and painting their bodies with ochre and hematite. As regards personal identity D. Bailey writes, “Flint-cutting tools as well as time and effort needed to produce such tools testify to the expressions of identity and more flexible combinations of materials, which began to be used in the late Upper Palaeolithic and Mesolithic.”
The aforementioned allows us to speculate whether or not there was a period which could be described as Mesolithic in Southeastern Europe, rather than an extended Upper Palaeolithic. On the other hand, lack of research in a number of regions, and the fact that many of the sites were close to seashores (It is evident that the current sea level is 100 m higher, and a number of sites were covered by water.) means that the Mesolithic Southeastern Europe could be referred to as the Epipalaeolithic Southeastern Europe, which might describe better its gradual changes and poorly defined development.
The relative climatic stability in Southeastern Europe, compared to northern and western Europe, enabled continuous settlement in Southeastern Europe. Southeastern Europe therefore may have effectively functioned as an ice-age refuge from which much of Europe, especially eastern Europe, was re-populated.
Southeastern Europe was the site of major Neolithic cultures, including Butmir, Vinča, Varna, Karanovo, Hamangia and Sesklo.
The Vinča culture was an early culture of Southeastern Europe (between the 6th and the 3rd millennium BC), stretching around the course of the Danube in Serbia, Croatia, northern parts of Bosnia and Herzegovina and Montenegro, Romania, Bulgaria, and the Republic of North Macedonia, although traces of it can be found all around the Southeastern Europe, parts of Central Europe and in Asia Minor.
The Varna Necropolis, Bulgaria, is a burial site in the western industrial zone of Varna (approximately 4 km from the city centre), internationally considered one of the key archaeological sites in world prehistory. The oldest gold treasure in the world, dating from 4,600 BC to 4,200 BC, was discovered at the site.[18] The gold piece dating from 4,500 BC, recently founded in Durankulak, near Varna is another important example.[19][20][21][22]
"Kurganization" of the eastern Southeastern Europe (and the Cucuteni-Trypillian culture adjacent to the north) during the Eneolithic is associated with an early expansion of Indo-Europeans.
Neolithic settlements are also spotted in modern day Greece, trading routes that are based in the late Mesolithic period exist all over the Aegean sea. Some major settlements of Neolithic Greece are Sesklo, Dimini, Early Knossos and Nea Nikomedeia close to Krya Vrysi.
The Bronze Age in Southeastern Europe is divided as follows (Boardman p. 166):
The Bronze Age in the central and eastern part of Southeastern Europe begins late, around 1800 BCE.
The transition to the Iron Age gradually sets in over the 13th century BCE.
The "East Balkan Complex" (Karanovo VII, Ezero culture) covers all of Thrace (modern Bulgaria). The Bronze Age cultures of the central and western Southeastern Europe are less clearly delineated and stretch to Pannonia, the Carpathians and into Hungary.
The Minoan civilization based on the Greek island of Crete becomes Europe's first actual civilization.
The culture of Mycenaean Greece (1600-1100 BC) offers the first written evidence of the Greek language.[23] Several Mycenaean attributes and achievements were borrowed or held in high regard in later periods.[23] while their religion already included several deities that can also be found in the Olympic Pantheon. Mycenaean Greece was dominated by a warrior elite society and consisted of a network of palace states.[23] It was followed by the Greek Dark Ages and the introduction of iron.
After the period that followed the arrival of the Dorians, known as the Greek Dark Ages or Submycenaean Period, the classical Greek culture began to develop in Southeastern Europe, the Aegean islands and the western Asia Minor Greek colonies starting around the 9–8th century (the Geometric Period) and peaking with the 5th century BC Athens democracy. The Greeks were the first to establish a system of trade routes in Southeastern Europe and, in order to facilitate trade with the natives between 700 BC and 300 BC, they founded several colonies on the Black Sea (Pontus Euxinus) coast, Asia Minor, Dalmatia etc.
Other notable groups of peoples and tribes of Southeast Europe organised themselves in large tribal unions such as the Thracian Odrysian kingdom in the east of Southeastern Europe in the 5th century BC. By the 6th century BC the first written sources dealing with the territory north of the Danube appear in Greek sources. By this time the Getae (and later the Daci) had branched out from the Thracian-speaking populations.
The Illyrian kingdom in the west of Southeastern Europe from the early 4th century was organised by the Illyrian tribes situated in the area corresponding to today's Montenegro and Albania. The name Illyrii was originally used to refer to a people occupying an area centered on Lake Skadar, situated between Albania and Montenegro (see List of ancient tribes in Illyria). The term Illyria was subsequently used by the Greeks and Romans as a generic name to refer to different peoples within a well defined but much greater area.[24]
Other tribal unions existed in Dacia at least as early as the beginning of the 2nd century BC under King Oroles. In the beginning of 1st century BC under Burebista's rule, Dacia expanded its territory from Central Europe to the Southern Europe.
Hellenistic culture spread throughout the Macedonian Empire created by Alexander the Great from the later 4th century BC. By the end of the 4th century BC Greek language and culture were dominant not only in Southeastern Europe but also around the whole Eastern Mediterranean.

Ethiopia is considered the area from which anatomically modern humans emerged.[1] Archeological discoveries in the country's sites have garnered specific fossil evidence of early human succession, including the hominins Australopithecus afarensis (3.2 million years ago) and Ardipithecus ramidus (4.4 million years ago). Human settlements in present-day Ethiopia began at least in the Late Stone Age, and the agricultural revolution took place in the third millennium BCE. Ethnolinguistic groups of Afroasiatic speakers (namely Semitic, Cushitic, and Omotic) and Nilo-Saharan speakers—defined by new ethnic, cultural, and linguistic identities—emerged around 2000–1000 BCE.
One of the most prominent and precursor excavations in Ethiopia was conducted by Gerard Dekker at the Stone Age site of Melka Kunture in 1963. There, he recognized that the exposed layered rock on the bank of a flood-prone river represented a fossil record dating back 1.7 million years.[2] French paleoanthropologist Jean Chavallion conducted a systematic expedition of the site between 1965 and 1995. Several Homo erectus fossils aged 1.5 million – 1.7 million years have been uncovered at the site, as well as cranial fragments of early Homo sapiens.[2]
Between 1967 and 1974, the Omo remains were excavated in the southwestern Omo Kibish area and have been dated to the Middle Paleolithic, around 200,000 years ago.[1]
In 1974, American paleoanthropologist Donald Johnson excavated a 3.2-million-year-old early female Australopithecus afarensis (nicknamed "Lucy") in Hadar in the Awash Valley. Ethiopians refer to the fossil as "Dinqnesh". Lucy weighed about 60 pounds and stood three and a half feet tall.[3]
Between 1992 and 1994, a team led by paleoanthropologist Tim White discovered the first Ardipithecus ramidus (nicknamed "Ardi") fossil in the Middle Awash area of Ethiopia, dating to 4.4 million years ago. Subsequently, 100 fossil specimens of Ardi were uncovered. These hominins lived in Early Pliocene and were likely omnivores, consuming plants, meat, and fruit but not hard abrasive foods like nuts and tubers. In 2009, scientists formally announced and published their findings on Ardi. Ardi has certain human features—including smaller diamond-shaped canines and some evidence of upright walking—and is said to descend from the predecessor Ardipithecus kadabba, found in the same area.[4]
In 2000, scientists led by paleoanthropologist Zeresenay Alemseged discovered a veiled 3.3-million-year-old Australopithecus afarensis baby nicknamed "Selam". Searches continued in 2001 when Yohannes Haile-Selassie discovered fossils of a 5.2-million-year-old Ardipithecus ramidus family, 15 mi (24 km) from Aramis. Recently, Yohannes has also found fossil specimens of human ancestors in the Afar Region.[5]
During the Late Stone Age in 9000 BCE,[6] there were at least two bladelet-making archeological cultures in Greater Ethiopia:[7] the Wilton culture (small stone blades) and the Hargeistan (long obsidian bladelet).[8] Humans associated with these cultures were semblance of "Afro-Mediterranean", while there is no Khoisan and Negroid population in the region.[9][10][clarification needed]
It is unclear whether or not these people[which?] transitioned to animal husbandry until their successors. Rock paintings of human and animal figures in Hararghe region and Eritrea illustrate the domestication of cattle and crafting of pottery, hand axes, hoes, and grinding stones. This suggests that by the late third millennium BC, rudimentary agriculture commenced in many parts of western Ethiopia.[9] There is evidence of stone hand tools, blade instruments, and drawings in the limestone caves found in Dire Dawa.[11][12] Recent findings suggest that crafting was introduced to Ethiopia from Sudan via the Blue Nile Valley.
The early inhabitants of Ethiopia began domesticating grains during the Chalcolithic Age (6200–3000 BCE). The development of plough-based agriculture may imply the domestication of cattle around the same time. By the Early Bronze Age (3000 BCE), animals such as cattle, sheep, goats, and donkeys were being domesticated.[13]
Linguistic analysis indicates that proto-Ethiopians spoke Hamito-Semitic or Afroasiatic languages in the third millennium BCE; these languages probably originated from the Eastern Sahara after its desertification.[14] In the 1950s, scholars agreed that only the Afroasiatic language was an ancestor of five[inconsistent] major languages: Ancient Egyptian, Berber, Semitic, and Cushitic.[15] Harold Fleming proposed including a sixth language group, Omotic (previously considered a branch of Cushitic), and speculated that the Afroasiatic homeland might be in southwestern Ethiopia.
The split between proto-Cushitic and proto-Omotic began by the fourth or fifth millennium BCE, and proto-Semitic separated to Asia Minor.[9] Inconclusive research suggests the Afroasiatic superfamily began to diffuse by 13,000 BCE, a period of Omotic slight migration southward.[clarification needed] I.M Diakonoff surmised Afroasiatic people in Ethiopia were attributed to North Africa, specifically from the Sahara to the Nile Delta and over the Sinai Peninsula. Gover Hudson hypothesized that proto–Semitic migrated to West Asia across Bab-el-Mandeb into South Arabia. Also, independent linguistic analysis shows the presence of Semitic speakers in Ethiopia in at least 2000 BCE.[16] Linguistic and cultural fission of proto-Ethiopians occurred in this period. Proto–Ethiopians are classified into five stable groups:
The Nilotic peoples of Sudan migrated to Greater Ethiopia in different phases. Pre-Nilotes arrived in Ethiopia about the third millennium BCE. They were mostly agriculturalists who developed the cultivation of sorghum and tuberous plants like enset and yams. Today, they are settled in western parts of Ethiopia namely Berta, Gumuz, and Koma. The second phase of Nilotic migration took place in the first millennium BCE. They are marked by cattle raising, millet cultivation, and dualistic social organization. Their settlement constitutes the western periphery of Ethiopia. The Nilotic represents two tribes, known as Anyuak and Nuer.[18]

Classically excluded but cladistically included:
Australopithecus (/ˌɒstrələˈpɪθɪkəs, -loʊ-/, OS-trə-lə-PITH-i-kəs, -⁠loh-;[1] or (/ɒsˌtrələpɪˈθiːkəs/, os-TRA-lə-pi-THEE-kəs[2] from Latin  australis 'southern' and Ancient Greek  πίθηκος (pithekos) 'ape'[3]) is a genus of early hominins that existed in Africa during the Pliocene and Early Pleistocene. The genera Homo (which includes modern humans), Paranthropus, and Kenyanthropus evolved from some Australopithecus species. Australopithecus is a member of the subtribe Australopithecina,[4][5] which sometimes also includes Ardipithecus,[6] though the term "australopithecine" is sometimes used to refer only to members of Australopithecus. Species include A. garhi, A. africanus, A. sediba, A. afarensis, A. anamensis, A. bahrelghazali, and A. deyiremeda. Debate exists as to whether some Australopithecus species should be reclassified into new genera, or if Paranthropus and Kenyanthropus are synonymous with Australopithecus, in part because of the taxonomic inconsistency.[7][8]
Furthermore, because e.g. A. africanus is more closely related to humans, or their ancestors at the time, than e.g. A. anamensis and many more Australopithecus branches, Australopithecus cannot be consolidated into a coherent grouping without also including the Homo genus and other genera.
The earliest known member of the genus, A. anamensis, existed in eastern Africa around 4.2 million years ago. Australopithecus fossils become more widely dispersed throughout eastern and southern Africa (the Chadian A. bahrelghazali indicates that the genus was much more widespread than the fossil record suggests), before eventually becoming extinct 1.9 million years ago (or 1.2 to 0.6 million years ago if Paranthropus is included). While none of the groups normally directly assigned to this group survived, Australopithecus gave rise to living descendants, as the genus Homo emerged from an Australopithecus species[7][9][10][11][12][excessive citations] at some time between 3 and 2 million years ago.[13]
Australopithecus possessed two of the three duplicated genes derived from SRGAP2 roughly 3.4 and 2.4 million years ago (SRGAP2B and SRGAP2C), the second of which contributed to the increase in number and migration of neurons in the human brain.[14][15] Significant changes to the hand first appear in the fossil record of later A. afarensis about 3 million years ago (fingers shortened relative to thumb and changes to the joints between the index finger and the trapezium and capitate).[16]
The first Australopithecus specimen, the type specimen, was discovered in 1924 in a lime quarry by workers at Taung, South Africa. The specimen was studied by the Australian anatomist Raymond Dart, who was then working at the University of the Witwatersrand in Johannesburg. The fossil skull was from a three-year-old bipedal primate (nicknamed Taung Child) that he named Australopithecus africanus. The first report was published in Nature in February 1925. Dart realised that the fossil contained a number of humanoid features, and so he came to the conclusion that this was an early human ancestor.[17] Later, Scottish paleontologist Robert Broom and Dart set out to search for more early hominin specimens, and several more A. africanus remains from various sites. Initially, anthropologists were largely hostile to the idea that these discoveries were anything but apes, though this changed during the late 1940s.[17]
In 1950, evolutionary biologist Ernst Walter Mayr said that all bipedal apes should be classified into the genus Homo, and considered renaming Australopithecus to Homo transvaalensis.[18] However, the contrary view taken by J.T. Robinson in 1954, excluding australopiths from Homo, became the prevalent view.[18] The first australopithecine fossil discovered in eastern Africa was an A. boisei skull excavated by Mary Leakey in 1959 in Olduvai Gorge, Tanzania. Since then, the Leakey family has continued to excavate the gorge, uncovering further evidence for australopithecines, as well as for Homo habilis and Homo erectus.[17] The scientific community took 20 more years to widely accept Australopithecus as a member of the human family tree.
In 1997, an almost complete Australopithecus skeleton with skull was found in the Sterkfontein caves of Gauteng, South Africa. It is now called "Little Foot" and it is around 3.7 million years old. It was named Australopithecus prometheus[19][20] which has since been placed within A. africanus. Other fossil remains found in the same cave in 2008 were named Australopithecus sediba, which lived 1.9 million years ago. A. africanus probably evolved into A. sediba, which some scientists think may have evolved into H. erectus,[21] though this is heavily disputed.
In 2003, Spanish writer Camilo José Cela Conde and evolutionary biologist Francisco J. Ayala proposed resurrecting the genus Praeanthropus to house Orrorin, A. afarensis, A. anamensis, A. bahrelghazali, and A. garhi,[22] but this genus has been largely dismissed.[23]
With the apparent emergence of the genera Homo, Kenyanthropus, and Paranthropus in the genus Australopithecus, taxonomy runs into some difficulty, as the name of species incorporates their genus. According to cladistics, groups should not be left paraphyletic, where it is kept not consisting of a common ancestor and all of its descendants.[24][25][26][27][28][29] Resolving this problem would cause major ramifications in the nomenclature of all descendent species. Possibilities suggested have been to rename Homo sapiens to Australopithecus sapiens[30] (or even Pan sapiens[31][32]), or to move some Australopithecus species into new genera.[8] A study reported in 2025 reported preliminary success in extracting ancient proteins from an Australopithic tooth, suggesting that paleoproteomics has the potential to provide information about the genetic affinities of the species.[33]
In 2002 and again in 2007, Camilo José Cela Conde et al. suggested that A. africanus be moved to Paranthropus.[7] On the basis of craniodental evidence, Strait and Grine (2004) suggest that A. anamensis and A. garhi should be assigned to new genera.[34] It is debated whether or not A. bahrelghazali should be considered simply a western variant of A. afarensis instead of a separate species.[35][36]
A. anamensis may have descended from or was closely related to Ardipithecus ramidus.[37] A. anamensis shows some similarities to both Ar. ramidus and Sahelanthropus.[37]
Australopiths shared several traits with modern apes and humans, and were widespread throughout Eastern and Northern Africa by 3.5 million years ago (mya). The earliest evidence of fundamentally bipedal hominins is a (3.6 mya) fossil trackway in Laetoli, Tanzania, which bears a remarkable similarity to those of modern humans. The footprints have generally been classified as australopith, as they are the only form of prehuman hominins known to have existed in that region at that time.[38]
According to the Chimpanzee Genome Project, the human–chimpanzee last common ancestor existed about five to six million years ago, assuming a constant rate of mutation. However, hominin species dated to earlier than the date could call this into question.[39] Sahelanthropus tchadensis, commonly called "Toumai", is about seven million years old and Orrorin tugenensis lived at least six million years ago. Since little is known of them, they remain controversial among scientists since the molecular clock in humans has determined that humans and chimpanzees had a genetic split at least a million years later.[citation needed] One theory suggests that the human and chimpanzee lineages diverged somewhat at first, then some populations interbred around one million years after diverging.[39]
The brains of most species of Australopithecus were roughly 35% of the size of a modern human brain[40] with an endocranial volume average of 466 cc (28.4 cu in).[13]  Although this is more than the average endocranial volume of chimpanzee brains at 360 cc (22 cu in)[13] the earliest australopiths (A. anamensis) appear to have been within the chimpanzee range,[37] whereas some later australopith specimens have a larger endocranial volume than that of some early Homo fossils.[13]
Most species of Australopithecus were diminutive and gracile, usually standing 1.2 to 1.4 m (3 ft 11 in to 4 ft 7 in) tall. It is possible that they exhibited a considerable degree of sexual dimorphism, males being larger than females.[41] In modern populations, males are on average a mere 15% larger than females, while in Australopithecus, males could be up to 50% larger than females by some estimates. However, the degree of sexual dimorphism is debated due to the fragmentary nature of australopith remains.[41] One paper finds that A. afarensis had a level of dimorphism close to modern humans.[42]
According to A. Zihlman, Australopithecus body proportions closely resemble those of bonobos (Pan paniscus),[43] leading evolutionary biologist Jeremy Griffith to suggest that bonobos may be phenotypically similar to Australopithecus.[44] Furthermore, thermoregulatory models suggest that australopiths were fully hair covered, more like chimpanzees and bonobos, and unlike humans.[45]
The fossil record seems to indicate that Australopithecus is ancestral to Homo and modern humans. It was once assumed that large brain size had been a precursor to bipedalism, but the discovery of Australopithecus with a small brain but developed bipedality upset this theory. Nonetheless, it remains a matter of controversy as to how bipedalism first emerged. The advantages of bipedalism were that it left the hands free to grasp objects (e.g., carry food and young), and allowed the eyes to look over tall grasses for possible food sources or predators, but it is also argued that these advantages were not significant enough to cause the emergence of bipedalism.[citation needed] Earlier fossils, such as Orrorin tugenensis, indicate bipedalism around six million years ago, around the time of the split between humans and chimpanzees indicated by genetic studies. This suggests that erect, straight-legged walking originated as an adaptation to tree-dwelling.[46] Major changes to the pelvis and feet had already taken place before Australopithecus.[47] It was once thought that humans descended from a knuckle-walking ancestor,[48] but this is not well-supported.[49]
Australopithecines have thirty-two teeth, like modern humans. Their molars were parallel, like those of great apes, and they had a slight pre-canine gap (diastema). Their canines were smaller, like modern humans, and with the teeth less interlocked than in previous hominins. In fact, in some australopithecines, the canines are shaped more like incisors.[50] The molars of Australopithecus fit together in much the same way those of humans do, with low crowns and four low, rounded cusps used for crushing. They have cutting edges on the crests.[50] However, australopiths generally evolved a larger postcanine dentition with thicker enamel.[51] Australopiths in general had thick enamel, like Homo, while other great apes have markedly thinner enamel.[50] Robust australopiths wore their molar surfaces down flat, unlike the more gracile species, who kept their crests.[50]
Australopithecus species are thought to have eaten mainly fruit, vegetables, and tubers, and perhaps easy-to-catch animals such as small lizards. Much research has focused on a comparison between the South African species A. africanus and Paranthropus robustus. Early analyses of dental microwear in these two species showed, compared to P. robustus, A. africanus had fewer microwear features and more scratches as opposed to pits on its molar wear facets.[52] Microwear patterns on the cheek teeth of A. afarensis and A. anamensis indicate that A. afarensis predominantly ate fruits and leaves, whereas A. anamensis included grasses and seeds (in addition to fruits and leaves).[53] The thickening of enamel in australopiths may have been a response to eating more ground-bound foods such as tubers, nuts, and cereal grains with gritty dirt and other small particulates which would wear away enamel.  Gracile australopiths had larger incisors, which indicates tearing food was important, perhaps eating scavenged meat. Nonetheless, the wearing patterns on the teeth support a largely herbivorous diet.[50]
In 1992, trace-element studies of the strontium/calcium ratios in robust australopith fossils suggested the possibility of animal consumption, as they did in 1994 using stable carbon isotopic analysis.[54] In 2005, fossil animal bones with butchery marks dating to 2.6 million years old were found at the site of Gona, Ethiopia. This implies meat consumption by at least one of three species of hominins occurring around that time: A. africanus, A. garhi, and/or P. aethiopicus.[55] In 2010, fossils of butchered animal bones dated 3.4 million years old were found in Ethiopia, close to regions where australopith fossils were found.[56] However, a 2025 study measuring nitrogen isotope ratios in fossilized teeth determined that Australopithecus was almost entirely vegetarian.[57][58]
Robust australopithecines (Paranthropus) had larger cheek teeth than gracile australopiths, possibly because robust australopithecines had more tough, fibrous plant material in their diets, whereas gracile australopiths ate more hard and brittle foods.[50] However, such divergence in chewing adaptations may instead have been a response to fallback food availability. In leaner times, robust and gracile australopithecines may have turned to different low-quality foods (fibrous plants for the former, and hard food for the latter), but in more bountiful times, they had more variable and overlapping diets.[59][60] In a 1979 preliminary microwear study of Australopithecus fossil teeth, anthropologist Alan Walker theorized that robust australopiths ate predominantly fruit (frugivory).[61]
A study in 2018 found non-carious cervical lesions, caused by acid erosion, on the teeth of A. africanus, probably caused by consumption of acidic fruit.[62]
It is debated if the Australopithecus hand was anatomically capable of producing stone tools.[63] A. garhi was associated with large mammal bones bearing evidence of processing by stone tools, which may indicate australopithecine tool production.[64][65][66][67] Stone tools dating to roughly the same time as A. garhi (about 2.6 mya) were later discovered at the nearby Gona and Ledi-Geraru sites, but the appearance of Homo at Ledi-Geraru (LD 350-1) casts doubt on australopithecine authorship.[68]
In 2010, cut marks dating to 3.4 mya on a bovid leg were found at the Dikaka site, which were at first attributed to butchery by A. afarensis,[69] but because the fossil came from a sandstone unit (and were modified by abrasive sand and gravel particles during the fossilisation process), the attribution to butchery is dubious.[70]
In 2015, the Lomekwi culture was discovered at Lake Turkana dating to 3.3 mya, possibly attributable to Kenyanthropus[71] or A. deyiremeda.[72]

The Neolithic or New Stone Age (from Greek νέος néos 'new' and λίθος líthos 'stone') is an archaeological period, the final division of the Stone Age in Europe, Asia, Mesopotamia and Africa (c. 10,000 BC to c. 2,000 BC). It saw the Neolithic Revolution, a wide-ranging set of developments that appear to have arisen independently in several parts of the world. This "Neolithic package" included the introduction of farming, domestication of animals, and change from a hunter-gatherer lifestyle to one of settlement. The term 'Neolithic' was coined by Sir John Lubbock in 1865 as a refinement of the three-age system.[1]
The Neolithic began about 12,000 years ago, when farming appeared in the Epipalaeolithic Near East and Mesopotamia, and later in other parts of the world. It lasted in the Near East until the transitional period of the Chalcolithic (Copper Age) from about 6,500 years ago (4500 BC), marked by the development of metallurgy, leading up to the Bronze Age and Iron Age.
In other places, the Neolithic followed the Mesolithic (Middle Stone Age) and then lasted until later. In Ancient Egypt, the Neolithic lasted until the Protodynastic period, c. 3150 BC.[2][3][4] In China, it lasted until circa 2000 BC with the rise of the pre-Shang Erlitou culture,[5] as it did in Scandinavia.[6][7][8]
Following the ASPRO chronology, the Neolithic started in around 10,200 BC in the Levant, arising from the Natufian culture, when pioneering use of wild cereals evolved into early farming. The Natufian period or "proto-Neolithic" lasted from 12,500 to 9,500 BC, and is taken to overlap with the Pre-Pottery Neolithic A (PPNA) of 10,200–8800 BC. As the Natufians had become dependent on wild cereals in their diet, and a sedentary way of life had begun among them, the climatic changes associated with the Younger Dryas (about 10,000 BC) are thought to have forced people to develop farming.
The founder crops of the Fertile Crescent were wheat, lentil, pea, chickpeas, bitter vetch, and flax. Among the other major crop domesticated were rice, millet, maize (corn), and potatoes. Crops were usually domesticated in a single location and ancestral wild species are still found.[1]
Early Neolithic age farming was limited to a narrow range of plants, both wild and domesticated, which included einkorn wheat, millet and spelt, and the keeping of dogs. By about 8000 BC, it included domesticated sheep and goats, cattle and pigs.
Not all of these cultural elements characteristic of the Neolithic appeared everywhere in the same order: the earliest farming societies in the Near East did not use pottery. In other parts of the world, such as Africa, South Asia and Southeast Asia, independent domestication events led to their own regionally distinctive Neolithic cultures, which arose completely independently of those in Europe and Southwest Asia. Early Japanese societies and other East Asian cultures used pottery before developing agriculture.[10][11]
In the Middle East, cultures identified as Neolithic began appearing in the 10th millennium BC.[12] Early development occurred in the Levant (e.g. Pre-Pottery Neolithic A and Pre-Pottery Neolithic B) and from there spread eastwards and westwards. Neolithic cultures are also attested in southeastern Anatolia and northern Mesopotamia by around 8000 BC.[citation needed]
Anatolian Neolithic farmers derived a significant portion of their ancestry from the Anatolian hunter-gatherers (AHG), suggesting that agriculture was adopted in site by these hunter-gatherers and not spread by demic diffusion into the region.[13]
The Neolithic 1 (PPNA) period began around 10,000 BC in the Levant.[12] A temple area in southeastern Turkey at Göbekli Tepe, dated to around 9500 BC, may be regarded as the beginning of the period. This site was developed by nomadic hunter-gatherer tribes, as evidenced by the lack of permanent housing in the vicinity, and may be the oldest known human-made place of worship.[17] At least seven stone circles, covering 25 acres (10 ha), contain limestone pillars carved with animals, insects, and birds. Stone tools were used by perhaps as many as hundreds of people to create the pillars, which might have supported roofs. Other early PPNA sites dating to around 9500–9000 BC have been found in Palestine, notably in Tell es-Sultan (ancient Jericho) and Gilgal in the Jordan Valley; Israel (notably Ain Mallaha, Nahal Oren, and Kfar HaHoresh); and in Byblos, Lebanon. The start of Neolithic 1 overlaps the Tahunian and Heavy Neolithic periods to some degree.[citation needed]
The major advance of Neolithic 1 was true farming. In the proto-Neolithic Natufian cultures, wild cereals were harvested, and perhaps early seed selection and re-seeding occurred. The grain was ground into flour. Emmer wheat was domesticated, and animals were herded and domesticated (animal husbandry and selective breeding).[citation needed]
In 2006, remains of figs were discovered in a house in Jericho dated to 9400 BC. The figs are of a mutant variety that cannot be pollinated by insects, and therefore the trees can only reproduce from cuttings. This evidence suggests that figs were the first cultivated crop and mark the invention of the technology of farming. This occurred centuries before the first cultivation of grains.[18]
Settlements became more permanent, with circular houses, much like those of the Natufians, with single rooms. However, these houses were for the first time made of mudbrick. The settlement had a surrounding stone wall and perhaps a stone tower (as in Jericho). The wall served as protection from nearby groups, as protection from floods, or to keep animals penned. Some of the enclosures also suggest grain and meat storage.[19]
The Neolithic 2 (PPNB) began around 8800 BC according to the ASPRO chronology in the Levant (Jericho, West Bank).[12] As with the PPNA dates, there are two versions from the same laboratories noted above. This system of terminology, however, is not convenient for southeast Anatolia and settlements of the middle Anatolia basin.[citation needed] A settlement of 3,000 inhabitants called 'Ain Ghazal was found in the outskirts of Amman, Jordan. Considered to be one of the largest prehistoric settlements in the Near East, it was continuously inhabited from approximately 7250 BC to approximately 5000 BC.[20]
Settlements have rectangular mud-brick houses where the family lived together in single or multiple rooms. Burial findings suggest an ancestor cult where people preserved skulls of the dead, which were plastered with mud to make facial features. The rest of the corpse could have been left outside the settlement to decay until only the bones were left, then the bones were buried inside the settlement underneath the floor or between houses.[citation needed]
Work at the site of 'Ain Ghazal in Jordan has indicated a later Pre-Pottery Neolithic C period. Juris Zarins has proposed that a Circum Arabian Nomadic Pastoral Complex developed in the period from the climatic crisis of 6200 BC, partly as a result of an increasing emphasis in PPNB cultures upon domesticated animals, and a fusion with Harifian hunter gatherers in the Southern Levant, with affiliate connections with the cultures of Fayyum and the Eastern Desert of Egypt. Cultures practicing this lifestyle spread down the Red Sea shoreline and moved east from Syria into southern Iraq.[21]
The Late Neolithic began around 6,400 BC in the Fertile Crescent.[12] By then distinctive cultures emerged, with pottery like the Halafian (Turkey, Syria, Northern Mesopotamia) and Ubaid (Southern Mesopotamia). This period has been further divided into PNA (Pottery Neolithic A) and PNB (Pottery Neolithic B) at some sites.[22]
The Chalcolithic (Stone-Bronze) period began about 4500 BC, then the Bronze Age began about 3500 BC, replacing the Neolithic cultures.[citation needed]
Around 10,000 BC the first fully developed Neolithic cultures belonging to the phase Pre-Pottery Neolithic A (PPNA) appeared in the Fertile Crescent.[12] Around 10,700–9400 BC a settlement was established in Tell Qaramel, 10 miles (16 km) north of Aleppo. The settlement included two temples dating to 9650 BC.[23] Around 9000 BC during the PPNA, one of the world's first towns, Jericho, appeared in the Levant. It was surrounded by a stone wall, may have contained a population of up to 2,000–3,000 people, and contained a massive stone tower.[24] Around 6400 BC the Halaf culture appeared in Syria and Northern Mesopotamia.
In 1981, a team of researchers from the Maison de l'Orient et de la Méditerranée, including Jacques Cauvin and Oliver Aurenche, divided Near East Neolithic chronology into ten periods (0 to 9) based on social, economic and cultural characteristics.[25] In 2002, Danielle Stordeur and Frédéric Abbès advanced this system with a division into five periods.
They also advanced the idea of a transitional stage between the PPNA and PPNB between 8800 and 8600 BC at sites like Jerf el Ahmar and Tell Aswad.[27]
Alluvial plains (Sumer/Elam). Low rainfall makes irrigation systems necessary. Ubaid culture originated from 6200 BC.[28]
The earliest evidence of Neolithic culture in northeast Africa was found in the archaeological sites of Bir Kiseiba and Nabta Playa in what is now southwest Egypt.[29] Domestication of sheep and goats reached Egypt from the Near East possibly as early as 6000 BC.[30][31][32] Graeme Barker states "The first indisputable evidence for domestic plants and animals in the Nile valley is not until the early fifth millennium BC in northern Egypt and a thousand years later further south, in both cases as part of strategies that still relied heavily on fishing, hunting, and the gathering of wild plants" and suggests that these subsistence changes were not due to farmers migrating from the Near East but was an indigenous development, with cereals either indigenous or obtained through exchange.[33] Other scholars argue that the primary stimulus for agriculture and domesticated animals (as well as mud-brick architecture and other Neolithic cultural features) in Egypt was from the Middle East.[34][35][36]
The neolithization of Northwestern Africa was initiated by Iberian, Levantine (and perhaps Sicilian) migrants around 5500–5300 BC.[37] During the Early Neolithic period, farming was introduced by Europeans and was subsequently adopted by the locals.[37] During the Middle Neolithic period, an influx of ancestry from the Levant appeared in Northwestern Africa, coinciding with the arrival of pastoralism in the region.[37] The earliest evidence for pottery, domestic cereals and animal husbandry is found in Morocco, specifically at Kaf el-Ghar.[37]
The Pastoral Neolithic was a period in Africa's prehistory marking the beginning of food production on the continent following the Later Stone Age. In contrast to the Neolithic in other parts of the world, which saw the development of farming societies, the first form of African food production was mobile pastoralism,[38][39] or ways of life centered on the herding and management of livestock. The term "Pastoral Neolithic" is used most often by archaeologists to describe early pastoralist periods in the Sahara,[40] as well as in eastern Africa.[41]
The Savanna Pastoral Neolithic or SPN (formerly known as the Stone Bowl Culture) is a collection of ancient societies that appeared in the Rift Valley of East Africa and surrounding areas during a time period known as the Pastoral Neolithic. They were South Cushitic speaking pastoralists, who tended to bury their dead in cairns whilst their toolkit was characterized by stone bowls, pestles, grindstones and earthenware pots.[42] Through archaeology, historical linguistics and archaeogenetics, they conventionally have been identified with the area's first Afroasiatic-speaking settlers. Archaeological dating of livestock bones and burial cairns has also established the cultural complex as the earliest center of pastoralism and stone construction in the region.[43]
In southeast Europe agrarian societies first appeared in the 7th millennium BC, attested by one of the earliest farming sites of Europe, discovered in Vashtëmi, southeastern Albania and dating back to 6500 BC.[44][45] In most of Western Europe in followed over the next two thousand years, but in some parts of Northwest Europe it is much later, lasting just under 3,000 years from c. 4500 BC–1700 BC. Recent advances in archaeogenetics have confirmed that the spread of agriculture from the Middle East to Europe was strongly correlated with the migration of early farmers from Anatolia about 9,000 years ago, and was not just a cultural exchange.[46][47]
Anthropomorphic figurines have been found in the Balkans from 6000 BC,[48] and in Central Europe by around 5800 BC (La Hoguette). Among the earliest cultural complexes of this area are the Sesklo culture in Thessaly, which later expanded in the Balkans giving rise to Starčevo-Körös (Cris), Linearbandkeramik, and Vinča. Through a combination of cultural diffusion and migration of peoples, the Neolithic traditions spread west and northwards to reach northwestern Europe by around 4500 BC. The Vinča culture may have created the earliest system of writing, the Vinča signs, though archaeologist Shan Winn believes they most likely represented pictograms and ideograms rather than a truly developed form of writing.[49]
The Cucuteni-Trypillian culture built enormous settlements in Romania, Moldova and Ukraine from 5300 to 2300 BC. The megalithic temple complexes of Ġgantija on the Mediterranean island of Gozo (in the Maltese archipelago) and of Mnajdra (Malta) are notable for their gigantic Neolithic structures, the oldest of which date back to around 3600 BC. The Hypogeum of Ħal-Saflieni, Paola, Malta, is a subterranean structure excavated around 2500 BC; originally a sanctuary, it became a necropolis, the only prehistoric underground temple in the world, and shows a degree of artistry in stone sculpture unique in prehistory to the Maltese islands. After 2500 BC, these islands were depopulated for several decades until the arrival of a new influx of Bronze Age immigrants, a culture that cremated its dead and introduced smaller megalithic structures called dolmens to Malta.[50] In most cases there are small chambers here, with the cover made of a large slab placed on upright stones. They are claimed to belong to a population different from that which built the previous megalithic temples. It is presumed the population arrived from Sicily because of the similarity of Maltese dolmens to some small constructions found there.[51]
With some exceptions, population levels rose rapidly at the beginning of the Neolithic until they reached the carrying capacity.[52] This was followed by a population crash of "enormous magnitude" after 5000 BC, with levels remaining low during the next 1,500 years.[52] Populations began to rise after 3500 BC, with further dips and rises occurring between 3000 and 2500 BC but varying in date between regions.[52] Around this time is the Neolithic decline, when populations collapsed across most of Europe, possibly caused by climatic conditions, plague, or mass migration.[53]
Settled life, encompassing the transition from foraging to farming and pastoralism, began in South Asia in the region of Balochistan, Pakistan, around 7,000 BC.[54][55][56] At the site of Mehrgarh, Balochistan, presence can be documented of the domestication of wheat and barley, rapidly followed by that of goats, sheep, and cattle.[57] In April 2006, it was announced in the scientific journal Nature that the oldest (and first Early Neolithic) evidence for the drilling of teeth in vivo (using bow drills and flint tips) was found in Mehrgarh.[58]
In South India, the Neolithic began by 6500 BC and lasted until around 1400 BC when the Megalithic transition period began. South Indian Neolithic is characterized by Ash mounds[clarification needed] from 2500 BC in Karnataka region, expanded later to Tamil Nadu.[59]
In East Asia, the earliest sites include the Nanzhuangtou culture around 9500–9000 BC,[60] Pengtoushan culture around 7500–6100 BC, and Peiligang culture around 7000–5000 BC. The prehistoric Beifudi site near Yixian in Hebei Province, China, contains relics of a culture contemporaneous with the Cishan and Xinglongwa cultures of about 6000–5000 BC, Neolithic cultures east of the Taihang Mountains, filling in an archaeological gap between the two Northern Chinese cultures. The total excavated area is more than 1,200 square yards (1,000 m2; 0.10 ha), and the collection of Neolithic findings at the site encompasses two phases.[61] Between 3000 and 1900 BC, the Longshan culture existed in the middle and lower Yellow River valley areas of northern China. Towards the end of the 3rd millennium BC, the population decreased sharply in most of the region and many of the larger centres were abandoned, possibly due to environmental change linked to the end of the Holocene Climatic Optimum.[62]
The 'Neolithic' (defined in this paragraph as using polished stone implements) remains a living tradition in small and extremely remote and inaccessible pockets of West Papua. Polished stone adze and axes are used in the present day (as of 2008[update]) in areas where the availability of metal implements is limited. This is likely to cease altogether in the next few years as the older generation die off and steel blades and chainsaws prevail.[citation needed]
In 2012, news was released about a new farming site discovered in Munam-ri, Goseong, Gangwon Province, South Korea, which may be the earliest farmland known to date in east Asia.[63] "No remains of an agricultural field from the Neolithic period have been found in any East Asian country before, the institute said, adding that the discovery reveals that the history of agricultural cultivation at least began during the period on the Korean Peninsula". The farm was dated between 3600 and 3000 BC. Pottery, stone projectile points, and possible houses were also found. "In 2002, researchers discovered prehistoric earthenware, jade earrings, among other items in the area". The research team will perform accelerator mass spectrometry (AMS) dating to retrieve a more precise date for the site.[64]
In Mesoamerica, a similar set of events (i.e., crop domestication and sedentary lifestyles) occurred by around 4500 BC in South America, but possibly as early as 11,000–10,000 BC. These cultures are usually not referred to as belonging to the Neolithic; in North America, different terms are used such as Formative stage instead of mid-late Neolithic, Archaic Era instead of Early Neolithic, and Paleo-Indian for the preceding period.[65]
The Formative stage is equivalent to the Neolithic Revolution period in Europe, Asia, and Africa. In the southwestern United States it occurred from 500 to 1200 AD when there was a dramatic increase in population and development of large villages supported by agriculture based on dryland farming of corn (maize), and later, beans, squash, and domesticated turkeys. During this period the bow and arrow and ceramic pottery were also introduced.[66] In later periods cities of considerable size developed, and some metallurgy by 700 BC.[67]
Australia, in contrast to New Guinea, has generally been held not to have had a Neolithic period, with a hunter-gatherer lifestyle continuing until the arrival of Europeans. This view can be challenged in terms of the definition of agriculture, but "Neolithic" remains a rarely used and not very useful concept in discussing Australian prehistory.[68]
During most of the Neolithic age of Eurasia, people lived in small tribes composed of multiple bands or lineages.[69] There is little scientific evidence of developed social stratification in most Neolithic societies; social stratification is more associated with the later Bronze Age.[70] Although some late Eurasian Neolithic societies formed complex stratified chiefdoms or even states, generally states evolved in Eurasia only with the rise of metallurgy, and most Neolithic societies on the whole were relatively simple and egalitarian.[69] Beyond Eurasia, however, states were formed during the local Neolithic in three areas, namely in the Preceramic Andes with the Caral-Supe Civilization,[71][72] Formative Mesoamerica and Ancient Hawaiʻi.[73] However, most Neolithic societies were noticeably more hierarchical than the Upper Paleolithic cultures that preceded them and hunter-gatherer cultures in general.[74][75]
The domestication of large animals (c. 8000 BC) resulted in a dramatic increase in social inequality in most of the areas where it occurred; New Guinea being a notable exception.[76] Possession of livestock allowed competition between households and resulted in inherited inequalities of wealth. Neolithic pastoralists who controlled large herds gradually acquired more livestock, and this made economic inequalities more pronounced.[77] However, evidence of social inequality is still disputed, as settlements such as Çatalhöyük reveal a lack of difference in the size of homes and burial sites, suggesting a more egalitarian society with no evidence of the concept of capital, although some homes do appear slightly larger or more elaborately decorated than others.[citation needed]
Families and households were still largely independent economically, and the household was probably the center of life.[78][79] However, excavations in Central Europe have revealed that early Neolithic Linear Ceramic cultures ("Linearbandkeramik") were building large arrangements of circular ditches between 4800 and 4600 BC. These structures (and their later counterparts such as causewayed enclosures, burial mounds, and henge) required considerable time and labour to construct, which suggests that some influential individuals were able to organise and direct human labour – though non-hierarchical and voluntary work remain possibilities.
There is a large body of evidence for fortified settlements at Linearbandkeramik sites along the Rhine, as at least some villages were fortified for some time with a palisade and an outer ditch.[80][81] Settlements with palisades and weapon-traumatized bones, such as those found at the Talheim Death Pit, have been discovered and demonstrate that "...systematic violence between groups" and warfare was probably much more common during the Neolithic than in the preceding Paleolithic period.[75] This supplanted an earlier view of the Linear Pottery Culture as living a "peaceful, unfortified lifestyle".[82] Violence increased toward the end of this culture which existed at 5500-4500 BCE.[83] In 2024, a study suggested a peaceful explanation to the reduction in the size of male population observed worldwide 5000-3000 years ago.[84]
Control of labour and inter-group conflict is characteristic of tribal groups with social rank that are headed by a charismatic individual – either a 'big man' or a proto-chief – functioning as a lineage-group head. Whether a non-hierarchical system of organization existed is debatable, and there is no evidence that explicitly suggests that Neolithic societies functioned under any dominating class or individual, as was the case in the chiefdoms of the European Early Bronze Age.[85] Possible exceptions to this include Iraq during the Ubaid period and England beginning in the Early Neolithic (4100–3000 BC).[86][87] Theories to explain the apparent implied egalitarianism of Neolithic (and Paleolithic) societies have arisen, notably the Marxist concept of primitive communism.[citation needed]
Phylogenies reconstructed from modern genetic data indicates an extreme drop in Y-chromosomal diversity occurred during the Neolithic, with effective population size for the mitochondria up to 17 times higher than for the Y-chromosomes during this period.[88] The causes of this bottleneck remain poorly understood. At a basic level, it can likely be attributed to a culture-induced change in the distribution of male reproductive success, with possible explanations ranging from an increased incidence of violence and male mortality during the Neolithic [89] to the rise of patrilineal segmentary groups with varying reproductive success due to polygyny.[90]
The shelter of early people changed dramatically from the Upper Paleolithic to the Neolithic era. In the Paleolithic, people did not normally live in permanent constructions. In the Neolithic, mud brick houses started appearing that were coated with plaster.[91] The growth of agriculture made permanent houses far more common. At Çatalhöyük 9,000 years ago, doorways were made on the roof, with ladders positioned both on the inside and outside of the houses.[91] Stilt-house settlements were common in the Alpine and Pianura Padana (Terramare) region.[92] Remains have been found in the Ljubljana Marsh in Slovenia and at the Mondsee and Attersee lakes in Upper Austria, for example.
A significant and far-reaching shift in human subsistence and lifestyle was to be brought about in areas where crop farming and cultivation were first developed: the previous reliance on an essentially nomadic hunter-gatherer subsistence technique or pastoral transhumance was at first supplemented, and then increasingly replaced by, a reliance upon the foods produced from cultivated lands. These developments are also believed to have greatly encouraged the growth of settlements, since it may be supposed that the increased need to spend more time and labor in tending crop fields required more localized dwellings. This trend would continue into the Bronze Age, eventually giving rise to permanently settled farming towns, and later cities and states whose larger populations could be sustained by the increased productivity from cultivated lands.
The profound differences in human interactions and subsistence methods associated with the onset of early agricultural practices in the Neolithic have been called the Neolithic Revolution, a term coined in the 1920s by the Australian archaeologist Vere Gordon Childe.
One potential benefit of the development and increasing sophistication of farming technology was the possibility of producing surplus crop yields, in other words, food supplies in excess of the immediate needs of the community. Surpluses could be stored for later use, or possibly traded for other necessities or luxuries. Agricultural life afforded securities that nomadic life could not, and sedentary farming populations grew faster than nomadic.
However, early farmers were also adversely affected in times of famine, such as may be caused by drought or pests. In instances where agriculture had become the predominant way of life, the sensitivity to these shortages could be particularly acute, affecting agrarian populations to an extent that otherwise may not have been routinely experienced by prior hunter-gatherer communities.[77] Nevertheless, agrarian communities generally proved successful, and their growth and the expansion of territory under cultivation continued.
Another significant change undergone by many of these newly agrarian communities was one of diet. Pre-agrarian diets varied by region, season, available local plant and animal resources and degree of pastoralism and hunting. Post-agrarian diet was restricted to a limited package of successfully cultivated cereal grains, plants and to a variable extent domesticated animals and animal products. Supplementation of diet by hunting and gathering was to variable degrees precluded by the increase in population above the carrying capacity of the land and a high sedentary local population concentration. In some cultures, there would have been a significant shift toward increased starch and plant protein. The relative nutritional benefits and drawbacks of these dietary changes and their overall impact on early societal development are still debated.
In addition, increased population density, decreased population mobility, increased continuous proximity to domesticated animals, and continuous occupation of comparatively population-dense sites would have altered sanitation needs and patterns of disease.
The identifying characteristic of Neolithic technology is the use of polished or ground stone tools, in contrast to the flaked stone tools used during the Paleolithic era.
Neolithic people were skilled farmers, manufacturing a range of tools necessary for the tending, harvesting and processing of crops (such as sickle blades and grinding stones) and food production (e.g. pottery, bone implements). They were also skilled manufacturers of a range of other types of stone tools and ornaments, including projectile points, beads, and statuettes. But what allowed forest clearance on a large scale was the polished stone axe above all other tools. Together with the adze, fashioning wood for shelter, structures and canoes for example, this enabled them to exploit the newly developed farmland.
Neolithic peoples in the Levant, Anatolia, Syria, northern Mesopotamia and Central Asia were also accomplished builders, utilizing mud-brick to construct houses and villages. At Çatalhöyük, houses were plastered and painted with elaborate scenes of humans and animals. In Europe, long houses built from wattle and daub were constructed. Elaborate tombs were built for the dead. These tombs are particularly numerous in Ireland, where there are many thousand still in existence. Neolithic people in the British Isles built long barrows and chamber tombs for their dead and causewayed camps, henges, flint mines and cursus monuments. It was also important to figure out ways of preserving food for future months, such as fashioning relatively airtight containers, and using substances like salt as preservatives.
The peoples of the Americas and the Pacific mostly retained the Neolithic level of tool technology until the time of European contact. Exceptions include copper hatchets and spearheads in the Great Lakes region.
Most clothing appears to have been made of animal skins, as indicated by finds of large numbers of bone and antler pins that are ideal for fastening leather. Wool cloth and linen might have become available during the later Neolithic,[93][94] as suggested by finds of perforated stones that (depending on size) may have served as spindle whorls or loom weights.[95][96][97]
Paleolithic
Epipalaeolithic
Mesolithic
Neolithic
Neolithic human settlements include:
The world's oldest known engineered roadway, the Post Track in England, dates from 3838 BC and the world's oldest freestanding structure is the Neolithic temple of Ġgantija in Gozo, Malta.
Note: Dates are very approximate, and are only given for a rough estimate; consult each culture for specific time periods.
Early Neolithic 
Periodization: The Levant: 9500–8000 BC; Europe: 7000–4000 BC; Elsewhere: varies greatly, depending on region.
Middle Neolithic
Periodization: The Levant: 8000–6500 BC; Europe: 5500–3500 BC; Elsewhere: varies greatly, depending on region.
Later Neolithic 
Periodization[broken anchor]: 6500–4500 BC; Europe: 5000–3000 BC; Elsewhere: varies greatly, depending on region.
Periodization: Near East: 6000–3500 BC; Europe: 5000–2000 BC; Elsewhere: varies greatly, depending on region. In the Americas, the Chalcolithic ended as late as the 19th century AD for some peoples.

Bryan Clifford Sykes (9 September 1947 – 10 December 2020) was a British geneticist and science writer who was a Fellow of Wolfson College and Emeritus Professor of human genetics at the University of Oxford.[1][2]
Sykes published the first report on retrieving DNA from ancient bone (Nature, 1989). He was involved in a number of high-profile cases dealing with ancient DNA, including that of Ötzi the Iceman. He also suggested a Florida accountant by the name of Tom Robinson was a direct descendant of Genghis Khan, a claim that was subsequently disproved.[3][4][5][6]
Sykes is best known outside the community of geneticists for his two popular books on the investigation of human history and prehistory through studies of mitochondrial DNA.
Sykes was educated at Eltham College, received his BSc from the University of Liverpool, his PhD from the University of Bristol, and his DSc from the University of Oxford.[2]
In 2001 (Banta Press Hardback) Sykes published a book for the popular audience, The Seven Daughters of Eve, in which he explained how the dynamics of maternal mitochondrial DNA (mtDNA) inheritance leave their mark on the human population in the form of genetic clans sharing common maternal descent.  He notes that the majority of Europeans can be classified in seven such clans, known scientifically as haplogroups, distinguishable by differences in their mtDNA that are unique to each group, with each clan descending from a separate prehistoric female-line ancestor.  He referred to these seven 'clan mothers' as 'daughters of Eve', a reference to the mitochondrial Eve to whom the mtDNA of all modern humans traces.  Based on the geographical and ethnological distribution of the modern descendants of each clan he assigned provisional homelands for the seven clan mothers, and used the degree to which each clan diverges to approximate the time period when the clan mother would have lived.  He then uses these deductions to give 'biographies' for each of the clan mothers, assigning them arbitrary names based on the scientific designation of their haplogroup (for example, using the name Xenia for the founder of haplogroup X).
In his 2006 book Blood of the Isles (published in the United States and Canada as Saxons, Vikings and Celts: The Genetic Roots of Britain and Ireland), Sykes examines British genetic "clans". He presents evidence from mitochondrial DNA, inherited by both sexes from their mothers, and the Y chromosome, inherited by men from their fathers, and makes the following claims:
Sykes used a similar approach to that used in The Seven Daughters of Eve to identify the nine "clan mothers" of Japanese ancestry, "all different from the seven European equivalents."[7]
With the advent of whole-genome sequencing and analysis of ancient DNA, many of Sykes' theories regarding the origins of the British have been largely invalidated. A 2018 study argues that over 90% of the DNA of the Neolithic population of Britain was overturned by a North European Bell Beaker population, originating from the Pontic Steppes, as part of an ongoing migration process that brought large amounts of Steppe DNA (including the R1b haplogroup) to North and West Europe.[8] Modern autosomal genetic clustering is testament to this fact, as both modern and Iron Age British and Irish samples cluster genetically very closely with other North European populations, rather than Iberians, Galicians, Basques or those from the south of France.[9][10] Similar studies have concluded that the Anglo-Saxons, while not replacing the previous populations outright, may have contributed more to the gene pool in much of England than Sykes had claimed.[11][12][13]
Sykes and his team at Oxford University carried out DNA analysis of presumed Yeti samples and thinks the samples may have come from a hybrid species of bear produced from a mating between a brown bear and a polar bear. Sykes told BBC News:
I think this bear, which nobody has seen alive, may still be there and may have quite a lot of polar bear in it. It may be some sort of hybrid and if its behaviour is different from normal bears, which is what eyewitnesses report, then I think that may well be the source of the mystery and the source of the legend.
He conducted another similar survey in 2014, this time examining samples attributed not just to yeti but also to Bigfoot and other "anomalous primates." The study concluded that two of the 30 samples tested most closely resembled the genome of a palaeolithic polar bear, and that the other 28 were from living mammals.[16]
The samples were subsequently re-analysed by Ceiridwen Edwards and Ross Barnett. They concluded that the mutation that had led to the match with a polar bear was a damaged artefact, and suggested that the two hair samples were in fact from Himalayan brown bears (U. arctos isabellinus). These bears are known in Nepal as Dzu-the (a Nepalese term meaning cattle-bear), and have been associated with the myth of the yeti.[17][18] Sykes and Melton acknowledged that their GenBank search was in error  but suggested that the hairs were instead a match to a modern polar bear specimen "from the Diomede Islands in the Bering Sea reported in the same paper". They maintained that they did not see any sign of damage in their sequences and commented that they had "no reason to doubt the accuracy of these two sequences any more than the other 28 presented in the paper".[19] Multiple further analyses, including replication of the single analysis conducted by Sykes and his team, were carried out in a study conducted by Eliécer E. Gutiérrez, a researcher at the Smithsonian Institution and Ronald H. Pine, affiliated at the University of Kansas. All of these analyses found that the relevant genetic variation in brown bears makes it impossible to assign, with certainty, the Himalayan samples to either that species or to the polar bear.  Because brown bears occur in the Himalayas, Gutiérrez and Pine stated that there is no reason to believe that the samples in question came from anything other than ordinary Himalayan brown bears.[20]
Sykes married Sue Foden, whom he met as a student in Oxford. They were married from 1978 to 1984, but remained close, and their son Richard was born in 1991. Sykes was a keen croquet player, representing Ireland in the 1984 Home Internationals.[21] Sykes died in December 2020. [22]

Most information about Taiwan before the arrival of the Dutch East India Company in 1624 comes from archaeological finds throughout the island.  The earliest evidence of human habitation dates back 20,000 to 30,000 years, when lower sea levels exposed the Taiwan Strait as a land bridge. Around 5,000 years ago, farmers from what is now the southeast coast of China settled on the island. These people are believed to have been speakers of Austronesian languages, which dispersed from Taiwan across the islands of the Pacific and Indian Oceans.  The current Taiwanese aborigines are believed to be their descendants.
The island of Taiwan was formed approximately 4 to 5 million years ago on a complex convergent boundary between the continental Eurasian Plate and the oceanic Philippine Sea Plate.  The boundary continues southwards in the Luzon Volcanic Arc, a chain of islands between Taiwan and the Philippine island of Luzon including Green Island and Orchid Island.  From the northern part of the island the eastward continuation of the boundary is marked by the Ryukyu chain of volcanic islands.[1][2]
The island is separated from the coast of Fujian to the west by the Taiwan Strait, which is 130 km (81 mi) wide at its narrowest point.  The most significant islands in the Strait are the Penghu islands 45 km (28 mi) from the southwest coast of Taiwan and 140 km (87 mi) from the Chinese coast.  Part of the continental shelf, the Strait is no more than 100 m (330 ft) deep, and has become a land bridge during glacial periods.[3]
Taiwan is a tilted fault block, with rugged longitudinal mountain ranges making up most of the eastern two-thirds of the island.  They include more than two hundred peaks with elevations of over 3,000 m (9,800 ft).  The western side of the island slopes down to fertile coastal plains.  The island straddles the Tropic of Cancer, and has a humid subtropical climate.[4]
The original vegetation ranged from tropical rainforest in the lowlands through temperate forests, boreal forest and alpine plants with increasing altitude.[5]
During the Late Pleistocene glaciation, sea levels in the area were about 140 m (460 ft) lower than in the present day.  As a result, the floor of the Taiwan Strait was exposed as a broad land bridge that was crossed by mainland fauna until the beginning of the Holocene 10,000 years ago.[3]
A concentration of vertebrate fossils has been found in the channel between the Penghu Islands and Taiwan, including a partial jawbone designated Penghu 1, apparently belonging to a previously unknown species of genus Homo.
These fossils are likely to date from one of the two most recent periods when the Strait was exposed, 10–70 kya and 130–190 kya.[6]
The Ryukyu Islands to the northeast of Taiwan were settled during marine isotope stage (MIS) 3, which ended around 30,000 years ago. It is likely that the southern (and possibly central) Ryukyus were settled via voyages from Taiwan.[7]
In 1972, fragmentary fossils of anatomically modern humans were found at Chouqu and Gangzilin, in Zuojhen District, Tainan, in fossil beds exposed by erosion of the Cailiao River.  Though some of the fragments are believed to be more recent, three cranial fragments and a molar tooth have been dated as between 20,000 and 30,000 years old.  The find has been dubbed "Zuozhen Man".  No associated artifacts have been found at the site.[8][9]
The oldest known artifacts are chipped-pebble tools of the Changbin culture (長濱文化), found at cave sites on the southeast coast of the island.  The sites are dated 15,000 to 5,000 years ago, and similar to contemporary sites in Fujian.  The primary site of Baxiandong (八仙洞), in Changbin, Taitung was first excavated in 1968.  The same culture has been found at sites at Eluanbi on the southern tip of Taiwan, persisting until 5,000 years ago.  The earliest layers feature large stone tools, and suggest a hunting and gathering lifestyle.  Later layers have small stone tools of quartz, as well as tools made from bone, horn and shell, and suggest a shift to intensive fishing and shellfish collection.[10][11]
The distinct Wangxing culture (網形) was discovered in Miaoli County in northwest Taiwan in the 1980s.  The assemblage consists of flake tools, becoming smaller and more standardized over time, and indicating a shift from gathering to hunting.[12]
Analysis of spores and pollen grains in sediment of Sun Moon Lake suggests that traces of slash-and-burn agriculture started in the area since 11,000 years ago, and ended 4,200 years ago, when abundant remains of rice cultivation were found.[13]
The only Paleolithic burial that has been found on Taiwan was in Xiaoma cave in Chenggong in the southeast of the island, dating from about 4000 BC, of a male similar in type to Negritos found in the Philippines.  There are also references in Chinese texts and Formosan Aboriginal oral traditions to pygmies on the island at some time in the past.[14][15]
In December 2011, a skeleton dated about 8,000 years ago was found on Liang Island, off the north coast of Fujian. In 2014, the mitochondrial DNA of the Liangdao Man skeleton was found to belong to Haplogroup E, which is today found throughout Maritime Southeast Asia. Moreover, it had two of the four mutations characteristic of the E1 subgroup.
From this, Ko et al. infer that Haplogroup E arose 8,000 to 11,000 years ago on the north Fujian coast, travelled to Taiwan with Neolithic settlers 6,000 years ago, and from there spread to Maritime Southeast Asia with the Austronesian language dispersal.[16]
Soares et al. caution against overemphasizing a single sample, and maintain that a constant molecular clock implies an earlier date (and more southerly origin) for Haplogroup E remains more likely.[17]
Between 4000 and 3000 BC, the Dapenkeng culture (named after a site in Taipei county) abruptly appeared and quickly spread around the coast of the island, as well as Penghu.
Dapenkeng sites are relatively homogeneous, characterized by pottery impressed with cord marks, pecked pebbles, highly polished stone adzes and thin points of greenish slate.
The inhabitants cultivated rice and millet, and engaged in hunting, but were also heavily reliant on marine shells and fish.
Most scholars believe this culture is not derived from the Changbin culture, but was brought across the Strait by the ancestors of today's Taiwanese aborigines, speaking early Austronesian languages.
No ancestral culture on the mainland has been identified, but a number of shared features suggest ongoing contacts.[18][19] However, the overall neolithic-era of Taiwan strait is said, by scholars, to have been descended from Neolithic cultures in the lower Yangtze area, particularly the Hemudu and Majiabang cultures.[20] Physical similarity has been noted between the people of these cultures and the Neolithic inhabitants of Taiwan.[21]
In the following millennium, these technologies appeared on the northern coast of the Philippine island of Luzon (250 km south of Taiwan), where they, and presumably Austronesian languages, were adopted by the local population.
This migration created a branch of Austronesian, the Malayo-Polynesian languages, which have since dispersed across a huge area from Madagascar to Hawaii, Easter Island and New Zealand.
All other primary branches of Austronesian are found only on Taiwan, the urheimat of the family.[22][23][24]
The successors of the Dapenkeng culture throughout Taiwan were locally differentiated.
The Fengpitou (鳳鼻頭) culture, characterized by fine red cord-marked pottery, was found in Penghu and the central and southern parts of the western side of the island, and a culture with similar pottery occupied the eastern coastal areas.
These later differentiated into the Niumatou and Yingpu cultures in central Taiwan, the Niuchouzi (牛稠子) and Dahu cultures in the southwest, the Beinan Culture in the southeast and the Qilin (麒麟) culture in the central east.
The Yuanshan culture (圓山) in the northeast does not appear to be closely related to these, featuring sectioned adzes, shouldered-stone adzes and pottery without cord impressions.
Some scholars suggest that it represents another wave of immigration from southeast China, but no similar culture is known from there either.[25]
Archaeological evidence of prehistoric cultures dating back 4500 years before present was found in Nangang Village, Cimei, Penghu in 1983.[26]: 314
The Niuchouzi Culture flourished around what is now Tainan 2,500 BC to 1,000 BC. They are known for orange pottery decorated with rope patterns.[27]
In the early Neolithic period, jade was used only for tools such are adzes, axes and spear points.
From about 2500 BC, jade ornaments began to be produced, peaking in sophistication between 1500 BC and 1 AD, particularly in the Beinan Culture of southern Taiwan.
All the jade found on Taiwan came from a deposit of green nephrite at Fengtian, near modern Hualien City.
Nephrite from Taiwan began to appear in the northern Philippines between 1850 and 1350 BC, spawning the Philippine jade culture.
Around the beginning of the Common Era, artisans in Taiwan switched from jade to metal, glass and carnelian. 
However, Philippine craftsmen continued to work jade from Taiwan until around 1000 AD, producing lingling-o pendants and other ornaments, which have been found throughout southeast Asia.[28][29]
Artifacts of iron and other metals appeared on Taiwan around the beginning of the Common Era.
At first these were trade goods, but by around AD 400 wrought iron was being produced locally using bloomeries, a technology possibly introduced from the Philippines.
Distinct Iron Age cultures have been identified in different parts of the island: the Shihsanhang Culture (十三行文化) in the north, the Fanzaiyuan Culture (番仔園) in the northwest, the Daqiuyuan Culture (大邱園) in the hills of southwest Nantou County, the Kanding Culture in the central west, the Niaosung Culture in the southwest, the Guishan Culture (龜山) at the southern tip of the island, and the Jingpu Culture (靜浦) on the east coast.
The earliest trade goods from China found on the island date from the Tang dynasty (618–907 AD).[30][31]
Prehistoric groups in Taiwan practiced a wide variety of burial practices with each culture having distinct practices. Excavations of ancient gravesites are key to archeologists understanding of these early Taiwanese cultures. Grave goods buried with the dead also provide concrete evidence of complex trade linkages and intercultural exchange. Some of these ancient funerary customs are practiced by modern Taiwanese indigenous cultures but many have been lost.[32]

Timeline of Polish history
The prehistory and protohistory of Poland can be traced from the first appearance of Homo species on the territory of modern-day Poland, to the establishment of the Polish state in the 10th century AD, a span of roughly 500,000 years.
The area of present-day Poland went through the stages of socio-technical development known as the Stone, Bronze and Iron Ages after experiencing the climatic shifts of the glacial periods. The best known archeological discovery from the prehistoric period is the Lusatian-culture Biskupin fortified settlement. As ancient civilizations began to appear in southern and western Europe, the cultures of the area of present-day Poland were influenced by them to various degrees.
Among the peoples that inhabited various parts of Poland up to the Iron Age stage of development were Scythian, Celtic, Germanic, Sarmatian, Roman, Avar, Vlach and Baltic tribes. In the Early Middle Ages, the area came to be dominated by West Slavic tribes and finally became home to a number of Lechitic Polish tribes that formed small states in the region beginning in the 8th century.
As with other early periods areas of human history, knowledge of these times is limited, since few written ancient and medieval sources are available; research therefore relies primarily on archeology. Written language came to the Poles only after 966 AD, when the ruler of the Polish lands, Duke Mieszko I, converted to Christianity and educated foreign clerics arrived.[1]
Poland's Stone Age is divided into the Paleolithic, Mesolithic and Neolithic eras.
The Paleolithic era extended from c. 500,000 BC to 8,000 BC and is subdivided into four periods: the Lower Paleolithic, c. 500,000 to 350,000 BC; the Middle Paleolithic, c. 350,000 to 40,000 BC; the Upper Paleolithic, c. 40,000 to 10,000 BC; and the Final Paleolithic, c. 10,000 to 8000 BC.
The Mesolithic era lasted from c. 8000 to 5500 BC and the Neolithic from c. 5500 to 2300 BC.
The Neolithic is subdivided into the Neolithic proper, c. 5500 – 2900 BC, and the Copper Age, c. 2900 – 2300 BC.[2]
Poland's Stone Age lasted approximately 500,000 years and saw the appearance of three distinct Homo species: Homo erectus, Homo neanderthalensis and Homo sapiens (humans). The Stone Age cultures ranged from early human groups with primitive tools to advanced agricultural and stratified societies that used sophisticated stone tools, built fortified settlements and developed copper metallurgy.
As elsewhere in Central Europe, the Paleolithic, Mesolithic and Neolithic stages of Poland's Stone Age were each characterized by refinements in stone-tool-making techniques. Paleolithic human activities (whose earliest sites are 500,000 years old) were intermittent because of recurring glaciations. A general climate warming and a resulting increase in ecologic diversity were characteristic of the Mesolithic era (9000–8000 BC).
The Neolithic era ushered in the first settled agricultural communities, whose founders had migrated from the Danube River area beginning about 5500 BC. Later, the native post-Mesolithic populations would also adopt and further develop the agricultural way of life (between 4400 and about 2000 BC).[3]
Poland's Bronze Age comprised Period I, c. 2300–1600 BC; Period II, c. 1600–1350 BC; Period III, c. 1350–1100 BC; Period IV, c. 1100–900 BC; and Period V, c. 900–700 BC. The Early Iron Age included Hallstatt Period C, c. 700–600 BC, and Hallstatt Period D, c. 600–450 BC.[2]
Poland's Bronze- and Iron-Age cultures are known mainly from archeological research. Poland's Early Bronze Age cultures began around 2300-2400 BC,[4] whereas the Iron Age commenced c. 700-750 BC.[5] By the beginning of the Common Era, the Iron Age archeological cultures described in the main article no longer existed. Given the absence of written records, the ethnicities and linguistic affiliations of the groups living in Central and Eastern Europe at that time are speculative; there is considerable disagreement about their identities. In Poland, the Lusatian culture, which spanned the Bronze and Iron Ages, became particularly prominent. The most famous archeological discovery from that period is the Biskupin fortified settlement (gród) that represented early-Iron-Age Lusatian culture.[6]
Bronze objects were brought to Poland around 2300 BC from the Carpathian Basin. The native Early Bronze Age that followed was dominated by the innovative Unetice culture in western Poland and the conservative Mierzanowice culture in eastern Poland. These were replaced in their respective territories for the duration of the subsequent Older Bronze Period by the (pre-Lusatian) Tumulus culture and the Trzciniec culture.
Characteristic of the remaining bronze periods were the Urnfield cultures, in which skeletal burials were replaced by cremation throughout much of Europe. In Poland, the Lusatian culture settlements dominated the landscape for nearly a thousand years, continuing into the Early Iron Age. A series of Scythian invasions beginning in the 6th century BC, precipitated their demise. The Hallstatt Period D was a time of expansion for the Pomeranian culture, while the Western Baltic Kurgan culture dominated Poland's Masuria-Warmia region.[7][8]
The period of the La Tène culture is subdivided into La Tène A, c. 450–400 BC; La Tène B, c. 400–250 BC; La Tène C, c. 250–150 BC; and La Tène D, c. 150–0 BC. The period from 200 to 0 BC may also be considered a younger pre-Roman period. It was followed by a period of Roman influence whose early stage lasted from c. 0 to 150 AD and its later stage from c. 150 to 375 AD. The period from 375 to 500 AD constitutes the (pre-Slavic) Migration Period.[2]
Peoples belonging to numerous archeological cultures identified with Celtic, Germanic, Baltic, and in some regions Slavic[9][10][11] tribes inhabited parts of Poland during the era of classical antiquity, from about 400 BC to 450-500 AD. Other groups, difficult to identify, were most likely also present, as the ethnic composition of archeological cultures is often poorly recognized. Short of using a written language to any appreciable degree, many of them developed a relatively advanced material culture and social organization, as evidenced by the archeological record, for example by richly furnished, dynastic "princely" graves. Characteristic of this period were high rates of migration, often involving large groups of people.[12]
Celtic peoples established settlements beginning in the early 4th century BC, mostly in southern Poland, the outer limit of their expansion, as representatives of the La Tène culture. With their developed economy and crafts, they exerted a lasting cultural influence disproportionate to their small numbers in the region.[13]
Germanic peoples lived in what is now Poland for several centuries, during which many of their tribes also migrated southward and eastward (see Wielbark culture). With the expansion of the Roman Empire, the Germanic tribes came under Roman cultural influence. Some written remarks by Roman authors that are relevant to developments on Polish lands have been preserved; they provide additional insights in conjunction with the archeological record. In the end, as the Roman Empire was nearing its collapse and the nomadic peoples invading from the east destroyed, damaged or destabilized the various Germanic cultures and societies, the Germanic peoples left Eastern and Central Europe for the safer and wealthier southern and western parts of the European continent.[14] According to Tacitus and Ptolemy, the Goths left the lower Vistula region in the mid-2nd century AD.[15]
The northeast corner of what is now Poland remained populated by Baltic tribes. They were at the outer limits of any substantial cultural influence from the Roman Empire.[16]
Slavic peoples may have lived in the southern and southeastern regions, some perhaps associated with the ancient Przeworsk and Zarubintsy cultures of the 3rd century BC[17][18] (with the Przeworsk culture being considered likely of Slavic or of mixed Slavic and Germanic origin[19][20][21]). It has been suggested that the early Slavic peoples and languages may have originated in the region of Polesia, which includes the area around the Belarus–Ukraine border, parts of Western Russia, and parts of far Eastern Poland.[11] More of Poland would be settled by Slavic tribes in later periods, in the early centuries of the common era.

The history of writing traces the development of writing systems[1] and how their use transformed and was transformed by different societies. The use of writing prefigures various social and psychological consequences associated with literacy and literary culture.
Each historical invention of writing emerged from systems of proto-writing that used ideographic and mnemonic symbols but were not capable of fully recording spoken language. True writing, where the content of linguistic utterances can be accurately reconstructed by later readers, is a later development. As proto-writing is not capable of fully reflecting the grammar and lexicon used in languages, it is often difficult or impossible to deduce what the author intended to communicate.
Early uses of writing included documenting agricultural transactions and contracts, but it was soon used in the areas of finance, religion, government, and law. Writing allowed the spread of these social modalities and their associated knowledge, and ultimately the further centralization of political power.[2]
Writing systems typically satisfy three criteria. Firstly, the writing must have some purpose or meaning to it, and a point must be communicated by the text. Secondly, writing systems make use of specific symbols which may be recorded on some writing medium. Thirdly, the symbols used in writing generally correspond to elements of spoken language.[3] In general, systems of symbolic communication like signage, painting, maps, and mathematical notation are distinguished from writing systems, which require knowledge of an associated language to read a text.[4]
The norms of writing generally evolve more slowly than those of speech; as a result, linguistic features are frequently preserved in the written form of a language after they cease to appear in the corresponding spoken language.[5]
Before the 20th century, most scholarly theories of the origins of writing involved some form of monogenesis,[6] the assumption that writing had been invented only once as cuneiform in ancient Sumer, and spread across the world from there via cultural diffusion.[7] According to these theories, writing was such a particular technology that exposure through activities like trade was a much more likely means of acquisition than independent reinvention. Specifically, many theories were dependent on a literal account of the Book of Genesis, including the emphases it placed on Mesopotamia.[8] Over time, greater awareness of the systems of pre-Columbian Mesoamerica conclusively established that writing had been independently invented multiple times. Four independent inventions of writing are most commonly recognized[9]—in Mesopotamia (c. 3400–3100 BC), Egypt (c. 3250 BC),[10][11][7] China (before c. 1250 BC),[12] and Mesoamerica (before c. 1 AD).[13]
Sumerian cuneiform and Egyptian hieroglyphs both gradually evolved from proto-writing between 3400 and 3100 BC. The Proto-Elamite script is also believed to have been in use during this period.[14] Regarding Egyptian hieroglyphs,[10][15][16] scholars point to very early differences with Sumerian cuneiform "in structure and style" as to why the two systems "(must) have developed independently," and if any "stimulus diffusion" of writing did occur, it only served to transmit the bare idea of writing between cultures.[10][17] Due to the lack of direct evidence for the transfer of writing, "no definitive determination has been made as to the origin of hieroglyphics in ancient Egypt."[18]
During the 1990s, symbols originally inscribed between 3400 and 3200 BC were discovered at Abydos, which shed some doubt on the previous notion that the Mesopotamian sign system predated the Egyptian one.[19] However, scholars have noted that the attestation at Abydos is singular and sudden, while the gradual evolution of the Mesopotamian system is lengthy and well-documented, with its predecessor token system used in agriculture and accounting attested as early as 8000 BC.[20]
As there is no evidence of contact between the Chinese Shang dynasty (c. 1600 – c. 1050 BC) and the literate civilizations of the Near East,[21] and the methods of logographic and phonetic representation in Chinese characters are distinct from those used in cuneiform and hieroglyphs, written Chinese is considered to be an independent development.[9]
In each case where writing was invented independently, it emerged from systems of proto-writing, which used ideographic and mnemonic symbols to communicate information, but did not record human language directly. Historically, most proto-writing systems did not produce writing systems; the earliest writing dates to the Early Bronze Age (3300–2100 BC), but proto-writing is attested as early as the 7th millennium BC. Examples of proto-writing during the Neolithic and Bronze Age include:
Later examples include quipu, a system of knotted cords used as mnemonic devices within the Inca Empire (15th century AD).[29]
The origins of writing are more generally attributed to the start of the Late Neolithic, when clay tokens were used to record specific amounts of livestock or commodities. These tokens were initially impressed on the surface of round clay envelopes and then stored in them. The tokens were then progressively replaced by flat tablets, on which signs were recorded with a stylus. Actual writing is first recorded in Uruk (modern Iraq), at the end of the 4th millennium BC, and soon after in various parts of the Near East.[30]
An ancient Sumerian poem gives the first known story of the invention of writing:
Because the messenger's mouth was heavy and he couldn't repeat (the message), the Lord of Kulaba patted some clay and put words on it, like a tablet. Until then, there had been no putting words on clay.
The emergence of writing in a given area is usually followed by several centuries of fragmentary inscriptions. Historians mark the "historicity" of a culture by the presence of coherent texts written by the culture.[33] Scholars have disagreed concerning when prehistory becomes history and when proto-writing became true writing.[34]
Sumerian writing evolved from a system of clay tokens used to represent commodities. By the end of the 4th millennium BC, this had evolved into a method of keeping accounts, which recorded numbers using a round stylus pressed into the clay at different angles. This system was gradually augmented with pictographic marks indicating what was being counted, which were made using a sharp stylus. By the 29th century BC, writing used a wedge-shaped stylus and included phonetic elements representing syllables of the Sumerian language, and gradually replaced round-stylus and sharp-stylus markings during the 27th and 26th centuries BC.[35] Finally, cuneiform became a general-purpose writing system with logograms, syllables, and numerals. From the 26th century BC, the system was adapted to write the Akkadian language, and from there to others, such as Hurrian and Hittite. Scripts similar in appearance to this writing system include those for Ugaritic and Old Persian.[36]
Geoffrey Sampson states that Egyptian hieroglyphs "came into existence a little after Sumerian script, and, probably [were], invented under the influence of the latter",[39] and that it is "probable that the general idea of expressing words of a language in writing was brought to Egypt from Sumerian Mesopotamia".[40][41][18] However, more recent scholars have held that the evidence for direct influence is sparse. During the 1990s, the discovery of glyphs at Abydos dated between 3400 and 3200 BC has challenged the hypothesis that writing diffused from Mesopotamia to Egypt, pointing instead to the independent development of writing within Egypt. The Abydos glyphs, found in tomb U-J, are written on ivory and are likely labels for other goods found in the grave.[42] While sign usage in Mesopotamian tokens is attested c. 8000 BC, Egyptian writing appears suddenly in the late 4th millennium BC.[19][32][43]
Frank J. Yurco states that depictions of pharaonic iconography such as the royal crowns, Horus falcons and victory scenes were concentrated in the Upper Egyptian Naqada and A-Group cultures. He further elaborates that "Egyptian writing arose in Naqadan Upper Egypt and A-Group Nubia, and not in the Delta cultures, where the direct Western Asian contact was made, [which] further vitiates the Mesopotamian-influence argument".[44]
Egyptian scholar Gamal Mokhtar argues that the inventory of hieroglyphic symbols derived from "fauna and flora used in the signs [which] are essentially African" and in "regards to writing, we have seen that a purely Nilotic, hence African origin not only is not excluded, but probably reflects the reality", although he acknowledges the geographical location of Egypt made it a receptacle for many influences.[45]
Writing was of political importance to the Egyptian empire, and literacy was concentrated among an educated elite of scribes.[46] Only people from certain backgrounds were allowed to train as scribes, in the service of temple, royal, and military authorities.[47]
The first alphabetic writing was developed by workers in the Sinai Peninsula to write West Semitic languages c. 1800 BC, "in the context of cultural exchanges between Semitic-speaking people from the Levant and communities in Egypt".[48] This earliest attested form is known as the Proto-Sinaitic script, and it adapted concepts and at least some of its written letterforms from Egyptian hieroglyphic writing; it adopted wholly West Semitic sound values for its letters, as opposed to adapting existing Egyptian ones.[49] Precise dating of its origin, as well as the graphical origins of many letterforms (if any) remain unclear, and the script remains undeciphered.[50] These early abjads (where letters generally represent only the consonantal sounds of a language) remained of marginal importance for several centuries; it is only towards the end of the Bronze Age that forms of Proto-Sinaitic script split into the Proto-Canaanite alphabet (c. 1400 BC), the undeciphered Byblos syllabary, and the Ancient South Arabian script (c. 1200 BC). Proto-Canaanite, which was probably influenced by the Byblos syllabary, in turn inspired the Ugaritic alphabet (c. 1300 BC).[51]
The Phoenician alphabet (c. 1050 BC), which was ultimately adapted into the Greek alphabet, is another direct descendant of Proto-Sinaitic.[52]
The Geʽez script native to Ethiopia and Eritrea descends from the Ancient South Arabian script, which had initially been used to write early Geʽez texts.[53]
Anatolian hieroglyphs are an indigenous script native to western Anatolia, used to record the Hieroglyphic Luwian language. It first appeared on Luwian royal seals from the 13th century BC.[54]
The earliest attested Chinese writing comprise the body of inscriptions on oracle bones and bronze vessels dating to the Late Shang period (c. 1200 – c. 1050 BC), with the earliest of these dated c. 1250 BC.[55][56]
Several syllabic and logographic writing systems were used in the Bronze Age Aegean civilizations (the Mycenaean civilization on the Greek mainland and the Minoan civilization on Crete), which ultimately fell out of use and were forgotten centuries prior to the introduction of the alphabet to the region by the Phoenicians:[57][58]
Of several symbol systems used in pre-Columbian Mesoamerica, the Maya script appears to be the best developed, and has been fully deciphered. The earliest inscriptions identifiable as Maya date to the 3rd century BC, and the earliest that can be deciphered and read dates to 199 AD.[59] The system was in continuous use from the 1st century AD until shortly after the arrival of the Spanish conquistadors in the 16th century. Maya writing used logograms complemented by a set of syllabic glyphs.[60]
The Phoenician alphabet gave rise to the Aramaic and Greek alphabets. Most of the writing systems used throughout Afro-Eurasia descend from either Aramaic or Greek. The Greek alphabet was the first to introduce letters representing vowel sounds.[61] It and its descendant in the Latin alphabet gave rise to several European scripts in the first several centuries AD, including the runic, Gothic, and Cyrillic alphabets. The Aramaic alphabet evolved into the Brahmic scripts of India, as well as the Hebrew, Arabic and Syriac abjads—with descendants spread as far as the Mongolian script. The South Arabian alphabet gave rise to the Geʽez abugida.[62]
The history of the Greek alphabet began as early as the 8th century BC, when the Greeks adapted the Phoenician alphabet for their own use.[63] The letters of the Greek alphabet generally visually correspond to those of the Phoenician alphabet, and both came to be arranged using the same alphabetical order.[63] Those adapting the Phoenician system added three letters to the end of the series, called the "supplementals". Several varieties of the Greek alphabet developed. One, known as the Cumae alphabet, was used west of Athens and in southern Italy. The other variation, known as Eastern Greek, was used in present-day Turkey and by the Athenians, and eventually the rest of the world that spoke Greek adopted this variation. After first writing right to left, like the Phoenicians, the Greeks eventually chose to write from left to right. Occasionally however, the writer would start the next line where the previous line finished, so that the lines would read alternately left to right, then right to left, and so on. This is known as boustrophedon writing, which imitated the path of an ox-drawn plough, and was used until the 6th century.[64]
The Greek alphabet is the progenitor of each script currently used to write the languages of Europe.[65] The most widespread descendant of Greek is the Latin script, named for the Latins, a central Italian people who came to dominate Europe with the rise of Rome. Around the 5th century BC, the Romans adopted writing from the Etruscan civilization, who wrote in a number of Italic scripts derived from the western Greeks. Due to the cultural dominance of the Roman state, the other Old Italic scripts have not survived in any great quantity, and the Etruscan language is mostly lost.[66]
After the fall of the Western Roman Empire in the 5th century, the production and transmission of literature that had previously been widespread across the Roman world became largely confined to the Byzantine and Sasanian empires, where the primary literary languages were Greek and Persian respectively—though other languages such as Syriac and Coptic were also important.[67]
The spread of Islam in the 7th century brought about the rapid establishment of Arabic as a major literary language in much of the Mediterranean and Central Asia. Arabic and Persian quickly began to overshadow Greek's role as a language of scholarship. Arabic script was adopted to write the Persian and  Old Turkic languages. This script also heavily influenced the development of the cursive scripts of Greek, the Slavic languages, Latin, and other languages.[68] The influence of Arabic writing during the Crusades also resulted in the Hindu–Arabic numeral system being adopted throughout Europe.[69] By the 11th century, the city of Córdoba, Andalusia in what is now southern Spain had become one of the world's foremost intellectual centres, and was the site of the largest library in Europe.[70]
By the 14th century, the Renaissance in Europe led to a revival of the importance of Greek, as well as of Latin as a significant literary language.[68] A similar though smaller emergence occurred in Eastern Europe, especially in Russia. At the same time Arabic and Persian began a slow decline in importance as the Islamic Golden Age ended. The revival of literacy development in Western Europe led to many innovations in the Latin alphabet and the diversification of the alphabet to codify the spoken forms of the various languages.
Within a society, the mediums, materials, and technologies used for writing shape what it is used for and what social impact it has.[71] For example, the physical durability of different writing materials directly determines which historical examples of writing have survived for later analysis: while bodies of inscriptions made in stone, bone, or metal from each literate society in antiquity have survived, many contemporaneous manuscript cultures are only attested indirectly.[72][73]
The common materials in the Mesopotamian world were the tablet and the roll, the former probably having a Chaldean origin, the latter an Egyptian. The tablets of the Chaldeans are small pieces of clay, somewhat crudely shaped into a form resembling a pillow, and thickly inscribed with cuneiform characters. Similar use has been seen in hollow cylinders, or prisms of six or eight sides, formed of fine terracotta, sometimes glazed, on which the characters were traced with a small stylus, in some specimens so minutely as to require the aid of a magnifying glass.[74]
In Egypt, the primary materials used for writing had different properties. Monuments depict the use of wooden tablets for writing; as early as the 4th millennium BC during the earliest Theban dynasties, papyrus had also been adopted.[75] The papyrus reed grew chiefly in Lower Egypt and could be adapted into a more economical writing medium—its pith was extracted and divided into thin pieces that were flattened under pressure and glued together. By adjoining other strips at right angles, papyrus rolls of any length could be produced. Compared to Mesopotamia, the cheapness of writing materials used in Egypt seemingly resulted in writing itself becoming more widespread as a practice.[76]
Egyptian papyrus was exported across the world, and ultimately became the subject of great demand. As a result, it became costly and was replaced by other materials including leather and parchment.[77] With the invention of wood-pulp paper, the cost of writing material began to steadily decline. While advancements in the mass production of paper throughout the 20th century improved the strength, ink retention, and other qualities of wood-pulp paper, it became one of the cheapest commodities in the global economy, resulting in its unprecedented availability as a writing medium as well as its widespread adoption for other applications.[78]
According to Denise Schmandt-Besserat, writing had its origins in the counting, cataloguing, and trade of agricultural produce.[79] Government tax rolls followed thereafter. Written documents became essential for the accumulation and accounting of wealth by individuals, the state, and religious organizations as well as the transactions of trade, loans, inheritance, and documentation of ownership.[80] With such documentation and accounting larger accumulations of wealth became more possible, along with the power that accompanied wealth, most prominently to the benefit of royalty, the state, and religions. Contracts and loans supported the growth of long-distance international trade with accompanying networks for import and export, supporting the rise of capitalism.[81] Paper money was first used in China during the 11th century;[82] it and other financial instruments relied on writing, initially in the form of letters and later as specialized genres designed to facilitate specific types of transactions and guarantees of value between individuals, banks, or governments.[83] With the growth of economic activity in late Medieval and Renaissance Europe, sophisticated methods of accounting and calculating value emerged, with such calculations both carried out in writing and explained in manuals.[84] The creation of corporations then proliferated documents surrounding organization, management, the distribution of shares, and records management.[85]
During the late 18th century, François Quesnay and Adam Smith developed systematic theories of economics for the first time. The works of Quesnay, Smith, and their colleagues introduced the concept of an economy as such—as well as the concept of a national economy.[86] Economics has since developed as a field with many authors contributing texts to the professional literature, and governments collecting data, instituting policies and creating institutions to manage and advance their economies. Deirdre McCloskey has examined the rhetorical strategies and discursive construction of modern economic theory.[87][88][89] Graham Smart has examined in depth how the Bank of Canada uses writing to cooperatively produce policies based on economic data and then to communicate strategically with relevant publics.[90]
Private legal documents for the sale of land appeared in Mesopotamia in the early 3rd millennium BC, not long after the appearance of cuneiform writing.[91] The first codes of law were written in Mesopotamia c. 2100 BC, exemplified in the Code of Hammurabi (c. 1750 BC) that was inscribed on stone stelae throughout the Old Babylonian Empire.[92] While the ancient Egyptian state did not codify its laws, legal documents such as official decrees and private contracts were used during the Old Kingdom c. 2150 BC. The Torah, comprising the first five books of the Hebrew Bible, codified the laws of Ancient Israel. Laws were frequently codified in ancient Greek and Roman polities, with Roman law ultimately serving as a model for both church canon law and secular law used throughout much of Europe during the Middle Ages.[93][94]
In China, the earliest evidence for the codification of laws or punishments are bronze inscriptions made in 536 BC.[95] The earliest law codes to be preserved in their entirety were those of the Qin and Western Han dynasties (221–9 BC), which articulated a full system of social control and governance, with criminal procedures and accountability for both government officials and citizens. These laws required complex reporting and documenting procedures to facilitate hierarchical supervision from the village up to the imperial centre.[96]
While common law developed in a mostly oral environment in England after the Roman period, with the return of the church and the Norman Conquest, customary law began to be inscribed as were precedents of the courts; however, many elements remained oral, with documents only memorializing public oaths, wills, land transfers, court judgements, and ceremonies. During the late medieval period, however, documents gained authority for agreements, transactions, and laws.[97]
Writing has been central to expanding many of the core functions of governance through law, regulation, taxation, and documentary surveillance of citizens; all dependent on growth of bureaucracy which elaborates and administers rules and policies and maintains records. These developments which rely on writing increase the power and extent of states.[98] At the same time writing has increased the ability of citizens to become informed about the operations of the state, to become more organized in expressing needs and concerns, to identify with regions and states, and to form constituencies with particular views and interests; the history of journalism is closely linked to citizen information, regional and national identity, and expression of interests. These changes have greatly influenced the nature of states, increasing the visibility of people and their views no matter what the form of governance is.[99]
in the ancient Near East and China, states developed extensive bureaucracies which relied on a literate class of scribes and bureaucrats.[100][101][102] In the Ancient Near East this was carried out through the formation of scribal schools;[103] in China, this led to the institution of written imperial examinations based on classic texts that effectively defined traditional Chinese education for millennia.[104] Literacy was associated with the government bureaucracy; following its emergence, printing was tightly controlled by the government, with texts written in vernacular Chinese being comparatively rare until the written vernacular Chinese movement that followed the end of the Qing dynasty (1644–1912).[105] In ancient Greece and Rome, class distinctions between citizen and slave, wealthy and poor limited education and participation. During the Middle Ages and early modern period, the church dominated education in Europe, reflecting the central role religious life had in the maintenance of state power and bureaucracy.[106]
In Europe and its American colonies, the introduction of the printing press and decreasing cost of paper and printing allowed for greater access of ordinary citizens to gain information about the government and conditions in other regions within the jurisdictions.[107] The Reformation's emphasis on the individual reading of sacred texts eventually increased the spread of literacy beyond the ruling classes, and opened the door to a wider awareness and criticism of government policy. Growing divisions along confessional and political lines in English society during 16th and 17th centuries culminated in the English Civil War that resulted in the sovereignty of Parliament being prioritized over the prerogatives of the British monarchy.[108] In this and other periods of conflict throughout the 16th and 17th centuries, cheap and disposable printed materials facilitated propaganda campaigns appealing to segments of the emerging public, pamphlet wars with authors attacking one another, and challenges to the status quo, often illicit, able to be published clandestinely.[109]
Newspaper publishing and journalism, having origins in commercial information, soon was to offer political information and was instrumental to the formation of a public sphere.[110][111] Newspapers were instrumental in the spread of information, fostering discussion and the formation of political identities in the American Revolution. During the late 19th century, the circulation of regional newspapers encouraged adoption and articulation of urban or localized identities by readers. A focus on national news that followed telegraphy and the emergence of newspapers with national circulation along with scripted national radio and television news broadcasts also created horizons of attention through the 20th century, with both benefits and costs.[112]
Much of what is considered knowledge is inscribed in written text and is the result of communal processes of production, sharing, and evaluation among social groups and institutions bound together with the aim of producing and disseminating knowledge-bearing texts; the contemporary world identifies such social groups as disciplines and their products as disciplinary literatures. The invention of writing facilitated the sharing, comparing, criticizing, and evaluating of texts, resulting in knowledge becoming a more communal property across wider geographic and temporal domains. Religious texts formed the common knowledge of scriptural religions, and knowledge of those sacred scriptures became the focus of institutions of religious belief, interpretation, and schooling.[113]
Scholars have disagreed concerning when written record-keeping became more like literature, but the oldest surviving literary texts date from a full millennium after the invention of writing. The earliest literary author known by name is Enheduanna, who is credited as the author of a number of works of Sumerian literature, including Exaltation of Inanna, in the Sumerian language during the 24th century BC.[114][115] The next earliest named author is Ptahhotep, who is credited with authoring The Maxims of Ptahhotep, an instructional book for young men in Old Egyptian composed in the 23rd century BC.[116] The Epic of Gilgamesh is a notable early poem, but it can also be seen as a political glorification of the historical King Gilgamesh of Sumer whose natural and supernatural accomplishments are recounted.
The identification of sacred religious texts codified distinct belief systems, and became the basis of the modern concept of religion.[117] The reproduction and spread of these texts became associated with these scriptural religions and their spread, and thus were central to proselytizing.[118] Their status created expectations that believers either read or otherwise respect their contents; priests charged with reading, interpretation and application of texts were especially vital in societies prior to the advent of mass literacy.
In Mesopotamia and Egypt, scribes became important for roles beyond the initiating roles in the economy, governance and law. They became the producers and stewards of astronomy and calendars, divination, and literary culture. Schools developed in tablet houses, which also archived repositories of knowledge.[119] In ancient India, the Brahman caste became stewards of texts that aggregated and codified oral knowledge.[120] Those texts then became the authoritative basis for a continuing tradition of oral education. A case in point is the work of Pāṇini, a linguist who analysed and codified knowledge of Sanskrit syntax, prosody, and grammar. Mathematics, astronomy, and medicine were also subjects of classic Indian learning and were codified in classic texts.[121] Less is known about Mayan, Aztec, and other Mesoamerican learning because of the destruction of texts by the conquistadors, but it is known that scribes were revered, elite children attended schools, and the study of astronomy, map making, historical chronicles, and genealogy flourished.[122][123]
In China, after the Qin dynasty attempted to remove all traces of the competing Confucian tradition, the Han dynasty made philological knowledge the qualification for the government bureaucracy, so as to restore knowledge that was in danger of vanishing. The imperial examination system for the civil service functioned for two millennia, and consisted of a written exam based on knowledge of classical texts. To support students obtaining government positions through the written examination, schools focused on those same texts and the associated philological knowledge.[104] These texts covered philosophical, religious, legal, astronomical, hydrological, mathematical, military, and medical knowledge.[124] Printing as it emerged largely served the knowledge needs of the bureaucracy and the monastery, with substantial vernacular printing only emerging around the 15th century.[105]
While Socrates thought writing an inferior means of transmission of learning (recounted in the Phaedrus), we know of his works through Plato's written accounts of his dialogues. Havelock also connects the philosophical work of Plato, Socrates, and Aristotle with literacy, as it enabled the development of critical thinking via the analysis of permanent texts written both by the author and their peers.[125][126][127] Aristotle wrote treatises and lectures which were the core of education at the Lyceum, along with the may volumes collected in the Lyceum's library. The Stoics and Epicureans also wrote and taught during the same period in Athens, although we now have only fragments of their works.
Greek writers were the founders of many other fields of knowledge. Herodotus and Thucydides, who wrote during the 5th century BC in Athens, are considered founders of the Western historiographical tradition, incorporating genealogy and mythic accounts into systematic investigations of events. Thucydides developed a more critical, neutral history through greater examination of documents, transcription of speeches, and interviews.[128] During the same period, Hippocrates authored several works codifying what was known within the field of medicine. The works of Galen, a Greek physician living in Rome during the 2nd century AD, were important in European medical practice through the Renaissance. Hellenized writers in Egypt also produced compendia of knowledge using the resources of the Library of Alexandria, such as Euclid's Elements, which remains a standard reference work in geometry. Ptolemy's Almagest, an astronomy treatise, was used throughout the Middle Ages.
Roman scholars continued the practice of writing compendia of knowledge, including Varro, Pliny the Elder, and Strabo. While much of Roman accomplishment was in material culture of construction, Vitruvius documented much of the contemporary practice to influence design until today. Agriculture also became an important area for manuals, such as Palladius's compendium. Numerous manuals of rhetoric and rhetorical education that were to influence future generations also appeared, such as the anonymous Rhetorica ad Herennium, Cicero's De Oratore and Quintilian's Institutio Oratoria.
With the fall of Rome, the Middle East became the crossroads for learning, with knowledge bearing texts from the West and East meeting in Constantinople, Damascus, and then Baghdad. The House of Wisdom with a large library was founded, where Greek works of medicine, philosophy, mathematics and astronomy were translated into Arabic, along with Indian works on mathematics and therapeutics.[129] To these texts, philosophers such as Al-Kindi and Avicenna and astronomers such as Al-Farghani made new contributions. Al-Khwarizmi authored the first work on algebra, drawing on both Greek and Indian resources. The centrality of the Quran within Islam also led to growth of Arabic linguistics.[130] From Baghdad, knowledge and texts were to flow back to South Asia and down through Africa, with a large collection of books and an educational center around the Sankoré Madrasah in Timbuktu, the seat of the Songhai Empire. During this period the deposed Abbasid Caliphate moved its seat of power and learning to Córdoba, now in Spain, where they founded a major library which reintroduced many of the classic texts back into Europe along with texts of Arab learning.
The reintroduction of classical texts into Europe through the library and intercultural intellectual culture in Córdoba, including the classical Greek canon, as well as Arabic texts by Avicenna and Al-Khwarizmi, created a need for interpretation and scholarship to make those works more accessible to scholars in monasteries and urban centres. During the 12th century, universities emerged from clusters of scholars in Italy at Bologna, in Spain at Salamanca, in France at Paris and in England at Oxford.[131][132] By 1500, there were at least 60 universities throughout Europe[133] enrolling at least 750,000 students.[134] Each of the four faculties—liberal arts, theology, law, and medicine—was oriented around transmission of and commentary on classical texts, rather than the production of new knowledge. This form of scholastic education continued well into the 17th century in some locations and disciplines.[135][136][137][138][139]
Johannes Gutenberg's introduction of the moveable type printing press to Europe c. 1450 created new opportunities for the production and widespread distribution of books, fostering much new writing, with particular consequences for the development of knowledge, as documented by Elizabeth Eisenstein.[140] The production and distribution of knowledge was no longer tied to monasteries or universities with their libraries and collections of scribal copies. In the ensuing centuries a politically and increasingly religiously divided Europe, no single authority was able to censor or control the production of books. While universities remained attached to disseminating traditional texts, publishing houses became the new centres of knowledge production, and publishing houses in different jurisdictions led to a diversity of ideas becoming available as books moved across borders and scholars came to see themselves as citizens of the Republic of Letters.[141]
The comparison of multiple editions of traditional texts led to improved textual scholarship.[142] The ability to share and compare results from many regions and enlist more people into the production of science soon led to the development of early modern science.[143] Books of medicine began to incorporate observations from contemporary surgery and dissections, including printed plates providing illustrations, to improve knowledge of anatomy.[144] With many copies of traditional books and new books appearing, debates arose over the value of each in what became known as the "battle of the books".[145] Maps and discoveries of exploration and colonization also were recorded in books and governmental records,[146] often with the purpose of economic exploitation as in the General Archive of the Indies in Seville but also to satisfy curiosity about the world.[147]
Printing also made possible the invention and development of scientific journals, with the Journal des sçavans appearing in France and the Philosophical Transactions of the Royal Society in England, both in 1665.[148] Over the years, these journals proliferated and became the basis of disciplines and disciplinary literature.[149] Genres reporting experiments and other scientific observations and theories developed over the ensuing centuries to produce modern practices of disciplinary publication with the extensive intertexts which represent the collective pursuits of disciplinary knowledge. The availability of scientific and disciplinary books and journals also facilitated the development of modern practices of scientific reference and citation. These developments from the impact of printing on the growth of knowledge contributed to the Scientific Revolution, science in the Renaissance and during the Enlightenment.
In the 18th century, dissident Scottish and English universities began offering practical instruction in rhetoric and writing to enable non-elite students to influence contemporary events. Only in the 19th century did the universities in some countries begin making place for the writing of new knowledge, turning them in the ensuing years from primarily disseminating knowledge through the reading of classical texts to becoming institutions devoted to both reading and writing. The creation of research seminars and the associated seminar papers in history and philology in German universities were a significant starting point for the reform of the university.[150] Professorships in philology, history, economy, theology, psychology, sociology, mathematics and the sciences were to emerge over the century, and the German model of disciplinary research university was to influence the organization of universities in England and the United States, with another model developing in France. Both emphasized production of new knowledge by faculty and acquisition thereof by students. In elite British universities, writing instruction was supported by the tutorial system with weekly writing by students for their tutors, while in the United States regular courses in writing were often required starting in the late 19th century, with writing across the curriculum becoming an increasing focus, particularly towards the end of the 20th century.
Walter J. Ong, Jack Goody, and Eric A. Havelock were among the earliest to systematically argue for the psychological and intellectual consequences of literacy. Ong argued that the introduction of writing changed the form of human consciousness from sensing the immediacy of the spoken word to the critical distance and systematization of words, which could be graphically displayed and ordered,[151][152] such as in the works of Petrus Ramus.[153] Havelock attributed the emergence of Greek philosophic thought to the use of the written word which allowed the comparison of beliefs and belief systems and the critical examination of concepts.[154][155] Jack Goody argued that written language fostered such practices as categorization, making lists, following formulas, developing recipes and prescriptions, and ultimately making and recording experiments. These practices changed the intellectual and psychological orientation of those who engaged with them.[156][157]
While recognizing the possibilities of all these psychological and intellectual changes that accompanied these literate practices, Sylvia Scribner and Michael Cole argued that these changes did not come universally or automatically with literacy, but rather were dependent on the social uses made of literacy in their local contexts.[158] They carried out field observation and experiments among the Vai people of West Africa, for whom the psychological impacts of literacy vary due to the three different contexts in which locals learn to read and write the Vai language, English, and Arabic—practical skills, secular education, and religious education, respectively. European literacy was associated with European-style schooling, and fostered among other things syllogistic reasoning and logical problem solving. Arabic literacy was associated with the religious training of madrasas and fostered, among other things, heightened rote memory. Literacy in the written forms of Vai associated with daily practices of making requests and explaining tasks, increased anticipation of audience knowledge and needs along with rebus solving (as the written language used rebus-like icons).

The prehistory of Stockholm is the continuous development and series of events that made the mouth of Lake Mälaren strategically important; a location which by the mid 13th century had become the centre of the Swedish kingdom.  The origin of Stockholm pre-dates its written history, and several mythological stories and modern myths have attempted to explain both its emergence and its name.
Stockholm stands on a bedrock of gneiss and granite approximately 2 billion years old.  Over millions of years, north-west to south-east oriented cracks appeared in the rock, which rivers transformed into the valleys still present in the landscape, for example the lakes Långsjön, Magelungen, and Drevviken.  All around Stockholm, such open fields are separated by forest-laden ridges.  Late in this geological process, east to west-oriented faults appeared, resulting in for example the tall, dark cliffs along the northern waterfront of Södermalm.[1]
Three million years ago, a series of ice ages started to grind down the north-bound faults, leaving the south-bound formations intact.[1]  During the latest ice age (70,000-9,500 BCE), the area surrounding Stockholm was covered by an ice layer up to two kilometres thick.  While the ice effectively eliminated every trace of pre-ice age life, it is assumed humans probably did inhabit the area before the ice age, notwithstanding no archaeological traces can confirm it.  Nevertheless, bones from a mammoth have been found in the Brunkebergsåsen esker stretching north to south through central Stockholm.[2]
As ice lightened its grip of the area about 11,500 BCE, the area was inundated by melt water before the land started to rise and the first islets rose over the water surface (at the time located about 40 metres over the present sea level).[3]  The retiring ice left behind a cover of sand, gravel, and rocks forming moraine ridges still witnessing how the ice gave up some 250 m annually over two centuries.  Under the ice sheet, streams formed eskers, most notably the huge Brunkebergsåsen, the steep slopes of which still form barriers in central Stockholm.[1]
After some 1000 years the first humans settled in the area to start the Stone Age era characterized by a climate similar to that of the present Mediterranean Sea.[3]  Due to land elevation, the archaeological traces of these first coastal settlements are today found far from the coast and the modern metropolitan area.  The traces consists of various tools, including quartz and flint arrowheads used by these hunter-gatherers to catch mostly seals.[4]  During the end of the Stone Age (4200–1800 BCE) humans started to use more stationary settlements, solid buildings standing on strong poles drilled into the ground, even if the access to food still made migratory periods necessary.  Graves got more elaborate as grinned axes made of carefully selected and often imported rocks accompanied the dead together with ceramics, fancy garments, and other impressive objects.[5]
The first town in the Lake Mälaren region was undoubtedly Birka on Björkö island about 30 km west of present-day Stockholm.[citation needed]  Founded in the late 8th century, it was described by Rimbert, Archbishop of Hamburg-Bremen, who wrote about his predecessor Ansgar's missionary journeys c. 830 and 850.  For unknown reasons, Birka was deserted around 975.  Shortly thereafter, Sigtuna appeared on the northern shores of Mälaren.  Located on the main navigable approach to Uppsala, Sigtuna is believed to have been designed as missionary outpost and a Christian trade centre rivalling the still pagan Uppsala.  While Sigtuna saw its heyday during the 10th century, the Blackfriars decided to construct their first monastery in Sweden at Sigtuna in the 1230s (inaugurated 1247), which seem to indicate Sigtuna was still the city dominating the Mälaren region at that time.[6]
Notwithstanding surviving records makes it difficult to see exactly when and in what order events took place, several causes for the development to occur in the 13th century can be distinguished:
Of the half dozen trade posts in Sweden described in 1120 as cathedral cities or cities with a potential to become such, Sigtuna is believed to be the only one with the density and status of a city in the proper sense.  This quickly changed as German merchants introduced developed forms of production and 'industrialised' Swedish mining, mostly during the second half of the 13th century.  This rewrote the regional map and resulted in the gradual development of a Swedish urbanity. As trade routes moved westward in the Mälaren region, Sigtuna found itself left astern.[6][7]
Birger jarl's elimination of the "true Folkungs" between 1247 (Battle of Sparrsätra) and 1251 (Battle of Herrevadsbro) lead to increased consolidation of the Swedish kingdom and the introduction of a continental feudal society.[7]  Additionally, the Swedish dominion expanded east as Birger jarl and Torgils Knutsson conquered Tavastland and Karelia (later Finland) which placed present-day Stockholm, until then an insignificant peripheral island, in the absolute centre of the small empire.[6]
Lastly, navigation on Lake Mälaren changed dramatically as land elevation isolated it from the Baltic Sea during the 13th century and made the strait in what is today central Stockholm the only navigable passage into the Lake Mälaren region.  It remains uncertain when this happened more precisely, but the development was further accelerated by the growth of international trade in the Baltic.  The streams around Stadsholmen can't have been much of a problem for Viking longships or traditional knarrs, but as German merchants introduced the sea-going deep-draught cogs, the straits and streams at the mouth of Lake Mälaren became insurmountable obstacles which thus created a need for a trading post at the location.[6][7]
While the oldest traces of human activities in present Stockholm are considerably older, the development as described above preceded the foundation of the city on its present location.  However, Birka, Sigtuna, and Stockholm forms a series of urban structures which can be thought of as the capital of the Lake Mälaren region, at a few occasions relocated to fit into the socio-economic structure of the day.[6]
The watercourse passing south of the old town of Stockholm first appears in historical records as the somewhat cryptic phrase: "What split off is called Stockholm" (Stockholm heter det som sprack av), found in a version of the Saga of the Saint Olaf by the Icelandic author Stymer Frode, preserved through a manuscript from the 14th century.  Stymer explains, what today are the islands  Södermalm and Stadsholmen was at the time united by an isthmus, and Saint Olaf of Norway (995-1030) produced the strait, in the saga called Konungssund ("King's strait"), by summoning assistance from superior forces.  A slightly different version, undoubtedly the most famous, is the account of the Icelandic historian Snorri Sturluson (1178–1241).  He retells, while King Olav of Norway raided the Lake Mälaren area, the Swedish king Olof Skötkonung (960s-1021/1022) hoped to trap him by pulling an iron chain over Stocksund ("Log Strait", e.g. modern Norrström passing north of the old town), a strait in addition guarded by a castellum and an army on either sides.  The Norwegian king then dug himself through the southern isthmus and, helped by vivid streams produced by spring flood and favourable winds, managed to have his ships break through the foreshore and shoals, and finally escaped to the Baltic Sea.  Snorri however adds, the Swedes refuted this version as drivel.[9]
Stockholm derives its mythological origin from a dwelling place called Agnefit. The second element fit means 'moist meadow' and, arguably, the only possible location for a meadow in present-day Stockholm at the time was on the western shore of today's Stadsholmen.  The first element of this name is, explains the historian Snorri Sturluson (1178–1241), derived from King Agne, a presumably mythological king who in a dim and distant past (around 400 A.D. according to some historians) encamped here after having successfully raided Finland.  His intentions were to marry Skjalf, the daughter of the defeated Finnish tribal chief.  The young woman, however, tricked him to arrange a celebration including prominent guests which eventually turned into a boozing party, and, while Agne slept sober, Skjalf had him hung in his gold necklace before escaping.[9]
A modern story dated back to the mid 17th century, tells how the population of Birka, a historical city on Lake Mälaren, grew too rapidly, and the Gods then consulted urged parts of the population to emigrate to a new site. To determine where to build the new city, it was decided a log bound with gold should point out where to settle by sailing ashore on the site, and, occasionally, it landed on an islet in what is today central Stockholm.[10]  According to a 17th-century myth, the tower Birger Jarls torn, often and erroneously said to be the oldest building in Stockholm, was built on this location.[10]
According to the Chronicle of Eric, written in the 1320s, Stockholm was founded by Birger Jarl around 1250 as a lock to the Lake Mälaren region (laas fore then sio, "lock before that lake") in order to prevent pirates from reaching the seven cities and nineteen parishes around it.  Another medieval source (Visbyannalerna), however, claims the city was founded in 1187 following a pagan pillaged the city of Sigtuna, and as there was an "Earl" named Birger around at that time too, the disputed origin of the city are likely to remain obscure and some historians choose a diplomatic interpretation saying there was some sort of fortification around by the mouth of Lake Mälaren when the city was founded during the second half of the 13th century.  As Snorri mentions no city in his account but some sort of fortification called a kastali (in various manuscripts curiously said to be located east and west of Stocksund), it has generally been assumed this fortification eventually developed into the castle Tre Kronor located where still is the Stockholm Palace.[11]
While the reliability of these stories remains disputed, dendrochronological examinations of logs driven into the seabed in Norrström, square oak logs, and sunken logs found on Helgeandsholmen just north of Stadsholmen in 1978-1980, concluded these trees were cut down during the period 970-1020, most of them from around 1010, and these logs presumably gave the entire city its present name, Stock-holm, "Log-Islet".[9]
Anyhow, any hypothesis on the origin of the city necessarily need to depart from three poorly documented defensive structures:[12]
It is often said the keep of the Three Crown Castle originated from at least parts of these defensive structures.  As historical and archaeological records are fragmentary, the origin of this tower and the castle remain open for various interpretations, as do the size and extent of the city at the time.  Some researchers conclude Stockholm evolved into the Swedish capital and an important trade city before 1200 (Kumlien), while others suggest the location remained a mostly rural area around 1250 to quickly expand into the large city before the castle was completed (Hansson, Ödman).[12]
The first, verified, mention of the name 'Stockholm' is from two letters written in Latin in 1252; the first, written in July, is a letter where the King Valdemar and Birger Jarl offered their royal patronage to Fogdö Abbey; and the other, written by Birger Jarl in August, urged the peasantry in Attundaland to pay their tithes to the Uppsala Cathedral.  Both letters were written in Stockholm, but give no further information of the city itself or any explanation on the background of the name.[10]  However, it can be assumed at least some sort of dwelling in consistence with the station of a Swedish jarl existed at that time.[11]
While the name itself easily splits into two distinct elements - stokker, or in modern Swedish stock, meaning "log", and holme, meaning "islet" - a matter-of-fact explanation for the name is much harder to produce, and over the years many popular myths have, accordingly, attempted to give a background. The first attempt to a more serious explanation was put forward by the German humanist Jacob Ziegler in his work Schondia (Scandinavia) printed in 1532. Writing in Latin, he describes the city as the stronghold and trade post of the Swedes, located among paludibus, meaning either marshes or lakes, and - like Venice - resting on poles. Most likely, while in Rome Ziegler must have come in contact with prominent Swedes like Johannes Magnus who supplied him with the description of the city, which still today styles itself "The Venice of the Nordic countries" (Nordens Venedig).[10]
Other interpretations includes stock being an allusion to:[10]
To add to the enigma, Stockholm has been called Eken ("The oak") in many contexts. While it is mostly associated with slang today, it is supposedly derived from Stockhäcken, the name the city was given by traders from Västergötland (called Västgötaknallar).[10]

The gorilla–human last common ancestor (GHLCA, GLCA, or G/H LCA) is the last species that the tribes Hominini and Gorillini (i.e. the chimpanzee–human last common ancestor on one hand and gorillas on the other) share as a common ancestor. It is estimated to have lived 8 to 10 million years ago (TGHLCA) during the late Miocene.[1][2][3][4]
The fossil find of Nakalipithecus nakayamai are closest in age to the GHLCA.[3][4]
The GHLCA marks a pivotal evolutionary split within the Homininae subfamily, separating the lineage that led to gorillas (Gorilla gorilla and Gorilla beringei) from the lineage that eventually gave rise to chimpanzees, bonobos and humans.
This ancestor is part of the larger African ape lineage, which also includes the chimpanzee—human last common ancestor (Pan and Homo genera)
The divergence of the gorilla lineage likely coincided with significant environmental changes, such as the shrinking of tropical forests during the Miocene
This prehistoric primate-related article is a stub. You can help Wikipedia by expanding it.

The earliest human occupation of what is now China dates to the Lower Paleolithic c. 1.7 million years ago—attested by archaeological finds such as the Yuanmou Man. The Erlitou (c. 1900 – c. 1500 BCE) and Erligang cultures (c. 1600 – c. 1400 BCE) inhabiting the Yellow River valley were Bronze Age civilizations predating the historical record—which first emerges c. 1250 BCE at Yinxu, during the Late Shang.
The Paleolithic period in palaeogeography refers to the stage of civilization development in which humans began to use stone tools as their main means of labor, and is the early stage of the Stone Age. This period is generally defined as from about 2.6 million[1] or 2.5 million years ago (the first production of stone tools by archaic humans) to 12,000 years ago[2] (the emergence of agricultural civilization). The division of this period is generally threefold: the Lower, Middle, and Upper Paleolithic, followed by the Mesolithic.[3]
Paleolithic humans usually lived together in tribes and subsisted by collecting plants and hunting wildlife.[4] Although humans also used wooden and bone tools within this era, the Paleolithic period is typified by the use of stone tools made by hammering stones. Other materials, such as leather and plant fibers, are also suitable for tooling, but the nature of these materials has prevented them from being used in a wider range of applications. During the Paleolithic period, humans evolved dramatically from early homo sapiens to anatomically modern humans.[5] During the latter stages of the Paleolithic  humans began to create the earliest art and became involved in religious and spiritual realms, such as funerals and rituals.[4][5][6]
The distribution of early Paleolithic cultures in China has been widespread. Chinese Paleolithic cultures dating back to 1 million years BP include the Xihoudou Culture, the Yuanmou Man stone tools, the Kehe Culture, the Lantian Human Culture, and the Donggutuo site.[7] Fewer than 1 million years ago, sites are represented in the north by the Peking Man culture at Zhoukoudian site 1, and in the south by the Guanyin Cave culture at Guanyin Cave in Qianxi, Guizhou. All in all, Chinese Early Paleolithic cultures are basically of a type similar to the Oldowan cultures, and do not seem to display the technological advancements of the Acheulean Culture of the West. However, some scholars believe that there may have been exchanges between the Chinese Paleolithic culture and the Western Acheulean culture during this period.[8]
The Middle Paleolithic culture in China can be represented by the Dingcun site found in Xiangfen, Shanxi Province. Also more important are the Zhoukoudian Locality 15 culture and the Shanxi Yanggao Xujiayao Culture.[9] The Middle Paleolithic cultures of China maintained the types and processing techniques of earlier cultures. Despite slight changes in types and slight advances in technology, they were all slow in progress. One notable feature is the lack of significant development of Levallois technique.[10]
Into the Late Palaeolithic, the number of sites increased, artefacts of visual culture became more abundant and elaborate, technological change took place and the material evidence of cultural types became more diverse from one another. In North China and South China, as well as in other regions, there are cultural types of a similar periodization but with different technological traditions.[8]: 237–239
In north China, there is a tradition of small stone tools inherited from the previous period, the important representatives of which are the Sarawusu site, the Zhiyu culture, the Xiaonanhai site, and the Shandingdong site.[11] There is the Blade culture type, represented by the Shuidonggou culture in Lingwu, Ningxia, which has more than a few similarities with its Western contemporaries. There are also typical microlithic cultures discovered after the 1970s, such as the Xiachuan culture in Qinshui, Shanxi, and the Hutouliang culture at the Hutouliang site in Yangyuan, Hebei.[12]
In the northeast, important sites belonging to this period include the Xiaogushan site in Haicheng, Liaoning, and the Yanjiagang site in Harbin, Heilongjiang.[13]
In the south, several regional cultures emerged during this period, such as the Fulin culture type named after the Fulin site in Hanyuan, Sichuan, the Tongliang culture type represented by the Zhang Ertang site in Tongliang, Chongqing, and the Maomao culture type initially discovered at the Maomao site in Xingyi, Guizhou.[14]
In addition, a number of Paleolithic cultural sites belonging to this period or slightly later have been found in Tibet, Xinjiang and Qinghai. In general, the main characteristic of the cultures of this period is that, with the exception of a few sites, the production of blade crafts and bone and horn tools was not very well developed.[15]
The Mesolithic period in China, between the Upper Paleolithic and Early Neolithic, was characterized by the manufacture of microliths, and is therefore also known as the "Microlith Period". China was in the Mesolithic period from about 10,000 to 7,000 years ago, with cultures successively entering the Neolithic period after one or two millennia, a relatively brief span of time. There are also the Shayuan Culture of Shayuan in Dali, Shaanxi, the Lingjing Culture of Lingjing in Xuchang, Henan, and the Xiachuan Culture of Xiachuan in Qinshui, Shanxi.[16]
The Neolithic period was characterized by a gradual progression from food gathering to food production, and by the development of stone tools from rudimentary to specialized. With the advent of agriculture, people had a more reliable food supply; The emergence of long-term tribes led to the complexity of living in groups; people began to use abstract symbols to represent concrete things, and representational art emerged. The Neolithic 'revolution' was a major juncture in human history.[17]: 17
The Peiligang culture and the Cishan culture are the earliest Neolithic cultures in China that have been seen in archaeology so far.[18] Both cultures already had agriculture, but the large number of excavations of tree seeds, fish bones, and animal bones indicate that gathering food was still of considerable importance. The presence of agricultural tools, grains, and domestic animals signifies that a significant portion of food was already supplied by production. Early Neolithic pottery was handmade, but it was fired to more than 900 °C (1,650 °F), and the forms were already quite complex, with a number of motifs and even a few paintings. Many of the pottery forms of the Peiligang culture and the Cishan culture are also found in the later Yangshao culture, where rope patterns and colorful paintings are even more prevalent. Round and square semisubterranian dwelling sites also originated in the Peiligang and Cishan cultures, while the same is seen in the village sites of the Yangshao culture.[19] All of this shows that Peiligang culture and Cishan culture are the predecessors of Yangshao culture. The Peiligang culture sites are mainly located in the central part of Henan. In terms of the age determined by carbon-14 dating, Peiligang culture has three data of 5935±480 BCE, 5195±300 BCE (5879 BCE after dendrochronological correction) and 7350±1000 BCE. Roughly contemporaneous with Peiligang, the Egou Beigang culture is 5315–5025 BCE (corrected to 5916–5737 BCE). The main artifacts of Peiligang culture are stone grinding discs with feet, stone grinding poles, narrow and flat stone shovels with double curved edges and stone scythes with serrated teeth, which are obviously tools related to agricultural production, and their pottery assemblage includes figurines of pigs' heads.[17]: 20
Yangshao culture is the mainstream of Neolithic culture in the Yellow River basin, distributed throughout Henan, Shanxi, Shaanxi, Hebei, Liaodong, Ningxia, southern Inner Mongolia, Henan and Hubei's northwestern part, including the entire Central Plains and the Guanshan area. Carbon-14 dating has been widely employed, with results about 4515–2460 BCE (corrected to 5150–2960 BCE), with a continuation of more than two thousand years. The Yangshao culture can be represented by the Banpo Village site in Shaanxi; of course, there are several types of Yangshao culture presented across temporal and spatial divides.
The Yangshao culture significantly developed agriculture. The villages were quite large, on the order of square kilometers. Dwelling sites are usually square or round and semisubterranian, divided into inner and outer chambers, with flat or even chalky floors. Chambers often have the remains of burnt fires. Tribes were often located on river terraces. In some cases, sites in good condition can encompass several non-contiguous cultural layers, suggesting that agriculture was practiced in the form of nomadic cultivation. But tribal migration often depended on conditions favorable to farming, so that the same site could be occupied successively by people moving in to establish a settled tribe.
The site at Banpo, Xi'an, Shaanxi, has at least two cultural layers, with the ruins and cellars of one layer superimposed on the remains of the other, separated by layers of soil alternately lush with grass seeds and tree pollen. Only with the "slash-and-burn" method of cultivation is it possible to have this kind of alternation of trees and grass on the same site. The Banpo Tribe may have had hundreds of dwelling sites, with the domiciles and storage caves concentrated in the center of the settlement, surrounded by a deep ditch. To the north of the Banpo site, there is a communal cemetery, where the remains of children and adults are buried, and the territory of the living and the dead is clearly separated. There is also a large house in the village, which may have served as a meeting place for the whole village, or some other public function. It is inferred that the tribes of Yangshao culture seem to have had a certain degree of Political organisation and a sense of self-grouping. The phenomenon of burial in cemeteries reflects group consciousness having transcended the boundaries of time.[23]
Agriculture at Yangshao was dominated by the cultivation of millet and other grains; storage caves at several sites have yielded millet-type grains. Pottery jars for storing vegetable seeds have also been excavated at the Banpo site. Livestock were mostly pigs and dogs, with fewer cows and sheep. Agricultural tools include stone hoes and shovels for plowing, stone knives and axes for felling, and stone sharpeners for general scraping. Yangshao agriculture was productive to such a level that storage caves are found distributed all throughout the villages, clearly indicating surplus food supply. Yangshao culture pottery often have painted decoration. For this reason, archaeologists once referred to Yangshao culture as "colored pottery culture". Motifs include geometric shapes and flowing irregular lines, as well as fairly realistic or pictorial images, such as fish, pigs, frogs, sheep, human heads, and the like. Several of these rudimentary engravings and paintings have symbolic functions, to the point that certain scholars consider Yangshao pottery patterns a form of writing. On the whole, Yangshao culture in the social organization, production level and the use of abstract symbols have a considerable degree of development. Yangshao culture has a long history and is dominant in the Central Plains, and has had a non-negligible influence on the Neolithic cultures of the surrounding neighboring regions.[23]
Immediately following the Yangshao culture, the late Neolithic culture in the Central Plains was the Longshan culture, which was more widely distributed and richer in content.
During the Yangshao culture, the development of agriculture stabilized the food supply, leading to an increase in population. Thus, on the one hand, there was an overflow of population to form more settlements and spread the culture to areas never before inhabited by people. On the other hand, also because of the limited space available for habitation, the inhabitants of the cluster had to settle in the same place permanently. The Longshan culture of each region is thus quite localized. As to Longshan culture in the Central Plains, there existed Henan Longshan culture, Shaanxi Longshan culture, Shandong Longshan culture, of which Henan Longshan culture formed the direct descendants of Yangshao culture.[24]
The Longshan culture of Henan occupied the Jinnan and Ji'nan regions, mainly along the middle and lower reaches of the Yellow River. The intermediate type from Yangshao culture to Longshan culture in Henan is the Miaodigou II in the border area of Henan, Jin and Shaanxi, whose carbon 14 date is 2310±95 years BCE (corrected to 2780±145 years BCE). In contrast, the entire Henan Longshan culture dates from 2100 to 1810 BCE (corrected to 2515 to 2155 BCE). The Longshan culture of Shaanxi is comparable in age and also inherited the Neolithic culture of Miaodigou II.[24]
The Longshan culture of Henan developed on the basis of the Yangshao culture, while there were several changes in its content. Farming tools include the Lei Si, sickle and bone spatula. Agricultural products were still primarily corn and grain, but the harvest seems to have been much larger. Carpenters' tools were no longer axes for felling, but more processing tools for cutting. Pottery in the wheel system components increased greatly. Villages had walls made of rammed earth for self-defense, and there were apparently wars between villages. Some wounded skeletons  were thrown in heaps in pits and caves, probably also victims of war. Religious beliefs emerged, and bone divination and special burial rites are sufficient to indicate the direction of this development. Eggshell pottery, with its thin, hard walls, was not intended for everyday use, and this specialized utensil was developed for religious ceremonies as well. Ancestor worship was also institutionalized. The division of social status and occupations within the same community created a phenomenon of social differentiation, which is also linked to the growing complexity and organization of the community. Settlements were more permanent, and therefore had a clearer sense of community, which is also expressed in the individual characteristics of local cultures. However, the density of the distribution of settlements increased from former times, and group-to-group contacts and exchanges were inevitable. Neighboring settlements must interact with each other, so the characteristics of the local culture of neighboring districts are often similar, from east to west, or from south to north, the visible cultural differences show a gradual process, and it is difficult to find a clear-cut line of demarcation of the local culture. As a whole, the influence of the Longshan culture radiated beyond the Central Plains.[24]
China entered the Bronze Age and went through several different stages of development: early, middle and late. Some scholars have divided China's Bronze Age from the Shang and Zhou dynasties to the Warring States period into four phases: the heyday, the decadence, the mid-emergence, and the decline. Some scholars have also divided this period into five stages: pre-Yin Shang, late Yin Shang, Western Zhou, pre-Eastern Zhou, and late Eastern Zhou.[25]
Bronze cultures throughout China have their own characteristics and styles, and can be divided into different regional types. The earliest excavated bronze objects in China (2900 BCE to 2740 BCE) belong to the Majiayao culture of Gansu. Academician Zhang Guangzhi, in his book The Bronze Age of China, was the first to clearly point out that the Xia, Shang, and Zhou dynasties were the heyday of the Bronze Age, followed by the Spring and Autumn period and the Warring States period. The Houmuwu ding is the largest and heaviest bronze vessel ever found in China. Academician Du Zhengsheng pointed out in Ancient Society and the State that from the Stone Age to the Bronze Age, there was no breakthrough in the development of the tools of production in China, but only class and social changes, and that bronzes were mainly used as ceremonial weapons and weapons of war as a symbol of political power.[25]
The Erlitou culture is considered the beginning of the Bronze Age in China.[26] The Erlitou culture includes both  the culture of the Erlitou site and the cultural landscape reflected in hundreds of sites that have the cultural characteristics of the Erlitou site in addition to the Erlitou site. The Erlitou site and the Erlitou culture became recognized as key research objects for ascertaining the historicity of the Xia dynasty. The broad geographic dispersal of Erlitou material culture may indicate that the society of the time had moved from a pluralistic situation where a number of competing political entities existed side by side to the period of a wide-area dynastic royal state.[27]
As the earliest remnant of a wide-area royal state in East Asia, the Erlitou culture can be called the "earliest China". The civilizational underpinnings of the Erlitou culture became the mainstay of Chinese civilization through the inheritance and abandonment between the dynasties of the Shang and Zhou eras. The Erlitou site is the namesake of the Erlitou culture and has been tentatively recognized as the site of a capital city in the middle to late Xia dynasty.[28]
Erligang culture is a culture type named after the Shang dynasty cultural remains at the Erligang site in Zhengzhou, Henan. Also known as the Erligang period Shang culture, it is an early Bronze Age archaeological culture between the Erlitou Xia culture and the Yinxu late Shang culture.[29]
In 1950, archaeologists discovered a new kind of remains for the first time at the Erligang site, which is located two miles southeast of the old city of Zhengzhou, and since the Erligang site is the earliest typical site where this kind of cultural remains can be found, the name of Erligang culture was proposed in 1954.[29]
According to archaeological discoveries, the pre-Shang dynasty, centered on Erligang in Zhengzhou, the sites of the Yin–Shang period, such as Liulige in Huixian and Donggangou in Luoyang, belong to this era.[30]
Following a brief occupation at Huanbei,[31] the final part of the Shang dynasty centered on Xiaotun Village in Anyang, the upper levels of the Zhengzhou Park area, and the Taishan Temple site and burials in Luoyang. There are also tombs belonging to this era. Yinxu is the first documented and archaeologically and oracle-bone proven capital city site in Chinese history, dating from around the end of the 14th century to the middle of the 11th century BCE. Since the discovery of Yinxu, about 150,000 pieces of oracle bones with characters have been unearthed. More than 5,000 individual characters have been identified from these inscribed oracle bones, and about 1,700 Chinese characters can be recognized. The information recorded in the oracle bones advances the credible history of written records in China to the Shang dynasty, and modern Chinese characters evolved from the oracle bones.[32] There are also tens of thousands of pieces of pottery, about 1,500 pieces of bronze ceremonial objects, about 3,500 pieces of bronze weapons, about 2,600 pieces of jade, more than 6,500 pieces of stone tools, and more than 30,000 pieces of bone tools.[33]

